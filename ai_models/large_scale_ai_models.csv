Model,Domain,Task,Authors,Model accessibility,Link,Citations,Reference,Publication date,Organization,Parameters,Parameters notes,Training compute (FLOP),Training compute notes,Training dataset,Training dataset notes,(DEPRECATED) Training dataset size (datapoints),Dataset size notes,Training time (hours),Training time notes,Training hardware,Confidence,Abstract,Country (of organization),Base model,Finetune compute (FLOP),Finetune compute notes,Hardware quantity,Hardware utilization (MFU),Training code accessibility,Accessibility notes,Hardware utilization (HFU),Training compute cost (cloud),Training compute cost (upfront)
K-EXAONE,Language,"Language modeling/generation,Chat,Question answering,Code generation,Mathematical reasoning,Instruction interpretation","Eunbi Choi, Kibong Choi, Seokhee Hong, Junwon Hwang, Hyojin Jeon, Hyunjik Jo, Joonkee Kim, Seonghwan Kim, Soyeon Kim, Sunkyoung Kim, Yireun Kim, Yongil Kim, Haeju Lee, Jinsik Lee, Kyungmin Lee, Sangha Park, Heuiyeen Yeen",API access,https://huggingface.co/LGAI-EXAONE,,K-EXAONE Technical Report,2025-12-31,LG AI Research,236000000000.0,236B,1.52e+24,,Unspecified unreleased,,,11T tokens,3240.0,"K-EXAONE was trained using a cloud-based GPU environment. The total training duration was 135 days, utilizing 512 B200 GPUs. This corresponds to a total of 1,658,880 GPU hours, calculated as follows:
512 GPUs Ã— 135 days Ã— 24 hours = 1,658,880 GPU hours",NVIDIA B200 GPUs,Confident,"K-EXAONE is a large-scale multilingual language model developed by LG AI Research. K-EXAONE is built on a Mixture-of-Experts architecture with 236B total parameters, activating 23B parameters during inference. It supports a 256K-token context window and covers six languages: Korean, English, Spanish, German, Japanese, and Vietnamese. We evaluate K-EXAONE on a comprehensive benchmark suite spanning reasoning, agentic, general, Korean, and multilingual abilities. Across these evaluations, K-EXAONE demonstrates performance comparable to open-weight models of similar size. K-EXAONE, designed to advance AI for a better life, is positioned as a powerful proprietary AI foundation model for a wide range of industrial and research applications.",Korea (Republic of),,,,512.0,,Unreleased,,,,
Emu3.5,"Video,Multimodal,Image generation,Vision,Language,Speech","Text-to-video,Image-to-video,Image generation,Text-to-image,Visual question answering,Language modeling/generation,Question answering,Speech recognition (ASR),Video description",,Open weights (unrestricted),https://emu.world/Emu35_tech_report.pdf,,Emu3.5: Native Multimodal Models are World Learners,2025-10-30,Beijing Academy of Artificial Intelligence / BAAI,34100000000.0,"""Overall, the model contains 34.1 billion(B) parameters, including 31.2 B in the transformer layers and 2.9 B in the embedding layers. """,9.86736e+24,7.084799999999999e+24 FLOP [base model compute] + 2.78256e+24 FLOP = 9.86736e+24 FLOP,,,,,,,,Confident,"We introduce Emu3.5, a large-scale multimodal world model that natively predicts the next state across vision and language. Emu3.5 is pre-trained end-to-end with a unified next-token prediction objective on a corpus of vision-language interleaved data containing over 10 trillion tokens, primarily derived from sequential frames and transcripts of internet videos. The model naturally accepts interleaved vision-language inputs and generates interleaved vision-language outputs. Emu3.5 is further post-trained with large-scale reinforcement learning to enhance multimodal reasoning and generation. To improve inference efficiency, we propose Discrete Diffusion Adaptation (DiDA), which converts token-by-token decoding into bidirectional parallel prediction, accelerating per-image inference by about 20Ã— without sacrificing performance. Emu3.5 exhibits strong native multimodal capabilities, including long-horizon vision-language generation, any-to-image (X2I) generation, and complex text-rich image generation. It also exhibits generalizable world-modeling abilities, enabling spatiotemporally consistent world exploration
and open-world embodied manipulation across diverse scenarios and tasks. For comparison, Emu3.5 achieves performance comparable to Gemini 2.5 Flash Image (Nano Banana) on image
generation and editing tasks and demonstrates superior results on a suite of interleaved generation tasks. We open-source Emu3.5 at https://github.com/baaivision/Emu3.5 to support
community research.",China,"Whisper v2,Qwen3-32B",2.78256e+24,6 FLOP/parameter/token * 34100000000 parameters * 13600000000000 tokens = 2.78256e+24 FLOP,,,Unreleased,"Apache 2.0
https://github.com/baaivision/Emu3.5

Apache 2.0
https://huggingface.co/collections/BAAI/emu35",,,
Kimi Linear,Language,"Language modeling/generation,Question answering,Mathematical reasoning",,Open weights (unrestricted),https://github.com/MoonshotAI/Kimi-Linear/blob/master/tech_report.pdf,,"KIMI LINEAR:
AN EXPRESSIVE, EFFICIENT ATTENTION ARCHITECTURE",2025-10-30,Moonshot,48000000000.0,3B activated parameters and 48B total parameters,1.026e+23,6 FLOP/parameter/token * 3000000000 activated parameters * 5700000000000 tokens = 1.026e+23 FLOP,Unspecified unreleased,,,"""Our final released Kimi Linear checkpoint is pretrained using the same procedure, but with an expanded total of 5.7 trillion tokens to match the pretraining tokens of Moonlight. In addition, the final checkpoint supports a context length of up to 1 million tokens""",,,,Confident,"We introduce Kimi Linear, a hybrid linear attention architecture that, for the first time, outperforms full attention under fair comparisons across various scenariosâ€”including short-context, long-context, and reinforcement learning (RL) scaling regimes. At its core lies Kimi Delta Attention (KDA), an expressive linear attention module that extends Gated DeltaNet with a finer-grained gating mechanism, enabling more effective use of limited finite-state RNN memory. Our bespoke chunkwise algorithm achieves high hardware efficiency through a specialized variant of the Diagonal-Plus-LowRank (DPLR) transition matrices, which substantially reduces computation compared to the general
DPLR formulation while remaining more consistent with the classical delta rule.
We pretrain a Kimi Linear model with 3B activated parameters and 48B total parameters, based on a layerwise hybrid of KDA and Multi-Head Latent Attention (MLA). Our experiments show
that with an identical training recipe, Kimi Linear outperforms full MLA with a sizeable margin across all evaluated tasks, while reducing KV cache usage by up to 75% and achieving up to 6Ã—
decoding throughput for a 1M context. These results demonstrate that Kimi Linear can be a drop-in replacement for full attention architectures with superior performance and efficiency, including tasks with longer input and output lengths.
To support further research, we open-source the KDA kernel and vLLM implementations, and release the pre-trained and instruction-tuned model checkpoints. ",China,,,,,,Unreleased,"MIT license
https://huggingface.co/moonshotai/Kimi-Linear-48B-A3B-Instruct
https://github.com/MoonshotAI/Kimi-Linear/tree/master",,,
"Lapa LLM

","Language,Multimodal",Language modeling/generation,,Open weights (unrestricted),https://huggingface.co/spaces/lapa-llm/lapa,,a cutting-edge open large language model based on Gemma-3-12B with a focus on Ukrainian language processing,2025-10-25,"Ukrainian Catholic University,Igor Sikorsky Kyiv Polytechnic Institute,AGH University of Krakow,Lviv Polytechnic",12000000000.0,12B,8.64e+23,"Not including finetuning compute
",Unspecified unreleased,,,at least 12T tokens,,,,Speculative,"Today, we proudly present Lapa LLM â€” a cutting-edge open large language model based on Gemma-3-12B with a focus on Ukrainian language processing. The project is the result of many months of work by a team of Ukrainian researchers in artificial intelligence from the Ukrainian Catholic University, AGH University of Krakow, Igor Sikorsky Kyiv Polytechnic Institute, and Lviv Polytechnic, who united to create the best model for Ukrainian language processing.","Ukraine,Ukraine,Poland,Ukraine",,,,,,Unreleased,,,,
Odyssey 102B,Biology,"Protein or nucleotide language model (pLM/nLM),Protein generation","Ankit Singhal, Shyam Venkatasubramanian, Sean Moushegian, Steven Strutt, Michael Lin, Connor Lee",Unreleased,https://www.biorxiv.org/content/10.1101/2025.10.15.682677v1,,Odyssey: reconstructing evolution through emergent consensus in the global proteome,2025-10-18,Anthrogen,102000000000.0,102B,1.1e+23,"""trained over 1.1 Ã— 10^23 FLOPs"" from the abstract","UniRef100,MERC,SRC,AntiRef,OMG,PDB (Protein Data Bank),ESM3 Dataset, AlphaFold database (AFDB)",Table 2,,"Table 2
3.662B proteins",,,,Confident,"We present Odyssey, a family of multimodal protein language models for sequence and structure generation, protein editing and design. We scale Odyssey to more than 102 billion parameters, trained over 1.1 Ã— 1023 FLOPs. The Odyssey architecture uses context modalities, categorized as structural cues, semantic descriptions, and orthologous group metadata, and comprises two main components: a finite scalar quantizer for tokenizing continuous atomic coordinates, and a transformer stack for multimodal representation learning. Odyssey is trained via discrete diffusion, and characterizes the generative process as a time-dependent unmasking procedure. The finite scalar quantizer and transformer stack leverage the consensus mechanism, a replacement for attention that uses an iterative propagation scheme informed by local agreements between residues. Across various benchmarks, Odyssey achieves landmark performance for protein generation and protein structure discretization. Our empirical findings are supported by theoretical analysis.",United States of America,,,,,,Unreleased,,,,
Ling-1T,Language,"Language modeling/generation,Question answering,Quantitative reasoning,Code generation,Mathematical reasoning",,Open weights (unrestricted),"https://huggingface.co/inclusionAI/Ling-1T
https://arxiv.org/pdf/2510.22115",,Ling-1T,2025-10-10,Ant Group,1000000000000.0,1 trillion total parameters with 50 billion activated parameters,6.000001e+24,6 FLOP/parameter/token * 50000000000 active parameters * 20000000000000 tokens = 6e+24 FLOP,Unspecified unreleased,,20000000000000.0,"""Pre-trained on 20 trillion+ high-quality, reasoning-dense tokens""",,,,Confident,"Ling-1T is the first flagship non-thinking model in the Ling 2.0 series, featuring 1 trillion total parameters with â‰ˆ 50 billion active parameters per token. Built on the Ling 2.0 architecture, Ling-1T is designed to push the limits of efficient reasoning and scalable cognition.

Pre-trained on 20 trillion+ high-quality, reasoning-dense tokens, Ling-1T-base supports up to 128K context length and adopts an evolutionary chain-of-thought (Evo-CoT) process across mid-training and post-training. This curriculum greatly enhances the modelâ€™s efficiency and reasoning depth, allowing Ling-1T to achieve state-of-the-art performance on multiple complex reasoning benchmarksâ€”balancing accuracy and efficiency.",China,,,,,,Unreleased,"MIT license
https://huggingface.co/inclusionAI/Ling-1T",,,
Granite-4.0-H-Tiny,Language,"Language modeling/generation,Question answering,Text summarization,Text classification,Retrieval-augmented generation,Code generation",,Open weights (unrestricted),"https://www.ibm.com/new/announcements/ibm-granite-4-0-hyper-efficient-high-performance-hybrid-models#:~:text=We're%20launching%20Granite%204,costs%20compared%20to%20conventional%20LLMs.",,"IBM Granite 4.0: hyper-efficient, high performance hybrid models for enterprise",2025-10-02,IBM,7000000000.0,a hybrid MoE with 7B total parameters (1B active),1.35e+23,6 FLOP/parameter/token * 1000000000 active parameters * 22500000000000 tokens = 1.35e+23 FLOP,,"""Granite 4.0 was pre-trained on a broad spectrum of samples curated from DataComp-LM (DCLM), GneissWeb, TxT360 subsets, Wikipedia and other enterprise-relevant sources. They were further post-trained to excel at enterprise tasks, leveraging both synthetic and open datasets across domains including language, code, math and reasoning, multilinguality, safety, tool calling, RAG and cybersecurity. All training datasets were prepared with the open-source Data Prep Kit framework.""",22500000000000.0,"""all Granite 4.0 models are trained on samples drawn from the same carefully compiled 22T-token corpus of enterprise-focused training data, as well the same improved pre-training methodologies, post-training regimen and chat template.""

4 training stages: 15T+5T+2T+0.5T = 22.5T",,,NVIDIA GB200,Confident,"Weâ€™re launching Granite 4, the next generation of IBM language models. Granite 4.0 features a new hybrid Mamba/transformer architecture that greatly reduces memory requirements without sacrificing performance. They can be run on significantly cheaper GPUs and at significantly reduced costs compared to conventional LLMs.
These new Granite 4.0 offerings, open sourced under a standard Apache 2.0 license, are the worldâ€™s first open models to receive ISO 42001 certification and are cryptographically signed, confirming their adherence to internationally recognized best practices for security, governance and transparency.
Granite 4.0 models are available on IBM watsonx.ai, as well as through platform partners including (in alphabetical order) Dell Technologies on Dell Pro AI Studio and Dell Enterprise Hub, Docker Hub, Hugging Face, Kaggle, LM Studio, NVIDIA NIM, Ollama, OPAQUE and Replicate. Access through Amazon SageMaker JumpStart and Microsoft Azure AI Foundry is coming soon.",United States of America,,,,,,Unreleased,"Apache 2.0
https://huggingface.co/ibm-granite/granite-4.0-h-tiny",,,
Granite-4.0-H-Micro,Language,"Language modeling/generation,Question answering,Text summarization,Text classification,Retrieval-augmented generation,Code generation",,Open weights (unrestricted),"https://www.ibm.com/new/announcements/ibm-granite-4-0-hyper-efficient-high-performance-hybrid-models#:~:text=We're%20launching%20Granite%204,costs%20compared%20to%20conventional%20LLMs.",,"IBM Granite 4.0: hyper-efficient, high performance hybrid models for enterprise",2025-10-02,IBM,3000000000.0, dense hybrid model with 3B parameters.,3.15e+23,6 FLOP/parameter/token * 3000000000 parameters * 17500000000000 tokens = 3.15e+23 FLOP,,"""Granite 4.0 was pre-trained on a broad spectrum of samples curated from DataComp-LM (DCLM), GneissWeb, TxT360 subsets, Wikipedia and other enterprise-relevant sources. They were further post-trained to excel at enterprise tasks, leveraging both synthetic and open datasets across domains including language, code, math and reasoning, multilinguality, safety, tool calling, RAG and cybersecurity. All training datasets were prepared with the open-source Data Prep Kit framework.""",17500000000000.0,"""all Granite 4.0 models are trained on samples drawn from the same carefully compiled 22T-token corpus of enterprise-focused training data, as well the same improved pre-training methodologies, post-training regimen and chat template.""

4 training stages: 10T+5T+2T+0.5T = 17.5T",,,NVIDIA GB200,Confident,"Weâ€™re launching Granite 4, the next generation of IBM language models. Granite 4.0 features a new hybrid Mamba/transformer architecture that greatly reduces memory requirements without sacrificing performance. They can be run on significantly cheaper GPUs and at significantly reduced costs compared to conventional LLMs.
These new Granite 4.0 offerings, open sourced under a standard Apache 2.0 license, are the worldâ€™s first open models to receive ISO 42001 certification and are cryptographically signed, confirming their adherence to internationally recognized best practices for security, governance and transparency.
Granite 4.0 models are available on IBM watsonx.ai, as well as through platform partners including (in alphabetical order) Dell Technologies on Dell Pro AI Studio and Dell Enterprise Hub, Docker Hub, Hugging Face, Kaggle, LM Studio, NVIDIA NIM, Ollama, OPAQUE and Replicate. Access through Amazon SageMaker JumpStart and Microsoft Azure AI Foundry is coming soon.",United States of America,,,,,,Unreleased,"Apache 2.0
https://huggingface.co/ibm-granite/granite-4.0-h-micro",,,
Granite-4.0-H-Small,Language,"Language modeling/generation,Question answering,Text summarization,Text classification,Retrieval-augmented generation,Code generation",,Open weights (unrestricted),"https://www.ibm.com/new/announcements/ibm-granite-4-0-hyper-efficient-high-performance-hybrid-models#:~:text=We're%20launching%20Granite%204,costs%20compared%20to%20conventional%20LLMs.",,"IBM Granite 4.0: hyper-efficient, high performance hybrid models for enterprise",2025-10-02,IBM,32000000000.0,a hybrid mixture of experts (MoE) model with 32B total parameters (9B active),1.215e+24,6 FLOP/parameter/token * 9000000000 active parameters * 22500000000000 tokens = 1.215e+24 FLOP,,"""Granite 4.0 was pre-trained on a broad spectrum of samples curated from DataComp-LM (DCLM), GneissWeb, TxT360 subsets, Wikipedia and other enterprise-relevant sources. They were further post-trained to excel at enterprise tasks, leveraging both synthetic and open datasets across domains including language, code, math and reasoning, multilinguality, safety, tool calling, RAG and cybersecurity. All training datasets were prepared with the open-source Data Prep Kit framework.""",22500000000000.0,"""all Granite 4.0 models are trained on samples drawn from the same carefully compiled 22T-token corpus of enterprise-focused training data, as well the same improved pre-training methodologies, post-training regimen and chat template.""

4 training stages: 15T+5T+2T+0.5T = 22.5T",,,NVIDIA GB200,Confident,"Weâ€™re launching Granite 4, the next generation of IBM language models. Granite 4.0 features a new hybrid Mamba/transformer architecture that greatly reduces memory requirements without sacrificing performance. They can be run on significantly cheaper GPUs and at significantly reduced costs compared to conventional LLMs.
These new Granite 4.0 offerings, open sourced under a standard Apache 2.0 license, are the worldâ€™s first open models to receive ISO 42001 certification and are cryptographically signed, confirming their adherence to internationally recognized best practices for security, governance and transparency.
Granite 4.0 models are available on IBM watsonx.ai, as well as through platform partners including (in alphabetical order) Dell Technologies on Dell Pro AI Studio and Dell Enterprise Hub, Docker Hub, Hugging Face, Kaggle, LM Studio, NVIDIA NIM, Ollama, OPAQUE and Replicate. Access through Amazon SageMaker JumpStart and Microsoft Azure AI Foundry is coming soon.",United States of America,,,,,,Unreleased,"Apache 2.0
https://huggingface.co/ibm-granite/granite-4.0-h-small",,,
GLM-4.6,Language,"Language modeling/generation,Question answering,Code generation,Quantitative reasoning,System control",,Open weights (unrestricted),https://z.ai/blog/glm-4.6,,"GLM-4.6: Advanced Agentic, Reasoning and Coding Capabilities",2025-09-30,"Z.ai (Zhipu AI),Tsinghua University",357000000000.0,Similarly to GLM 4.5: 355 billion total parameters (reported) with 32 billion active parameters (assumed),4.42e+24,6 FLOP/parameter/token * 32000000000 active parameters [very likely assumption - everything else is reported to be same as at GLM 4.5] * 23000000000000 tokens = 4.42e24 FLOP,Unspecified unreleased,,,23T tokens (from Jaime's correspondence with the GLM team),2880.0,"4 months (from Jaime's correspondence with the GLM team)

4 months * 30 days * 24 hours = 2880 hours",,Likely,"Today, we are releasing the latest version of our flagship model: GLM-4.6. Compared with GLM-4.5, this generation brings several key improvements:

Longer context window: The context window has been expanded from 128K to 200K tokens, enabling the model to handle more complex agentic tasks.
Superior coding performance: The model achieves higher scores on code benchmarks and demonstrates better real-world performance in applications such as Claude Codeã€Clineã€Roo Code and Kilo Code, including improvements in generating visually polished front-end pages.
Advanced reasoning: GLM-4.6 shows a clear improvement in reasoning performance and supports tool use during inference, leading to stronger overall capability.
More capable agents: GLM-4.6 exhibits stronger performance in tool using and search-based agents, and integrates more effectively within agent frameworks.
Refined writing: Better aligns with human preferences in style and readability, and performs more naturally in role-playing scenarios.
We evaluated GLM-4.6 across eight public benchmarks covering agents, reasoning, and coding. Results show clear gains over GLM-4.5, with GLM-4.6 also holding competitive advantages over leading domestic and international models such as DeepSeek-V3.2-Exp and Claude Sonnet 4, but still lags behind Claude Sonnet 4.5 in coding ability.","China,China",,,,,,Unreleased,"MIT license
https://huggingface.co/zai-org/GLM-4.6",,,
DeepSeek-V3.2-Exp,Language,"Language modeling/generation,Code generation,Quantitative reasoning,Question answering,Search,System control,Instruction interpretation",,Open weights (unrestricted),https://github.com/deepseek-ai/DeepSeek-V3.2-Exp/blob/main/DeepSeek_V3_2.pdf,,Introducing DeepSeek-V3.2-Exp,2025-09-29,DeepSeek,671000000000.0,"671B total, 37B active (assuming same as in the base model)",4.18e+24,"3.594058e+24 FLOP [base model] + 2.095014e+23 FLOP = 3.8035594e+24 FLOP. 

Then, ""this framework allocates a
post-training computational budget exceeding 10% of the pre-training cost"", so 4.18e24",Unspecified unreleased,,943700000000.0,"""We train both the main model and the indexer for 15000 steps, with each step consisting of 480 sequences of 128K tokens, resulting in a total of 943.7B tokens.""",,,,Confident,"We are excited to announce the official release of DeepSeek-V3.2-Exp, an experimental version of our model. As an intermediate step toward our next-generation architecture, V3.2-Exp builds upon V3.1-Terminus by introducing DeepSeek Sparse Attentionâ€”a sparse attention mechanism designed to explore and validate optimizations for training and inference efficiency in long-context scenarios.

This experimental release represents our ongoing research into more efficient transformer architectures, particularly focusing on improving computational efficiency when processing extended text sequences.",China,DeepSeek-V3.1-Terminus,5.9e+23,"Continued pretraining compute: 6 * 37B * 943.7B = 2.095e23 FLOP. In addition, V3.2 states post-training compute budget exceeds 10% of total pretraining cost, implying additional post-training FLOPs > 3.8e23 (so finetune total beyond V3.1-Terminus is >5.9e23 FLOP).",,,Unreleased,"MIT license
https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Exp",,,
Gemini 2.5 Flash (Sep 2025),"Language,Multimodal,Vision,Speech,Video","Language modeling/generation,Question answering,Code generation,Quantitative reasoning,Visual question answering,Translation,Image captioning,Speech recognition (ASR),Video description,Search,Text summarization,Chat",,API access,https://deepmind.google/models/gemini/flash/,,Our powerful and most efficient workhorse model designed for speed and low-cost.,2025-09-25,Google DeepMind,,,,,Unspecified unreleased,Knowledge cutoff January 2025,,,,,,Unknown,"Speed and value at scale
Ideal for tasks like summarization, chat applications, data extraction, and captioning.
Thinking budget: Control how much 2.5 Flash reasons to balance latency and cost.
Natively multimodal: Understands input across text, audio, images and video.
Long context: Explore vast datasets with a 1-million token context window.
Adaptive and budgeted thinking
Adaptive controls and adjustable thinking budgets allow you to balance performance and cost.
Calibrated: The model explores diverse thinking strategies, leading to more accurate and relevant outputs.
Controllable: Developers have fine-grained control over the model's thinking process, allowing them to manage resource usage.
Adaptive: When no thinking budget is set, the model assesses the complexity of a task and calibrates the amount of thinking accordingly.","United States of America,United Kingdom of Great Britain and Northern Ireland",,,,,,Unreleased,"Availability	
Google AI Studio
Gemini API
Gemini App",,,
DeepSeek-V3.1-Terminus,Language,"Language modeling/generation,Code generation,Quantitative reasoning,Question answering,Search,System control,Instruction interpretation",,Open weights (unrestricted),https://api-docs.deepseek.com/news/news250922,,The latest update builds on V3.1â€™s strengths while addressing key user feedback.,2025-09-22,DeepSeek,671000000000.0,"671B total, 37B active",3.594058e+24,"This was based on V3 which was trained on 14.8T tokens for a total of 14.8T + 840B = 15.64T pretraining tokens. Adding the FLOPs from continued pretraining onto our estimate for V3, which is 3.4e24 FLOPs, we get 3.4e24 + 1.9e23 = 3.59e24",Unspecified unreleased,,839000000000.0,"From V3.1 training dataset information:
""840B tokens continued pretraining for long context extension on top of V3""

""The 32K extension phase has been increased 10-fold to 630B tokens, while the 128K extension phase has been extended by 3.3x to 209B tokens.""

839B in total",,,,Likely," Whatâ€™s improved?
 Language consistency: fewer CN/EN mix-ups & no more random chars.
 Agent upgrades: stronger Code Agent & Search Agent performance.
 DeepSeek-V3.1-Terminus delivers more stable & reliable outputs across benchmarks compared to the previous version.",China,DeepSeek-V3,1.86258e+23,"6 FLOP/parameter/token * 671000000000 parameters * 839000000000 tokens = 3.377814e+24 FLOP

",,,Unreleased,"MIT license
https://huggingface.co/deepseek-ai/DeepSeek-V3.1-Terminus",,,
Qwen3-Next-80B-A3B,Language,"Language modeling/generation,Question answering,System control,Code generation",,Open weights (unrestricted),https://qwen.ai/blog?id=4074cca80393150c248e508aa62983f9cb7d27cd&from=research.latest-advancements-list,,Qwen3-Next: Towards Ultimate Training & Inference Efficiency,2025-09-10,Alibaba,80000000000.0,80B - A3B,2.7e+23,"6 FLOP / parameter / token * 3 * 10^9 active parameters * 15 * 10^12 pre-training tokens = 2.7e+23 FLOP

""It uses less than 80% of the GPU hours needed by Qwen3-30A-3B, and only 9.3% of the compute cost of Qwen3-32B â€” while achieving better performance.""

(Qwen3-30A-3B compute estimation: 6.48e+23 FLOP, Qwen3-32B - 7.0848e+24 FLOP)",Unspecified unreleased,,15000000000000.0,"Training Stage: Pretraining (15T tokens) & Post-training

""Qwen3-Next is trained on a uniformly sampled subset (15T tokens) of Qwen3â€™s 36T-token pretraining corpus""",,,,Confident,"We believe that Context Length Scaling and Total Parameter Scaling are two major trends in the future of large models. To further improve training and inference efficiency under long-context and large-parameter settings, we design a brand-new model architecture called Qwen3-Next.Compared to the MoE structure of Qwen3, Qwen3-Next introduces several key improvements: a hybrid attention mechanism, a highly sparse Mixture-of-Experts (MoE) structure, training-stability-friendly optimizations, and a multi-token prediction mechanism for faster inference.
Based on this new architecture, we train the Qwen3-Next-80B-A3B-Base model â€” an 80-billion-parameter model that activates only 3 billion parameters during inference. This base model achieves performance comparable to (or even slightly better than) the dense Qwen3-32B model, while using less than 10% of its training cost (GPU hours). In inference, especially with context lengths over 32K tokens, it delivers more than 10x higher throughput â€” achieving extreme efficiency in both training and inference.
We develop and release two post-trained versions based on Qwen3-Next-80B-A3B-Base: Qwen3-Next-80B-A3B-Instruct and Qwen3-Next-80B-A3B-Thinking. We solve the long-standing stability and efficiency issues in reinforcement learning (RL) training caused by the hybrid attention + high-sparsity MoE architecture. This led to improvements in both RL training speed and final performance.",China,,,,,,Unreleased,"Apache 2.0
https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Instruct",,,
Ling-mini-base-2.0-20T,Language,"Language modeling/generation,Question answering","Today, we are excited to announce the open-sourcing of Ling 2.0 â€” a family of MoE-based large language models that combine SOTA performance with high efficiency. The first released version, Ling-mini-2.0, is compact yet powerful. It has 16B total parameters, but only 1.4B are activated per input token (non-embedding 789M). Trained on more than 20T tokens of high-quality data and enhanced through multi-stage supervised fine-tuning and reinforcement learning, Ling-mini-2.0 achieves remarkable improvements in complex reasoning and instruction following. With just 1.4B activated parameters, it still reaches the top-tier level of sub-10B dense LLMs and even matches or surpasses much larger MoE models.",Open weights (unrestricted),https://huggingface.co/inclusionAI/Ling-mini-base-2.0-20T,,Ling-mini-base-2.0-20T,2025-09-10,Ant Group,16000000000.0,"It has 16B total parameters, but only 1.4B are activated per input token (non-embedding 789M).",1.68e+23,"6 FLOP/parameter/token * 1400000000 active parameters * 20000000000000 tokens = 1.68e+23 FLOP

",Unspecified unreleased,,,Trained on more than 20T tokens of high-quality data and enhanced through multi-stage supervised fine-tuning and reinforcement learning,,,,Confident,"Today, we are excited to announce the open-sourcing of Ling 2.0 â€” a family of MoE-based large language models that combine SOTA performance with high efficiency. The first released version, Ling-mini-2.0, is compact yet powerful. It has 16B total parameters, but only 1.4B are activated per input token (non-embedding 789M). Trained on more than 20T tokens of high-quality data and enhanced through multi-stage supervised fine-tuning and reinforcement learning, Ling-mini-2.0 achieves remarkable improvements in complex reasoning and instruction following. With just 1.4B activated parameters, it still reaches the top-tier level of sub-10B dense LLMs and even matches or surpasses much larger MoE models.

",China,,,,,,Unreleased,"MIT license
https://huggingface.co/inclusionAI/Ling-mini-base-2.0-20T",,,
Ling-flash-base-2.0-20T,Language,"Language modeling/generation,Question answering","Today, Ling-flash-2.0 is officially open-sourced! ðŸš€ Following the release of the language model Ling-mini-2.0 and the thinking model Ring-mini-2.0, we are now open-sourcing the third MoE LLM under the Ling 2.0 architecture: Ling-flash-2.0, a language model with 100B total parameters and 6.1B activated parameters (4.8B non-embedding). Trained on 20T+ tokens of high-quality data, together with supervised fine-tuning and multi-stage reinforcement learning, Ling-flash-2.0 achieves SOTA performance among dense models under 40B parameters, despite activating only ~6B parameters. Compared to MoE models with larger activation/total parameters, it also demonstrates strong competitiveness. Notably, it delivers outstanding performance in complex reasoning, code generation, and frontend development.",Open weights (unrestricted),https://huggingface.co/inclusionAI/Ling-flash-base-2.0,,Ling-flash-base-2.0-20T,2025-09-10,Ant Group,100000000000.0,100B total parameters and 6.1B activated parameters (4.8B non-embedding). ,7.32e+23,"6 FLOP/parameter/token * 6100000000 active parameters * 20000000000000 tokens = 7.32e+23 FLOP

",Unspecified unreleased,,,"Trained on 20T+ tokens of high-quality data, together with supervised fine-tuning and multi-stage reinforcement learning",,,,Confident,"Today, Ling-flash-2.0 is officially open-sourced! ðŸš€ Following the release of the language model Ling-mini-2.0 and the thinking model Ring-mini-2.0, we are now open-sourcing the third MoE LLM under the Ling 2.0 architecture: Ling-flash-2.0, a language model with 100B total parameters and 6.1B activated parameters (4.8B non-embedding). Trained on 20T+ tokens of high-quality data, together with supervised fine-tuning and multi-stage reinforcement learning, Ling-flash-2.0 achieves SOTA performance among dense models under 40B parameters, despite activating only ~6B parameters. Compared to MoE models with larger activation/total parameters, it also demonstrates strong competitiveness. Notably, it delivers outstanding performance in complex reasoning, code generation, and frontend development.",China,,,,,,Unreleased,"MIT license
https://huggingface.co/inclusionAI/Ling-flash-base-2.0",,,
Qwen3-Max,Language,"Language modeling/generation,Question answering,Mathematical reasoning,Code generation,Quantitative reasoning,Retrieval-augmented generation,Translation",,API access,"https://modelstudio.console.alibabacloud.com/?tab=doc#/doc/?type=model&url=2840914_2&modelId=qwen3-max-preview

https://qwen.ai/blog?id=87dc93fc8a590dc718c77e1f6e84c07b474f6c5a&from=home.latest-research-list",,"Introducing Qwen3-Max-Preview (Instruct) â€” our biggest model yet, with over 1 trillion parameters! ",2025-09-05,Alibaba,1000000000000.0,MoE architecture,1.512e+25,"6ND with:
36T tokens is taken from the qwen3 technical report
70B active params is based on it having >1T params, and the architectures of Qwen3-235B-A22B and Qwen3-Coder-480B-A35B",Unspecified unreleased,,36000000000000.0,"""was pretrained on 36 trillion tokens""",,,,Speculative,"Following the release of the Qwen3-2507 series, we are thrilled to introduce Qwen3-Max â€” our largest and most capable model to date. The preview version of Qwen3-Max-Instruct currently ranks third on the Text Arena leaderboard, surpassing GPT-5-Chat. The official release further enhances performance in coding and agent capabilities, achieving state-of-the-art results across a comprehensive suite of benchmarks â€” including knowledge, reasoning, coding, instruction following, human preference alignment, agent tasks, and multilingual understanding. We invite you to try Qwen3-Max-Instruct via its API on Alibaba Cloud or explore it directly on Qwen Chat. Meanwhile, Qwen3-Max-Thinking â€” still under active training â€” is already demonstrating remarkable potential. When augmented with tool usage and scaled test-time compute, the Thinking variant has achieved 100% on challenging reasoning benchmarks such as AIME 25 and HMMT. We look forward to releasing it publicly in the near future.",China,,,,,,Unreleased,,,,
Apertus 70B,Language,"Language modeling/generation,Question answering,Code generation,Translation",,Open weights (unrestricted),"https://ethz.ch/en/news-and-events/eth-news/news/2025/09/press-release-apertus-a-fully-open-transparent-multilingual-language-model.html

https://github.com/swiss-ai/apertus-tech-report/blob/main/Apertus_Tech_Report.pdf",,"Apertus: a fully open, transparent, multilingual language model",2025-09-02,"ETH Zurich,Ecole Polytechnique FÂ´edÂ´erale de Lausanne (EPFL),Swiss National Supercomputing Centre (CSCS),Swisscom",70000000000.0,70B,6.74e+24,"6.74 Â· 10^24 FLOPs - reported

6 FLOP / parameter / token * 70 * 10^9 parameters * 15 * 10^12 tokens = 6.3e+24 FLOP 

989500000000000 FLOP / GPU / sec [GH200, bf16 reported] * 6*10^6 GPU-hours * 3600 sec / hour * 0.3 [assumed utilization] = 6.41196e+24 FLOP","FineWeb,DCLM-baseline,Wikipedia,StarCoderData,The Stack v2,FineMath,MegaMath",,15000000000000.0,"""Trained on 15 trillion tokens across more than 1,000 languages â€“ 40% of the data is non-English""",,"""6 million GPU hours""",NVIDIA GH200,Confident,"Researchers at EPFL, ETH Zurich and CSCS have developed Apertus, a fully open Large Language Model (LLM) â€“ one of the largest of its kind.
As a foundational technology, Apertus enables innovation and strengthens AI expertise across research, society and industry by allowing others to build upon it.
Apertus is currently available through strategic partner Swisscom, the AI platform Hugging Face, and the Public AI network.","Switzerland,Switzerland,Switzerland,Switzerland",,,,4096.0,,Open source,"Apache 2.0
https://huggingface.co/swiss-ai/Apertus-8B-Instruct-2509

Apache 2.0 
https://github.com/swiss-ai/pretrain-code",,,
LongCat-Flash,Language,"Language modeling/generation,Question answering,Chat,Code generation,Quantitative reasoning,Instruction interpretation","Meituan LongCat Team, Bayan, Bei Li, Bingye Lei, Bo Wang, Bolin Rong, Chao Wang, Chao Zhang, Chen Gao, Chen Zhang, Cheng Sun, Chengcheng Han, Chenguang Xi, Chi Zhang, Chong Peng, Chuan Qin, Chuyu Zhang, Cong Chen, Congkui Wang, Dan Ma, Daoru Pan, Defei Bu, Dengchang Zhao, Deyang Kong, Dishan Liu, Feiye Huo, Fengcun Li, Fubao Zhang, Gan Dong, Gang Liu, Gang Xu, Ge Li, Guoqiang Tan, Guoyuan Lin, Haihang Jing, Haomin Fu, Haonan Yan, Haoxing Wen, Haozhe Zhao, Hong Liu, Hongmei Shi, Hongyan Hao, Hongyin Tang, Huantian Lv, Hui Su, Jiacheng Li, Jiahao Liu, Jiahuan Li, Jiajun Yang, Jiaming Wang, Jian Yang, Jianchao Tan, Jiaqi Sun, Jiaqi Zhang, Jiawei Fu, Jiawei Yang, Jiaxi Hu, Jiayu Qin, Jingang Wang, Jiyuan He, Jun Kuang, Junhui Mei, Kai Liang, Ke He, Kefeng Zhang, Keheng Wang, Keqing He, Liang Gao, Liang Shi, Lianhui Ma, Lin Qiu, Lingbin Kong, Lingtong Si, Linkun Lyu, Linsen Guo, Liqi Yang, Lizhi Yan, Mai Xia, Man Gao, Manyuan Zhang, Meng Zhou, Mengxia Shen, Mingxiang Tuo, Mingyang Zhu, Peiguang Li, Peng Pei, Peng Zhao, Pengcheng Jia, Pingwei Sun, Qi Gu, Qianyun Li, Qingyuan Li, Qiong Huang, Qiyuan Duan, Ran Meng, Rongxiang Weng, Ruichen Shao, Rumei Li, Shizhe Wu, Shuai Liang et al. (82 additional authors not shown)",Open weights (unrestricted),https://arxiv.org/abs/2509.01322,,LongCat-Flash Technical Report,2025-09-01,Meituan Inc,560000000000.0,"""560 billion total parameters, featuring an innovative Mixture-of-Experts (MoE) architecture. The model incorporates a dynamic computation mechanism that activates 18.6Bâˆ¼31.3B parameters (averagingâˆ¼27B)""",3.726e+24,"6 FLOP/parameter/token * 27000000000 active parameters * 23000000000000 tokens [""Likely"" confidence, see dataset size notes] = 3.726e+24 FLOP",Unspecified unreleased,,23000000000000.0,"""(1) We train the model on approximately 20 trillion tokens with 8192 sequence length to establish a robust base model. 
(2) Reasoning and coding capabilities are further enhanced using trillions of data. 
(3) The context length is extended to 128k through training on long context corpora.""

With ""Likely"" confidence we may assume theytrained on total of 23T tokens",720.0,"""completing training within 30 days"" (=720 hours)",,Likely,"We introduce LongCat-Flash, a 560-billion-parameter Mixture-of-Experts (MoE) language model designed for both computational efficiency and advanced agentic capabilities. Stemming from the need for scalable efficiency, LongCat-Flash adopts two novel designs: (a) Zero-computation Experts, which enables dynamic computational budget allocation and activates 18.6B-31.3B (27B on average) per token depending on contextual demands, optimizing resource usage. (b) Shortcut-connected MoE, which enlarges the computation-communication overlap window, demonstrating notable gains in inference efficiency and throughput compared to models of a comparable scale. We develop a comprehensive scaling framework for large models that combines hyperparameter transfer, model-growth initialization, a multi-pronged stability suite, and deterministic computation to achieve stable and reproducible training. Notably, leveraging the synergy among scalable architectural design and infrastructure efforts, we complete model training on more than 20 trillion tokens within 30 days, while achieving over 100 tokens per second (TPS) for inference at a cost of $0.70 per million output tokens. To cultivate LongCat-Flash towards agentic intelligence, we conduct a large-scale pre-training on optimized mixtures, followed by targeted mid- and post-training on reasoning, code, and instructions, with further augmentation from synthetic data and tool use tasks. Comprehensive evaluations demonstrate that, as a non-thinking foundation model, LongCat-Flash delivers highly competitive performance among other leading models, with exceptional strengths in agentic tasks. The model checkpoint of LongCat-Flash is open-sourced to foster community research.",China,,,,,,Unreleased,"MIT license
https://huggingface.co/meituan-longcat/LongCat-Flash-Chat",,,
DeepSeek-V3.1,Language,"Language modeling/generation,Code generation,Quantitative reasoning,Question answering,Search,System control,Instruction interpretation",,Open weights (unrestricted),"https://api-docs.deepseek.com/news/news250821

https://huggingface.co/deepseek-ai/DeepSeek-V3.1",,Introducing DeepSeek-V3.1: our first step toward the agent era!,2025-08-21,DeepSeek,671000000000.0,"671B total, 37B active",3.594058e+24,"This was based on V3 which was trained on 14.8T tokens for a total of 14.8T + 840B = 15.64T pretraining tokens. Adding the FLOPs from continued pretraining onto our estimate for V3, which is 3.4e24 FLOPs, we get 3.4e24 + 1.9e23 = 3.59e24",Unspecified unreleased,,839000000000.0,"""840B tokens continued pretraining for long context extension on top of V3""

""The 32K extension phase has been increased 10-fold to 630B tokens, while the 128K extension phase has been extended by 3.3x to 209B tokens.""

839B in total",,,,Confident,"DeepSeek-V3.1 is a hybrid model that supports both thinking mode and non-thinking mode. Compared to the previous version, this upgrade brings improvements in multiple aspects:

Hybrid thinking mode: One model supports both thinking mode and non-thinking mode by changing the chat template.

Smarter tool calling: Through post-training optimization, the model's performance in tool usage and agent tasks has significantly improved.

Higher thinking efficiency: DeepSeek-V3.1-Think achieves comparable answer quality to DeepSeek-R1-0528, while responding more quickly.

DeepSeek-V3.1 is post-trained on the top of DeepSeek-V3.1-Base, which is built upon the original V3 base checkpoint through a two-phase long context extension approach, following the methodology outlined in the original DeepSeek-V3 report. We have expanded our dataset by collecting additional long documents and substantially extending both training phases. The 32K extension phase has been increased 10-fold to 630B tokens, while the 128K extension phase has been extended by 3.3x to 209B tokens.",China,DeepSeek-V3,1.86258e+23,6 FLOP / parameter / token * 37* 10^9 active parameters * 839 * 10^9 tokens = 1.86258e+23 FLOP,,,Unreleased,"MIT license
https://huggingface.co/deepseek-ai/DeepSeek-V3.1",,,
Seed-OSS-36B-Base,Language,"Language modeling/generation,Question answering,System control,Code generation,Mathematical reasoning",,Open weights (unrestricted),https://seed.bytedance.com/en/blog/seed-oss-open-source-models-release,,Seed-OSS Open-Source Models Release,2025-08-21,ByteDance,,,2.592e+24,6 FLOP / parameter / token * 36 * 10^9 parameters * 12 * 10^12 tokens = 2.592e+24 FLOP,Unspecified unreleased,,12000000000.0,"""Trained with 12T tokens""",,,,Confident,"Seed-OSS is a series of open-source language models developed by ByteDance's Seed Team, designed for powerful long-context, reasoning, agent and general capabilities, and versatile developer-friendly features.

We have released Seed-OSS-36B to the open-source community under the Apache-2.0 license. Trained with 12T tokens, Seed-OSS-36B has achieved impressive results on mainstream benchmarks while maintaining good practical performance at a low cost.

Key Features
Native Long Context: Trained with up-to-512K long context natively.

Flexible Control of Thinking Budget: Allowing users to flexibly adjust the reasoning length as needed. This capability of dynamically controlling the reasoning length enhances inference efficiency in practical application scenarios.

Enhanced Reasoning Capability: Specifically optimized for reasoning tasks while maintaining balanced and excellent general capabilities.

Agentic Intelligence: Performs well in agentic tasks such as tool-using and issue resolving.

Research-Friendly: Given that the inclusion of synthetic instruction data in pre-training may affect the post-training research, we released pre-trained models both with and without instruction data, providing the research community with more diverse options.",China,,,,,,Unreleased,"Apache 2.0
https://huggingface.co/ByteDance-Seed/Seed-OSS-36B-Base",,,
Teuken 7B,Language,"Language modeling/generation,Question answering","Mehdi Ali, Michael Fromm, Klaudia Thellmann, Jan Ebert, Alexander Arno Weber, Richard Rutmann, Charvi Jain, Max LÃ¼bbering, Daniel Steinigen, Johannes Leveling, Katrin Klug, Jasper Schulze Buschhoff, Lena Jurkschat, Hammam Abdelwahab, Benny JÃ¶rg Stein, Karl-Heinz Sylla, Pavel Denisov, Nicolo' Brandizzi, Qasid Saleem, Anirban Bhowmick, Lennard Helmer, Chelsea John, Pedro Ortiz Suarez, Malte Ostendorff, Alex Jude, Lalith Manjunath, Samuel Weinbach, Carolin Penke, Oleg Filatov, Fabio Barth, Paramita Mirza, Lucas Weber, Ines Wendler, Rafet Sifa, Fabian KÃ¼ch, Andreas Herten, RenÃ© JÃ¤kel, Georg Rehm, Stefan Kesselheim, Joachim KÃ¶hler, Nicolas Flores-Herr",Open weights (non-commercial),https://arxiv.org/abs/2410.03730,,Teuken-7B-Base & Teuken-7B-Instruct: Towards European LLMs,2025-08-21,"OpenGPT-X,Fraunhofer Institute for Algorithms and Scientific Computing,Forschungszentrum Julich,Technische UniversitÃ¤t Dresden",7000000000.0,7B,2.1444092e+23,"6 FLOP/parameter/token * 7000000000 parameters * 4000000000000 tokens = 1.68e+23 FLOP

312000000000000 FLOP/GPU/sec * 812321 GPU-hours * 3600 sec / hour * 0.3 [assumed utilization] = 2.7371968415999997e+23 FLOP

sqrt(1.68e+23 * 2.7371968415999997e+23) = 2.1444092e+23 FLOP","OpenOrca,WizardLM",Teuken-7B-instruct-research-v0.4 was pre-trained on 4 trillion tokens of data from publicly available sources. The pretraining data has a cutoff of September 2023.,4000000000000.0,pre-trained with 4T tokens,,"""We trained our models for 812.321 GPU hours on a supercomputer, which comprises 936 compute nodes, each containing 4Ã— NVIDIA A100 (40 GB) GPUs connected via NVLink3 intra-node and through Mellanox HDR200 InfiniBand (IB) inter-node.""",NVIDIA A100 SXM4 40 GB,Confident,"We present two multilingual LLMs, Teuken 7B-base and Teuken 7B-instruct, designed to embrace Europe's linguistic diversity by supporting all 24 official languages of the European Union. Trained on a dataset comprising around 60% non-English data and utilizing a custom multilingual tokenizer, our models address the limitations of existing LLMs that predominantly focus on English or a few high-resource languages. We detail the models' development principles, i.e., data composition, tokenizer optimization, and training methodologies. The models demonstrate strong performance across multilingual benchmarks, as evidenced by their performance on European versions of ARC, HellaSwag, and TruthfulQA.","Germany,Germany,Germany,Germany",,,,512.0,,Unreleased,https://huggingface.co/openGPT-X/Teuken-7B-instruct-research-v0.4,,,
NVIDIA-Nemotron-Nano-9B-v2,Language,"Language modeling/generation,Question answering",,Open weights (restricted use),https://build.nvidia.com/nvidia/nvidia-nemotron-nano-9b-v2/modelcard,,NVIDIA-Nemotron-Nano-9B-v2 Overview,2025-08-18,NVIDIA,9000000000.0,9B,1.53e+24,"Cumulative compute : 1.53E+24 FLOPS [reported]

6 FLOP/parameter/token * 9000000000 parameters * 21100000000000 tokens = 1.1394e+24 FLOP",Nemotron-Pre-Training-Dataset,"""The pre-training corpus for NVIDIA-Nemotron-Nano-9B-v2 consists of high-quality curated and synthetically-generated data. It is trained in the English language, as well as 15 multilingual languages and 43 programming languages. Our sources cover a variety of document types such as: webpages, dialogue, articles, and other written materials. The corpus spans domains including legal, math, science, finance, and more. We also include a small portion of question-answering, and alignment style data to improve model accuracy.""",21100000000000.0,21.1T,,,,Confident,"NVIDIA-Nemotron-Nano-9B-v2 is a large language model (LLM) trained from scratch by NVIDIA, and designed as a unified model for both reasoning and non-reasoning tasks. It responds to user queries and tasks by first generating a reasoning trace and then concluding with a final response. The model's reasoning capabilities can be controlled via a system prompt. If the user prefers the model to provide its final answer without intermediate reasoning traces, it can be configured to do so, albeit with a slight decrease in accuracy for harder prompts that require reasoning. Conversely, allowing the model to generate reasoning traces first generally results in higher-quality final solutions to queries and tasks.

The model uses a hybrid architecture consisting primarily of Mamba-2 and MLP layers combined with just four Attention layers. For the architecture, please refer to the Nemotron-H tech report.

The supported languages include: English, German, Spanish, French, Italian, and Japanese. Improved using Qwen.

This model is ready for commercial use.",United States of America,,,,,,Unreleased,"GOVERNING TERMS: Your use of this model is governed by the NVIDIA Open Model License. Additional Information: Llama 3.1 Community License Agreement. Built with Llama.

""Models are commercially usable""

https://huggingface.co/nvidia/NVIDIA-Nemotron-Nano-9B-v2",,,
NVIDIA-Nemotron-Nano-12B-v2,Language,"Language modeling/generation,Question answering",,Open weights (restricted use),https://huggingface.co/nvidia/NVIDIA-Nemotron-Nano-12B-v2,,NVIDIA-Nemotron-Nano-12B-v2,2025-08-18,NVIDIA,12000000000.0,12B,1.5192e+24,6 FLOP/parameter/token * 12000000000 parameters * 21100000000000 tokens = 1.5192e+24 FLOP,Nemotron-Pre-Training-Dataset,"""The pre-training corpus for NVIDIA-Nemotron-Nano-9B-v2 consists of high-quality curated and synthetically-generated data. It is trained in the English language, as well as 15 multilingual languages and 43 programming languages. Our sources cover a variety of document types such as: webpages, dialogue, articles, and other written materials. The corpus spans domains including legal, math, science, finance, and more. We also include a small portion of question-answering, and alignment style data to improve model accuracy.""",21100000000000.0,21.1T,,,,Confident,"NVIDIA-Nemotron-Nano-12B-v2 is a large language model (LLM) trained from scratch by NVIDIA, and designed as a unified model for both reasoning and non-reasoning tasks. It responds to user queries and tasks by first generating a reasoning trace and then concluding with a final response. The model's reasoning capabilities can be controlled via a system prompt. If the user prefers the model to provide its final answer without intermediate reasoning traces, it can be configured to do so, albeit with a slight decrease in accuracy for harder prompts that require reasoning. Conversely, allowing the model to generate reasoning traces first generally results in higher-quality final solutions to queries and tasks. The model was fine-tuned from NVIDIA-Nemotron-Nano-12B-v2-Base was further compressed into NVIDIA-Nemotron-Nano-9B-v2.

The model uses a hybrid architecture consisting primarily of Mamba-2 and MLP layers combined with just six Attention layers. For the architecture, please refer to the Nemotron-H tech report. The model was trained using Megatron-LM and NeMo-RL.

The supported languages include: English, German, Spanish, French, Italian, and Japanese. Improved using Qwen.

This model is ready for commercial use.",United States of America,,,,,,Unreleased,"GOVERNING TERMS: Your use of this model is governed by the NVIDIA Open Model License. Additional Information: Llama 3.1 Community License Agreement. Built with Llama.

""Models are commercially usable""

https://huggingface.co/nvidia/NVIDIA-Nemotron-Nano-12B-v2",,,
GLM-4.5V,"Multimodal,Vision,Language,Video","Language modeling/generation,Question answering,Visual question answering,Image captioning,Video description,Table tasks,Character recognition (OCR)","GLM-V Team: Wenyi Hong, Wenmeng Yu, Xiaotao Gu, Guo Wang, Guobing Gan, Haomiao Tang, Jiale Cheng, Ji Qi, Junhui Ji, Lihang Pan, Shuaiqi Duan, Weihan Wang, Yan Wang, Yean Cheng, Zehai He, Zhe Su, Zhen Yang, Ziyang Pan, Aohan Zeng, Baoxu Wang, Bin Chen, Boyan Shi, Changyu Pang, Chenhui Zhang, Da Yin, Fan Yang, Guoqing Chen, Jiazheng Xu, Jiale Zhu, Jiali Chen, Jing Chen, Jinhao Chen, Jinghao Lin, Jinjiang Wang, Junjie Chen, Leqi Lei, Letian Gong, Leyi Pan, Mingdao Liu, Mingde Xu, Mingzhi Zhang, Qinkai Zheng, Sheng Yang, Shi Zhong, Shiyu Huang, Shuyuan Zhao, Siyan Xue, Shangqin Tu, Shengbiao Meng, Tianshu Zhang, Tianwei Luo, Tianxiang Hao, Tianyu Tong, Wenkai Li, Wei Jia, Xiao Liu, Xiaohan Zhang, Xin Lyu, Xinyue Fan, Xuancheng Huang, Yanling Wang, Yadong Xue, Yanfeng Wang, Yanzi Wang, Yifan An, Yifan Du, Yiming Shi, Yiheng Huang, Yilin Niu, Yuan Wang, Yuanchang Yue, Yuchen Li, Yutao Zhang, Yuting Wang, Yu Wang, Yuxuan Zhang, Zhao Xue, Zhenyu Hou, Zhengxiao Du, Zihan Wang, Peng Zhang, Debing Liu, Bin Xu, Juanzi Li, Minlie Huang, Yuxiao Dong, Jie Tang",Open weights (unrestricted),https://arxiv.org/abs/2507.01006,,GLM-4.5V and GLM-4.1V-Thinking: Towards Versatile Multimodal Reasoning with Scalable Reinforcement Learning,2025-08-15,"Z.ai (Zhipu AI),Tsinghua University",108000000000.0,"108 total, 12 B active",1.8e+24,1.6560000000000004e+24 FLOP [backbone model compute] + 1.44e+23 FLOP = 1.8e+24 FLOP,"LAION,DataComp-1B,DFN,Wukong,MINT-1T,MMC4 / Multimodal C4,OmniCorpus",""" The process begins with the aggregation of an initial pool of over 10 billion image-text pairs from diverse sources, including public datasets like LAION[40], DataComp [12], DFN[8], and Wukong [14], supplemented by data from web search engines.""

"" Our pipeline for web data begins with the aggregation of raw content from large-scale open-source datasets, including MINT [3], MMC4 [73], and OmniCorpus [29]""

""To bolster the modelâ€™s OCR capabilities, we construct a large-scale pre-training dataset comprising 220 million images""",2013265900000.0,"Pre-training: ""The training utilizes a sequence length of 8,192 and a global batch size of 1,536 for a total of 120,000 steps.""

Continual training: ""To accommodate these longer inputs, we increase the sequence length to 32,768 and enhance our parallelism strategy by setting the context parallel size to 4 in addition to the base parallel configuration. This stage is run for an additional 10,000 steps, while maintaining the global batch size of 1,536.""

8192*1536*120000 + 32768*1536*10000 = 2.0132659e+12 gradient updates",,,,Confident,"We present GLM-4.1V-Thinking and GLM-4.5V, a family of vision-language models (VLMs) designed to advance general-purpose multimodal understanding and reasoning. In this report, we share our key findings in the development of the reasoning-centric training framework. We first develop a capable vision foundation model with significant potential through large-scale pre-training, which arguably sets the upper bound for the final performance. We then propose Reinforcement Learning with Curriculum Sampling (RLCS) to unlock the full potential of the model, leading to comprehensive capability enhancement across a diverse range of tasks, including STEM problem solving, video understanding, content recognition, coding, grounding, GUI-based agents, and long document interpretation. In a comprehensive evaluation across 42 public benchmarks, GLM-4.5V achieves state-of-the-art performance on nearly all tasks among open-source models of similar size, and demonstrates competitive or even superior results compared to closed-source models such as Gemini-2.5-Flash on challenging tasks including Coding and GUI Agents. Meanwhile, the smaller GLM-4.1V-9B-Thinking remains highly competitive-achieving superior results to the much larger Qwen2.5-VL-72B on 29 benchmarks. We open-source both GLM-4.1V-9B-Thinking and GLM-4.5V. Code, models and more information are released at this https URL.","China,China",GLM-4.5-Air,1.44e+23,6 FLOP / token / parameter * 12 * 10^9 active parameters * 2 *10^12 tokens [see dataset size notes] = 1.44e+23 FLOP,,,Unreleased,"MIT license
https://huggingface.co/zai-org/GLM-4.5V

Apache 2.0
https://github.com/zai-org/GLM-V",,,
GLM-4.1V-Thinking,"Multimodal,Vision,Language","Language modeling/generation,Question answering,Visual question answering,Image captioning","GLM-V Team: Wenyi Hong, Wenmeng Yu, Xiaotao Gu, Guo Wang, Guobing Gan, Haomiao Tang, Jiale Cheng, Ji Qi, Junhui Ji, Lihang Pan, Shuaiqi Duan, Weihan Wang, Yan Wang, Yean Cheng, Zehai He, Zhe Su, Zhen Yang, Ziyang Pan, Aohan Zeng, Baoxu Wang, Bin Chen, Boyan Shi, Changyu Pang, Chenhui Zhang, Da Yin, Fan Yang, Guoqing Chen, Jiazheng Xu, Jiale Zhu, Jiali Chen, Jing Chen, Jinhao Chen, Jinghao Lin, Jinjiang Wang, Junjie Chen, Leqi Lei, Letian Gong, Leyi Pan, Mingdao Liu, Mingde Xu, Mingzhi Zhang, Qinkai Zheng, Sheng Yang, Shi Zhong, Shiyu Huang, Shuyuan Zhao, Siyan Xue, Shangqin Tu, Shengbiao Meng, Tianshu Zhang, Tianwei Luo, Tianxiang Hao, Tianyu Tong, Wenkai Li, Wei Jia, Xiao Liu, Xiaohan Zhang, Xin Lyu, Xinyue Fan, Xuancheng Huang, Yanling Wang, Yadong Xue, Yanfeng Wang, Yanzi Wang, Yifan An, Yifan Du, Yiming Shi, Yiheng Huang, Yilin Niu, Yuan Wang, Yuanchang Yue, Yuchen Li, Yutao Zhang, Yuting Wang, Yu Wang, Yuxuan Zhang, Zhao Xue, Zhenyu Hou, Zhengxiao Du, Zihan Wang, Peng Zhang, Debing Liu, Bin Xu, Juanzi Li, Minlie Huang, Yuxiao Dong, Jie Tang",Open weights (unrestricted),https://arxiv.org/abs/2507.01006,,GLM-4.5V and GLM-4.1V-Thinking: Towards Versatile Multimodal Reasoning with Scalable Reinforcement Learning,2025-08-15,"Z.ai (Zhipu AI),Tsinghua University",9000000000.0,9B,9.18e+23,"8.1e+23 FLOP [backbone model compute, ""Likely"" confidence] + 1.08e+23 FLOP [""Confident""] = 9.18e+23 FLOP","LAION,DataComp-1B,DFN,Wukong,MINT-1T,MMC4 / Multimodal C4,OmniCorpus",""" The process begins with the aggregation of an initial pool of over 10 billion image-text pairs from diverse sources, including public datasets like LAION[40], DataComp [12], DFN[8], and Wukong [14], supplemented by data from web search engines.""

"" Our pipeline for web data begins with the aggregation of raw content from large-scale open-source datasets, including MINT [3], MMC4 [73], and OmniCorpus [29]""

""To bolster the modelâ€™s OCR capabilities, we construct a large-scale pre-training dataset comprising 220 million images""",2013265900000.0,"Pre-training: ""The training utilizes a sequence length of 8,192 and a global batch size of 1,536 for a total of 120,000 steps.""

Continual training: ""To accommodate these longer inputs, we increase the sequence length to 32,768 and enhance our parallelism strategy by setting the context parallel size to 4 in addition to the base parallel configuration. This stage is run for an additional 10,000 steps, while maintaining the global batch size of 1,536.""

8192*1536*120000 + 32768*1536*10000 = 2.0132659e+12 gradient updates",,,,Likely,"We present GLM-4.1V-Thinking and GLM-4.5V, a family of vision-language models (VLMs) designed to advance general-purpose multimodal understanding and reasoning. In this report, we share our key findings in the development of the reasoning-centric training framework. We first develop a capable vision foundation model with significant potential through large-scale pre-training, which arguably sets the upper bound for the final performance. We then propose Reinforcement Learning with Curriculum Sampling (RLCS) to unlock the full potential of the model, leading to comprehensive capability enhancement across a diverse range of tasks, including STEM problem solving, video understanding, content recognition, coding, grounding, GUI-based agents, and long document interpretation. In a comprehensive evaluation across 42 public benchmarks, GLM-4.5V achieves state-of-the-art performance on nearly all tasks among open-source models of similar size, and demonstrates competitive or even superior results compared to closed-source models such as Gemini-2.5-Flash on challenging tasks including Coding and GUI Agents. Meanwhile, the smaller GLM-4.1V-9B-Thinking remains highly competitive-achieving superior results to the much larger Qwen2.5-VL-72B on 29 benchmarks. We open-source both GLM-4.1V-9B-Thinking and GLM-4.5V. Code, models and more information are released at this https URL.","China,China",GLM-4-9B-0414,1.08e+23,6 FLOP / token / parameter * 9 * 10^9 parameters * 2 *10^12 tokens [see dataset size notes] = 1.08e+23 FLOP,,,Unreleased,"MIT license
https://huggingface.co/zai-org/GLM-4.1V-9B-Thinking

Apache 2.0
https://github.com/zai-org/GLM-V",,,
GPT-5,"Multimodal,Language,Vision",,,API access,"https://cdn.openai.com/gpt-5-system-card.pdf
https://openai.com/index/introducing-gpt-5/ ",,GPT-5 System Card,2025-08-07,OpenAI,,,6.6e+25,"Likely around 6e25 [CI: 2e25 to 2e26] FLOP. See document below for details

https://docs.google.com/document/d/1V2jIk365LnhH4WDoCw5dYJjZr1Htw8IHaK1noMf5Y48/edit?tab=t.z871imftkus",,,,,,,,Speculative,"""We are introducing GPTâ€‘5, our best AI system yet. GPTâ€‘5 is a significant leap in intelligence over all our previous models, featuring state-of-the-art performance across coding, math, writing, health, visual perception, and more. It is a unified system that knows when to respond quickly and when to think longer to provide expert-level responses. GPTâ€‘5 is available to all users, with Plus subscribers getting more usage, and Pro subscribers getting access to GPTâ€‘5 pro, a version with extended reasoning for even more comprehensive and accurate answers.""",United States of America,,,,,,Unreleased,,,,
gpt-oss-120b,Language,"Language modeling/generation,Question answering",,Open weights (unrestricted),https://cdn.openai.com/pdf/419b6906-9da6-406c-a19d-1bb078ac7637/oai_gpt-oss_model_card.pdf,,gpt-oss-120b & gpt-oss-20b Model Card,2025-08-05,OpenAI,116830000000.0,Total parameters: 116.83B,4.94e+24,"""The training run for gpt-oss-120b required 2.1 million H100-hours to complete""
(2.1e6 hours)*(1,979 H100 FLOP/s)*(30% utilization)*(60*60) = 4.49e24
They also do post training similar to o3, which we assume adds at least 10% as much compute, so we multiply this estimate by 1.1 to get 4.94e24

",,,146732030000000.0,(pretraining FLOPs)/(6*5.1B active parameters),,,NVIDIA H100 SXM5 80GB,Confident,,United States of America,,,,,,,,,,
gpt-oss-20b,Language,"Language modeling/generation,Question answering",,Open weights (unrestricted),https://cdn.openai.com/pdf/419b6906-9da6-406c-a19d-1bb078ac7637/oai_gpt-oss_model_card.pdf,,gpt-oss-120b & gpt-oss-20b Model Card,2025-08-05,OpenAI,20910000000.0,Total parameters: 20.91B,5.49e+23,"""The training run for gpt-oss-120b required 2.1
million H100-hours to complete, with gpt-oss-20b needing almost 10x fewer""
assuming ""almost 10x fewer"" means ~9x fewer: 4.94e24/9 = 5.49e23",,,23096708000000.0,(pretraining FLOPs)/(6*3.6B active parameters),,,NVIDIA H100 SXM5 80GB,Confident,,United States of America,,,,,,,,,,
GLM-4.5,Language,"Language modeling/generation,Question answering,Code generation,Quantitative reasoning","Bin Chen, Chengxing Xie, Cunxiang Wang, Da Yin, Hao Zeng, Jiajie Zhang, Kedong Wang, Lucen
Zhong, Mingdao Liu, Rui Lu, Shulin Cao, Xiaohan Zhang, Xuancheng Huang, Yao Wei, Yean Cheng,
Yifan An, Yilin Niu, Yuanhao Wen, Yushi Bai, Zhengxiao Du, Zihan Wang (æ±ªå­æ¶µ), Zilin Zhu",Open weights (unrestricted),https://arxiv.org/abs/2508.06471,,"GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models",2025-08-05,"Z.ai (Zhipu AI),Tsinghua University",355000000000.0,355 billion total parameters with 32 billion active parameters,4.42e+24,(6 FLOP/token/parameter) * (23T tokens) * (32B active parameters) = 4.42e24 FLOP,Unspecified unreleased,,23000000000000.0,23T tokens,,,,Confident,"We present GLM-4.5, an open-source Mixture-of-Experts (MoE) large language model with 355B total parameters and 32B activated parameters, featuring a hybrid reasoning method that supports both thinking and direct response modes.
Through multi-stage training on 23T tokens and comprehensive post-training with expert model iteration and reinforcement learning, GLM-4.5 achieves strong performance across agentic, reasoning, and coding (ARC) tasks, scoring 70.1% on
TAU-Bench, 91.0% on AIME 24, and 64.2% on SWE-bench Verified. With much fewer parameters than several competitors, GLM-4.5 ranks 3rd overall among all evaluated models and 2nd on agentic benchmarks. We release both GLM-4.5 (355B parameters) and a compact version, GLM-4.5-Air (106B parameters), to advance research in reasoning and agentic AI systems. Code, models, and more information are available at https://github.com/zai-org/GLM-4.5.","China,China",,,,,,Unreleased,"MIT license
https://huggingface.co/zai-org/GLM-4.5

Apache 2.0 (Inference code)
https://github.com/zai-org/GLM-4.5?tab=readme-ov-file",,,
GLM-4.5-Air,Language,"Language modeling/generation,Question answering,Visual question answering,Image captioning,Video description,Table tasks,Character recognition (OCR)","Bin Chen, Chengxing Xie, Cunxiang Wang, Da Yin, Hao Zeng, Jiajie Zhang, Kedong Wang, Lucen
Zhong, Mingdao Liu, Rui Lu, Shulin Cao, Xiaohan Zhang, Xuancheng Huang, Yao Wei, Yean Cheng,
Yifan An, Yilin Niu, Yuanhao Wen, Yushi Bai, Zhengxiao Du, Zihan Wang (æ±ªå­æ¶µ), Zilin Zhu",Open weights (unrestricted),https://arxiv.org/abs/2508.06471,,"GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models",2025-08-05,"Z.ai (Zhipu AI),Tsinghua University",106000000000.0,"106B parameters, 12B active",1.656e+24,6 FLOP / parameter / token * 12 * 10^9 active parameters * 23 * 10^12 tokens = 1.656e+24 FLOP,Unspecified unreleased,,23000000000000.0,23T tokens,,,,Confident,"We present GLM-4.5, an open-source Mixture-of-Experts (MoE) large language model with 355B total parameters and 32B activated parameters, featuring a hybrid reasoning method that supports both thinking and direct response modes.
Through multi-stage training on 23T tokens and comprehensive post-training with expert model iteration and reinforcement learning, GLM-4.5 achieves strong performance across agentic, reasoning, and coding (ARC) tasks, scoring 70.1% on
TAU-Bench, 91.0% on AIME 24, and 64.2% on SWE-bench Verified. With much fewer parameters than several competitors, GLM-4.5 ranks 3rd overall among all evaluated models and 2nd on agentic benchmarks. We release both GLM-4.5 (355B parameters) and a compact version, GLM-4.5-Air (106B parameters), to advance research in reasoning and agentic AI systems. Code, models, and more information are available at https://github.com/zai-org/GLM-4.5.","China,China",,,,,,Unreleased,"MIT license
https://huggingface.co/zai-org/GLM-4.5-Air

Apache 2.0 (Inference code)
https://github.com/zai-org/GLM-4.5?tab=readme-ov-file",,,
Tri-21B,Language,"Language modeling/generation,Question answering,Code generation,Quantitative reasoning",,Open weights (restricted use),https://huggingface.co/trillionlabs/Tri-21B,,,2025-08-01,Trillion Labs,20730000000.0,"20.73B
",2.95e+23,"2.95E+23 FLOPs (reported)

6 FLOP/parameter/token * 20730000000 parameters * 2300000000000 tokens = 2.86074e+23 FLOP",Unspecified unreleased,,2300000000000.0,2.3T training tokens,,,,Confident,"We introduce Tri-21B, our flagship large language model that redefines the efficiency frontier in LLM training. By achieving state-of-the-art performance with only 2.3T training tokens, we demonstrate that exceptional capabilities don't require excessive computational resources.",Korea (Republic of),,,,,,Unreleased,"Trillion license (MAU are fewer than 1 million or ARR is less than $10 million USD)
https://huggingface.co/trillionlabs/Tri-21B",,,
Wan 2.2 14B T2V,Video,"Video generation,Text-to-video",,Open weights (unrestricted),https://huggingface.co/Wan-AI/Wan2.2-T2V-A14B,,"We are excited to introduce Wan2.2, a major upgrade to our foundational video models.",2025-07-28,Alibaba,14000000000.0,14B,4.2e+23,"6 FLOP / token / parameter * 14 * 10^9 active parameters * 5 * 10^12 tokens [assumed, see dataset size notes] = 4.2e+23 FLOP",Unspecified unreleased,,5000000000000.0,"""Wan2.2 is trained on a significantly larger data, with +65.6% more images and +83.2% more videos.""

for Wan 2.1, they are reporting ""Wan has seen large-scale data comprising billions of images and videos, amounting to O(1) trillions of tokens in total."" -->
this model was trained on ~5 trillion tokens (with ""Likely"" confidence)",,,,Likely,"This repository contains our T2V-A14B model, which supports generating 5s videos at both 480P and 720P resolutions. Built with a Mixture-of-Experts (MoE) architecture, it delivers outstanding video generation quality. On our new benchmark Wan-Bench 2.0, the model surpasses leading commercial models across most key evaluation dimensions.",China,,,,,,Unreleased,"Apache 2.0
https://huggingface.co/Wan-AI/Wan2.2-T2V-A14B

INference code:
https://github.com/Wan-Video/Wan2.2",,,
Qwen3-235B-A22B-Thinking (Jul 2025),Language,"Language modeling/generation,Question answering,Mathematical reasoning,Quantitative reasoning,Code generation,Translation",,Open weights (unrestricted),https://qwenlm.github.io/blog/qwen3/,,"Qwen3: Think Deeper, Act Faster",2025-07-25,Alibaba,235000000000.0,"235 billion total parameters and 22 billion activated parameters

Number of Layers: 94
Number of Attention Heads (GQA): 64 for Q and 4 for KV
Number of Experts: 128
Number of Activated Experts: 8
Context Length: 32,768 natively and 131,072 tokens with YaRN.",4.752e+24,6 FLOP / parameter / token * 22*10^9 active parameters * 36000000000000 tokens = 4.752e+24 FLOP,Unspecified unreleased,"""While Qwen2.5 was pre-trained on 18 trillion tokens, Qwen3 uses nearly twice that amount, with approximately 36 trillion tokens covering 119 languages and dialects. To build this large dataset, we collected data not only from the web but also from PDF-like documents. We used Qwen2.5-VL to extract text from these documents and Qwen2.5 to improve the quality of the extracted content. To increase the amount of math and code data, we used Qwen2.5-Math and Qwen2.5-Coder to generate synthetic data. This includes textbooks, question-answer pairs, and code snippets.

The pre-training process consists of three stages. In the first stage (S1), the model was pretrained on over 30 trillion tokens with a context length of 4K tokens. This stage provided the model with basic language skills and general knowledge. In the second stage (S2), we improved the dataset by increasing the proportion of knowledge-intensive data, such as STEM, coding, and reasoning tasks. The model was then pretrained on an additional 5 trillion tokens. In the final stage, we used high-quality long-context data to extend the context length to 32K tokens. """,36000000000000.0,36T,,,,Likely,"Today, we are excited to announce the release of Qwen3, the latest addition to the Qwen family of large language models. Our flagship model, Qwen3-235B-A22B, achieves competitive results in benchmark evaluations of coding, math, general capabilities, etc., when compared to other top-tier models such as DeepSeek-R1, o1, o3-mini, Grok-3, and Gemini-2.5-Pro. Additionally, the small MoE model, Qwen3-30B-A3B, outcompetes QwQ-32B with 10 times of activated parameters, and even a tiny model like Qwen3-4B can rival the performance of Qwen2.5-72B-Instruct.",China,,,,,,Unreleased,"Apache 2.0

https://huggingface.co/Qwen/Qwen3-235B-A22B",,,
Qwen3-235B-A22B-Instruct (Jul 2025),Language,"Language modeling/generation,Question answering,Mathematical reasoning,Quantitative reasoning,Code generation,Translation",,Open weights (unrestricted),https://qwenlm.github.io/blog/qwen3/,,"Qwen3: Think Deeper, Act Faster",2025-07-25,Alibaba,235000000000.0,"235 billion total parameters and 22 billion activated parameters

Number of Layers: 94
Number of Attention Heads (GQA): 64 for Q and 4 for KV
Number of Experts: 128
Number of Activated Experts: 8
Context Length: 32,768 natively and 131,072 tokens with YaRN.",4.752e+24,6 FLOP / parameter / token * 22*10^9 active parameters * 36000000000000 tokens = 4.752e+24 FLOP,Unspecified unreleased,"""While Qwen2.5 was pre-trained on 18 trillion tokens, Qwen3 uses nearly twice that amount, with approximately 36 trillion tokens covering 119 languages and dialects. To build this large dataset, we collected data not only from the web but also from PDF-like documents. We used Qwen2.5-VL to extract text from these documents and Qwen2.5 to improve the quality of the extracted content. To increase the amount of math and code data, we used Qwen2.5-Math and Qwen2.5-Coder to generate synthetic data. This includes textbooks, question-answer pairs, and code snippets.

The pre-training process consists of three stages. In the first stage (S1), the model was pretrained on over 30 trillion tokens with a context length of 4K tokens. This stage provided the model with basic language skills and general knowledge. In the second stage (S2), we improved the dataset by increasing the proportion of knowledge-intensive data, such as STEM, coding, and reasoning tasks. The model was then pretrained on an additional 5 trillion tokens. In the final stage, we used high-quality long-context data to extend the context length to 32K tokens. """,36000000000000.0,36T,,,,Likely,"Today, we are excited to announce the release of Qwen3, the latest addition to the Qwen family of large language models. Our flagship model, Qwen3-235B-A22B, achieves competitive results in benchmark evaluations of coding, math, general capabilities, etc., when compared to other top-tier models such as DeepSeek-R1, o1, o3-mini, Grok-3, and Gemini-2.5-Pro. Additionally, the small MoE model, Qwen3-30B-A3B, outcompetes QwQ-32B with 10 times of activated parameters, and even a tiny model like Qwen3-4B can rival the performance of Qwen2.5-72B-Instruct.",China,,,,,,Unreleased,"Apache 2.0

https://huggingface.co/Qwen/Qwen3-235B-A22B",,,
Qwen3-Coder-480B-A35B,Language,"Language modeling/generation,Question answering,Code generation,System control",,Open weights (unrestricted),https://qwenlm.github.io/blog/qwen3-coder/,,Qwen3-Coder: Agentic Coding in the World,2025-07-22,Alibaba,480000000000.0,"""a 480B-parameter Mixture-of-Experts model with 35B active parameters which supports the context length of 256K tokens natively and 1M tokens with extrapolation methods""",1.575e+24,6 FLOP / parameter / token * 35 * 10^9 active parameters * 7.5 * 10^12 tokens = 1.575e+24 FLOP`,Unspecified unreleased,,7500000000000.0,"""Scaling Tokens: 7.5T tokens """,,,,Confident,"Today, we're announcing Qwen3-Coder, our most agentic code model to date. Qwen3-Coder is available in multiple sizes, but we're excited to introduce its most powerful variant first: Qwen3-Coder-480B-A35B-Instruct. featuring the following key enhancements:

Significant Performance among open models on Agentic Coding, Agentic Browser-Use, and other foundational coding tasks, achieving results comparable to Claude Sonnet.
Long-context Capabilities with native support for 256K tokens, extendable up to 1M tokens using Yarn, optimized for repository-scale understanding.
Agentic Coding supporting for most platform such as Qwen Code, CLINE, featuring a specially designed function call format.",China,,,,,,Unreleased,"Apache 2.0
https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct",,,
EXAONE 4.0 (32B),Language,"Language modeling/generation,Question answering,Code generation,Quantitative reasoning,Translation","LG AI Research: Kyunghoon Bae, Eunbi Choi, Kibong Choi, Stanley Jungkyu Choi, Yemuk Choi, Kyubeen Han, Seokhee Hong, Junwon Hwang, Taewan Hwang, Joonwon Jang, Hyojin Jeon, Kijeong Jeon, Gerrard Jeongwon Jo, Hyunjik Jo, Jiyeon Jung, Euisoon Kim, Hyosang Kim, Jihoon Kim, Joonkee Kim, Seonghwan Kim, Soyeon Kim, Sunkyoung Kim, Yireun Kim, Yongil Kim, Youchul Kim, Edward Hwayoung Lee, Gwangho Lee, Haeju Lee, Honglak Lee, Jinsik Lee, Kyungmin Lee, Sangha Park, Young Min Paik, Yongmin Park, Youngyong Park, Sanghyun Seo, Sihoon Yang, Heuiyeen Yeen, Sihyuk Yi, Hyeongu Yun",Open weights (non-commercial),https://arxiv.org/abs/2507.11407,,EXAONE 4.0: Unified Large Language Models Integrating Non-reasoning and Reasoning Modes,2025-07-15,LG AI Research,32000000000.0,32B,2.69000000000001e+24,"Reported in Table 2.

from communication with the authors:
""EXAONE 4.0 32B: NVIDIA H200 GPUs x 512 EA for 19 weeks (FP8 mode training)""",Unspecified unreleased,Knowledge cut-off Nov. 2024,14000000000000.0,"max sequence length 131,072 (Table 1)

size of pretraining data: 14T (Table 2)",3192.0,"from communication with the authors:
""EXAONE 4.0 32B: NVIDIA H200 GPUs x 512 EA for 19 weeks (FP8 mode training)""

19 weeks = 3192 hours",NVIDIA H200 SXM,Confident,"This technical report introduces EXAONE 4.0, which integrates a Non-reasoning mode and a Reasoning mode to achieve both the excellent usability of EXAONE 3.5 and the advanced reasoning abilities of EXAONE Deep. To pave the way for the agentic AI era, EXAONE 4.0 incorporates essential features such as agentic tool use, and its multilingual capabilities are extended to support Spanish in addition to English and Korean. The EXAONE 4.0 model series consists of two sizes: a mid-size 32B model optimized for high performance, and a small-size 1.2B model designed for on-device applications. The EXAONE 4.0 demonstrates superior performance compared to open-weight models in its class and remains competitive even against frontier-class models. The models are publicly available for research purposes and can be easily downloaded via this https URL.",Korea (Republic of),,,,512.0,,Unreleased,"Exaone license (permits only academic, research, or educational usage) 
https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-32B",,,
Kimi K2,Language,"Language modeling/generation,Code generation,Question answering,Quantitative reasoning,Search,Table tasks",,Open weights (restricted use),https://moonshotai.github.io/Kimi-K2/,,Kimi K2: Open Agentic Intelligence,2025-07-11,Moonshot,1000000000000.0,MoE with 1T total parameters and 32B parameters active per forward pass,2.976e+24,6 FLOP / parameter / token * 32 * 10^9 activated parameters * 15.5 * 10^12 tokens = 2.976e+24 FLOP,Unspecified unreleased,,15500000000000.0,"""Kimi K2 was pre-trained on 15.5T tokens""",,,NVIDIA H800 SXM5,Confident,"Kimi K2 is our latest Mixture-of-Experts model with 32 billion activated parameters and 1 trillion total parameters. It achieves state-of-the-art performance in frontier knowledge, math, and coding among non-thinking models. But it goes further â€” meticulously optimized for agentic tasks, Kimi K2 does not just answer; it acts.
And now, it is within your reach. Today, we are open-sourcing:
Kimi-K2-Base: The foundation model, a strong start for researchers and builders who want full control for fine-tuning and custom solutions.
Kimi-K2-Instruct: The post-trained model best for drop-in, general-purpose chat and agentic experiences. It is a reflex-grade model without long thinking.
With Kimi K2, advanced agentic intelligence is more open and accessible than ever. We can't wait to see what you build.",China,,,,,,Unreleased,"Modified MIT license (separare license is required for entities with 100M+ MAU or $20M+ in annual revenue) for weights and inference code:
https://huggingface.co/moonshotai/Kimi-K2-Instruct
https://github.com/MoonshotAI/Kimi-K2?tab=readme-ov-file#4-deployment

API:
https://platform.moonshot.cn/docs/pricing/chat#%E8%AE%A1%E8%B4%B9%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5",,,
Grok 4,"Language,Multimodal,Vision","Language modeling/generation,Question answering,Search,Visual question answering,Character recognition (OCR),Image captioning,Quantitative reasoning",,API access,https://x.ai/news/grok-4,,Grok 4,2025-07-09,xAI,3000000000000.0,Rumored to be 2.4T params (https://x.com/kalomaze/status/1942996555088134592),5.0000000000001e+26,"We think that RL relative to pre-compute is between our estimate for o3 (10% of pre-training) and the 100% implied by this slide in the launch ( https://archive.is/f0vJU ). Assuming the same pre-training as Grok 3 (also implied by that slide, and much more consistent) and that Grok 3 used a tenth as much RL, we get:

2 * (grok3/1.1) in the high case (rl is 10% of grok 3, so grok3/1.1 is grok3 precompute, and in this case twice that is grok 4)
1.1 * (grok3/1.01) in the low case
The geometric mean is (rounded to one sig fig): 5e26
",Unspecified unreleased,,,,,,,Speculative,"Grok 4 is the most intelligent model in the world. It includes native tool use and real-time search integration, and is available now to SuperGrok and Premium+ subscribers, as well as through the xAI API. We are also introducing a new SuperGrok Heavy tier with access to Grok 4 Heavy - the most powerful version of Grok 4.",United States of America,,,,200000.0,,Unreleased,,,,10717356897.243427
dots.llm1,Language,"Language modeling/generation,Question answering","Bi Huo, Bin Tu, Cheng Qin, Da Zheng, Debing Zhang, Dongjie Zhang, En Li, Fu Guo, Jian Yao, Jie Lou, Junfeng Tian, Li Hu, Ran Zhu, Shengdong Chen, Shuo Liu, Su Guang, Te Wo, Weijun Zhang, Xiaoming Shi, Xinxin Peng, Xing Wu, Yawen Liu, Yuqiu Ji, Ze Wen, Zhenhai Liu, Zichao Li, Zilong Liao",Open weights (unrestricted),https://www.arxiv.org/abs/2506.05767,,dots.llm1 Technical Report,2025-07-06,Rednote,142000000000.0,"a large-scale MoE model that activates 14
billion parameters out of a total of 142 billion parameters",1.2164856e+24,"6 FLOP/parameter/token * 14000000000 active parameters * 11328000000000 tokens = 9.51552e+23 FLOP

989000000000000 FLOP/GPU/sec [H800 assumed] * 1456000 GPU-hours * 3600 sec / hour * 0.3 [assumed utilization] = 1.55518272e+24 FLOP

sqrt(9.51552e+23*1.55518272e+24) = 1.2164856e+24",Unspecified unreleased,,11328000000000.0,"pre-training: ""11.2T high-quality tokens""
long context: 128B tokens

11.328T tokens total",,"Table 4
Total GPU Hours: 1,456K ",NVIDIA H800 SXM5,Confident,"Mixture of Experts (MoE) models have emerged as a promising paradigm for scaling language models efficiently by activating only a subset of parameters for each input token. In this report, we present dots.llm1, a large-scale MoE model that activates 14B parameters out of a total of 142B parameters, delivering performance on par with state-of-the-art models while reducing training and inference costs. Leveraging our meticulously crafted and efficient data processing pipeline, dots.llm1 achieves performance comparable to Qwen2.5-72B after pretraining on 11.2T high-quality tokens and post-training to fully unlock its capabilities. Notably, no synthetic data is used during pretraining. To foster further research, we open-source intermediate training checkpoints at every one trillion tokens, providing valuable insights into the learning dynamics of large language models.",China,,,,,,Unreleased,"MIT license
https://huggingface.co/rednote-hilab/dots.llm1.inst",,,
ERNIE-4.5-300B-A47B,Language,"Language modeling/generation,Quantitative reasoning,Code generation,Translation,Question answering",,Open weights (unrestricted),https://yiyan.baidu.com/blog/publication/ERNIE_Technical_Report.pdf,,ERNIE 4.5 Technical Report,2025-06-29,Baidu,300000000000.0,"MoE
total parameters: 300B
active parameters: 47B",2.82e+24,"They say the model was trained on ""trillions of tokens"" 
Speculatively assuming 10T tokens:

6 FLOP / token / parameter * 47 * 10^9 active parameters * 10 * 10^12 assumed training tokens = 2.82e+24 FLOP",Unspecified unreleased,,,"""We commence with large-scale pre-training on trillions of pure-text tokens
sourced from diverse domains.""",,,NVIDIA H800 SXM5,Speculative,"In this report, we introduce ERNIE 4.5, a new family of large-scale multimodal models comprising 10 distinct variants. The model family consist of Mixture-of-Experts (MoE) models with 47B and 3B active parameters, with the largest model having 424B total parameters, as well as a 0.3B dense model. For the MoE architecture, we propose a novel heterogeneous modality structure, which supports parameter sharing across modalities while also allowing dedicated parameters for each individual modality.
This MoE architecture has the advantage to enhance multimodal understanding without compromising, and even improving, performance on text-related tasks. All of our models are trained with optimal efficiency using the PaddlePaddle deep learning framework, which also enables high-performance inference and streamlined deployment for them. We achieve 47% Model FLOPs Utilization (MFU) in our largest ERNIE 4.5 language model pre-training. Experimental results show that our models achieve state-of-the-art performance across multiple text and multimodal benchmarks, especially in instruction following, world knowledge memorization, visual understanding and multimodal reasoning. All models are publicly accessible under Apache 2.0 to support future research and development in the field. Additionally, we open source the development toolkits for ERNIE 4.5, featuring industrial-grade capabilities, resourceefficient training and inference workflows, and multi-hardware compatibility.",China,,,,,0.47,Unreleased,"Apache 2.0
https://huggingface.co/baidu/ERNIE-4.5-300B-A47B-Base-PT

Apache 2.0 for inference code
https://github.com/PaddlePaddle/ERNIE",,,
ERNIE-4.5-21B-A3B,Language,"Language modeling/generation,Quantitative reasoning,Code generation,Translation,Question answering",,Open weights (unrestricted),https://yiyan.baidu.com/blog/publication/ERNIE_Technical_Report.pdf,,ERNIE 4.5 Technical Report,2025-06-29,Baidu,21000000000.0,"MoE
total parameters: 21B
active parameters: 3B",1.8e+23,"They say the model was trained on """"trillions of tokens""
Speculatively assuming 10T tokens:

6 FLOP / token / parameter * 3 * 10^9 active parameters * 10 * 10^12 assumed training tokens = 1.8e+23 FLOP",Unspecified unreleased,,,"""We commence with large-scale pre-training on trillions of pure-text tokens
sourced from diverse domains.""",,,,Speculative,"In this report, we introduce ERNIE 4.5, a new family of large-scale multimodal models comprising 10 distinct variants. The model family consist of Mixture-of-Experts (MoE) models with 47B and 3B active parameters, with the largest model having 424B total parameters, as well as a 0.3B dense model. For the MoE architecture, we propose a novel heterogeneous modality structure, which supports parameter sharing across modalities while also allowing dedicated parameters for each individual modality.
This MoE architecture has the advantage to enhance multimodal understanding without compromising, and even improving, performance on text-related tasks. All of our models are trained with optimal efficiency using the PaddlePaddle deep learning framework, which also enables high-performance inference and streamlined deployment for them. We achieve 47% Model FLOPs Utilization (MFU) in our largest ERNIE 4.5 language model pre-training. Experimental results show that our models achieve state-of-the-art performance across multiple text and multimodal benchmarks, especially in instruction following, world knowledge memorization, visual understanding and multimodal reasoning. All models are publicly accessible under Apache 2.0 to support future research and development in the field. Additionally, we open source the development toolkits for ERNIE 4.5, featuring industrial-grade capabilities, resourceefficient training and inference workflows, and multi-hardware compatibility.",China,,,,,,Unreleased,"Apache 2.0
https://huggingface.co/baidu/ERNIE-4.5-21B-A3B-Base-PT

Apache 2.0 for inference code
https://github.com/PaddlePaddle/ERNIE",,,
Minimax Hailuo 02,Video,"Video generation,Text-to-video,Image-to-video",,API access,https://www.minimax.io/news/minimax-hailuo-02,,"MiniMax Hailuo 02, World-Class Quality, Record-Breaking Cost Efficiency",2025-06-18,"MiniMax,Hailuo AI",,"""We ultimately expanded the model's total parameter count to 3 times that of its predecessor.""",,"Hard to bound the training compute. It performs similarly to Veo-2/3, so plausibly has similar training budget. Minimax raised $600M in early 2024, so they plausibly could have afforded a 1e25 FLOP training run (https://siliconangle.com/2024/03/05/report-chinese-ai-startup-minimax-raises-600m-2-5b-valuation-led-alibaba/).",Unspecified unreleased,,,,,,,Unknown,"Today, we are thrilled to introduce MiniMax Hailuo 02, our highly anticipated new video generation model.
The video showcased above was a collaborative effort by three artists over the course of 1.5 days. They utilized MiniMax Hailuo 02 to generate multiple 6-10 second video clips, which were then skillfully edited into a final video.

Key Highlights of Hailuo 02:

- Native 1080p
- SOTA Instruction Following
- Extreme Physics Mastery","China,Singapore",,,,,,Unreleased,,,,
Gemini 2.5 Flash (Jun 2025),"Language,Multimodal,Vision,Speech,Video","Language modeling/generation,Question answering,Code generation,Quantitative reasoning,Visual question answering,Translation,Image captioning,Speech recognition (ASR),Video description,Search,Text summarization,Chat",,API access,https://deepmind.google/models/gemini/flash/,,Our powerful and most efficient workhorse model designed for speed and low-cost.,2025-06-17,Google DeepMind,,,,,Unspecified unreleased,Knowledge cutoff January 2025,,,,,,Unknown,"Speed and value at scale
Ideal for tasks like summarization, chat applications, data extraction, and captioning.
Thinking budget: Control how much 2.5 Flash reasons to balance latency and cost.
Natively multimodal: Understands input across text, audio, images and video.
Long context: Explore vast datasets with a 1-million token context window.
Adaptive and budgeted thinking
Adaptive controls and adjustable thinking budgets allow you to balance performance and cost.
Calibrated: The model explores diverse thinking strategies, leading to more accurate and relevant outputs.
Controllable: Developers have fine-grained control over the model's thinking process, allowing them to manage resource usage.
Adaptive: When no thinking budget is set, the model assesses the complexity of a task and calibrates the amount of thinking accordingly.","United States of America,United Kingdom of Great Britain and Northern Ireland",,,,,,Unreleased,"Availability	
Google AI Studio
Gemini API
Gemini App",,,
MiniMax-M1-80k,Language,"Language modeling/generation,Question answering,Code generation,Quantitative reasoning","MiniMax: Aili Chen, Aonian Li, Bangwei Gong, Binyang Jiang, Bo Fei, Bo Yang, Boji Shan, Changqing Yu, Chao Wang, Cheng Zhu, Chengjun Xiao, Chengyu Du, Chi Zhang, Chu Qiao, Chunhao Zhang, Chunhui Du, Congchao Guo, Da Chen, Deming Ding, Dianjun Sun, Dong Li, Enwei Jiao, Haigang Zhou, Haimo Zhang, Han Ding, Haohai Sun, Haoyu Feng, Huaiguang Cai, Haichao Zhu, Jian Sun, Jiaqi Zhuang, Jiaren Cai, Jiayuan Song, Jin Zhu, Jingyang Li, Jinhao Tian, Jinli Liu, Junhao Xu, Junjie Yan, Junteng Liu, Junxian He, Kaiyi Feng, Ke Yang, Kecheng Xiao, Le Han, Leyang Wang, Lianfei Yu, Liheng Feng, Lin Li, Lin Zheng, Linge Du, Lingyu Yang, Lunbin Zeng, Minghui Yu, Mingliang Tao, Mingyuan Chi, Mozhi Zhang, Mujie Lin, Nan Hu, Nongyu Di, Peng Gao, Pengfei Li, Pengyu Zhao, Qibing Ren, Qidi Xu, Qile Li, Qin Wang, Rong Tian, Ruitao Leng, Shaoxiang Chen, Shaoyu Chen, Shengmin Shi, Shitong Weng, Shuchang Guan, Shuqi Yu, Sichen Li, Songquan Zhu, Tengfei Li, Tianchi Cai, Tianrun Liang, Weiyu Cheng, Weize Kong, Wenkai Li, Xiancai Chen, Xiangjun Song, Xiao Luo, Xiao Su, Xiaobo Li, Xiaodong Han, Xinzhu Hou, Xuan Lu, Xun Zou, Xuyang Shen, Yan Gong, Yan Ma, Yang Wang, Yiqi Shi, Yiran Zhong, Yonghong Duan et al. (27 additional authors not shown)",Open weights (unrestricted),https://arxiv.org/abs/2506.13585,,MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention,2025-06-13,MiniMax,456000000000.0,"from the base model:
Total Parameters: 456B
Activated Parameters per Token: 45.9B",4.3240062e+24,1.9828799999999997e+24 FLOP [base model compute] + 2.3411262e+24 FLOP [see finetune compute notes] = 4.3240062e+24 FLOP,Unspecified unreleased,,7500000000000.0,"7.5T tokens

""To enhance the reasoning and long context capabilities of the foundation model while ensuring diversity, we continue training the MiniMax-Text-01 model with additional 7.5T tokens with optimized
data quality and mixture.""",,"Continual pre-training: ??
RL: ""These dual contributions yield an efficient and scalable RL framework for training M1, where the complete training cycle requires 3 weeks on 512 H800 GPUsâ€”equivalent to a rental cost of approximately $0.53M USD.""",NVIDIA H800 SXM5,Likely,"We introduce MiniMax-M1, the world's first open-weight, large-scale hybrid-attention reasoning model. MiniMax-M1 is powered by a hybrid Mixture-of-Experts (MoE) architecture combined with a lightning attention mechanism. The model is developed based on our previous MiniMax-Text-01 model, which contains a total of 456 billion parameters with 45.9 billion parameters activated per token. The M1 model natively supports a context length of 1 million tokens, 8x the context size of DeepSeek R1. Furthermore, the lightning attention mechanism in MiniMax-M1 enables efficient scaling of test-time compute. These properties make M1 particularly suitable for complex tasks that require processing long inputs and thinking extensively. MiniMax-M1 is trained using large-scale reinforcement learning (RL) on diverse problems including sandbox-based, real-world software engineering environments. In addition to M1's inherent efficiency advantage for RL training, we propose CISPO, a novel RL algorithm to further enhance RL efficiency. CISPO clips importance sampling weights rather than token updates, outperforming other competitive RL variants. Combining hybrid-attention and CISPO enables MiniMax-M1's full RL training on 512 H800 GPUs to complete in only three weeks, with a rental cost of just $534,700. We release two versions of MiniMax-M1 models with 40K and 80K thinking budgets respectively, where the 40K model represents an intermediate phase of the 80K training. Experiments on standard benchmarks show that our models are comparable or superior to strong open-weight models such as the original DeepSeek-R1 and Qwen3-235B, with particular strengths in complex software engineering, tool utilization, and long-context tasks. We publicly release MiniMax-M1 at this https URL.",China,MiniMax-Text-01,2.3411262e+24,"1. Continual pre-training for 7.5T tokens:

6 FLOP / token / active parameter * 45.9 * 10^9 activated parameters * 7.5 * 10^12 tokens = 2.0655e+24 FLOP

2. RL (3 weeks on 512 H800 GPUs):

989000000000000 FLOP / GPU / sec [bf16 assumed] * 512 GPUs * 3 weeks * 168 hours / week * 3600 sec / hour * 0.3 [assimed utilization] = 2.7562623e+23 FLOP

2.0655e+24 FLOP + 2.7562623e+23 FLOP = 2.3411262e+24 FLOP

*there was also SFT stage with no details provided, I assumed its compute is negligible",,,Unreleased,"Apache 2.0

https://huggingface.co/MiniMaxAI/MiniMax-M1-80k",,,
MiniMax-M1-40k,Language,"Language modeling/generation,Question answering,Code generation,Quantitative reasoning","MiniMax: Aili Chen, Aonian Li, Bangwei Gong, Binyang Jiang, Bo Fei, Bo Yang, Boji Shan, Changqing Yu, Chao Wang, Cheng Zhu, Chengjun Xiao, Chengyu Du, Chi Zhang, Chu Qiao, Chunhao Zhang, Chunhui Du, Congchao Guo, Da Chen, Deming Ding, Dianjun Sun, Dong Li, Enwei Jiao, Haigang Zhou, Haimo Zhang, Han Ding, Haohai Sun, Haoyu Feng, Huaiguang Cai, Haichao Zhu, Jian Sun, Jiaqi Zhuang, Jiaren Cai, Jiayuan Song, Jin Zhu, Jingyang Li, Jinhao Tian, Jinli Liu, Junhao Xu, Junjie Yan, Junteng Liu, Junxian He, Kaiyi Feng, Ke Yang, Kecheng Xiao, Le Han, Leyang Wang, Lianfei Yu, Liheng Feng, Lin Li, Lin Zheng, Linge Du, Lingyu Yang, Lunbin Zeng, Minghui Yu, Mingliang Tao, Mingyuan Chi, Mozhi Zhang, Mujie Lin, Nan Hu, Nongyu Di, Peng Gao, Pengfei Li, Pengyu Zhao, Qibing Ren, Qidi Xu, Qile Li, Qin Wang, Rong Tian, Ruitao Leng, Shaoxiang Chen, Shaoyu Chen, Shengmin Shi, Shitong Weng, Shuchang Guan, Shuqi Yu, Sichen Li, Songquan Zhu, Tengfei Li, Tianchi Cai, Tianrun Liang, Weiyu Cheng, Weize Kong, Wenkai Li, Xiancai Chen, Xiangjun Song, Xiao Luo, Xiao Su, Xiaobo Li, Xiaodong Han, Xinzhu Hou, Xuan Lu, Xun Zou, Xuyang Shen, Yan Gong, Yan Ma, Yang Wang, Yiqi Shi, Yiran Zhong, Yonghong Duan et al. (27 additional authors not shown)",Open weights (unrestricted),https://arxiv.org/abs/2506.13585,,MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention,2025-06-13,MiniMax,456000000000.0,"from the base model:
Total Parameters: 456B
Activated Parameters per Token: 45.9B

(matches safetensors)",4.1861931e+24,1.9828799999999997e+24 FLOP [base model compute] + 2.2033131e+24 FLOP [see finetune compute notes] = 4.1861931e+24 FLOP,Unspecified unreleased,,7500000000000.0,"7.5T tokens

""To enhance the reasoning and long context capabilities of the foundation model while ensuring diversity, we continue training the MiniMax-Text-01 model with additional 7.5T tokens with optimized
data quality and mixture.""",,"Continual pre-training: ??
RL: ""These dual contributions yield an efficient and scalable RL framework for training M1, where the complete training cycle requires 3 weeks on 512 H800 GPUsâ€”equivalent to a rental cost of approximately $0.53M USD.""

40k vesion training was stopped interemediately",NVIDIA H800 SXM5,Likely,"We introduce MiniMax-M1, the world's first open-weight, large-scale hybrid-attention reasoning model. MiniMax-M1 is powered by a hybrid Mixture-of-Experts (MoE) architecture combined with a lightning attention mechanism. The model is developed based on our previous MiniMax-Text-01 model, which contains a total of 456 billion parameters with 45.9 billion parameters activated per token. The M1 model natively supports a context length of 1 million tokens, 8x the context size of DeepSeek R1. Furthermore, the lightning attention mechanism in MiniMax-M1 enables efficient scaling of test-time compute. These properties make M1 particularly suitable for complex tasks that require processing long inputs and thinking extensively. MiniMax-M1 is trained using large-scale reinforcement learning (RL) on diverse problems including sandbox-based, real-world software engineering environments. In addition to M1's inherent efficiency advantage for RL training, we propose CISPO, a novel RL algorithm to further enhance RL efficiency. CISPO clips importance sampling weights rather than token updates, outperforming other competitive RL variants. Combining hybrid-attention and CISPO enables MiniMax-M1's full RL training on 512 H800 GPUs to complete in only three weeks, with a rental cost of just $534,700. We release two versions of MiniMax-M1 models with 40K and 80K thinking budgets respectively, where the 40K model represents an intermediate phase of the 80K training. Experiments on standard benchmarks show that our models are comparable or superior to strong open-weight models such as the original DeepSeek-R1 and Qwen3-235B, with particular strengths in complex software engineering, tool utilization, and long-context tasks. We publicly release MiniMax-M1 at this https URL.",China,MiniMax-Text-01,2.2033131e+24,"1. Continual pre-training for 7.5T tokens:

6 FLOP / token / active parameter * 45.9 * 10^9 activated parameters * 7.5 * 10^12 tokens = 2.0655e+24 FLOP

2. RL (3 weeks on 512 H800 GPUs for full training, 40k version was stopped in the ~middle of it):

989000000000000 FLOP / GPU / sec [bf16 assumed] * 512 GPUs * 1.5 weeks * 168 hours / week * 3600 sec / hour * 0.3 [assimed utilization] = 1.3781311e+23 FLOP

2.0655e+24 FLOP + 1.3781311e+23 FLOP = 2.2033131e+24 FLOP

*there was also SFT stage with no details provided, I assumed its compute is negligible",,,Unreleased,"Apache 2,0

https://huggingface.co/MiniMaxAI/MiniMax-M1-40k",,,
Cosmos-Predict2-2B-Text2Image ,Image generation,"Text-to-image,Image generation",,Open weights (restricted use),https://research.nvidia.com/labs/dir/cosmos-predict2/,,NVIDIA Releases New AI Models and Developer Tools to Advance Autonomous Vehicle Ecosystem,2025-06-11,NVIDIA,2000000000.0,,,"High quality (but not SOTA) text-to-image generation, plausibly over 1e23 FLOP, unlikely to be over 1e25 FLOP at this size.",Unspecified unreleased,,,,,,,Confident,"Training Physical AI systems in digital environments requires a physical world simulator. We introduce Cosmos-Predict2, the latest version of the Cosmos world model, designed for simulating and predicting the future state of the world as video.

Cosmos-Predict2 features four models: Cosmos-Predict2-2B-Text2Image and Cosmos-Predict2-14B-Text2Image for text-to-image generation for creating high-quality images from text descriptions, and Cosmos-Predict2-2B-Video2World and Cosmos-Predict2-14B-Video2World for video-to-world generation for producing visual simulations from image or video inputs. To accelerate the development of world models for Physical AI, we make our code, model weights, and the benchmark (PBench) available under the NVIDIA Open Model License.",United States of America,,,,,,Open source,"NVIDIA license (termination clause + attribution requirements) 
https://huggingface.co/nvidia/Cosmos-Predict2-2B-Text2Image

Apache 2.0 for code
https://github.com/nvidia-cosmos/cosmos-predict2/tree/main",,,
Cosmos-Predict2-14B-Text2Image,Image generation,"Text-to-image,Image generation",,Open weights (restricted use),https://research.nvidia.com/labs/dir/cosmos-predict2/,,NVIDIA Releases New AI Models and Developer Tools to Advance Autonomous Vehicle Ecosystem,2025-06-11,NVIDIA,14000000000.0,,,"High quality (but not SOTA) text-to-image generation, plausibly over 1e23 FLOP, unlikely to be over 1e25 FLOP.",Unspecified unreleased,,,,,,,Confident,"Training Physical AI systems in digital environments requires a physical world simulator. We introduce Cosmos-Predict2, the latest version of the Cosmos world model, designed for simulating and predicting the future state of the world as video.

Cosmos-Predict2 features four models: Cosmos-Predict2-2B-Text2Image and Cosmos-Predict2-14B-Text2Image for text-to-image generation for creating high-quality images from text descriptions, and Cosmos-Predict2-2B-Video2World and Cosmos-Predict2-14B-Video2World for video-to-world generation for producing visual simulations from image or video inputs. To accelerate the development of world models for Physical AI, we make our code, model weights, and the benchmark (PBench) available under the NVIDIA Open Model License.",United States of America,,,,,,Open source,"NVIDIA license (termination clause + attribution requirements) https://huggingface.co/nvidia/Cosmos-Predict2-14B-Text2Image

Apache 2.0 for code
https://github.com/nvidia-cosmos/cosmos-predict2/tree/main",,,
Cosmos-Predict2-2B-Video2World,"Video,Vision,Robotics","Robotic manipulation,System control,Video generation",,Open weights (restricted use),https://research.nvidia.com/labs/dir/cosmos-predict2/,,NVIDIA Releases New AI Models and Developer Tools to Advance Autonomous Vehicle Ecosystem,2025-06-11,NVIDIA,2000000000.0,,,"Likely under 1e25 FLOP in light of small size, maybe over 1e23 FLOP comparing to the 14B model.",Unspecified unreleased,,,,,,,Confident,"Training Physical AI systems in digital environments requires a physical world simulator. We introduce Cosmos-Predict2, the latest version of the Cosmos world model, designed for simulating and predicting the future state of the world as video.

Cosmos-Predict2 features four models: Cosmos-Predict2-2B-Text2Image and Cosmos-Predict2-14B-Text2Image for text-to-image generation for creating high-quality images from text descriptions, and Cosmos-Predict2-2B-Video2World and Cosmos-Predict2-14B-Video2World for video-to-world generation for producing visual simulations from image or video inputs. To accelerate the development of world models for Physical AI, we make our code, model weights, and the benchmark (PBench) available under the NVIDIA Open Model License.",United States of America,,,,,,Open source,"NVIDIA license (termination clause + attribution requirements) https://huggingface.co/nvidia/Cosmos-Predict2-2B-Video2World

Apache 2.0 for code
https://github.com/nvidia-cosmos/cosmos-predict2/tree/main",,,
Cosmos-Predict2-14B-Video2World,"Video,Vision,Robotics","Robotic manipulation,System control,Video generation",,Open weights (restricted use),https://research.nvidia.com/labs/dir/cosmos-predict2/,,NVIDIA Releases New AI Models and Developer Tools to Advance Autonomous Vehicle Ecosystem,2025-06-11,NVIDIA,14000000000.0,,,"Maybe over 1e25 FLOP as the original ""Cosmos-1.0-Diffusion-14B-Video2World"" was just under 1e25 FLOP, and this appears to be an improved model. But details not yet released.",Unspecified unreleased,,,,,,,Confident,"Training Physical AI systems in digital environments requires a physical world simulator. We introduce Cosmos-Predict2, the latest version of the Cosmos world model, designed for simulating and predicting the future state of the world as video.

Cosmos-Predict2 features four models: Cosmos-Predict2-2B-Text2Image and Cosmos-Predict2-14B-Text2Image for text-to-image generation for creating high-quality images from text descriptions, and Cosmos-Predict2-2B-Video2World and Cosmos-Predict2-14B-Video2World for video-to-world generation for producing visual simulations from image or video inputs. To accelerate the development of world models for Physical AI, we make our code, model weights, and the benchmark (PBench) available under the NVIDIA Open Model License.",United States of America,,,,,,Open source,"NVIDIA license (termination clause + attribution requirements) https://huggingface.co/nvidia/Cosmos-Predict2-14B-Video2World

Apache 2.0 for code
https://github.com/nvidia-cosmos/cosmos-predict2/tree/main",,,
Seed-1.6-Thinking,"Multimodal,Language,Vision","Language modeling/generation,Vision-language generation",,API access,https://seed.bytedance.com/en/blog/introduction-to-techniques-used-in-seed1-6,,"Introduction to Techniques Used in Seed1.6
",2025-06-11,ByteDance,230000000000.0,"Model has 230B parameters total, with 23B active parameters (sparse MoE setting).",,,Unspecified unreleased,,,,,,,Confident,"Seed1.6 is the latest general-purpose model series unveiled by the ByteDance Seed team. It incorporates multimodal capabilities, supporting adaptive deep thinking, multimodal understanding, GUI-based interaction, and deep reasoning with a 256K context window. Seed1.6 is now available through the open API of Volcano Engine. You can try it out via the links provided at the end of this article.


In Seed1.6, we introduced Adaptive Chain-of-Thought (AdaCoT), a novel technique for initiating thinking processes adaptively based on question difficulty. This adaptive approach enables a better trade-off between model effectiveness and reasoning performance.


Seed1.6 models demonstrated competitive performance across diverse benchmarks. They matched or even outperformed Seed1.5-VL in multiple visual tasks, while also achieving high scores in generalization tests such as college entrance exams in China and abroad.",China,,,,,,Unreleased,,,,
Seed 1.6,"Multimodal,Language,Vision,Video","Language modeling/generation,Image captioning,Video generation",,API access,https://www.aibase.com/news/18831,,"ByteDance Unveils DouBao 1.6 and Seedance 1.0 with Significantly Reduced Costs
",2025-06-11,ByteDance,230000000000.0,MoE,,,Unspecified unreleased,,,,,,,Confident,,China,,,,,,Unreleased,,,,
Seedance 1.0,Video,"Video generation,Text-to-video,Image-to-video","Yu Gao, Haoyuan Guo, Tuyen Hoang, Weilin Huang, Lu Jiang, Fangyuan Kong, Huixia Li, Jiashi Li, Liang Li, Xiaojie Li, Xunsong Li, Yifu Li, Shanchuan Lin, Zhijie Lin, Jiawei Liu, Shu Liu, Xiaonan Nie, Zhiwu Qing, Yuxi Ren, Li Sun, Zhi Tian, Rui Wang, Sen Wang, Guoqiang Wei, Guohong Wu, Jie Wu, Ruiqi Xia, Fei Xiao, Xuefeng Xiao, Jiangqiao Yan, Ceyuan Yang, Jianchao Yang, Runkai Yang, Tao Yang, Yihang Yang, Zilyu Ye, Xuejiao Zeng, Yan Zeng, Heng Zhang, Yang Zhao, Xiaozheng Zheng, Peihao Zhu, Jiaxin Zou, Feilong Zuo",API access,https://arxiv.org/abs/2506.09113,,Seedance 1.0: Exploring the Boundaries of Video Generation Models,2025-06-10,ByteDance,,,,"Unsure how much compute was used. Plausibly over 1e25 FLOP. Performs similarly to Veo-2/3, and Bytedance could afford a 1e25 FLOP training run.",Unspecified unreleased,,,,,,,Unknown,"Notable breakthroughs in diffusion modeling have propelled rapid improvements in video generation, yet current foundational model still face critical challenges in simultaneously balancing prompt following, motion plausibility, and visual quality. In this report, we introduce Seedance 1.0, a high-performance and inference-efficient video foundation generation model that integrates several core technical improvements: (i) multi-source data curation augmented with precision and meaningful video captioning, enabling comprehensive learning across diverse scenarios; (ii) an efficient architecture design with proposed training paradigm, which allows for natively supporting multi-shot generation and jointly learning of both text-to-video and image-to-video tasks. (iii) carefully-optimized post-training approaches leveraging fine-grained supervised fine-tuning, and video-specific RLHF with multi-dimensional reward mechanisms for comprehensive performance improvements; (iv) excellent model acceleration achieving ~10x inference speedup through multi-stage distillation strategies and system-level optimizations. Seedance 1.0 can generate a 5-second video at 1080p resolution only with 41.4 seconds (NVIDIA-L20). Compared to state-of-the-art video generation models, Seedance 1.0 stands out with high-quality and fast video generation having superior spatiotemporal fluidity with structural stability, precise instruction adherence in complex multi-subject contexts, native multi-shot narrative coherence with consistent subject representation.",China,,,,,,Unreleased,,,,
MiMo-7B-Base,Language,"Language modeling/generation,Question answering,Quantitative reasoning,Code generation","LLM-Core Xiaomi: Bingquan Xia, Bowen Shen, Cici, Dawei Zhu, Di Zhang, Gang Wang, Hailin Zhang, Huaqiu Liu, Jiebao Xiao, Jinhao Dong, Liang Zhao, Peidian Li, Peng Wang, Shihua Yu, Shimao Chen, Weikun Wang, Wenhan Ma, Xiangwei Deng, Yi Huang, Yifan Song, Zihan Jiang, Bowen Ye, Can Cai, Chenhong He, Dong Zhang, Duo Zhang, Guoan Wang, Hao Tian, Haochen Zhao, Heng Qu, Hongshen Xu, Jun Shi, Kainan Bao, Kai Fang, Kang Zhou, Kangyang Zhou, Lei Li, Menghang Zhu, Nuo Chen, Qiantong Wang, Shaohui Liu, Shicheng Li, Shuhao Gu, Shuhuai Ren, Shuo Liu, Sirui Deng, Weiji Zhuang, Weiwei Lv, Wenyu Yang, Xin Zhang, Xing Yong, Xing Zhang, Xingchen Song, Xinzhe Xu, Xu Wang, Yihan Yan, Yu Tu, Yuanyuan Tian, Yudong Wang, Yue Yu, Zhenru Lin, Zhichao Song, Zihao Yue",Open weights (unrestricted),https://arxiv.org/abs/2505.07608,,MiMo: Unlocking the Reasoning Potential of Language Model -- From Pretraining to Posttraining,2025-06-05,Xiaomi Corp,7000000000.0,"""We set the number of Transformer layers to 36 and the hidden
dimension to 4,096. The intermediate hidden dimension of FFN is set to 11,008. The number of attention heads is 32 and there are 8 key-value groups""",1.05e+24,6 FLOP / parameter / token * 7 * 10^9 parameters * 25 * 10^12 tokens = 1.05e+24 FLOP,Unspecified unreleased,"""diverse sources, including web pages, academic papers, books, programming code, and synthetic data.""",25000000000000.0,"""MiMo-7B-Base is pre-trained on 25 trillion tokens""",,,,Confident,"We present MiMo-7B, a large language model born for reasoning tasks, with optimization across both pre-training and post-training stages. During pre-training, we enhance the data preprocessing pipeline and employ a three-stage data mixing strategy to strengthen the base model's reasoning potential. MiMo-7B-Base is pre-trained on 25 trillion tokens, with additional Multi-Token Prediction objective for enhanced performance and accelerated inference speed. During post-training, we curate a dataset of 130K verifiable mathematics and programming problems for reinforcement learning, integrating a test-difficulty-driven code-reward scheme to alleviate sparse-reward issues and employing strategic data resampling to stabilize training. Extensive evaluations show that MiMo-7B-Base possesses exceptional reasoning potential, outperforming even much larger 32B models. The final RL-tuned model, MiMo-7B-RL, achieves superior performance on mathematics, code and general reasoning tasks, surpassing the performance of OpenAI o1-mini. The model checkpoints are available at this https URL.",China,,,,,,Unreleased,"Apache 2.0
https://github.com/xiaomimimo/MiMo

MIT license
https://huggingface.co/XiaomiMiMo/MiMo-7B-Base",,,
Gemini 2.5 Pro (Jun 2025),"Language,Vision,Video,Multimodal,Speech","Language modeling/generation,Question answering,Code generation,Quantitative reasoning,Visual question answering,Translation,Image captioning,Video description,Speech recognition (ASR)",,API access,https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/#gemini-2-5-thinking,,Gemini 2.5: Our most intelligent AI model,2025-06-05,Google DeepMind,,,,Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.,Unspecified unreleased,Knowledge cutoff	January 2025,,,,,,Unknown,"Gemini 2.5 Pro Experimental is our most advanced model for complex tasks. It tops the LMArena leaderboard â€” which measures human preferences â€” by a significant margin, indicating a highly capable model equipped with high-quality style. 2.5 Pro also shows strong reasoning and code capabilities, leading on common coding, math and science benchmarks.

Gemini 2.5 Pro is available now in Google AI Studio and in the Gemini app for Gemini Advanced users, and will be coming to Vertex AI soon. Weâ€™ll also introduce pricing in the coming weeks, enabling people to use 2.5 Pro with higher rate limits for scaled production use.","United States of America,United Kingdom of Great Britain and Northern Ireland",,,,,,Unreleased,"Availability	
Google AI Studio
Gemini API
Gemini App",,,
MiMo-VL-7B-SFT,"Vision,Multimodal,Language,Video","Character recognition (OCR),Image captioning,Language modeling/generation,Question answering,Visual question answering,Video,Video description","Xiaomi LLM-Core Team: Zihao Yue, Zhenru Lin, Yifan Song, Weikun Wang, Shuhuai Ren, Shuhao Gu, Shicheng Li, Peidian Li, Liang Zhao, Lei Li, Kainan Bao, Hao Tian, Hailin Zhang, Gang Wang, Dawei Zhu, Cici, Chenhong He, Bowen Ye, Bowen Shen, Zihan Zhang, Zihan Jiang, Zhixian Zheng, Zhichao Song, Zhenbo Luo, Yue Yu, Yudong Wang, Yuanyuan Tian, Yu Tu, Yihan Yan, Yi Huang, Xu Wang, Xinzhe Xu, Xingchen Song, Xing Zhang, Xing Yong, Xin Zhang, Xiangwei Deng, Wenyu Yang, Wenhan Ma, Weiwei Lv, Weiji Zhuang, Wei Liu, Sirui Deng, Shuo Liu, Shimao Chen, Shihua Yu, Shaohui Liu, Shande Wang, Rui Ma, Qiantong Wang, Peng Wang, Nuo Chen, Menghang Zhu, Kangyang Zhou, Kang Zhou, Kai Fang, Jun Shi, Jinhao Dong, Jiebao Xiao, Jiaming Xu, Huaqiu Liu, Hongshen Xu, Heng Qu, Haochen Zhao, Hanglong Lv, Guoan Wang, Duo Zhang, Dong Zhang, Di Zhang, Chong Ma, Chang Liu, Can Cai, Bingquan Xia",Open weights (unrestricted),https://arxiv.org/abs/2506.03569,,"MiMo-VL Technical Report
",2025-06-04,Xiaomi Corp,7000000000.0,,1.1508e+24,1.05e+24 FLOP [base model compute] + 1.008e+23 FLOP [finetune compute] = 1.1508e+24 FLOP,Unspecified unreleased,,2400000000000.0,"2.4 trillion tokens of high-quality, diverse multimodal data spanning images, videos, and text. This comprehensive dataset includes general image captions, interleaved data, Optical Character Recognition (OCR) data, grounding data, video content, GUI interactions, reasoning examples, and text-only sequences.",,,,Confident,"We open-source MiMo-VL-7B-SFT and MiMo-VL-7B-RL, two powerful vision-language models delivering state-of-the-art performance in both general visual understanding and multimodal reasoning. MiMo-VL-7B-RL outperforms Qwen2.5-VL-7B on 35 out of 40 evaluated tasks, and scores 59.4 on OlympiadBench, surpassing models with up to 78B parameters. For GUI grounding applications, it sets a new standard with 56.1 on OSWorld-G, even outperforming specialized models such as UI-TARS. Our training combines four-stage pre-training (2.4 trillion tokens) with Mixed On-policy Reinforcement Learning (MORL) integrating diverse reward signals. We identify the importance of incorporating high-quality reasoning data with long Chain-of-Thought into pre-training stages, and the benefits of mixed RL despite challenges in simultaneous multi-domain optimization. We also contribute a comprehensive evaluation suite covering 50+ tasks to promote reproducibility and advance the field. The model checkpoints and full evaluation suite are available at this https URL.
",China,MiMo-7B-Base,1.008e+23,6 FLOP / parameter / token * 7 * 10^9 parameters * 2.4 * 10^12 tokens = 1.008e+23 FLOP,,,Unreleased,"Apache 2.0
https://github.com/XiaomiMiMo/MiMo-VL

MIT license
https://huggingface.co/XiaomiMiMo/MiMo-VL-7B-SFT",,,
Eleven v3,Speech,"Text-to-speech (TTS),Speech synthesis",,API access,https://elevenlabs.io/v3,,Meet Eleven v3. The most expressive Text to Speech model.,2025-06-03,ElevenLabs,,,,,Unspecified unreleased,,,,,,,Unknown,"Eleven v3 is our latest and most advanced speech synthesis model. It is a state-of-the-art model that produces natural, life-like speech with high emotional range and contextual understanding across multiple languages.

This model works well in the following scenarios:

Character Discussions: Excellent for audio experiences with multiple characters that interact with each other.
Audiobook Production: Perfect for long-form narration with complex emotional delivery.
Emotional Dialogue: Generate natural, lifelike dialogue with high emotional range and contextual understanding.
With Eleven v3 comes a new Text to Dialogue API, which allows you to generate natural, lifelike dialogue with high emotional range and contextual understanding across multiple languages. Eleven v3 can also be used with the Text to Speech API to generate natural, lifelike speech with high emotional range and contextual understanding across multiple languages.",United States of America,,,,,,Unreleased,,,,
Kling 2.1,"Video,Vision","Image-to-video,Video generation",,API access,https://ir.kuaishou.com/news-releases/news-release-details/kling-ai-celebrates-first-anniversary-achieves-annualized,,"Greatly improved dynamics, aesthetics, and promt adherence",2025-05-29,Kuaishou Technology,,,,"Hard to bound the training compute. It performs similarly to Veo-2/3, so plausibly has similar training budget. Kuaishou, the developer, spent $1.7B on R&D in 2024, so plausibly could have afforded the compute for e.g. a 1e25 FLOP training run (https://kr-asia.com/kuaishou-turns-to-ai-and-kling-to-stay-sticky-in-a-flatlining-market).",Unspecified unreleased,,,,,,,Unknown,"Kling AI unveiled its latest 2.1 AI video model in May 2025. Compared to Kling 1.6 & Kling 2.0, it enhanced prompt adherence, frame consistency, and generation speed. Its Master tier also provides next-level cinematics with smoother motion dynamics and greater realism.",China,,,,,,Unreleased,,,,
Codestral Embed,Language,"Code generation,Code autocompletion,Retrieval-augmented generation",,API access,https://mistral.ai/news/codestral-embed,,The new state-of-the-art embedding model for code.,2025-05-28,Mistral AI,,,,"Unlikely to use >1e25 FLOP (lightweight embedding model). Meanwhile, >1e23 FLOP is likely given compute used for code LLMs generally.",Unspecified unreleased,,,,,,,Unknown,"We are excited to release Codestral Embed, our first embedding model specialized for code. It performs especially well for retrieval use cases on real-world code data. 

Codestral Embed significantly outperforms leading code embedders in the market today: Voyage Code 3, Cohere Embed v4.0 and OpenAIâ€™s large embedding model.

Codestral Embed can output embeddings with different dimensions and precisions, and the figure below illustrates the trade-offs between retrieval quality and storage costs. Codestral Embed with dimension 256 and int8 precision still performs better than any model from our competitors. The dimensions of our embeddings are ordered by relevance. For any integer target dimension n, you can choose to keep the first n dimensions for a smooth trade-off between quality and cost.",France,,,,,,Unreleased,"Codestral Embed is available on our API under the name `codestral-embed-2505` at a price of $0.15 per million tokens. It is also available on our batch API at a 50% discount. For on-prem deployments, please contact us to connect with our applied AI team. ",,,
Pangu Pro MoE,Language,"Language modeling/generation,Question answering,Quantitative reasoning,Mathematical reasoning,Code generation","Yehui Tang, Xiaosong Li, Fangcheng Liu, Wei Guo, Hang Zhou, Yaoyuan Wang, Kai Han, Xianzhi Yu, Jinpeng Li, Hui Zang, Fei Mi, Xiaojun Meng, Zhicheng Liu, Hanting Chen, Binfan Zheng, Can Chen, Youliang Yan, Ruiming Tang, Peifeng Qin, Xinghao Chen, Dacheng Tao, Yunhe Wang (and Other Contributors)",Open weights (unrestricted),https://arxiv.org/abs/2505.21411,,Pangu Pro MoE: Mixture of Grouped Experts for Efficient Sparsity,2025-05-28,Huawei,71990000000.0,a total of 71.99B parameters and 16.50 billion active parameters,1.287e+24,6 FLOP / token / parameter * 16.5 * 10^9 active parameters * 13 * 10^12 tokens = 1.287e+24 FLOP,Unspecified unreleased,,13000000000000.0,"""The pre-training dataset of Pangu Pro MoE contains a total number of 13 trillion tokens produced by our
tokenizer with a vocabulary size of 153,376 tokens.""

""the general phase (9.6T), the reasoning phase (3T), and the annealing phase (0.4T)""",,,Huawei Ascend 800T A2,Confident,"The surgence of Mixture of Experts (MoE) in Large Language Models promises a small price of execution cost for a much larger model parameter count and learning capacity, because only a small fraction of parameters are activated for each input token. However, it is commonly observed that some experts are activated far more often than others, leading to system inefficiency when running the experts on different devices in parallel. Therefore, we introduce Mixture of Grouped Experts (MoGE), which groups the experts during selection and balances the expert workload better than MoE in nature. It constrains tokens to activate an equal number of experts within each predefined expert group. When a model execution is distributed on multiple devices, this architectural design ensures a balanced computational load across devices, significantly enhancing throughput, particularly for the inference phase. Further, we build Pangu Pro MoE on Ascend NPUs, a sparse model based on MoGE with 72 billion total parameters, 16 billion of which are activated for each token. The configuration of Pangu Pro MoE is optimized for Ascend 300I Duo and 800I A2 through extensive system simulation studies. Our experiments indicate that MoGE indeed leads to better expert load balancing and more efficient execution for both model training and inference on Ascend NPUs. The inference performance of Pangu Pro MoE achieves 1148 tokens/s per card and can be further improved to 1528 tokens/s per card by speculative acceleration, outperforming comparable 32B and 72B Dense models. Furthermore, we achieve an excellent cost-to-performance ratio for model inference on Ascend 300I Duo. Our studies show that Ascend NPUs are capable of training Pangu Pro MoE with massive parallelization to make it a leading model within the sub-100B total parameter class, outperforming prominent open-source models like GLM-Z1-32B and Qwen3-32B.",China,,,,4000.0,,Unreleased,"CC-BY-4.0
https://gitcode.com/ascend-tribe/pangu-pro-moe",,,
DeepSeek-R1 (May 2025),Language,"Language modeling/generation,Code generation,Quantitative reasoning,Question answering",,Open weights (unrestricted),https://api-docs.deepseek.com/news/news250120,,DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning,2025-05-28,DeepSeek,671000000000.0,"671B total
37B activated
https://github.com/deepseek-ai/DeepSeek-R1/tree/main",4.020010000000001e+24,"Estimates by Ege Erdil in Gradient Updates:
https://epoch.ai/gradient-updates/what-went-into-training-deepseek-r1
""A dataset size of 14.8 trillion tokens is reasonable and in line with other models of this scale. Assuming thatâ€™s valid, the pretraining of this model would have required 6 * (37 billion) * (14.8 trillion) = 3e24 FLOP. If we assume DeepSeekâ€™s training cluster consists of H800s with the PCIe form factor, then each should be capable of 1.5e15 FP8 per second, and the implied model FLOP utilization (MFU) of DeepSeek v3â€™s 55 day training run ends up being around 23%.""

6 FLOP/token/param * 14.8T tokens * 37B active params = 3.29e24 FLOP (pretraining)
1.2e23 FLOP (post-training)
6.1e23 FLOP (fine-tuning)

Total compute: 3.29e24 + 1.2e23 + 6.1e23 = 4.02e24",Unspecified unreleased,"RL + SFT

When reasoning-oriented RL converges, we utilize the resulting checkpoint to collect SFT (Supervised Fine-Tuning) data for the subsequent round. Unlike the initial cold-start data, which primarily focuses on reasoning, this stage incorporates data from other domains to enhance the modelâ€™s capabilities in writing, role-playing, and other general-purpose tasks.",,,,,,Confident,"We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities.
Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors. However, it encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.",China,DeepSeek-V3,6.1e+23,6.1e23 FLOP from these estimations: https://epoch.ai/gradient-updates/what-went-into-training-deepseek-r1,,,Unreleased,"MIT licensed
https://huggingface.co/deepseek-ai/DeepSeek-R1",,,
Claude Opus 4,"Language,Multimodal,Vision","Code generation,Language modeling/generation,Quantitative reasoning,Search,Visual question answering,Translation,Image captioning,Instruction interpretation,Mathematical reasoning,Visual puzzles,Code autocompletion,Chat,Character recognition (OCR),Language modeling,Language generation,Text autocompletion,Retrieval-augmented generation,System control",,API access,"https://www.anthropic.com/claude/opus

https://www.anthropic.com/news/claude-4",,"Hybrid reasoning model that pushes the frontier for coding and AI agents, featuring a 200K context window",2025-05-22,Anthropic,,,,Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.,Unspecified unreleased,"Knowledge cutoff date is March 1, 2025, according to https://support.anthropic.com/en/articles/8114494-how-up-to-date-is-claude-s-training-data.",,,,,,Unknown,"Claude Opus 4 is our most powerful model yet and the best coding model in the world, leading on SWE-bench (72.5%) and Terminal-bench (43.2%). It delivers sustained performance on long-running tasks that require focused effort and thousands of steps, with the ability to work continuously for several hoursâ€”dramatically outperforming all Sonnet models and significantly expanding what AI agents can accomplish.",United States of America,,,,,,Unreleased,,,,
Claude Sonnet 4,"Language,Multimodal,Vision","Code generation,Language modeling/generation,Quantitative reasoning,Search,Visual question answering,Translation,Image captioning,Instruction interpretation,Mathematical reasoning,Visual puzzles,Code autocompletion,Chat,Character recognition (OCR),Language modeling,Language generation,Text autocompletion,Retrieval-augmented generation,System control",,API access,"https://www.anthropic.com/claude/sonnet

https://www.anthropic.com/news/claude-4",,"Hybrid reasoning model with superior intelligence for high-volume use cases, and 200K context window",2025-05-22,Anthropic,,,,Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.,Unspecified unreleased,"Knowledge cutoff date is March 1, 2025, according to https://support.anthropic.com/en/articles/8114494-how-up-to-date-is-claude-s-training-data.",,,,,,Unknown,"Claude Sonnet 4 can understand nuanced instructions and context, recognize and correct its own mistakes, and create sophisticated analysis and insights from complex data. Combined with superior coding, vision, and writing skills, you can use Claude Sonnet 4 for a variety of use cases.

Claude Sonnet 4 can produce near-instant responses or extended, step-by-step thinking that is made visible to the user. API users also have fine-grained control over how long the model thinks for.",United States of America,,,,,,Unreleased,,,,
Veo 3,"Video,Vision","Video generation,Image-to-video,Text-to-video","Abhishek Sharma, Alina Kuznetsova, Ali Razavi, Aleksander Holynski, Alina Kuznetsova, Ankush Gupta, Austin Waters, Ben Poole, Daniel Tanis, Derek Gasaway, Dumitru Erhan, Enric Corona, Frank Belletti, Gabe Barth-Maron, Hakan Erdogan, Henna Nandwani, Hernan Moraldo, Ilya Figotin, Igor Saprykin, Jason Baldridge, Jeff Donahue, Jimmy Shi, Kurtis David, Mai Gimenez, Medhini Narasimhan, Miaosen Wang, Mingda Zhang, Mohammad Babaeizadeh, Mukul Bhutani, Nikhil Khadke, Nilpa Jha, Pieter-Jan Kindermans, Poorva Rane, Rachel Hornung, Ricky Wong, Ruben Villegas, Ruiqi Gao, Ryan Poplin, Salah Zaiem, Sayna Ebrahimi, Scott Wisdom, Shlomi Fruchter, Sophia Sanchez, Vikas Verma, Viral Carpenter, Xinchen Yan, Xinyu Wang, Yiwen Luo, Zhichao Yin, Zu Kim",API access,https://deepmind.google/models/veo/,,Our state-of-the-art video generation model,2025-05-21,Google DeepMind,,,,,Unspecified unreleased,,,,,,,Unknown,"Re-designed for greater realism
Greater realism and fidelity, including 4k output and Veo 3â€™s real world physics and audio

Follows prompts like never before
Improved prompt adherence, meaning more accurate responses to your instructions.

Improved creative control
New capabilities to achieve new levels of control, consistency, and creativity.","United States of America,United Kingdom of Great Britain and Northern Ireland",,,,,,Unreleased,https://cloud.google.com/vertex-ai/generative-ai/docs/models/veo/3-0-generate-preview,,,
Falcon-H1,Language,"Language modeling/generation,Question answering,Code generation","Jingwei Zuo, Maksim Velikanov, Ilyas Chahed, Younes Belkada, Dhia Eddine Rhayem, Guillaume Kunsch, Hakim Hacid, Hamza Yous, Brahim Farhat, Ibrahim Khadraoui, Mugariya Farooq, Giulia Campesan, Ruxandra Cojocaru, Yasser Djilali, Shi Hu, Iheb Chaabane, Puneesh Khanna, Mohamed El Amine Seddik, Ngoc Dung Huynh, Phuc Le Khac, Leen AlQadi, Billel Mokeddem, Mohamed Chami, Abdalgader Abubaker, Mikhail Lubinets, Kacper Piskorski, Slim Frikha",Open weights (unrestricted),https://arxiv.org/abs/2507.22448,,Falcon-H1: A Family of Hybrid-Head Language Models Redefining Efficiency and Performance,2025-05-21,Technology Innovation Institute,34000000000.0,"""In this release, we feature six open-weight models: 0.5B, 1.5B, 1.5B-Deep, 3B, 7B, and 34B, along with their instruct versions"" (https://huggingface.co/blog/tiiuae/falcon-h1).",3.672e+24,6 FLOP / parameter / token * 34 * 10^9 parameters * 18 * 10^12 tokens = 3.672e+24 FLOP,"FineWeb,Common Crawl,GitHub,Proofpile 2,WebMath,Unspecified unreleased,Nemotron CC",,18000000000000.0,18T,,"""The Falcon-H1 model series was trained on a large-scale infrastructure comprising 4,096 NVIDIA H100 GPUs.""",NVIDIA H100 SXM5 80GB,Confident,"In this report, we introduce Falcon-H1, a new series of large language models (LLMs) featuring hybrid architecture designs optimized for both high performance and efficiency across diverse use cases. Unlike earlier Falcon models built solely on Transformer or Mamba architectures, Falcon-H1 adopts a parallel hybrid approach that combines Transformer-based attention with State Space Models (SSMs), known for superior long-context memory and computational efficiency. We systematically revisited model design, data strategy, and training dynamics, challenging conventional practices in the field. Falcon-H1 is released in multiple configurations, including base and instruction-tuned variants at 0.5B, 1.5B, 1.5B-deep, 3B, 7B, and 34B parameters. Quantized instruction-tuned models are also available, totaling over 30 checkpoints on Hugging Face Hub. Falcon-H1 models demonstrate state-of-the-art performance and exceptional parameter and training efficiency. The flagship Falcon-H1-34B matches or outperforms models up to 70B scale, such as Qwen3-32B, Qwen2.5-72B, and Llama3.3-70B, while using fewer parameters and less data. Smaller models show similar trends: the Falcon-H1-1.5B-Deep rivals current leading 7B-10B models, and Falcon-H1-0.5B performs comparably to typical 7B models from 2024. These models excel across reasoning, mathematics, multilingual tasks, instruction following, and scientific knowledge. With support for up to 256K context tokens and 18 languages, Falcon-H1 is suitable for a wide range of applications. All models are released under a permissive open-source license, underscoring our commitment to accessible and impactful AI research.",United Arab Emirates,,,,4096.0,,Unreleased,"FalconLLM license
https://huggingface.co/tiiuae/Falcon-H1-34B-Base
",,,
Gemma 3n,"Language,Multimodal,Speech,Vision","Language modeling/generation,Question answering,Chat,Speech recognition (ASR),Translation,Speech-to-text,Visual question answering,Mathematical reasoning,Code generation,Character recognition (OCR)","Lucas Gonzalez, Rakesh Shivanna",Open weights (restricted use),https://developers.googleblog.com/en/introducing-gemma-3n/,,"Announcing Gemma 3n preview: powerful, efficient, mobile-first AI",2025-05-20,Google,7850000000.0,7.85B (Safetensors),5.181e+23,6 FLOP / parameter / token * 7.85 * 10^9 parameters * 11 * 10^12 tokens = 5.181e+23 FLOP,Unspecified unreleased,"These models were trained on a dataset that includes a wide variety of sources totalling approximately 11 trillion tokens. The knowledge cutoff date for the training data was June 2024. Here are the key components:

Web Documents: A diverse collection of web text ensures the model is exposed to a broad range of linguistic styles, topics, and vocabulary. The training dataset includes content in over 140 languages.
Code: Exposing the model to code helps it to learn the syntax and patterns of programming languages, which improves its ability to generate code and understand code-related questions.
Mathematics: Training on mathematical text helps the model learn logical reasoning, symbolic representation, and to address mathematical queries.
Images: A wide range of images enables the model to perform image analysis and visual data extraction tasks.
Audio: A diverse set of sound samples enables the model to recognize speech, transcribe text from recordings, and identify information in audio data.",11000000000000.0,11T,,,,Confident,"Following the exciting launches of Gemma 3 and Gemma 3 QAT, our family of state-of-the-art open models capable of running on a single cloud or desktop accelerator, we're pushing our vision for accessible AI even further. Gemma 3 delivered powerful capabilities for developers, and we're now extending that vision to highly capable, real-time AI operating directly on the devices you use every day â€“ your phones, tablets, and laptops.

To power the next generation of on-device AI and support a diverse range of applications, including advancing the capabilities of Gemini Nano, we engineered a new, cutting-edge architecture. This next-generation foundation was created in close collaboration with mobile hardware leaders like Qualcomm Technologies, MediaTek, and Samsung's System LSI business, and is optimized for lightning-fast, multimodal AI, enabling truly personal and private experiences directly on your device.

Gemma 3n is our first open model built on this groundbreaking, shared architecture, allowing developers to begin experimenting with this technology today in an early preview. The same advanced architecture also powers the next generation of Gemini Nano, which brings these capabilities to a broad range of features in Google apps and our on-device ecosystem, and will become available later this year. Gemma 3n enables you to start building on this foundation that will come to major platforms such as Android and Chrome.",United States of America,,,,,,Unreleased,"Gemma license
https://huggingface.co/google/gemma-3n-E4B-it",,,
Imagen 4,Image generation,"Image generation,Text-to-image","Gabriel Barcik, Jakob Bauer, Dana Berman, Nicole Brichtova, Lluis Castrejon, Matan Cohen, Sander Dieleman, Yuqing Du, Praneet Dutta, Jess Gallegos, Yilin Gao, Evgeny Gladchenko, Susan Hao, Ruba Haroun, Ed Hirst, Tobenna Peter Igwe, Xuhui Jia, Siavash Khodadadeh, Pavel Khrushkov, Karol Langner, Rory Lawton, Yinxiao Li, Yandong Li, Shixin Luo, Michael Mathieu, SoÅˆa MokrÃ¡, AÃ¤ron van den Oord, Lily Pagan, Zarana Parekh, Noam Petrank, Jordi Pont-Tuset, Hang Qi, Deepak Ramachandran, Poorva Rane, Ali Razavi, Robert Riachi, Dirk Robinson, James Thornton, Felix Riedel, Evgeny Sluzhaev, Hansa Srinivasan, Srivatsan Srinivasan, Benigno Uria, Cristina Vasconcelos, Oliver Wang, Simon Wang, Austin Waters, Daniel Winter, Chris Wolff, Xin Yuan, Zhisheng Xiao, Keyang Xu, Andrew Xue, Katie Zhang, Yang Zhao",API access,https://storage.googleapis.com/deepmind-media/Model-Cards/Imagen-4-Model-Card.pdf,,Imagen 4 Model Card,2025-05-20,Google,,,,,Unspecified unreleased,,,,,,,Unknown,"Description: Imagen 4 is a latent diffusion model that generates high quality images from text prompts. Imagen 4 performs well in photorealistic composition seings and has improved spelling and typography, instruction following and richer colors, textures and details compared to previous Imagen models.
Inputs: The inputs consist of natural-language text strings (e.g. instructions for creating a synthetic image using a visual description) or image files.
Outputs: Outputs are generated high quality images in response to text and image inputs. 
Architecture: Imagen 4 utilises latent diffusion, which is the de facto standard approach for modern image and video models, achieving high quality performance in generative media applications.
",United States of America,,,,,,Unreleased,,,,
Gemini 2.5 Flash (May 2025),"Language,Multimodal,Vision,Speech,Video","Language modeling/generation,Question answering,Code generation,Quantitative reasoning,Visual question answering,Translation,Image captioning,Speech recognition (ASR),Video description,Search,Text summarization,Chat",,API access,https://deepmind.google/models/gemini/flash/,,Our powerful and most efficient workhorse model designed for speed and low-cost.,2025-05-20,Google DeepMind,,,,,Unspecified unreleased,Knowledge cutoff January 2025,,,,,,Unknown,"Speed and value at scale
Ideal for tasks like summarization, chat applications, data extraction, and captioning.
Thinking budget: Control how much 2.5 Flash reasons to balance latency and cost.
Natively multimodal: Understands input across text, audio, images and video.
Long context: Explore vast datasets with a 1-million token context window.
Adaptive and budgeted thinking
Adaptive controls and adjustable thinking budgets allow you to balance performance and cost.
Calibrated: The model explores diverse thinking strategies, leading to more accurate and relevant outputs.
Controllable: Developers have fine-grained control over the model's thinking process, allowing them to manage resource usage.
Adaptive: When no thinking budget is set, the model assesses the complexity of a task and calibrates the amount of thinking accordingly.","United States of America,United Kingdom of Great Britain and Northern Ireland",,,,,,Unreleased,"Availability	
Google AI Studio
Gemini API
Gemini App",,,
Marin 8B,Language,"Language modeling/generation,Question answering,Code generation,Quantitative reasoning",,Open weights (unrestricted),https://marin.readthedocs.io/en/latest/reports/marin-8b-retro/,,Marin 8B Retrospective,2025-05-19,Marin,8000000000.0,"8B

Architecture Details
Architecture: Llama 3 8B
Hidden size: 4096
Feedforward size: 14336
Number of layers: 32
Number of attention heads: 32
Number of KV heads: 8",6.12e+23,6 FLOP / parameter / token * 8 *10^9 parameters * 12.75 * 10^12 tokens = 6.12e+23 FLOP,"DCLM-baseline,StarCoder,Proofpile 2,Nemotron CC","""Retrospectively, we can partition the 8B run into several distinct phasesâ€”each nicknamed after an animal:

Kestrel (DCLM WSD-S Phase): In the first phase, we used the ""DCLM mix"" and WSD-S for about 2.7T tokens. We used 2x TPU v5e-256 coordinated with multislice for this. (0â†’2.7T tokens)
Ocelot (DCLM WSD Phase): We were given access to a v4-2048 slice and moved to that. To better utilize the hardware, we increased our batch size 50%. We also switched from WSD-S to WSD. We kept the learning rate high through 3.78T tokens.
Jellyfish (First Cooldown): It was time to cooldown as we were starting to run low on DCLM. Following recent work on midtraining (e.g. Olmo 2), we decided to fold in higher quality data during cooldown. (3.78Tâ†’4.78T tokens)
Phoenix (Reheated): We had more time for training, so we rapidly rewarmed the model and transitioned our mixture to Nemotron-CC (plus StarCoder Data). (4.78Tâ†’11.1T tokens)
Starling (Second Cooldown): Now we were running low on time, so we started another cooldown. We followed a similar process to the first cooldown, but added a few new datasets that we had created and also some that had dropped since our previous attempt. (11.1Tâ†’12.75T tokens)""",12750000000000.0,12.75T tokens,,,"Google TPU v5e,Google TPU v4",Confident,"The ""Tootsie Roll"" Process
A core premise of the Marin 8B run was that we didn't fully know the best recipeâ€” so we just started training with what we had, and planned to adapt along the way. Internally, we referred to this as the ""Tootsie"" process, a reference to Tootsie Rolls, which use a ""graining"" process where each day's batch contains a bit of the previous day's, seeding crystallization or something. (We are not food scientists.) This is admittedly a bit of a strained metaphor, but the idea was that we'd keep folding in new data, training techniques, and whatever else as the training process went on. (As it would turn out, dear reader, we would often change more than the data...)

Model Basics
Model Size
We decided to build a roughly 7-8 billion parameter model mostly out of pragmatism: we initially only had reserved capacity to train a model of that size for long enough.

Architecture
We settled on the Llama architecture for the usual reasons: it has been shown to work well, easier to plug into existing inference stacks, no one ever got fired for buying IBM, etc.

We used the same settings as Llama 3.1 8B.",United States of America,,,,,,Open source,"Apache 2.0

https://huggingface.co/marin-community/marin-8b-base",,,
Minimax-Speech-02-HD,Speech,"Text-to-speech (TTS),Speech synthesis","Bowen Zhang, Congchao Guo, Geng Yang, Hang Yu, Haozhe Zhang, Heidi Lei, Jialong Mai, Junjie Yan, Kaiyue Yang, Mingqi Yang, Peikai Huang, Ruiyang Jin, Sitan Jiang, Weihua Cheng, Yawei Li, Yichen Xiao, Yiying Zhou, Yongmao Zhang, Yuan Lu, Yucen He",API access,https://arxiv.org/abs/2505.07916,,MiniMax-Speech: Intrinsic Zero-Shot Text-to-Speech with a Learnable Speaker Encoder,2025-05-12,MiniMax,,,,,Unspecified unreleased,"""Minimax-Speech is trained on a multilingual speech dataset spanning 32 languages.""",,,,,,Unknown,"We introduce MiniMax-Speech, an autoregressive Transformer-based Text-to-Speech (TTS) model that generates high-quality speech. A key innovation is our learnable speaker encoder, which extracts timbre features from a reference audio without requiring its transcription. This enables MiniMax-Speech to produce highly expressive speech with timbre consistent with the reference in a zero-shot manner, while also supporting one-shot voice cloning with exceptionally high similarity to the reference voice. In addition, the overall quality of the synthesized audio is enhanced through the proposed Flow-VAE. Our model supports 32 languages and demonstrates excellent performance across multiple objective and subjective evaluations metrics. Notably, it achieves state-of-the-art (SOTA) results on objective voice cloning metrics (Word Error Rate and Speaker Similarity) and has secured the top position on the public TTS Arena leaderboard. Another key strength of MiniMax-Speech, granted by the robust and disentangled representations from the speaker encoder, is its extensibility without modifying the base model, enabling various applications such as: arbitrary voice emotion control via LoRA; text to voice (T2V) by synthesizing timbre features directly from text description; and professional voice cloning (PVC) by fine-tuning timbre features with additional data. We encourage readers to visit this https URL for more examples.",China,,,,,,Unreleased,,,,
Seed1.5-VL,"Vision,Language,Multimodal,Video","Visual question answering,Video description,Language modeling/generation,Question answering,Character recognition (OCR)","Dong Guo, Faming Wu, Feida Zhu, Fuxing Leng, Guang Shi, Haobin Chen, Haoqi Fan, Jian Wang, Jianyu Jiang, Jiawei Wang, Jingji Chen, Jingjia Huang, Kang Lei, Liping Yuan, Lishu Luo, Pengfei Liu, Qinghao Ye, Rui Qian, Shen Yan, Shixiong Zhao, Shuai Peng, Shuangye Li, Sihang Yuan, Sijin Wu, Tianheng Cheng, Weiwei Liu, Wenqian Wang, Xianhan Zeng, Xiao Liu, Xiaobo Qin, Xiaohan Ding, Xiaojun Xiao, Xiaoying Zhang, Xuanwei Zhang, Xuehan Xiong, Yanghua Peng, Yangrui Chen, Yanwei Li, Yanxu Hu, Yi Lin, Yiyuan Hu, Yiyuan Zhang, Youbin Wu, Yu Li, Yudong Liu, Yue Ling, Yujia Qin, Zanbo Wang, Zhiwu He, Aoxue Zhang, Bairen Yi, Bencheng Liao, Can Huang, Can Zhang, Chaorui Deng, Chaoyi Deng, Cheng Lin, Cheng Yuan, Chenggang Li, Chenhui Gou, Chenwei Lou, Chengzhi Wei, Chundian Liu, Chunyuan Li, Deyao Zhu, Donghong Zhong, Feng Li, Feng Zhang, Gang Wu, Guodong Li, Guohong Xiao, Haibin Lin, Haihua Yang, Haoming Wang, Heng Ji, Hongxiang Hao, Hui Shen, Huixia Li, Jiahao Li, Jialong Wu, Jianhua Zhu, Jianpeng Jiao, Jiashi Feng, Jiaze Chen, Jianhui Duan, Jihao Liu, Jin Zeng, Jingqun Tang, Jingyu Sun, Joya Chen, Jun Long, Junda Feng, Junfeng Zhan, Junjie Fang, Junting Lu, Kai Hua, Kai Liu, Kai Shen, Kaiyuan Zhang, Ke Shen et al. (97 additional authors not shown)",API access,https://arxiv.org/abs/2505.07062,,Seed1.5-VL Technical Report,2025-05-11,ByteDance,,"""Seed1.5-VL is composed with a 532M-parameter vision encoder and a Mixture-of-Experts (MoE) LLM of 20B active parameters.""",1.388556e+24,"989000000000000 FLOP / GPU / sec [H800, bf16 assumed] * 1300000 GPU-hours [see training time notes] * 3600 sec / hour * 0.3 [assumed utilization] = 1.388556e+24 FLOP",Unspecified unreleased,,3000000000000.0,"""more than 3T multimodal data tokens""",,""" The pretraining phase consumes 1.3
million GPU hours in total""

"" all computational costs mentioned in this report are normalized to GPU hours based on the H800""",NVIDIA H800 SXM5,Confident,"We present Seed1.5-VL, a vision-language foundation model designed to advance general-purpose multimodal understanding and reasoning. Seed1.5-VL is composed with a 532M-parameter vision encoder and a Mixture-of-Experts (MoE) LLM of 20B active parameters. Despite its relatively compact architecture, it delivers strong performance across a wide spectrum of public VLM benchmarks and internal evaluation suites, achieving the state-of-the-art performance on 38 out of 60 public benchmarks. Moreover, in agent-centric tasks such as GUI control and gameplay, Seed1.5-VL outperforms leading multimodal systems, including OpenAI CUA and Claude 3.7. Beyond visual and video understanding, it also demonstrates strong reasoning abilities, making it particularly effective for multimodal reasoning challenges such as visual puzzles. We believe these capabilities will empower broader applications across diverse tasks. In this report, we mainly provide a comprehensive review of our experiences in building Seed1.5-VL across model design, data construction, and training at various stages, hoping that this report can inspire further research. Seed1.5-VL is now accessible at this https URL (Volcano Engine Model ID: doubao-1-5-thinking-vision-pro-250428)",China,,,,,,Unreleased,"API (log in and select Doubao-1.5-thinking-vision-pro to experience): https://www.volcengine.com/experience/ark?model=doubao-1-5-thinking-vision-pro-250428


GitHub sample code (Apache 2.0): https://github.com/ByteDance-Seed/Seed1.5-VL",,,
Mistral Medium 3,"Multimodal,Language,Vision","Language modeling/generation,Visual question answering,Question answering,Quantitative reasoning,Code generation,Translation",,API access,https://mistral.ai/news/mistral-medium-3,,Mistral Medium 3 delivers state-of-the-art performance at 8X lower cost with radically simplified enterprise deployments.,2025-05-07,Mistral AI,,,,"Benchmarks match with models like GPT-4o, Mistral's previous largest runs were ~1e25 FLOP scale, so plausibly they might have trained Medium with this much compute.",Unspecified unreleased,,,,,,,Unknown,"Mistral Medium 3 introduces a new class of models that balances

SOTA performance
8X lower cost
simpler deployability to accelerate enterprise usage

The model leads in professional use cases such as coding and multimodal understanding

The model delivers a range of enterprise capabilities including:
Hybrid or on-premises / in-VPC deployment
Custom post-training 
Integration into enterprise tools and systems",France,,,,,,Unreleased,,,,
Pangu Ultra MoE,Language,"Language modeling/generation,Question answering","Yehui Tang, Yichun Yin, Yaoyuan Wang, Hang Zhou, Yu Pan, Wei Guo, Ziyang Zhang, Miao Rang, Fangcheng Liu, Naifu Zhang, Binghan Li, Yonghan Dong, Xiaojun Meng, Yasheng Wang, Dong Li, Yin Li, Dandan Tu, Can Chen, Youliang Yan, Fisher Yu, Ruiming Tang, Yunhe Wang, Botian Huang, Bo Wang, Boxiao Liu, Changzheng Zhang, Da Kuang, Fei Liu, Gang Huang, Jiansheng Wei, Jiarui Qin, Jie Ran, Jinpeng Li, Jun Zhao, Liang Dai, Lin Li, Liqun Deng, Peifeng Qin, Pengyuan Zeng, Qiang Gu, Shaohua Tang, Shengjun Cheng, Tao Gao, Tao Yu, Tianshu Li, Tianyu Bi, Wei He, Weikai Mao, Wenyong Huang, Wulong Liu, Xiabing Li, Xianzhi Yu, Xueyu Wu, Xu He, Yangkai Du, Yan Xu, Ye Tian, Yimeng Wu, Yongbing Huang, Yong Tian, Yong Zhu, Yue Li, Yufei Wang, Yuhang Gai, Yujun Li, Yu Luo, Yunsheng Ni, Yusen Sun, Zelin Chen, Zhe Liu, Zhicheng Liu, Zhipeng Tu, Zilin Ding, Zongyuan Zhan",Hosted access (no API),https://arxiv.org/abs/2505.04519,,Pangu Ultra MoE: How to Train Your Big MoE on Ascend NPUs,2025-05-07,Huawei,718000000000.0,"a sparse LLM with 718 billion parameters

39B activated parameters",3.0888e+24,"Speculatively assuming they trained MoE model on the same amount of tokens as Pangu Ultra (13,2 T): 

6 FLOP / parameter / token * 39 * 10^9 activated parameters * 13,2 * 10^12 tokens = 3.0888e+24 FLOP",Unspecified unreleased,,,,,,Huawei Ascend 910B,Speculative,"Sparse large language models (LLMs) with Mixture of Experts (MoE) and close to a trillion parameters are dominating the realm of most capable language models. However, the massive model scale poses significant challenges for the underlying software and hardware systems. In this paper, we aim to uncover a recipe to harness such scale on Ascend NPUs. The key goals are better usage of the computing resources under the dynamic sparse model structures and materializing the expected performance gain on the actual hardware. To select model configurations suitable for Ascend NPUs without repeatedly running the expensive experiments, we leverage simulation to compare the trade-off of various model hyperparameters. This study led to Pangu Ultra MoE, a sparse LLM with 718 billion parameters, and we conducted experiments on the model to verify the simulation results. On the system side, we dig into Expert Parallelism to optimize the communication between NPU devices to reduce the synchronization overhead. We also optimize the memory efficiency within the devices to further reduce the parameter and activation management overhead. In the end, we achieve an MFU of 30.0% when training Pangu Ultra MoE, with performance comparable to that of DeepSeek R1, on 6K Ascend NPUs, and demonstrate that the Ascend system is capable of harnessing all the training stages of the state-of-the-art language models. Extensive experiments indicate that our recipe can lead to efficient training of large-scale sparse language models with MoE. We also study the behaviors of such models for future reference.",China,,,,6000.0,0.3,Unreleased,,,,
Gemini 2.5 Pro (May 2025),"Language,Vision,Video,Multimodal,Speech","Language modeling/generation,Question answering,Code generation,Quantitative reasoning,Visual question answering,Translation,Image captioning,Video description,Speech recognition (ASR)",,API access,https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/#gemini-2-5-thinking,,Gemini 2.5: Our most intelligent AI model,2025-05-06,Google DeepMind,,,,Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.,Unspecified unreleased,Knowledge cutoff	January 2025,,,,,,Unknown,"Gemini 2.5 Pro Experimental is our most advanced model for complex tasks. It tops the LMArena leaderboard â€” which measures human preferences â€” by a significant margin, indicating a highly capable model equipped with high-quality style. 2.5 Pro also shows strong reasoning and code capabilities, leading on common coding, math and science benchmarks.

Gemini 2.5 Pro is available now in Google AI Studio and in the Gemini app for Gemini Advanced users, and will be coming to Vertex AI soon. Weâ€™ll also introduce pricing in the coming weeks, enabling people to use 2.5 Pro with higher rate limits for scaled production use.","United States of America,United Kingdom of Great Britain and Northern Ireland",,,,,,Unreleased,"Availability	
Google AI Studio
Gemini API
Gemini App",,,
MiniMax-Speech-02-turbo,Speech,"Text-to-speech (TTS),Speech synthesis",,API access,https://www.minimaxi.com/news/minimax-speech-02,,"Text-to-Audio (T2A) that offers voice synthesis, emotional expression, and multilingual capabilities. Designed for real-time applications with low latency",2025-05-02,MiniMax,,,,"Small speech models are unlikely to be above 1e25 FLOP, but might be over 1e23 FLOP.",Unspecified unreleased,,,,,,,Unknown,"Voice Cloning
10-second voice cloning with 99% reported vocal similarity
300+ pre-built voices across different demographics
Controls for pitch, speed, and volume
Emotion Control
Auto-detect mode that matches emotional tone to text context
Manual customization options for emotional expression
Language Support
30+ languages with native accents
English variants: US, UK, Australian, Indian
Asian languages: Mandarin, Cantonese, Japanese, Korean, Vietnamese, Indonesian
European languages: French, German, Spanish, Portuguese (Brazilian), Turkish, Russian, Ukrainian
Recently added: Thai, Polish, Romanian, Greek, Czech, Finnish, Hindi",China,,,,,,Unreleased,,,,
Amazon Nova Premier,"Multimodal,Language,Video,Vision","Language modeling/generation,Visual question answering,Video description,Question answering,Code generation,Character recognition (OCR),System control,Instruction interpretation,Retrieval-augmented generation",,API access,https://www.amazon.science/publications/amazon-nova-premier-technical-report-and-model-card,,Amazon Nova Premier: Our most capable model for complex tasks and teacher for model distillation,2025-04-30,Amazon,,,,"Benchmark performance better than Nova Pro, which was already close to 1e25 FLOP. Similar to Claude Sonnet 3.5. Hence could be >1e25 FLOP, but it's uncertain. Serving speed 80 tok/s, so not far off Llama-3 70B, and hence not obvious they would have trained with a >1e25 FLOP budget.",Unspecified unreleased,,,,,,,Unknown,"We present Amazon Nova Premier, our most capable multimodal foundation model and teacher for model distillation. Nova Premier processes text, images, and videos with a one-million token context window enabling analysis of large codebases, long documents, and long videos in a single prompt. It also enables customers to use Amazon Bedrock to create customized variants of Amazon Nova Pro, Nova Lite, and Nova Micro that maintain high accuracy while offering improved speed and cost efficiency. Like all Nova models, Nova Premier is built with integrated safety measures and responsible AI practices, maintaining our commitment to customer trust, security, and reliability.
With Nova Premier, we further extend the capabilities and price-performance advantages of the Amazon Nova model family",United States of America,,,,,,Unreleased,,,,
Qwen3-235B-A22B,Language,"Language modeling/generation,Question answering,Mathematical reasoning,Quantitative reasoning,Code generation,Translation",,Open weights (unrestricted),https://qwenlm.github.io/blog/qwen3/,,"Qwen3: Think Deeper, Act Faster",2025-04-29,Alibaba,235000000000.0,"235 billion total parameters and 22 billion activated parameters

Number of Layers: 94
Number of Attention Heads (GQA): 64 for Q and 4 for KV
Number of Experts: 128
Number of Activated Experts: 8
Context Length: 32,768 natively and 131,072 tokens with YaRN.",4.752e+24,6 FLOP / parameter / token * 22*10^9 active parameters * 36000000000000 tokens = 4.752e+24 FLOP,Unspecified unreleased,"""While Qwen2.5 was pre-trained on 18 trillion tokens, Qwen3 uses nearly twice that amount, with approximately 36 trillion tokens covering 119 languages and dialects. To build this large dataset, we collected data not only from the web but also from PDF-like documents. We used Qwen2.5-VL to extract text from these documents and Qwen2.5 to improve the quality of the extracted content. To increase the amount of math and code data, we used Qwen2.5-Math and Qwen2.5-Coder to generate synthetic data. This includes textbooks, question-answer pairs, and code snippets.

The pre-training process consists of three stages. In the first stage (S1), the model was pretrained on over 30 trillion tokens with a context length of 4K tokens. This stage provided the model with basic language skills and general knowledge. In the second stage (S2), we improved the dataset by increasing the proportion of knowledge-intensive data, such as STEM, coding, and reasoning tasks. The model was then pretrained on an additional 5 trillion tokens. In the final stage, we used high-quality long-context data to extend the context length to 32K tokens. """,36000000000000.0,36T,,,,Likely,"Today, we are excited to announce the release of Qwen3, the latest addition to the Qwen family of large language models. Our flagship model, Qwen3-235B-A22B, achieves competitive results in benchmark evaluations of coding, math, general capabilities, etc., when compared to other top-tier models such as DeepSeek-R1, o1, o3-mini, Grok-3, and Gemini-2.5-Pro. Additionally, the small MoE model, Qwen3-30B-A3B, outcompetes QwQ-32B with 10 times of activated parameters, and even a tiny model like Qwen3-4B can rival the performance of Qwen2.5-72B-Instruct.",China,,,,,,Unreleased,"Apache 2.0

https://huggingface.co/Qwen/Qwen3-235B-A22B",,,
Qwen3-30B-A3B,Language,"Language modeling/generation,Question answering,Mathematical reasoning,Quantitative reasoning,Code generation,Translation",,Open weights (unrestricted),https://qwenlm.github.io/blog/qwen3/,,"Qwen3: Think Deeper, Act Faster",2025-04-29,Alibaba,30000000000.0,"30 billion total parameters and 3 billion activated parameters

Number of Layers: 48
Number of Attention Heads (GQA): 32 for Q and 4 for KV
Number of Experts: 128
Number of Activated Experts: 8
Context Length: 32,768",6.48e+23,6 FLOP / parameter / token * 3*10^9 active parameters * 36000000000000 tokens = 6.48e+23 FLOP,Unspecified unreleased,"""While Qwen2.5 was pre-trained on 18 trillion tokens, Qwen3 uses nearly twice that amount, with approximately 36 trillion tokens covering 119 languages and dialects. To build this large dataset, we collected data not only from the web but also from PDF-like documents. We used Qwen2.5-VL to extract text from these documents and Qwen2.5 to improve the quality of the extracted content. To increase the amount of math and code data, we used Qwen2.5-Math and Qwen2.5-Coder to generate synthetic data. This includes textbooks, question-answer pairs, and code snippets.

The pre-training process consists of three stages. In the first stage (S1), the model was pretrained on over 30 trillion tokens with a context length of 4K tokens. This stage provided the model with basic language skills and general knowledge. In the second stage (S2), we improved the dataset by increasing the proportion of knowledge-intensive data, such as STEM, coding, and reasoning tasks. The model was then pretrained on an additional 5 trillion tokens. In the final stage, we used high-quality long-context data to extend the context length to 32K tokens. """,36000000000000.0,36T,,,,Likely,"Today, we are excited to announce the release of Qwen3, the latest addition to the Qwen family of large language models. Our flagship model, Qwen3-235B-A22B, achieves competitive results in benchmark evaluations of coding, math, general capabilities, etc., when compared to other top-tier models such as DeepSeek-R1, o1, o3-mini, Grok-3, and Gemini-2.5-Pro. Additionally, the small MoE model, Qwen3-30B-A3B, outcompetes QwQ-32B with 10 times of activated parameters, and even a tiny model like Qwen3-4B can rival the performance of Qwen2.5-72B-Instruct.",China,,,,,,Unreleased,"Apache 2.0

https://huggingface.co/Qwen/Qwen3-30B-A3B",,,
Qwen3-32B,Language,"Language modeling/generation,Question answering,Mathematical reasoning,Quantitative reasoning,Code generation,Translation",,Open weights (unrestricted),https://qwenlm.github.io/blog/qwen3/,,"Qwen3: Think Deeper, Act Faster",2025-04-29,Alibaba,32800000000.0,"Number of Parameters: 32.8B
Number of Paramaters (Non-Embedding): 31.2B
Number of Layers: 64
Number of Attention Heads (GQA): 64 for Q and 8 for KV
Context Length: 32,768 natively and 131,072 tokens with YaRN.",7.0848e+24,6 FLOP / parameter / token * 36 * 10^12 tokens * 32.8  * 10^9 parameters = 7.0848e+24 FLOP,Unspecified unreleased,"""While Qwen2.5 was pre-trained on 18 trillion tokens, Qwen3 uses nearly twice that amount, with approximately 36 trillion tokens covering 119 languages and dialects. To build this large dataset, we collected data not only from the web but also from PDF-like documents. We used Qwen2.5-VL to extract text from these documents and Qwen2.5 to improve the quality of the extracted content. To increase the amount of math and code data, we used Qwen2.5-Math and Qwen2.5-Coder to generate synthetic data. This includes textbooks, question-answer pairs, and code snippets.

The pre-training process consists of three stages. In the first stage (S1), the model was pretrained on over 30 trillion tokens with a context length of 4K tokens. This stage provided the model with basic language skills and general knowledge. In the second stage (S2), we improved the dataset by increasing the proportion of knowledge-intensive data, such as STEM, coding, and reasoning tasks. The model was then pretrained on an additional 5 trillion tokens. In the final stage, we used high-quality long-context data to extend the context length to 32K tokens. """,36000000000000.0,36T,,,,Confident,"Qwen3 is the latest generation of large language models in Qwen series, offering a comprehensive suite of dense and mixture-of-experts (MoE) models. Built upon extensive training, Qwen3 delivers groundbreaking advancements in reasoning, instruction-following, agent capabilities, and multilingual support, with the following key features:

Uniquely support of seamless switching between thinking mode (for complex logical reasoning, math, and coding) and non-thinking mode (for efficient, general-purpose dialogue) within single model, ensuring optimal performance across various scenarios.
Significantly enhancement in its reasoning capabilities, surpassing previous QwQ (in thinking mode) and Qwen2.5 instruct models (in non-thinking mode) on mathematics, code generation, and commonsense logical reasoning.
Superior human preference alignment, excelling in creative writing, role-playing, multi-turn dialogues, and instruction following, to deliver a more natural, engaging, and immersive conversational experience.
Expertise in agent capabilities, enabling precise integration with external tools in both thinking and unthinking modes and achieving leading performance among open-source models in complex agent-based tasks.
Support of 100+ languages and dialects with strong capabilities for multilingual instruction following and translation.",China,,,,,,Unreleased,"Apache 2.0

https://huggingface.co/Qwen/Qwen3-32B-Base",,,
Qwen3-14B,Language,"Language modeling/generation,Question answering,Mathematical reasoning,Quantitative reasoning,Code generation,Translation",,Open weights (unrestricted),https://qwenlm.github.io/blog/qwen3/,,"Qwen3: Think Deeper, Act Faster",2025-04-29,Alibaba,14800000000.0,"Number of Parameters: 14.8B
Number of Paramaters (Non-Embedding): 13.2B
Number of Layers: 40
Number of Attention Heads (GQA): 40 for Q and 8 for KV
Context Length: 32,768 natively and 131,072 tokens with YaRN.",3.1968e+24,6 FLOP / parameter / token * 36 * 10^12 tokens * 14.8  * 10^9 parameters = 3.1968e+24 FLOP,Unspecified unreleased,"""While Qwen2.5 was pre-trained on 18 trillion tokens, Qwen3 uses nearly twice that amount, with approximately 36 trillion tokens covering 119 languages and dialects. To build this large dataset, we collected data not only from the web but also from PDF-like documents. We used Qwen2.5-VL to extract text from these documents and Qwen2.5 to improve the quality of the extracted content. To increase the amount of math and code data, we used Qwen2.5-Math and Qwen2.5-Coder to generate synthetic data. This includes textbooks, question-answer pairs, and code snippets.

The pre-training process consists of three stages. In the first stage (S1), the model was pretrained on over 30 trillion tokens with a context length of 4K tokens. This stage provided the model with basic language skills and general knowledge. In the second stage (S2), we improved the dataset by increasing the proportion of knowledge-intensive data, such as STEM, coding, and reasoning tasks. The model was then pretrained on an additional 5 trillion tokens. In the final stage, we used high-quality long-context data to extend the context length to 32K tokens. """,36000000000000.0,36T,,,,Confident,"Qwen3 is the latest generation of large language models in Qwen series, offering a comprehensive suite of dense and mixture-of-experts (MoE) models. Built upon extensive training, Qwen3 delivers groundbreaking advancements in reasoning, instruction-following, agent capabilities, and multilingual support, with the following key features:

Uniquely support of seamless switching between thinking mode (for complex logical reasoning, math, and coding) and non-thinking mode (for efficient, general-purpose dialogue) within single model, ensuring optimal performance across various scenarios.
Significantly enhancement in its reasoning capabilities, surpassing previous QwQ (in thinking mode) and Qwen2.5 instruct models (in non-thinking mode) on mathematics, code generation, and commonsense logical reasoning.
Superior human preference alignment, excelling in creative writing, role-playing, multi-turn dialogues, and instruction following, to deliver a more natural, engaging, and immersive conversational experience.
Expertise in agent capabilities, enabling precise integration with external tools in both thinking and unthinking modes and achieving leading performance among open-source models in complex agent-based tasks.
Support of 100+ languages and dialects with strong capabilities for multilingual instruction following and translation.",China,,,,,,Unreleased,"Apache 2.0

https://huggingface.co/Qwen/Qwen3-14B-Base",,,
Qwen3-8B,Language,"Language modeling/generation,Question answering,Mathematical reasoning,Quantitative reasoning,Code generation,Translation",,Open weights (unrestricted),https://qwenlm.github.io/blog/qwen3/,,"Qwen3: Think Deeper, Act Faster",2025-04-29,Alibaba,8200000000.0,"Number of Parameters: 8.2B
Number of Paramaters (Non-Embedding): 6.95B
Number of Layers: 36
Number of Attention Heads (GQA): 32 for Q and 8 for KV
Context Length: 32,768 natively and 131,072 tokens with YaRN.",1.7712e+24,6 FLOP / parameter / token * 36 * 10^12 tokens * 8.2  * 10^9 parameters = 1.7712e+24 FLOP,Unspecified unreleased,"""While Qwen2.5 was pre-trained on 18 trillion tokens, Qwen3 uses nearly twice that amount, with approximately 36 trillion tokens covering 119 languages and dialects. To build this large dataset, we collected data not only from the web but also from PDF-like documents. We used Qwen2.5-VL to extract text from these documents and Qwen2.5 to improve the quality of the extracted content. To increase the amount of math and code data, we used Qwen2.5-Math and Qwen2.5-Coder to generate synthetic data. This includes textbooks, question-answer pairs, and code snippets.

The pre-training process consists of three stages. In the first stage (S1), the model was pretrained on over 30 trillion tokens with a context length of 4K tokens. This stage provided the model with basic language skills and general knowledge. In the second stage (S2), we improved the dataset by increasing the proportion of knowledge-intensive data, such as STEM, coding, and reasoning tasks. The model was then pretrained on an additional 5 trillion tokens. In the final stage, we used high-quality long-context data to extend the context length to 32K tokens. """,36000000000000.0,36T,,,,Confident,"Qwen3 is the latest generation of large language models in Qwen series, offering a comprehensive suite of dense and mixture-of-experts (MoE) models. Built upon extensive training, Qwen3 delivers groundbreaking advancements in reasoning, instruction-following, agent capabilities, and multilingual support, with the following key features:

Uniquely support of seamless switching between thinking mode (for complex logical reasoning, math, and coding) and non-thinking mode (for efficient, general-purpose dialogue) within single model, ensuring optimal performance across various scenarios.
Significantly enhancement in its reasoning capabilities, surpassing previous QwQ (in thinking mode) and Qwen2.5 instruct models (in non-thinking mode) on mathematics, code generation, and commonsense logical reasoning.
Superior human preference alignment, excelling in creative writing, role-playing, multi-turn dialogues, and instruction following, to deliver a more natural, engaging, and immersive conversational experience.
Expertise in agent capabilities, enabling precise integration with external tools in both thinking and unthinking modes and achieving leading performance among open-source models in complex agent-based tasks.
Support of 100+ languages and dialects with strong capabilities for multilingual instruction following and translation.",China,,,,,,Unreleased,"Apache 2.0

https://huggingface.co/Qwen/Qwen3-8B-Base",,,
Qwen3-4B,Language,"Language modeling/generation,Question answering,Mathematical reasoning,Quantitative reasoning,Code generation,Translation",,Open weights (unrestricted),https://qwenlm.github.io/blog/qwen3/,,"Qwen3: Think Deeper, Act Faster",2025-04-29,Alibaba,4000000000.0,"Number of Parameters: 4.0B
Number of Paramaters (Non-Embedding): 3.6B
Number of Layers: 36
Number of Attention Heads (GQA): 32 for Q and 8 for KV
Context Length: 32,768 natively and 131,072 tokens with YaRN.",8.64e+23,6 FLOP / parameter / token * 36 * 10^12 tokens * 4  * 10^9 parameters = 8.64e+23 FLOP,Unspecified unreleased,"""While Qwen2.5 was pre-trained on 18 trillion tokens, Qwen3 uses nearly twice that amount, with approximately 36 trillion tokens covering 119 languages and dialects. To build this large dataset, we collected data not only from the web but also from PDF-like documents. We used Qwen2.5-VL to extract text from these documents and Qwen2.5 to improve the quality of the extracted content. To increase the amount of math and code data, we used Qwen2.5-Math and Qwen2.5-Coder to generate synthetic data. This includes textbooks, question-answer pairs, and code snippets.

The pre-training process consists of three stages. In the first stage (S1), the model was pretrained on over 30 trillion tokens with a context length of 4K tokens. This stage provided the model with basic language skills and general knowledge. In the second stage (S2), we improved the dataset by increasing the proportion of knowledge-intensive data, such as STEM, coding, and reasoning tasks. The model was then pretrained on an additional 5 trillion tokens. In the final stage, we used high-quality long-context data to extend the context length to 32K tokens. """,36000000000000.0,36T,,,,Confident,"Qwen3 is the latest generation of large language models in Qwen series, offering a comprehensive suite of dense and mixture-of-experts (MoE) models. Built upon extensive training, Qwen3 delivers groundbreaking advancements in reasoning, instruction-following, agent capabilities, and multilingual support, with the following key features:

Uniquely support of seamless switching between thinking mode (for complex logical reasoning, math, and coding) and non-thinking mode (for efficient, general-purpose dialogue) within single model, ensuring optimal performance across various scenarios.
Significantly enhancement in its reasoning capabilities, surpassing previous QwQ (in thinking mode) and Qwen2.5 instruct models (in non-thinking mode) on mathematics, code generation, and commonsense logical reasoning.
Superior human preference alignment, excelling in creative writing, role-playing, multi-turn dialogues, and instruction following, to deliver a more natural, engaging, and immersive conversational experience.
Expertise in agent capabilities, enabling precise integration with external tools in both thinking and unthinking modes and achieving leading performance among open-source models in complex agent-based tasks.
Support of 100+ languages and dialects with strong capabilities for multilingual instruction following and translation.",China,,,,,,Unreleased,"Apache 2.0

https://huggingface.co/Qwen/Qwen3-4B-Base",,,
Qwen3-1.7B,Language,"Language modeling/generation,Question answering,Mathematical reasoning,Quantitative reasoning,Code generation,Translation",,Open weights (unrestricted),https://qwenlm.github.io/blog/qwen3/,,"Qwen3: Think Deeper, Act Faster",2025-04-29,Alibaba,1700000000.0,"Number of Parameters: 1.7B
Number of Paramaters (Non-Embedding): 1.4B
Number of Layers: 28
Number of Attention Heads (GQA): 16 for Q and 8 for KV
Context Length: 32,768
",3.672e+23,6 FLOP / parameter / token * 36 * 10^12 tokens * 1.7  * 10^9 parameters = 3.672e+23 FLOP,Unspecified unreleased,"""While Qwen2.5 was pre-trained on 18 trillion tokens, Qwen3 uses nearly twice that amount, with approximately 36 trillion tokens covering 119 languages and dialects. To build this large dataset, we collected data not only from the web but also from PDF-like documents. We used Qwen2.5-VL to extract text from these documents and Qwen2.5 to improve the quality of the extracted content. To increase the amount of math and code data, we used Qwen2.5-Math and Qwen2.5-Coder to generate synthetic data. This includes textbooks, question-answer pairs, and code snippets.

The pre-training process consists of three stages. In the first stage (S1), the model was pretrained on over 30 trillion tokens with a context length of 4K tokens. This stage provided the model with basic language skills and general knowledge. In the second stage (S2), we improved the dataset by increasing the proportion of knowledge-intensive data, such as STEM, coding, and reasoning tasks. The model was then pretrained on an additional 5 trillion tokens. In the final stage, we used high-quality long-context data to extend the context length to 32K tokens. """,36000000000000.0,36T,,,,Confident,"Qwen3 is the latest generation of large language models in Qwen series, offering a comprehensive suite of dense and mixture-of-experts (MoE) models. Built upon extensive training, Qwen3 delivers groundbreaking advancements in reasoning, instruction-following, agent capabilities, and multilingual support, with the following key features:

Uniquely support of seamless switching between thinking mode (for complex logical reasoning, math, and coding) and non-thinking mode (for efficient, general-purpose dialogue) within single model, ensuring optimal performance across various scenarios.
Significantly enhancement in its reasoning capabilities, surpassing previous QwQ (in thinking mode) and Qwen2.5 instruct models (in non-thinking mode) on mathematics, code generation, and commonsense logical reasoning.
Superior human preference alignment, excelling in creative writing, role-playing, multi-turn dialogues, and instruction following, to deliver a more natural, engaging, and immersive conversational experience.
Expertise in agent capabilities, enabling precise integration with external tools in both thinking and unthinking modes and achieving leading performance among open-source models in complex agent-based tasks.
Support of 100+ languages and dialects with strong capabilities for multilingual instruction following and translation.",China,,,,,,Unreleased,"Apache 2.0

https://huggingface.co/Qwen/Qwen3-1.7B-Base",,,
Qwen3-0.6B,Language,"Language modeling/generation,Question answering,Mathematical reasoning,Quantitative reasoning,Code generation,Translation",,Open weights (unrestricted),https://qwenlm.github.io/blog/qwen3/,,"Qwen3: Think Deeper, Act Faster",2025-04-29,Alibaba,600000000.0,"Number of Parameters: 0.6B
Number of Paramaters (Non-Embedding): 0.44B
Number of Layers: 28
Number of Attention Heads (GQA): 16 for Q and 8 for KV
Context Length: 32,768",1.296e+23,6 FLOP / parameter / token * 36 * 10^12 tokens * 0.6 * 10^9 parameters = 1.296e+23 FLOP,Unspecified unreleased,"""While Qwen2.5 was pre-trained on 18 trillion tokens, Qwen3 uses nearly twice that amount, with approximately 36 trillion tokens covering 119 languages and dialects. To build this large dataset, we collected data not only from the web but also from PDF-like documents. We used Qwen2.5-VL to extract text from these documents and Qwen2.5 to improve the quality of the extracted content. To increase the amount of math and code data, we used Qwen2.5-Math and Qwen2.5-Coder to generate synthetic data. This includes textbooks, question-answer pairs, and code snippets.

The pre-training process consists of three stages. In the first stage (S1), the model was pretrained on over 30 trillion tokens with a context length of 4K tokens. This stage provided the model with basic language skills and general knowledge. In the second stage (S2), we improved the dataset by increasing the proportion of knowledge-intensive data, such as STEM, coding, and reasoning tasks. The model was then pretrained on an additional 5 trillion tokens. In the final stage, we used high-quality long-context data to extend the context length to 32K tokens. """,36000000000000.0,36T,,,,Confident,"Qwen3 is the latest generation of large language models in Qwen series, offering a comprehensive suite of dense and mixture-of-experts (MoE) models. Built upon extensive training, Qwen3 delivers groundbreaking advancements in reasoning, instruction-following, agent capabilities, and multilingual support, with the following key features:

Uniquely support of seamless switching between thinking mode (for complex logical reasoning, math, and coding) and non-thinking mode (for efficient, general-purpose dialogue) within single model, ensuring optimal performance across various scenarios.
Significantly enhancement in its reasoning capabilities, surpassing previous QwQ (in thinking mode) and Qwen2.5 instruct models (in non-thinking mode) on mathematics, code generation, and commonsense logical reasoning.
Superior human preference alignment, excelling in creative writing, role-playing, multi-turn dialogues, and instruction following, to deliver a more natural, engaging, and immersive conversational experience.
Expertise in agent capabilities, enabling precise integration with external tools in both thinking and unthinking modes and achieving leading performance among open-source models in complex agent-based tasks.
Support of 100+ languages and dialects with strong capabilities for multilingual instruction following and translation.",China,,,,,,Unreleased,"Apache 2.0

https://huggingface.co/Qwen/Qwen3-0.6B-Base",,,
Foundation-sec-8b,Language,"Language modeling/generation,Question answering","Paul Kassianik, Baturay Saglam, Alexander Chen, Blaine Nelson, Anu Vellore, Massimo Aufiero, Fraser Burch, Dhruv Kedia, Avi Zohary, Sajana Weerawardhena, Aman Priyanshu, Adam Swanda, Amy Chang, Hyrum Anderson, Kojin Oshiba, Omar Santos, Yaron Singer, Amin Karbasi",Open weights (unrestricted),https://arxiv.org/abs/2504.21039,,Llama-3.1-FoundationAI-SecurityLLM-Base-8B Technical Report,2025-04-28,Cisco,8000000000.0,"""Our newly launched Foundation AI groupâ€”formed to bring world-class, domain-specific AI infrastructure to the cybersecurity spaceâ€”is proud to introduce its first release: Llama-3.1-FoundationAI-SecurityLLM-base-8B (Foundation-sec-8b), an 8-billion parameter, open-weight Large Language Model (LLM) purpose-built for security.""",1.4688e+24,1.224e+24 FLOP [base model compute] + 2.448e+23 FLOP [finetune compute] = 1.4688e+24 FLOP,Unspecified unreleased,cybersecurity-specific corpus,5100000000.0,"""Finally, we split the resulting 5.1 billion tokens into a
99% training set and a 1% test set.""",,,,Confident,"As transformer-based large language models (LLMs) increasingly permeate society, they have revolutionized domains such as software engineering, creative writing, and digital arts. However, their adoption in cybersecurity remains limited due to challenges like scarcity of specialized training data and complexity of representing cybersecurity-specific knowledge. To address these gaps, we present Foundation-Sec-8B, a cybersecurity-focused LLM built on the Llama 3.1 architecture and enhanced through continued pretraining on a carefully curated cybersecurity corpus. We evaluate Foundation-Sec-8B across both established and new cybersecurity benchmarks, showing that it matches Llama 3.1-70B and GPT-4o-mini in certain cybersecurity-specific tasks. By releasing our model to the public, we aim to accelerate progress and adoption of AI-driven tools in both public and private cybersecurity contexts.",United States of America,Llama 3.1-8B,2.448e+23,6 FLOP / parameter / token * 8 * 10^9 tokens * 5.1 * 10^12 tokens = 2.448e+23 FLOP,,,Unreleased,"apache 2.0
https://huggingface.co/fdtn-ai/Foundation-Sec-8B",,,
gpt-image-1,"Image generation,Vision","Image generation,Text-to-image",,API access,https://openai.com/index/image-generation-api/,,Introducing our latest image generation model in the API,2025-04-23,OpenAI,,,,,Unspecified unreleased,,,,,,,Unknown,"Today, weâ€™re bringing the natively multimodal model that powers this experience in ChatGPT to the API via gpt-image-1, enabling developers and businesses to easily integrate high-quality, professional-grade image generation directly into their own tools and platforms. The modelâ€™s versatility allows it to create images across diverse styles, faithfully follow custom guidelines, leverage world knowledge, and accurately render textâ€”unlocking countless practical applications across multiple domains.",United States of America,,,,,,Unreleased,,,,
"MamayLM
",Language,"Language modeling/generation,Language generation","Anton Alexandrov, Hannah Yukhymenko, Martin Vechev
",Open weights (unrestricted),https://huggingface.co/INSAIT-Institute/MamayLM-Gemma-2-9B-IT-v0.1,,"INSAIT introduces MamayLM-Gemma-2-9B-IT-v0.1, the best performing Ukrainian language model based on google/gemma-2-9b and google/gemma-2-9b-it.",2025-04-23,"INSAIT,ETH Zurich",9000000000.0,9B,4.36e+23,Gemma 2 9B pretrain on 8T tokens + MamayLM continual pretrain on 75B tokens. Unclear about SFT compute,"FineWeb,CulturaX,Wikipedia",,,,,,,Speculative,"The model was built on top of Googleâ€™s Gemma 2 9B open models. It was continuously pre-trained on a large pre-filtered dataset (75B tokens of Ukrainian and English data in total) using the combination of data mixing and model merging, allowing the model to gain outstanding Ukrainian cultural and linguistic capabilities while retaining its English performance. During the pre-training stage, we use various datasets, including Ukrainian web crawl data (FineWeb2), freely available datasets such as Wikipedia, a range of specialized Ukrainian datasets, and machine translations of popular English datasets. The model was then instruction-fine-tuned on a newly constructed Ukrainian instruction dataset created using machine translations of current best English datasets and specialized Ukrainian datasets, prepared by Ukrainian community. For more information check our blogpost (English, Ukrainian).","Bulgaria,Switzerland",,,,,,Unreleased,,,,
Gemini 2.5 Flash (Apr 2025),"Language,Multimodal,Vision,Speech,Video","Language modeling/generation,Question answering,Code generation,Quantitative reasoning,Visual question answering,Translation,Image captioning,Speech recognition (ASR),Video description,Search,Text summarization,Chat",,API access,https://deepmind.google/models/gemini/flash/,,Our powerful and most efficient workhorse model designed for speed and low-cost.,2025-04-17,Google DeepMind,,,,,Unspecified unreleased,Knowledge cutoff January 2025,,,,,,Unknown,"Speed and value at scale
Ideal for tasks like summarization, chat applications, data extraction, and captioning.
Thinking budget: Control how much 2.5 Flash reasons to balance latency and cost.
Natively multimodal: Understands input across text, audio, images and video.
Long context: Explore vast datasets with a 1-million token context window.
Adaptive and budgeted thinking
Adaptive controls and adjustable thinking budgets allow you to balance performance and cost.
Calibrated: The model explores diverse thinking strategies, leading to more accurate and relevant outputs.
Controllable: Developers have fine-grained control over the model's thinking process, allowing them to manage resource usage.
Adaptive: When no thinking budget is set, the model assesses the complexity of a task and calibrates the amount of thinking accordingly.","United States of America,United Kingdom of Great Britain and Northern Ireland",,,,,,Unreleased,"Availability	
Google AI Studio
Gemini API
Gemini App",,,
o4-mini,"Multimodal,Language,Vision","Language modeling/generation,Search,Question answering,Quantitative reasoning,Chat,Translation,Code generation,Visual question answering,Instruction interpretation,Visual puzzles",,API access,https://openai.com/index/introducing-o3-and-o4-mini/,,Introducing OpenAI o3 and o4-mini: Our smartest and most capable models to date with full tool access,2025-04-16,OpenAI,,"Can't get an exact estimate, but we suspect total parameter count around 60B-120B, active parameters around 10B-30B. 

Given these models are served at 150-200 tok/s, at $4.40/Mtok output, inference economics (https://epoch.ai/blog/inference-economics-of-language-models) suggests total parameter count around 60-120B parameters, with mixture-of-experts active parameters around 10-30B. MoEs make a given model roughly comparable to a ~50% smaller dense model (https://epoch.ai/gradient-updates/moe-vs-dense-models-inference), which lines up decently with Magistral Small pricing (24B dense, served at a similar speed for the cheaper $1.50/Mtok). ",,"We canâ€™t make a precise estimate, but seems unlikely to exceed 10^25 FLOP. We think active parameter count is 10-30B. This would require >55T tokens to reach 10^25 FLOP at the large size, i.e. well beyond 10x overtraining relative to Chinchilla.",Unspecified unreleased,"""The OpenAI o-series models are trained with large-scale reinforcement learning on chains of thought.""

""OpenAI o3 and o4-mini were trained on diverse datasets,
including information that is publicly available on the internet, information that we partner with third parties to access, and information that our users or human trainers and researchers provide or generate. Our data processing pipeline includes rigorous filtering to maintain data quality and mitigate potential risks. We use advanced data filtering processes to reduce personal information
from training data. We also employ a combination of our Moderation API and safety classifiers to help prevent the use of harmful or sensitive content, including explicit materials such as sexual content involving a minor.""
This has a knowledge cutoff date of May 31, 2024, according to https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models?tabs=global-standard%2Cstandard-chat-completions#o-series-models.",,,,,,Unknown,"Today, weâ€™re releasing OpenAI o3 and o4-mini, the latest in our o-series of models trained to think for longer before responding. These are the smartest models weâ€™ve released to date, representing a step change in ChatGPT's capabilities for everyone from curious users to advanced researchers. For the first time, our reasoning models can agentically use and combine every tool within ChatGPTâ€”this includes searching the web, analyzing uploaded files and other data with Python, reasoning deeply about visual inputs, and even generating images. Critically, these models are trained to reason about when and how to use tools to produce detailed and thoughtful answers in the right output formats, typically in under a minute, to solve more complex problems. This allows them to tackle multi-faceted questions more effectively, a step toward a more agentic ChatGPT that can independently execute tasks on your behalf. The combined power of state-of-the-art reasoning with full tool access translates into significantly stronger performance across academic benchmarks and real-world tasks, setting a new standard in both intelligence and usefulness.
<..>
OpenAI o4-mini is a smaller model optimized for fast, cost-efficient reasoningâ€”it achieves remarkable performance for its size and cost, particularly in math, coding, and visual tasks. It is the best-performing benchmarked model on AIME 2024 and 2025. Although access to a computer meaningfully reduces the difficulty of the AIME exam, we also found it notable that o4-mini achieves 99.5% pass@1 (100% consensus@8) on AIME 2025 when given access to a Python interpreter. While these results should not be compared to the performance of models without tool access, they are one example of how effectively o4-mini leverages available tools; o3 shows similar improvements on AIME 2025 from tool use (98.4% pass@1, 100% consensus@8).",United States of America,,,,,,Unreleased,"""Both o3 and o4-mini are also available to developers today via the Chat Completions API and Responses API (some developers will need to verify their organizationsâ (opens in a new window) to access these models)""",,,
Seedream 3.0,Image generation,"Image generation,Text-to-image","Yu Gao, Lixue Gong, Qiushan Guo, Xiaoxia Hou, Zhichao Lai, Fanshi Li, Liang Li, Xiaochen Lian, Chao Liao, Liyang Liu, Wei Liu, Yichun Shi, Shiqi Sun, Yu Tian, Zhi Tian, Peng Wang, Rui Wang, Xuanda Wang, Xun Wang, Ye Wang, Guofeng Wu, Jie Wu, Xin Xia, Xuefeng Xiao, Zhonghua Zhai, Xinyu Zhang, Qi Zhang, Yuwei Zhang, Shijia Zhao, Jianchao Yang, Weilin Huang",API access,https://arxiv.org/abs/2504.11346,,Seedream 3.0 Technical Report,2025-04-16,ByteDance,,,,"Performs similarly well on video generation leaderboards to Veo-3, developer has plenty of funding (billions of dollars in annual R&D), plausibly may have trained with >1e25 FLOP.",Unspecified unreleased,,,,,,,Unknown,"We present Seedream 3.0, a high-performance Chinese-English bilingual image generation foundation model. We develop several technical improvements to address existing challenges in Seedream 2.0, including alignment with complicated prompts, fine-grained typography generation, suboptimal visual aesthetics and fidelity, and limited image resolutions. Specifically, the advancements of Seedream 3.0 stem from improvements across the entire pipeline, from data construction to model deployment. At the data stratum, we double the dataset using a defect-aware training paradigm and a dual-axis collaborative data-sampling framework. Furthermore, we adopt several effective techniques such as mixed-resolution training, cross-modality RoPE, representation alignment loss, and resolution-aware timestep sampling in the pre-training phase. During the post-training stage, we utilize diversified aesthetic captions in SFT, and a VLM-based reward model with scaling, thereby achieving outputs that well align with human preferences. Furthermore, Seedream 3.0 pioneers a novel acceleration paradigm. By employing consistent noise expectation and importance-aware timestep sampling, we achieve a 4 to 8 times speedup while maintaining image quality. Seedream 3.0 demonstrates significant improvements over Seedream 2.0: it enhances overall capabilities, in particular for text-rendering in complicated Chinese characters which is important to professional typography generation. In addition, it provides native high-resolution output (up to 2K), allowing it to generate images with high visual quality.",China,,,,,,Unreleased,,,,
Kling 2.0 Video Generation,"Video,Vision","Video generation,Image-to-video,Video-to-video,Text-to-video",,API access,https://ir.kuaishou.com/news-releases/news-release-details/kling-ai-advances-20-era-empowering-everyone-tell-great-stories,,"Kling AI Advances to the 2.0 Era, Empowering Everyone to Tell Great Stories with AI",2025-04-15,Kuaishou Technology,,,,"Hard to bound the training compute. It performs similarly to Veo-2/3, so plausibly has similar training budget. Kuaishou, the developer, spent $1.7B on R&D in 2024, so plausibly could have afforded the compute for e.g. a 1e25 FLOP training run (https://kr-asia.com/kuaishou-turns-to-ai-and-kling-to-stay-sticky-in-a-flatlining-market).",Unspecified unreleased,,,,,,,Unknown,"As the worldâ€™s first user-accessible DiT video generation model, in the 10 months since its initial launch in June of last year, its global user base has surpassed 22 million. On March 27, Artificial Analysis, a globally renowned AI benchmarking organization, released the latest global rankings for video generation large models. Kuaishou Kling 1.6 Pro (high-quality mode) topped the Image to Video category with an Arena ELO benchmark score of 1,000, while Google Veo 2 and Pika Art ranked second and third, respectively.

In this 2.0 model iteration, Kling AI officially introduces multi-modal visual language (MVL), a new interactive concept for AI video generation. This feature allows users to integrate multimodal inputs, such as image references and video clips, enabling them to convey complex creative ideas effectively and directly to AI, covering aspects such as identity, appearance, style, scenarios, actions, expressions, camera movements, and other elements.",China,,,,,,Unreleased,,,,
GPT-4.1,"Multimodal,Language,Vision,Video","Language modeling/generation,Code generation,Question answering,Quantitative reasoning,Instruction interpretation,System control,Visual question answering,Video description","Research leads
Ananya Kumar, Jiahui Yu, John Hallman, Michelle Pokrass

Research core contributors
Adam Goucher, Adi Ganesh, Bowen Cheng, Brandon McKinzie, Brian Zhang, Chris Koch, Colin Wei, David Medina, Edmund Wong, Erin Kavanaugh, Florent Bekerman, Haitang Hu, Hongyu Ren, Ishaan Singal, Jamie Kiros, Jason Ai, Ji Lin, Jonathan Chien, Josh McGrath, Julian Lee, Julie Wang, Kevin Lu, Kristian Georgiev, Kyle Luther, Li Jing, Max Schwarzer, Miguel Castro, Nitish Keskar, Rapha Gontijo Lopes, Shengjia Zhao, Sully Chen, Suvansh Sanjeev, Taylor Gordon, Ted Sanders, Wenda Zhou, Yang Song, Yujia Xie, Yujia Jin, Zhishuai Zhang",API access,https://openai.com/index/gpt-4-1/,,"Introducing GPT-4.1 in the API: A new series of GPT models featuring major improvements on coding, instruction following, and long contextâ€”plus our first-ever nano model.",2025-04-14,OpenAI,,,,Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.,Unspecified unreleased,"This has a knowledge cutoff date of May 31, 2024, according to https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models?tabs=global-standard%2Cstandard-chat-completions#o-series-models.",,,,,,Unknown,"Today, weâ€™re launching three new models in the API: GPTâ€‘4.1, GPTâ€‘4.1 mini, and GPTâ€‘4.1 nano. These models outperform GPTâ€‘4o and GPTâ€‘4o mini across the board, with major gains in coding and instruction following. They also have larger context windowsâ€”supporting up to 1 million tokens of contextâ€”and are able to better use that context with improved long-context comprehension. They feature a refreshed knowledge cutoff of June 2024.

GPTâ€‘4.1 excels at the following industry standard measures: 

Coding: GPTâ€‘4.1 scores 54.6% on SWE-bench Verified, improving by 21.4%abs over GPTâ€‘4o and 26.6%abs over GPTâ€‘4.5â€”making it a leading model for coding.
Instruction following: On Scaleâ€™s MultiChallengeâ (opens in a new window) benchmark, a measure of instruction following ability, GPTâ€‘4.1 scores 38.3%, a 10.5%abs increase over GPTâ€‘4o.
Long context: On Video-MMEâ (opens in a new window), a benchmark for multimodal long context understanding, GPTâ€‘4.1 sets a new state-of-the-art resultâ€”scoring 72.0% on the long, no subtitles category, a 6.7%abs improvement over GPTâ€‘4o.",United States of America,,,,,,Unreleased,"""GPTâ€‘4.1 will only be available via the API""",,,
GPT-4.1 mini,"Multimodal,Language,Vision,Video","Language modeling/generation,Code generation,Question answering,Quantitative reasoning,Instruction interpretation,System control,Visual question answering,Video description","Research leads
Ananya Kumar, Jiahui Yu, John Hallman, Michelle Pokrass

Research core contributors
Adam Goucher, Adi Ganesh, Bowen Cheng, Brandon McKinzie, Brian Zhang, Chris Koch, Colin Wei, David Medina, Edmund Wong, Erin Kavanaugh, Florent Bekerman, Haitang Hu, Hongyu Ren, Ishaan Singal, Jamie Kiros, Jason Ai, Ji Lin, Jonathan Chien, Josh McGrath, Julian Lee, Julie Wang, Kevin Lu, Kristian Georgiev, Kyle Luther, Li Jing, Max Schwarzer, Miguel Castro, Nitish Keskar, Rapha Gontijo Lopes, Shengjia Zhao, Sully Chen, Suvansh Sanjeev, Taylor Gordon, Ted Sanders, Wenda Zhou, Yang Song, Yujia Xie, Yujia Jin, Zhishuai Zhang",API access,https://openai.com/index/gpt-4-1/,,"Introducing GPT-4.1 in the API: A new series of GPT models featuring major improvements on coding, instruction following, and long contextâ€”plus our first-ever nano model.",2025-04-14,OpenAI,,,,,Unspecified unreleased,"This has a knowledge cutoff date of May 31, 2024, according to https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models?tabs=global-standard%2Cstandard-chat-completions#o-series-models.",,,,,,Unknown,"GPTâ€‘4.1 mini is a significant leap in small model performance, even beating GPTâ€‘4o in many benchmarks. It matches or exceeds GPTâ€‘4o in intelligence evals while reducing latency by nearly half and reducing cost by 83%. ",United States of America,,,,,,Unreleased,"""GPTâ€‘4.1 will only be available via the API""",,,
GPT-4.1 nano,"Multimodal,Language,Vision,Video","Language modeling/generation,Code generation,Question answering,Quantitative reasoning,Instruction interpretation,System control,Visual question answering,Video description","Research leads
Ananya Kumar, Jiahui Yu, John Hallman, Michelle Pokrass

Research core contributors
Adam Goucher, Adi Ganesh, Bowen Cheng, Brandon McKinzie, Brian Zhang, Chris Koch, Colin Wei, David Medina, Edmund Wong, Erin Kavanaugh, Florent Bekerman, Haitang Hu, Hongyu Ren, Ishaan Singal, Jamie Kiros, Jason Ai, Ji Lin, Jonathan Chien, Josh McGrath, Julian Lee, Julie Wang, Kevin Lu, Kristian Georgiev, Kyle Luther, Li Jing, Max Schwarzer, Miguel Castro, Nitish Keskar, Rapha Gontijo Lopes, Shengjia Zhao, Sully Chen, Suvansh Sanjeev, Taylor Gordon, Ted Sanders, Wenda Zhou, Yang Song, Yujia Xie, Yujia Jin, Zhishuai Zhang",API access,https://openai.com/index/gpt-4-1/,,"Introducing GPT-4.1 in the API: A new series of GPT models featuring major improvements on coding, instruction following, and long contextâ€”plus our first-ever nano model.",2025-04-14,OpenAI,,,,,Unspecified unreleased,"This has a knowledge cutoff date of May 31, 2024, according to https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models?tabs=global-standard%2Cstandard-chat-completions#o-series-models.",,,,,,Unknown,"For tasks that demand low latency, GPTâ€‘4.1 nano is our fastest and cheapest model available. It delivers exceptional performance at a small size with its 1 million token context window, and scores 80.1% on MMLU, 50.3% on GPQA, and 9.8% on Aider polyglot codingâ€”even higher than GPTâ€‘4o mini. Itâ€™s ideal for tasks like classification or autocompletion.",United States of America,,,,,,Unreleased,"""GPTâ€‘4.1 will only be available via the API""",,,
Nemotron-H 8B,Language,"Language modeling/generation,Question answering,Translation,Quantitative reasoning,Code generation","NVIDIA: Aaron Blakeman, Aarti Basant, Abhinav Khattar, Adithya Renduchintala, Akhiad Bercovich, Aleksander Ficek, Alexis Bjorlin, Ali Taghibakhshi, Amala Sanjay Deshmukh, Ameya Sunil Mahabaleshwarkar, Andrew Tao, Anna Shors, Ashwath Aithal, Ashwin Poojary, Ayush Dattagupta, Balaram Buddharaju, Bobby Chen, Boris Ginsburg, Boxin Wang, Brandon Norick, Brian Butterfield, Bryan Catanzaro, Carlo del Mundo, Chengyu Dong, Christine Harvey, Christopher Parisien, Dan Su, Daniel Korzekwa, Danny Yin, Daria Gitman, David Mosallanezhad, Deepak Narayanan, Denys Fridman, Dima Rekesh, Ding Ma, Dmytro Pykhtar, Dong Ahn, Duncan Riach, Dusan Stosic, Eileen Long, Elad Segal, Ellie Evans, Eric Chung, Erick Galinkin, Evelina Bakhturina, Ewa Dobrowolska, Fei Jia, Fuxiao Liu, Gargi Prasad, Gerald Shen, Guilin Liu, Guo Chen, Haifeng Qian, Helen Ngo, Hongbin Liu, Hui Li, Igor Gitman, Ilia Karmanov, Ivan Moshkov, Izik Golan, Jan Kautz, Jane Polak Scowcroft, Jared Casper, Jarno Seppanen, Jason Lu, Jason Sewall, Jiaqi Zeng, Jiaxuan You, Jimmy Zhang, Jing Zhang, Jining Huang, Jinze Xue, Jocelyn Huang, Joey Conway, John Kamalu, Jon Barker, Jonathan Cohen, Joseph Jennings, Jupinder Parmar, Karan Sapra, Kari Briski, Kateryna Chumachenko, Katherine Luna, Keshav Santhanam, Kezhi Kong, Kirthi Sivamani, Krzysztof Pawelec, Kumar Anik, Kunlun Li, Lawrence McAfee, Leon Derczynski, Lindsey Pavao, Luis Vega, Lukas Voegtle, Maciej Bala, Maer Rodrigues de Melo, Makesh Narsimhan Sreedhar, Marcin Chochowski, Markus Kliegl et al. (100 additional authors not shown)",Open weights (non-commercial),https://arxiv.org/abs/2504.03624,,Nemotron-H: A Family of Accurate and Efficient Hybrid Mamba-Transformer Models,2025-04-14,NVIDIA,8000000000.0,"Model Architecture
Architecture Type: Hybrid Mamba-Transformer
Network Architecture: Nemotron-H
This model has 8B model parameters.",7.2e+23,6 FLOP / parameter / token * 8 * 10^9 parameters * 15 * 10^12 tokens = 7.2e+23 FLOP,"Common Crawl,Unspecified unreleased","""The training corpus for Nemotron-H-8B-Base-8K consists of English and multilingual text (German, Spanish, French, Italian, Korean, Portuguese, Russian, Japanese, Chinese and English), as well as code. Our sources cover a variety of document types such as: webpages, dialogue, articles, and other written materials. This model was also improved using synthetic data from Qwen (Built with Qwen). The corpus spans domains including legal, math, science, finance, and more. We also include a small portion of question-answering, and alignment style data to improve model accuracy.""",15000000000000.0,"""We trained Nemotron-H-8B-Base on a token horizon of 15 trillion tokens""",,,,Confident,"As inference-time scaling becomes critical for enhanced reasoning capabilities, it is increasingly becoming important to build models that are efficient to infer. We introduce Nemotron-H, a family of 8B and 56B/47B hybrid Mamba-Transformer models designed to reduce inference cost for a given accuracy level. To achieve this goal, we replace the majority of self-attention layers in the common Transformer model architecture with Mamba layers that perform constant computation and require constant memory per generated token. We show that Nemotron-H models offer either better or on-par accuracy compared to other similarly-sized state-of-the-art open-sourced Transformer models (e.g., Qwen-2.5-7B/72B and Llama-3.1-8B/70B), while being up to 3Ã— faster at inference. To further increase inference speed and reduce the memory required at inference time, we created Nemotron-H-47B-Base from the 56B model using a new compression via pruning and distillation technique called MiniPuzzle. Nemotron-H-47B-Base achieves similar accuracy to the 56B model, but is 20% faster to infer. In addition, we introduce an FP8-based training recipe and show that it can achieve on par results with BF16-based training. This recipe is used to train the 56B model. We are releasing Nemotron-H base model checkpoints with support in Hugging Face and NeMo.",United States of America,,,,,,Unreleased,"https://huggingface.co/nvidia/Nemotron-H-8B-Base-8K
nvidia-internal-scientific-research-and-development-model-license",,,
Nemotron-H 56B,Language,"Language modeling/generation,Question answering,Translation,Quantitative reasoning,Code generation","NVIDIA: Aaron Blakeman, Aarti Basant, Abhinav Khattar, Adithya Renduchintala, Akhiad Bercovich, Aleksander Ficek, Alexis Bjorlin, Ali Taghibakhshi, Amala Sanjay Deshmukh, Ameya Sunil Mahabaleshwarkar, Andrew Tao, Anna Shors, Ashwath Aithal, Ashwin Poojary, Ayush Dattagupta, Balaram Buddharaju, Bobby Chen, Boris Ginsburg, Boxin Wang, Brandon Norick, Brian Butterfield, Bryan Catanzaro, Carlo del Mundo, Chengyu Dong, Christine Harvey, Christopher Parisien, Dan Su, Daniel Korzekwa, Danny Yin, Daria Gitman, David Mosallanezhad, Deepak Narayanan, Denys Fridman, Dima Rekesh, Ding Ma, Dmytro Pykhtar, Dong Ahn, Duncan Riach, Dusan Stosic, Eileen Long, Elad Segal, Ellie Evans, Eric Chung, Erick Galinkin, Evelina Bakhturina, Ewa Dobrowolska, Fei Jia, Fuxiao Liu, Gargi Prasad, Gerald Shen, Guilin Liu, Guo Chen, Haifeng Qian, Helen Ngo, Hongbin Liu, Hui Li, Igor Gitman, Ilia Karmanov, Ivan Moshkov, Izik Golan, Jan Kautz, Jane Polak Scowcroft, Jared Casper, Jarno Seppanen, Jason Lu, Jason Sewall, Jiaqi Zeng, Jiaxuan You, Jimmy Zhang, Jing Zhang, Jining Huang, Jinze Xue, Jocelyn Huang, Joey Conway, John Kamalu, Jon Barker, Jonathan Cohen, Joseph Jennings, Jupinder Parmar, Karan Sapra, Kari Briski, Kateryna Chumachenko, Katherine Luna, Keshav Santhanam, Kezhi Kong, Kirthi Sivamani, Krzysztof Pawelec, Kumar Anik, Kunlun Li, Lawrence McAfee, Leon Derczynski, Lindsey Pavao, Luis Vega, Lukas Voegtle, Maciej Bala, Maer Rodrigues de Melo, Makesh Narsimhan Sreedhar, Marcin Chochowski, Markus Kliegl et al. (100 additional authors not shown)",Open weights (non-commercial),https://arxiv.org/abs/2504.03624,,Nemotron-H: A Family of Accurate and Efficient Hybrid Mamba-Transformer Models,2025-04-14,NVIDIA,56000000000.0,"Model Architecture
Architecture Type: Hybrid Mamba-Transformer
Network Architecture: Nemotron-H
This model has 56B model parameters.",6.72e+24,6 FLOP / parameter / token * 56 * 10^9 parameters * 20 * 10^12 tokens = 6.72e+24 FLOP,"Common Crawl,Unspecified unreleased",,20000000000000.0,"""We trained Nemotron-H-8B-Base on a token horizon of 15 trillion tokens and Nemotron-H-56B-Base on a token horizon of 20 trillion tokens. We used a sequence length of 8192 and global batch size of
768 (6291456 tokens per batch).""",,,NVIDIA H100 SXM5 80GB,Confident,"As inference-time scaling becomes critical for enhanced reasoning capabilities, it is increasingly becoming important to build models that are efficient to infer. We introduce Nemotron-H, a family of 8B and 56B/47B hybrid Mamba-Transformer models designed to reduce inference cost for a given accuracy level. To achieve this goal, we replace the majority of self-attention layers in the common Transformer model architecture with Mamba layers that perform constant computation and require constant memory per generated token. We show that Nemotron-H models offer either better or on-par accuracy compared to other similarly-sized state-of-the-art open-sourced Transformer models (e.g., Qwen-2.5-7B/72B and Llama-3.1-8B/70B), while being up to 3Ã— faster at inference. To further increase inference speed and reduce the memory required at inference time, we created Nemotron-H-47B-Base from the 56B model using a new compression via pruning and distillation technique called MiniPuzzle. Nemotron-H-47B-Base achieves similar accuracy to the 56B model, but is 20% faster to infer. In addition, we introduce an FP8-based training recipe and show that it can achieve on par results with BF16-based training. This recipe is used to train the 56B model. We are releasing Nemotron-H base model checkpoints with support in Hugging Face and NeMo.",United States of America,,,,6144.0,,Unreleased,"https://huggingface.co/nvidia/Nemotron-H-56B-Base-8K
nvidia-internal-scientific-research-and-development-model-license",,,
GLM-Z1-Rumination-32B-0414,Language,"Language modeling/generation,Question answering,Quantitative reasoning,Code generation",,Open weights (unrestricted),https://huggingface.co/THUDM/GLM-Z1-Rumination-32B-0414,,GLM-4-Z1-Rumination-32B-0414,2025-04-14,Tsinghua University,32000000000.0,32B,2.88e+24,6 FLOP / parameter / token * 32 * 10^9 parameters * 15 * 10^12 tokens = 2.88e+24 FLOP,Unspecified unreleased,"""GLM-4-32B-Base-0414 was pre-trained on 15T of high-quality data, including substantial reasoning-type synthetic data. This lays the foundation for subsequent reinforcement learning extensions. In the post-training stage, we employed human preference alignment for dialogue scenarios. ""

""Z1-Rumination is trained through scaling end-to-end reinforcement learning with responses graded by the ground truth answers or rubrics and can make use of search tools during its deep thinking process to handle complex tasks.""",15000000000000.0,15T (base model) + RL,,,,Confident,"The GLM family welcomes a new generation of open-source models, the GLM-4-32B-0414 series, featuring 32 billion parameters. Its performance is comparable to OpenAI's GPT series and DeepSeek's V3/R1 series, and it supports very user-friendly local deployment features. GLM-4-32B-Base-0414 was pre-trained on 15T of high-quality data, including a large amount of reasoning-type synthetic data, laying the foundation for subsequent reinforcement learning extensions. In the post-training stage, in addition to human preference alignment for dialogue scenarios, we also enhanced the model's performance in instruction following, engineering code, and function calling using techniques such as rejection sampling and reinforcement learning, strengthening the atomic capabilities required for agent tasks. GLM-4-32B-0414 achieves good results in areas such as engineering code, Artifact generation, function calling, search-based Q&A, and report generation. Some benchmarks even rival larger models like GPT-4o and DeepSeek-V3-0324 (671B).

GLM-Z1-Rumination-32B-0414 is a deep reasoning model with rumination capabilities (benchmarked against OpenAI's Deep Research). Unlike typical deep thinking models, the rumination model employs longer periods of deep thought to solve more open-ended and complex problems (e.g., writing a comparative analysis of AI development in two cities and their future development plans). The rumination model integrates search tools during its deep thinking process to handle complex tasks and is trained by utilizing multiple rule-based rewards to guide and extend end-to-end reinforcement learning. Z1-Rumination shows significant improvements in research-style writing and complex retrieval tasks.",China,,,,,,Unreleased,"MIT license

https://huggingface.co/THUDM/GLM-Z1-Rumination-32B-0414",,,
GLM-4-9B-0414,Language,"Language modeling/generation,Question answering,Quantitative reasoning,Code generation",,Open weights (unrestricted),https://huggingface.co/THUDM/GLM-4-9B-0414,,GLM-4-9B-0414,2025-04-14,Tsinghua University,9000000000.0,9B,8.1e+23,"Assuming it was trained on the same 15T dataset as 32B model:

6 FLOP / parameter / token * 9 * 10^9 parameters * 15 * 10^12 tokens = 8.1e+23 FLOP 

""Likely"" confidence due to the uncertain dataset size",Unspecified unreleased,,,,,,,Likely,"Finally, GLM-Z1-9B-0414 is a surprise. We employed all the aforementioned techniques to train a small model (9B). GLM-Z1-9B-0414 exhibits excellent capabilities in mathematical reasoning and general tasks. Its overall performance is top-ranked among all open-source models of the same size. Especially in resource-constrained scenarios, this model achieves an excellent balance between efficiency and effectiveness, providing a powerful option for users seeking lightweight deployment.",China,,,,,,Unreleased,"MIT license

https://huggingface.co/THUDM/GLM-4-9B-0414",,,
GLM-4-32B-0414,Language,"Language modeling/generation,Question answering,Code generation,Search",,Open weights (unrestricted),https://huggingface.co/zai-org/GLM-4-32B-0414,,GLM-4-32B-0414,2025-04-14,"Z.ai (Zhipu AI),Tsinghua University",32000000000.0,32B,2.88e+24,6 FLOP / parameter / token * 32 * 10^9 parameters * 15 * 10^12 tokens = 2.88e+24 FLOP,Unspecified unreleased,,15000000000000.0,"""pre-trained on 15T of high-quality data, including substantial reasoning-type synthetic data.""",,,,Confident,"The GLM family welcomes new members, the GLM-4-32B-0414 series models, featuring 32 billion parameters. Its performance is comparable to OpenAIâ€™s GPT series and DeepSeekâ€™s V3/R1 series. It also supports very user-friendly local deployment features. GLM-4-32B-Base-0414 was pre-trained on 15T of high-quality data, including substantial reasoning-type synthetic data. This lays the foundation for subsequent reinforcement learning extensions. In the post-training stage, we employed human preference alignment for dialogue scenarios. Additionally, using techniques like rejection sampling and reinforcement learning, we enhanced the modelâ€™s performance in instruction following, engineering code, and function calling, thus strengthening the atomic capabilities required for agent tasks. GLM-4-32B-0414 achieves good results in engineering code, Artifact generation, function calling, search-based Q&A, and report generation. In particular, on several benchmarks, such as code generation or specific Q&A tasks, GLM-4-32B-Base-0414 achieves comparable performance with those larger models like GPT-4o and DeepSeek-V3-0324 (671B).","China,China",,,,,,Unreleased,"MIT license
https://huggingface.co/zai-org/GLM-4-32B-0414

Apche 2.0
https://github.com/zai-org/GLM-4",,,
SenseNova V6,"Multimodal,Language,Video,Vision","Language modeling/generation,Quantitative reasoning,Question answering,Visual question answering,Video description,Character recognition (OCR),Code generation,Chat",,API access,"https://www.prnewswire.com/apac/news-releases/sensetimes-sensenova-v6-chinas-most-advanced-multimodal-model-with-the-lowest-cost-in-the-industry-302426998.html
https://platform.sensenova.cn/technology/fusion/Pro/",,SenseTime's SenseNova V6: China's Most Advanced Multimodal Model with the Lowest Cost in the Industry,2025-04-12,SenseTime,600000000000.0,"""Mixture of Experts (MoE)-based multimodal general foundation model with over 600 billion parameters,""",,,,,,,,,,Confident,"HONG KONG, April 12, 2025 /PRNewswire/ -- SenseTime launched its newly upgraded large model series, SenseNova V6, at its Tech Day event held in several locations, including Shanghai and Shenzhen. Leveraging advances in the training of multimodal long chain-of-thought (CoT), global memory, and reinforcement learning, the model delivers industry-leading multimodal reasoning capabilities while setting a new benchmark for cost efficiency.

The capabilities of the SenseNova V6 model have been greatly enhanced, with strong advantages in long CoT, reasoning, mathematical capabilities, and global memory. Its multimodal reasoning capabilities ranked first in China when benchmarked against GPT-o1, while its data analysis performance outpaced GPT-4o. It also combines high performance with cost efficiency. Its multimodal training efficiency is aligned with that of language models, providing the lowest training costs in the industry. Its reasoning costs are also the lowest in the industry. The new lightweight full-modal interactive model, SenseNova V6 Omni, delivers the most advanced multimodal interactive capabilities in China. It is China's first large model that supports in-depth analysis of 10-minute mid-to-long form videos, benchmarked against Gemini 2.5 Turbo to be among the strongest in its class",Hong Kong,,,,,,Unreleased,"""The SenseChat app is available for preview and SenseNova V6 is now available for trial via the SenseChat web platform at https://chat.sensetime.com/wb/chat.""

API: https://platform.sensenova.cn/technology/fusion/Pro/",,,
Seaweed-7B,Video,"Video generation,Text-to-video,Image-to-video,Audio generation","Team Seawead, Ceyuan Yang, Zhijie Lin, Yang Zhao, Shanchuan Lin, Zhibei Ma, Haoyuan Guo, Hao Chen, Lu Qi, Sen Wang, Feng Cheng, Feilong Zuo Xuejiao Zeng, Ziyan Yang, Fangyuan Kong, Zhiwu Qing, Fei Xiao, Meng Wei, Tuyen Hoang, Siyu Zhang, Peihao Zhu, Qi Zhao, Jiangqiao Yan, Liangke Gui, Sheng Bi, Jiashi Li, Yuxi Ren, Rui Wang, Huixia Li, Xuefeng Xiao, Shu Liu, Feng Ling, Heng Zhang, Houmin Wei, Huafeng Kuang, Jerry Duncan, Junda Zhang, Junru Zheng, Li Sun, Manlin Zhang, Renfei Sun, Xiaobin Zhuang, Xiaojie Li, Xin Xia, Xuyan Chi, Yanghua Peng, Yuping Wang, Yuxuan Wang, Zhongkai Zhao, Zhuo Chen, Zuquan Song, Zhenheng Yang, Jiashi Feng, Jianchao Yang, Lu Jiang",Unreleased,https://arxiv.org/abs/2504.08685,61.0,"Seaweed-7B: Cost-Effective Training of Video Generation Foundation Model
",2025-04-11,ByteDance,7000000000.0,,9.0007697e+23,"989400000000000 FLOP / GPU / sec [H100 reported, bf16 assumed] * 665000 GPU-hours * 3600 sec / hour * 0.38 [reported utilization] = 9.0007697e+23 FLOP",,,,,,"We train the model from scratch using 665,000 H100 GPU hours, equivalent to 27.7 days of training on 1,000 H100 GPUs.",NVIDIA H100 SXM5 80GB,Confident,"This technical report presents a cost-efficient strategy for training a video generation foundation model. We present a mid-sized research model with approximately 7 billion parameters (7B) called Seaweed-7B trained from scratch using 665,000 H100 GPU hours. Despite being trained with moderate computational resources, Seaweed-7B demonstrates highly competitive performance compared to contemporary video generation models of much larger size. Design choices are especially crucial in a resource-constrained setting. This technical report highlights the key design decisions that enhance the performance of the medium-sized diffusion model. Empirically, we make two observations: (1) Seaweed-7B achieves performance comparable to, or even surpasses, larger models trained on substantially greater GPU resources, and (2) our model, which exhibits strong generalization ability, can be effectively adapted across a wide range of downstream applications either by lightweight fine-tuning or continue training. See the project page at this https URL",China,,,,,0.38,Unreleased,,,,
Pangu Ultra,Language,"Code generation,Language modeling/generation","Yichun Yin, Wenyong Huang, Kaikai Song, Yehui Tang, Xueyu Wu, Wei Guo, Peng Guo, Yaoyuan Wang, Xiaojun Meng, Yasheng Wang, Dong Li, Can Chen, Dandan Tu, Yin Li, Fisher Yu, Ruiming Tang, Yunhe Wang, Baojun Wang, Bin Wang, Bo Wang, Boxiao Liu, Changzheng Zhang, Duyu Tang, Fei Mi, Hui Jin, Jiansheng Wei, Jiarui Qin, Jinpeng Li, Jun Zhao, Liqun Deng, Lin Li, Minghui Xu, Naifu Zhang, Nianzu Zheng, Qiang Li, Rongju Ruan, Shengjun Cheng, Tianyu Guo, Wei He, Wei Li, Weiwen Liu, Wulong Liu, Xinyi Dai, Yonghan Dong, Yu Pan, Yue Li, Yufei Wang, Yujun Li, Yunsheng Ni, Zhe Liu, Zhenhe Zhang, Zhicheng Liu",Hosted access (no API),https://arxiv.org/abs/2504.07866,12.0,Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs,2025-04-10,Huawei,135000000000.0,,1.0692e+25,"When compared to Llama 3.1 405B, Pangu Ultra achieves better scores on most of the challenging benchmarks, while utilizing only about 29% of the training FLOPs required by Llama 405B.

Compute = 6 FLOP/token/param *  135e9 params *13.2e12 tokens = 1.069200e+25 FLOP
This is consistent with 29% of Llama 405B's compute: 3.8e25*0.29=1.1e25.",Unspecified unreleased,,13200000000000.0, 13.2 trillion tokens,,,Huawei Ascend 910B,Confident,"We present Pangu Ultra, a Large Language Model (LLM) with 135 billion parameters and dense Transformer modules trained on Ascend Neural Processing Units (NPUs). Although the field of LLM has been witnessing unprecedented advances in pushing the scale and capability of LLM in recent years, training such a large-scale model still involves significant optimization and system challenges. To stabilize the training process, we propose depth-scaled sandwich normalization, which effectively eliminates loss spikes during the training process of deep models. We pre-train our model on 13.2 trillion diverse and high-quality tokens and further enhance its reasoning capabilities during post-training. To perform such large-scale training efficiently, we utilize 8,192 Ascend NPUs with a series of system optimizations. Evaluations on multiple diverse benchmarks indicate that Pangu Ultra significantly advances the state-of-the-art capabilities of dense LLMs such as Llama 405B and Mistral Large 2, and even achieves competitive results with DeepSeek-R1, whose sparse model structure contains much more parameters. Our exploration demonstrates that Ascend NPUs are capable of efficiently and effectively training dense models with more than 100 billion parameters. Our model and system will be available for our commercial customers.",China,,,,8192.0,0.52,Unreleased,"""Our model and system will be available for our commercial customers""",,,
Gen-4 Turbo,"Video,Vision","Video generation,Image-to-video",,API access,https://help.runwayml.com/hc/en-us/articles/37327109429011-Creating-with-Gen-4-Video,,"Introducing Runway Gen-4
Our next-generation series of AI models for media generation and world consistency.",2025-04-09,Runway,,,,,Unspecified unreleased,,,,,,,Unknown,"Gen-4 Turbo is now available on the Runway API, allowing developers to easily integrate our most powerful and efficient video model directly into their apps and products.

Gen-4 Turbo offers the same price, scale and reliability of Gen-3 Alpha Turbo but with the state-of-the-art video generation capabilities of Gen-4.",United States of America,,,,,,Unreleased,,,,
QWQ-Plus,Language,"Language modeling/generation,Question answering,Quantitative reasoning,Code generation",,API access,https://www.alibabacloud.com/help/en/model-studio/qwq,,,2025-04-08,Alibaba,,,,,,,,,,,,Unknown,,China,,,,,,Unreleased,,,,
Amazon Nova Sonic,Speech,"Speech-to-text,Speech recognition (ASR),Speech synthesis,Speech-to-speech,Audio question answering",Danilo Poccia ,API access,https://aws.amazon.com/blogs/aws/introducing-amazon-nova-sonic-human-like-voice-conversations-for-generative-ai-applications/,,Introducing Amazon Nova Sonic: Human-like voice conversations for generative AI applications,2025-04-08,Amazon,,,,,Unspecified unreleased,,,,,,,Confident,"Amazon Nova Sonic is a proprietary foundation model that unifies speech understanding and generation capabilities into one model, to enable human-like voice conversations with artificial intelligence (AI) applications. Customers can use Amazon Nova Sonic to develop voice-based applications, such as customer service call automation and conversational AI agents across a broad range of industries, including travel, education, entertainment, and more. Customers can integrate these applications with Amazon Nova Sonic for real time speech-to-speech conversational AI using Amazon Bedrockâ€™s bidirectional streaming API.",United States of America,,,,,,Unreleased,,,,
Amazon Nova Reel,"Video,Vision","Video generation,Text-to-video,Image-to-video",,API access,"https://docs.aws.amazon.com/ai/responsible-ai/nova-reel/overview.html

https://assets.amazon.science/96/7d/0d3e59514abf8fdcfafcdc574300/nova-tech-report-20250317-0810.pdf",,Amazon Nova Reel is a video generation model that supports the generation of short videos from input text and images. Amazon Nova Reel provides camera motion controls using natural language inputs.,2025-04-07,Amazon,,,,,Unspecified unreleased,"""We curated a diverse set of prompts designed to capture various aspects of video generation. The prompts are distributed across 6 broad categories: human and activities, animals, natural scenery and landscapes, indoor scenes, objects interactions, and creative scenes and activities""",,,,,"Amazon Trainium1,NVIDIA A100,NVIDIA H100 SXM5 80GB",Unknown,"Amazon Nova Reel is a proprietary multimodal foundation model (FM) designed for enterprise use cases. Amazon Nova Reel generates a novel video from a descriptive natural language text string and an optional reference image (together, the â€œpromptâ€). Customers can use Amazon Nova Reel to create content within advertising, branding, product design, and social media workflows.
This AI Service Card applies to the use of Amazon Nova Reel via Amazon Bedrock Console and Amazon Bedrock API. Typically, customers use the Console to develop and test applications,
and the API for production loads at scale. Each Nova model is a managed subservice of Amazon Bedrock; customers can focus on executing prompts without having to provision or manage any
infrastructure such as instance types, network topology, and endpoints.
An Amazon Nova Reel <text prompt, <optional image prompts>, generated video> triple is said to be ""effective"" if a skilled human evaluator decides that the generated video: 1/ has the content
requested by the input prompts (the combination of text and optional image prompts); 2/ makes reasonable assumptions about elements not specified in the input prompts (for example, if asked
for a video of a kitchen, a refrigerator and microwave are present and not a couch or a tiger); 3/ is free from defects or image composition errors (for example, human body parts are attached in the correct places and objects are not warped); and 4/ is consistent with the standards of safety, fairness, and other properties valued by the evaluator. Otherwise, a triple is said to be ""ineffective.""
A customer's workflow must decide if a generated video is effective using human judgment, whether human judgement is applied on a case-by-case basis (as happens when the Console is
used as a productivity tool by itself) or is applied via the customer's choice of an acceptable score on an automated test",United States of America,,,,,,Unreleased,,,,
Llama 4 Scout,"Multimodal,Language,Vision","Chat,Code generation,Visual question answering,Language modeling/generation,Question answering",,Open weights (restricted use),https://ai.meta.com/blog/llama-4-multimodal-intelligence/,,The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation,2025-04-05,Meta AI,109000000000.0,"""Our smaller model, Llama 4 Scout, is a general purpose model with 17 billion active parameters, 16 experts, and 109 billion total parameters that delivers state-of-the-art performance for its class.""",4.08e+24,"40T training tokens per model card:

https://github.com/meta-llama/llama-models/blob/main/models/llama4/MODEL_CARD.md  

Estimating training compute from parameters and tokens:
6 FLOP per token per parameter * 17B active parameters * 40T tokens = 4.08e24 FLOP
(Implying mean throughput was 227 TFLOPS/GPU, or 11.5% MFU in FP8)


The model card also states that Llama 4 Scout used 5.0M H100-hours.
The blog post gives a figure of 390 TFLOPS/GPU, but this may have been the utilization rate for Behemoth, or all of the models together. Using this utilization, we have:
Compute = 390 TFLOP/s * 5 million hours = 7.02e24 FLOP
(This value is higher than the compute implied by parameters and tokens, and suggests utilization may have been lower for Scout than for Behemoth.)",,"Knowledge cutoff date is August 2024, according to https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E.",30000000000000.0,"""The overall data mixture for training consisted of more than 30 trillion tokens, which is more than double the Llama 3 pre-training mixture and includes diverse text, image, and video datasets.""",,,,Likely,"Weâ€™re sharing the first models in the Llama 4 herd, which will enable people to build more personalized multimodal experiences.
Llama 4 Scout, a 17 billion active parameter model with 16 experts, is the best multimodal model in the world in its class and is more powerful than all previous generation Llama models, while fitting in a single NVIDIA H100 GPU. Additionally, Llama 4 Scout offers an industry-leading context window of 10M and delivers better results than Gemma 3, Gemini 2.0 Flash-Lite, and Mistral 3.1 across a broad range of widely reported benchmarks.
Llama 4 Maverick, a 17 billion active parameter model with 128 experts, is the best multimodal model in its class, beating GPT-4o and Gemini 2.0 Flash across a broad range of widely reported benchmarks, while achieving comparable results to the new DeepSeek v3 on reasoning and codingâ€”at less than half the active parameters. Llama 4 Maverick offers a best-in-class performance to cost ratio with an experimental chat version scoring ELO of 1417 on LMArena.
These models are our best yet thanks to distillation from Llama 4 Behemoth, a 288 billion active parameter model with 16 experts that is our most powerful yet and among the worldâ€™s smartest LLMs. Llama 4 Behemoth outperforms GPT-4.5, Claude Sonnet 3.7, and Gemini 2.0 Pro on several STEM benchmarks. Llama 4 Behemoth is still training, and weâ€™re excited to share more details about it even while itâ€™s still in flight.
Download the Llama 4 Scout and Llama 4 Maverick models today on llama.com and Hugging Face. Try Meta AI built with Llama 4 in WhatsApp, Messenger, Instagram Direct, and on the web.",United States of America,,,,,,Unreleased,"Llama 4 license (branding requirements, size cap 700M MAU)
https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E

no training code here 
https://github.com/meta-llama/llama-models/tree/main/models/llama4",,,
Llama 4 Maverick,"Multimodal,Language,Vision","Chat,Code generation,Visual question answering,Language modeling/generation,Question answering",,Open weights (restricted use),https://ai.meta.com/blog/llama-4-multimodal-intelligence/,,The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation,2025-04-05,Meta AI,400000000000.0,"""Llama 4 Maverick models have 17B active parameters and 400B total parameters.""

https://ai.meta.com/blog/llama-4-multimodal-intelligence/",2.244000000001e+24,"22T training tokens per model card:

https://github.com/meta-llama/llama-models/blob/main/models/llama4/MODEL_CARD.md   

Maverick was trained using co-distillation from Llama 4 Behemoth. It isn't 100% clear that all 22T tokens used distillation, but we assume this for the time being.

Estimating training compute from parameters and tokens:
Compute = 6 FLOP per token per parameter * 17B active parameters * 22T tokens = 2.244e24 FLOP
(Implying mean throughput was 262 TFLOPS/GPU, or 13.2% MFU in FP8)


The model card also states that Llama 4 Maverick used 2.38M H100-hours.
The blog post gives a figure of 390 TFLOPS/GPU, but this may have been the utilization rate for Behemoth, or all of the models together. Using this utilization, we have:
Compute = 390 TFLOP/s * 2.38 million hours = 3.342e24 FLOP
(This value is higher than the compute implied by parameters and tokens, and suggests utilization may have been lower for Maverick than for Behemoth.)",,"Knowledge cutoff date is August 2024, according to https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E.",30000000000000.0,"""The overall data mixture for training consisted of more than 30 trillion tokens, which is more than double the Llama 3 pre-training mixture and includes diverse text, image, and video datasets.""",,,,Likely,"Weâ€™re sharing the first models in the Llama 4 herd, which will enable people to build more personalized multimodal experiences.
Llama 4 Scout, a 17 billion active parameter model with 16 experts, is the best multimodal model in the world in its class and is more powerful than all previous generation Llama models, while fitting in a single NVIDIA H100 GPU. Additionally, Llama 4 Scout offers an industry-leading context window of 10M and delivers better results than Gemma 3, Gemini 2.0 Flash-Lite, and Mistral 3.1 across a broad range of widely reported benchmarks.
Llama 4 Maverick, a 17 billion active parameter model with 128 experts, is the best multimodal model in its class, beating GPT-4o and Gemini 2.0 Flash across a broad range of widely reported benchmarks, while achieving comparable results to the new DeepSeek v3 on reasoning and codingâ€”at less than half the active parameters. Llama 4 Maverick offers a best-in-class performance to cost ratio with an experimental chat version scoring ELO of 1417 on LMArena.
These models are our best yet thanks to distillation from Llama 4 Behemoth, a 288 billion active parameter model with 16 experts that is our most powerful yet and among the worldâ€™s smartest LLMs. Llama 4 Behemoth outperforms GPT-4.5, Claude Sonnet 3.7, and Gemini 2.0 Pro on several STEM benchmarks. Llama 4 Behemoth is still training, and weâ€™re excited to share more details about it even while itâ€™s still in flight.
Download the Llama 4 Scout and Llama 4 Maverick models today on llama.com and Hugging Face. Try Meta AI built with Llama 4 in WhatsApp, Messenger, Instagram Direct, and on the web.",United States of America,,,,,,Unreleased,"Llama 4 license (branding requirements, size cap 700M MAU)
https://huggingface.co/meta-llama/Llama-4-Maverick-17B-128E-Original

no training code here 
https://github.com/meta-llama/llama-models/tree/main/models/llama4",,,
Llama 4 Behemoth (preview),"Multimodal,Language,Vision","Chat,Code generation,Visual question answering,Translation,Language modeling/generation,Quantitative reasoning,Question answering",,Unreleased,https://ai.meta.com/blog/llama-4-multimodal-intelligence/,,The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation,2025-04-05,Meta AI,2000000000000.0,"""Llama 4 Behemoth, a 288 billion active parameter model with 16 experts that is our most powerful yet and among the worldâ€™s smartest LLMs.""",5.18400000000001e+25,"Behemoth's training dataset is at least 30T tokens:
https://ai.meta.com/blog/llama-4-multimodal-intelligence/ 

6 FLOP / parameter / token * 288 * 10^9 activated parameters * 30 * 10^12 tokens = 5.184e+25 FLOP",,,30000000000000.0,"""The overall data mixture for training consisted of more than 30 trillion tokens, which is more than double the Llama 3 pre-training mixture and includes diverse text, image, and video datasets.""",,"Based on the model cards for Llama 4 Scout and Maverick, they seem to be using H100-80GB GPUs, despite the article saying that 390 TFLOPS/GPU was a high MFU (it is high throughput, but <20% MFU in FP8).",NVIDIA H100 SXM5 80GB,Likely,"Weâ€™re sharing the first models in the Llama 4 herd, which will enable people to build more personalized multimodal experiences.
...
Llama 4 Behemoth, a 288 billion active parameter model with 16 experts that is our most powerful yet and among the worldâ€™s smartest LLMs. Llama 4 Behemoth outperforms GPT-4.5, Claude Sonnet 3.7, and Gemini 2.0 Pro on several STEM benchmarks. Llama 4 Behemoth is still training, and weâ€™re excited to share more details about it even while itâ€™s still in flight.
Download the Llama 4 Scout and Llama 4 Maverick models today on llama.com and Hugging Face. Try Meta AI built with Llama 4 in WhatsApp, Messenger, Instagram Direct, and on the web.",United States of America,,,,32000.0,,Unreleased,"""While weâ€™re not yet releasing Llama 4 Behemoth as it is still training""",,,1780094856.8197687
Amazon Nova Act,Other,"System control,Instruction interpretation",Amazon AGI,API access,https://labs.amazon.science/blog/nova-act,,Introducing Amazon Nova Act,2025-03-31,Amazon,,,,,Unspecified unreleased,,,,,,,Unknown,"Nova Act is an early research preview of an SDK + model for building agents designed to reliably take actions in web browsers. Building with the SDK enables developers to break down complex workflows into smaller, reliable, commands, add more detail where needed, call APIs, and intersperse direct browser manipulation. Developers can interleave Python code, whether it be tests, breakpoints, asserts, or threadpooling for parallelization. Read more about the announcement: https://labs.amazon.science/blog/nova-act.",United States of America,,,,,,Unreleased,"Apache 2.0 for inference code:
https://github.com/aws/nova-act",,,
Gen-4,"Video,Vision","Video generation,Image-to-video,Text-to-video",,API access,"https://runwayml.com/research/introducing-runway-gen-4

https://help.runwayml.com/hc/en-us/articles/37327109429011-Creating-with-Gen-4-Video",,"Introducing Runway Gen-4
Our next-generation series of AI models for media generation and world consistency.",2025-03-31,Runway,,,,,Unspecified unreleased,,,,,,,Unknown,"Gen-4 sets a new standard for video generation and is a marked improvement over Gen-3 Alpha. It excels in its ability to generate highly dynamic videos with realistic motion as well as subject, object and style consistency with superior prompt adherence and best-in-class world understanding. 
Using visual references, combined with instructions, Gen-4 allows you to create new images and videos with consistent styles, subjects, locations and more. Allowing for continuity and control within your stories. ",United States of America,,,,,,Unreleased,,,,
QVQ-Max,"Language,Vision,Multimodal","Visual question answering,Video description,Language modeling/generation,Translation,Question answering,Character recognition (OCR),Quantitative reasoning","Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, Junyang Lin",,https://qwenlm.github.io/blog/qvq-max-preview/,,QVQ-Max: Think with Evidence,2025-03-28,Alibaba,,,,,Unspecified unreleased,,,,,,,Confident,"Last December, we launched QVQ-72B-Preview as an exploratory model, but it had many issues. Today, we are officially releasing the first version of QVQ-Max, our visual reasoning model. This model can not only â€œunderstandâ€ the content in images and videos but also analyze and reason with this information to provide solutions. From math problems to everyday questions, from programming code to artistic creation, QVQ-Max has demonstrated impressive capabilities. Though this is just our first version, its potential is already eye-catching.",China,,,,,,Unreleased,,,,
Ideogram 3.0,Image generation,"Image generation,Text-to-image",,API access,https://about.ideogram.ai/3.0,,"Meet Ideogram 3.0 â€” stunning realism, creative designs, and consistent styles, all in one powerful model. Now available to all users on ideogram.ai and our iOS app!",2025-03-26,Ideogram,,,,"Unlikely to exceed 10^25 FLOP as the developer raised $80M (https://betakit.com/midjourney-competitor-ideogram-closes-80-million-series-a-round-as-it-launches-latest-text-to-image-model/), and was unlikely to spend a significant fraction of that on a 10^25 FLOP training run.",Unspecified unreleased,,,,,,,Unknown,"Ideogram 3.0 pushes the boundaries of generative media through significant advancements in image-prompt alignment, photorealism, and text rendering quality. In human evaluations, Ideogram 3.0 consistently outperforms other text-to-image models, scoring highest in ELO rating over a set of diverse prompts that probe a wide variety of capabilities, subjects, styles, use cases, and composition difficulty.",Canada,,,,,,Unreleased,,,,
Gemini 2.5 Pro (Mar 2025),"Language,Vision,Video,Multimodal,Speech","Language modeling/generation,Question answering,Code generation,Quantitative reasoning,Visual question answering,Translation,Image captioning,Video description,Speech recognition (ASR)",,API access,https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/#gemini-2-5-thinking,,Gemini 2.5: Our most intelligent AI model,2025-03-25,Google DeepMind,,,,Flagship model from a leading developer in mid-2025; very likely it used >1e25 FLOP.,Unspecified unreleased,Knowledge cutoff	January 2025,,,,,,Unknown,"Gemini 2.5 Pro Experimental is our most advanced model for complex tasks. It tops the LMArena leaderboard â€” which measures human preferences â€” by a significant margin, indicating a highly capable model equipped with high-quality style. 2.5 Pro also shows strong reasoning and code capabilities, leading on common coding, math and science benchmarks.

Gemini 2.5 Pro is available now in Google AI Studio and in the Gemini app for Gemini Advanced users, and will be coming to Vertex AI soon. Weâ€™ll also introduce pricing in the coming weeks, enabling people to use 2.5 Pro with higher rate limits for scaled production use.","United States of America,United Kingdom of Great Britain and Northern Ireland",,,,,,Unreleased,"Availability	
Google AI Studio
Gemini API
Gemini App",,,
Llama Nemotron Ultra 253B,Language,"Language modeling/generation,Question answering,Quantitative reasoning,Code generation,Neural Architecture Search - NAS","Akhiad Bercovich, Itay Levy, Izik Golan, Mohammad Dabbah, Ran El-Yaniv, Omri Puny, Ido Galil, Zach Moshe, Tomer Ronen, Najeeb Nabwani, Ido Shahaf, Oren Tropp, Ehud Karpas, Ran Zilberstein, Jiaqi Zeng, Soumye Singhal, Alexander Bukharin, Yian Zhang, Tugrul Konuk, Gerald Shen, Ameya Sunil Mahabaleshwarkar, Bilal Kartal, Yoshi Suhara, Olivier Delalleau, Zijia Chen, Zhilin Wang, David Mosallanezhad, Adi Renduchintala, Haifeng Qian, Dima Rekesh, Fei Jia, Somshubra Majumdar, Vahid Noroozi, Wasi Uddin Ahmad, Sean Narenthiran, Aleksander Ficek, Mehrzad Samadi, Jocelyn Huang, Siddhartha Jain, Igor Gitman, Ivan Moshkov, Wei Du, Shubham Toshniwal, George Armstrong, Branislav Kisacanin, Matvei Novikov, Daria Gitman, Evelina Bakhturina, Jane Polak Scowcroft, John Kamalu, Dan Su, Kezhi Kong, Markus Kliegl, Rabeeh Karimi, Ying Lin, Sanjeev Satheesh, Jupinder Parmar, Pritam Gundecha, Brandon Norick, Joseph Jennings, Shrimai Prabhumoye, Syeda Nahida Akter, Mostofa Patwary, Abhinav Khattar, Deepak Narayanan, Roger Waleffe, Jimmy Zhang, Bor-Yiing Su, Guyue Huang, Terry Kong, Parth Chadha, Sahil Jain, Christine Harvey, Elad Segal, Jining Huang, Sergey Kashirsky, Robert McQueen, Izzy Putterman, George Lam, Arun Venkatesan, Sherry Wu, Vinh Nguyen, Manoj Kilaru, Andrew Wang, Anna Warno, Abhilash Somasamudramath, Sandip Bhaskar, Maka Dong, Nave Assaf, Shahar Mor, Omer Ullman Argov, Scot Junkin, Oleksandr Romanenko, Pedro Larroy, Monika Katariya, Marco Rovinelli, Viji Balas, Nicholas Edelman, Anahita Bhiwandiwalla, Muthu Subramaniam et al. (32 additional authors not shown)",Open weights (restricted use),https://arxiv.org/abs/2505.00949,,Ultra is 253B distilled from Llama 3.1 405B for maximum agentic accuracy on multi-GPU data center servers.,2025-03-18,NVIDIA,253000000000.0,"253B
""Dense decoder-only Transformer model Network Architecture: Llama-3.1-405B-Instruct, customized through Neural Architecture Search (NAS)

**This model was developed based on Llama-3.1-405B-Instruct
** This model has 253B model parameters.""",3.911001e+25,"Total training compute: 3.8e+25 FLOP (base model) + 1.11e+24 FLOP (fine-tuning) = 3.9e25 FLOP
See calculation in the finetune compute notes.","Unspecified unreleased,Llama Nemotron Post Training Dataset",,603000000000.0,"KD + Continued Training: 
""LN-Ultra is first trained with knowledge distillation for 65B tokens using the same distillation dataset, followed by 88B tokens of continued training on the Nemotron-H phase 4 pretraining dataset (NVIDIA et al., 2025)."" (from the paper)

Reasoning training data (SFT): 
for Super model (from the blog) ""60B tokens of synthetic data (representing 4M of the 30M generated samples)"" -> the entire dataset is ~450B tokens (Ultra model is likely to be trained on the entire dataset (""Likely"" confidence}

65b+88b+450b = 603b tokens

RL for Scientific Reasoning:140k h100 hours (240k samples)

RL for instruction following: 30k prompts

RL for chat: 50k prompts",,,,Likely,"Llama-3.1-Nemotron-Ultra-253B-v1 is a large language model (LLM) which is a derivative of Meta Llama-3.1-405B-Instruct (AKA the reference model). It is a reasoning model that is post trained for reasoning, human chat preferences, and tasks, such as RAG and tool calling. The model supports a context length of 128K tokens. This model fits on a single 8xH100 node for inference.

Llama-3.1-Nemotron-Ultra-253B-v1 is a model which offers a great tradeoff between model accuracy and efficiency. Efficiency (throughput) directly translates to savings. Using a novel Neural Architecture Search (NAS) approach, we greatly reduce the modelâ€™s memory footprint, enabling larger workloads, as well as reducing the number of GPUs required to run the model in a data center environment. This NAS approach enables the selection of a desired point in the accuracy-efficiency tradeoff. Furthermore, by using a novel method to vertically compress the model (see details here), it also offers a significant improvement in latency.

The model underwent a multi-phase post-training process to enhance both its reasoning and non-reasoning capabilities. This includes a supervised fine-tuning stage for Math, Code, Reasoning, Chat, and Tool Calling as well as multiple reinforcement learning (RL) stages using Group Relative Policy Optimization (GRPO) algorithms for reasoning, chat, and instruction-following.",United States of America,Llama 3.1-405B,1.114817000000001e+24,"Knowledge Distillation + Continued pre-training + SFT: 

6 FLOP / parameter / token * 253000000000 parameters * 6033000000000 tokens [see dataset size notes] = 9.15354e+23 FLOP

RL: ""the whole training takes approximately 140k H100 hours""

989400000000000 FLOP / sec / GPU [bf16] * 140000 GPU-hours * 3600 sec / hour * 0.4 [assumed utilization] = 1.9946304e+23 FLOP

Total: 9.15354e+23 FLOP + 1.9946304e+23 FLOP = 1.114817e+24 FLOP
",,,Unreleased,"GOVERNING TERMS: Your use of this model is governed by the NVIDIA Open Model License. Additional Information: Llama 3.3 Community License Agreement. Built with Llama.

https://huggingface.co/nvidia/Llama-3_1-Nemotron-Ultra-253B-v1",,,
Cosmos-Transfer1-7B,"Video,Vision,Robotics","Robotic manipulation,System control,Video generation","NVIDIA: Hassan Abu Alhaija, Jose Alvarez, Maciej Bala, Tiffany Cai, Tianshi Cao, Liz Cha, Joshua Chen, Mike Chen, Francesco Ferroni, Sanja Fidler, Dieter Fox, Yunhao Ge, Jinwei Gu, Ali Hassani, Michael Isaev, Pooya Jannaty, Shiyi Lan, Tobias Lasser, Huan Ling, Ming-Yu Liu, Xian Liu, Yifan Lu, Alice Luo, Qianli Ma, Hanzi Mao, Fabio Ramos, Xuanchi Ren, Tianchang Shen, Xinglong Sun, Shitao Tang, Ting-Chun Wang, Jay Wu, Jiashu Xu, Stella Xu, Kevin Xie, Yuchong Ye, Xiaodong Yang, Xiaohui Zeng, Yu Zeng",Open weights (restricted use),https://arxiv.org/abs/2503.14492,,Cosmos-Transfer1: Conditional World Generation with Adaptive Multimodal Control,2025-03-18,NVIDIA,7000000000.0,7B,3.6059017e+24,1.4e+24 FLOP [base model compute] + 2.205901651968e+24 FLOP = 3.6059017e+24 FLOP,Unspecified unreleased,,,,2016.0,"""We train each control branch with 1024 NVIDIA H100 GPUs for a period of 2 to 4 weeks depending on the modality""

4 modalities -> ~12 weeks of total training = 2016 hours",NVIDIA H100 SXM5 80GB,Likely,"We introduce Cosmos-Transfer, a conditional world generation model that can generate world simulations based on multiple spatial control inputs of various modalities such as segmentation, depth, and edge. In the design, the spatial conditional scheme is adaptive and customizable. It allows weighting different conditional inputs differently at different spatial locations. This enables highly controllable world generation and finds use in various world-to-world transfer use cases, including Sim2Real. We conduct extensive evaluations to analyze the proposed model and demonstrate its applications for Physical AI, including robotics Sim2Real and autonomous vehicle data enrichment. We further demonstrate an inference scaling strategy to achieve real-time world generation with an NVIDIA GB200 NVL72 rack. To help accelerate research development in the field, we open-source our models and code at https://github.com/nvidia-cosmos/cosmos-transfer1.",United States of America,Cosmos-Predict1-7b-Video2World,2.205901651968e+24,"989400000000000 FLOP/GPU/sec * 2016 hours [""Likely"" confidence, see training time notes] * 3600 sec / hour * 1024 GPUs * 0.3 [assumed utilization] = 2.205901651968e+24 FLOP",1024.0,,Unreleased,"NVIDIA license (termination clause + attribution requirements) 
https://huggingface.co/nvidia/Cosmos-Transfer1-7B",,,
Chirp 3 Speech-to-Text,Speech,"Speech recognition (ASR),Speech-to-text,Translation",,Hosted access (no API),https://cloud.google.com/speech-to-text/v2/docs/chirp_3-model,,Chirp 3: Enhanced multilingual accuracy,2025-03-17,"Google,Google DeepMind",,,,,Unspecified unreleased,,,,,,,Unknown,"Chirp 3 is the latest generation of Google's multilingual ASR-specific generative models, designed to meet user needs based on feedback and experience. It improves upon the original Chirp and Chirp 2 models in accuracy and speed, as well as expanding into key new features like diarization.","United States of America,United States of America,United Kingdom of Great Britain and Northern Ireland",,,,,,Unreleased,"""Chirp 3 model is in private preview. The online documentation is publicly available, but you won't be able to use the model until you contact a member of the sales team to be added to the allowlist.""",,,
Chirp 3 HD Text-to-Speech,Speech,"Text-to-speech (TTS),Speech synthesis",,API access,https://cloud.google.com/text-to-speech/docs/chirp3-hd,,Chirp 3: HD voices,2025-03-17,"Google,Google DeepMind",,,,,Unspecified unreleased,,,,,,,Unknown,"Text-to-Speech Chirp 3: HD voices represent the latest generation of Text-to-Speech technology. Powered by our cutting-edge LLMs, these voices deliver an unparalleled level of realism and emotional resonance.","United States of America,United States of America,United Kingdom of Great Britain and Northern Ireland",,,,,,Unreleased,,,,
ERNIE x1 (æ–‡å¿ƒå¤§æ¨¡åž‹X1),"Language,Multimodal","Code generation,Mathematical reasoning,Chat",,,https://www.datacamp.com/blog/ernie-4-5-x1,,"Baidu's ERNIE 4.5 & X1: Features, Access, DeepSeek Comparison
",2025-03-16,Baidu,,,,,,,,,,,,Unknown,,China,,,,,,Unreleased,,,,
Hunyuan T1,Language,Language modeling/generation,,,https://tencent.github.io/llm.hunyuan.T1/README_EN.html,,"Reasoning Efficiency Redefined! Meet Tencentâ€™s â€˜Hunyuan-T1â€™â€”The First Mamba-Powered Ultra-Large Model
",2025-03-15,Tencent,,,,,Unspecified unreleased,"We collected world science and reasoning problems, covering mathematics/logic reasoning/science/code, etc. These data sets cover everything from basic mathematical reasoning to complex scientific problem solving. Combined with ground-truth real feedback, we ensure that the model can demonstrate excellent capabilities when facing various reasoning tasks.",,,,,,Unverified,"Today, we are very pleased to announce that the in-depth thinking model of the Hunyuan large model series has been successfully upgraded to the Hunyuan-T1 official version , . This model is based on the TurboS fast-thinking base, the worldâ€™s first ultra-large-scale Hybrid-Transformer-Mamba MoE , large model released by us at the beginning of March. Through large-scale post-training, its reasoning ability has been significantly expanded and further aligned with human preferences.

Compared with the previous T1-preview model, Hunyuan-T1 has shown a significant overall performance improvement and is a leading cutting-edge strong reasoning large model in the industry.

Based on TurboS, T1 shows unique advantages in the direction of in-depth reasoning. TurboSâ€™s long-text capture ability helps Turbo-S effectively solve the problems of context loss and long-distance information dependence often encountered in long-text reasoning. Secondly, its Mamba architecture specifically optimizes the processing ability of long sequences. Through an efficient computing method, it can ensure the ability to capture long-text information while significantly reducing the consumption of computing resources. Under the same deployment conditions, the decoding speed is 2 times faster.

In the post-training phase of the model, we invested 96.7% of our computing power in reinforcement learning training, focusing on improving pure reasoning ability and optimizing alignment with human preferences.",China,,,,,,,,,,
Cohere Command A,Language,"Language modeling/generation,Chat,Code generation,Question answering",,Open weights (non-commercial),"https://cohere.com/blog/command-a
https://huggingface.co/CohereForAI/c4ai-command-a-03-2025?ref=cohere-ai.ghost.io",,"Introducing Command A: Max performance, minimal compute",2025-03-13,Cohere,111000000000.0,"a 111 billion parameter model optimized for demanding enterprises that require fast, secure, and high-quality AI",,"Unlikely to be >1e25 FLOP. Trained on ""trillions of tokens"", so C=6ND would suggest at most 6*111e9*10e12 = 7e24 FLOP if it used 10T tokens.",Unspecified unreleased,,,,,,,Confident,"Today, weâ€™re introducing Command A, a new state-of-the-art generative model optimized for demanding enterprises that require fast, secure, and high-quality AI. Command A delivers maximum performance with minimal hardware costs when compared to leading proprietary and open-weights models, such as GPT-4o and DeepSeek-V3. For private deployments, Command A excels on business-critical agentic and multilingual tasks, whileâ€¬ being deployable on just two GPUs, compared to other models that typically require as many as 32.

In head-to-head human evaluation across business, STEM, and coding tasks, Command A matches or outperforms its larger and slower competitors â€“ while offering superior throughput and increased efficiency. Human evaluations matter because they test on real-world enterprise data and situations.  ",Canada,,,,,,Unreleased,"Command A is available today on the Cohere platform, for research use on Hugging Face, and coming soon to major cloud providers.
https://huggingface.co/CohereLabs/c4ai-command-a-03-2025?ref=cohere-ai.ghost.io
License:CC-BY-NC",,,
OLMo 2 32B,Language,"Language modeling/generation,Question answering",,Open weights (unrestricted),https://allenai.org/blog/olmo2-32B,,OLMo 2 32B: First fully open model to outperform GPT 3.5 and GPT 4o mini,2025-03-13,Allen Institute for AI,32000000000.0,32B,1.3e+24,first table here: https://allenai.org/blog/olmo2-32B,"OLMo-Mix-1124,Dolmino-Mix-1124,Tulu 3 70B",,4000000000000.0,"""It is trained up to 6T tokens and post-trained using Tulu 3.1.""

""OLMo 2 32B is trained for 1.5 epochs, up to 6T tokens""

Pretraining Stage 1
(OLMo-Mix-1124)	6T tokens ( = 1.5 epochs)
Pretraining Stage 2
(Dolmino-Mix-1124) 100B tokens (3 runs)
300B tokens (1 run)
merged
Post-training
(Tulu 3 SFT OLMo mix)	SFT + DPO + PPO
(preference mix)",,,NVIDIA H100 SXM5 80GB,Confident,"Today we release OLMo 2 32B, the most capable and largest model in the OLMo 2 family, scaling up the OLMo 2 training recipe used for our 7B and 13B models released in November. It is trained up to 6T tokens and post-trained using Tulu 3.1. OLMo 2 32B is the first fully-open model (all data, code, weights, and details are freely available) to outperform GPT3.5-Turbo and GPT-4o mini on a suite of popular, multi-skill academic benchmarks. It is comparable to the leading open-weight models while requiring only a fraction of training compute. For example, OLMo 2 32B takes only one third of the cost of training Qwen 2.5 32B while reaching similar performance. The OLMo 2 family of modelsâ€”now available in 7B, 13B, and 32B parameter sizes, all can be finetuned on a single H100 GPU node, and all models are available on the Ai2 playground.",United States of America,,,,,0.38,Open source,"Apache 2.0
https://huggingface.co/allenai/OLMo-2-0325-32B (base)
allenai/OLMo-2-0325-32B-Instruct (base+SFT+DPO+RVLR)

Apache 2.0
https://github.com/allenai/OLMo-core",,,
Hunyuan-TurboS,Language,"Language modeling/generation,Quantitative reasoning,Question answering,Code generation,Text summarization","Tencent Hunyuan Team: Ao Liu, Botong Zhou, Can Xu, Chayse Zhou, ChenChen Zhang, Chengcheng Xu, Chenhao Wang, Decheng Wu, Dengpeng Wu, Dian Jiao, Dong Du, Dong Wang, Feng Zhang, Fengzong Lian, Guanghui Xu, Guanwei Zhang, Hai Wang, Haipeng Luo, Han Hu, Huilin Xu, Jiajia Wu, Jianchen Zhu, Jianfeng Yan, Jiaqi Zhu, Jihong Zhang, Jinbao Xue, Jun Xia, Junqiang Zheng, Kai Liu, Kai Zhang, Kai Zheng, Kejiao Li, Keyao Wang, Lan Jiang, Lixin Liu, Lulu Wu, Mengyuan Huang, Peijie Yu, Peiqi Wang, Qian Wang, Qianbiao Xiang, Qibin Liu, Qingfeng Sun, Richard Guo, Ruobing Xie, Saiyong Yang, Shaohua Chen, Shihui Hu, Shuai Li, Shuaipeng Li, Shuang Chen, Suncong Zheng, Tao Yang, Tian Zhang, Tinghao Yu, Weidong Han, Weijie Liu, Weijin Zhou, Weikang Wang, Wesleye Chen, Xiao Feng, Xiaoqin Ren, Xingwu Sun, Xiong Kuang, Xuemeng Huang, Xun Cao, Yanfeng Chen, Yang Du, Zhen Yang, Yangyu Tao, Yaping Deng, Yi Shen, Yigeng Hong, Yiqi Chen, Yiqing Huang, Yuchi Deng, Yue Mao, Yulong Wang, Yuyuan Zeng, Zenan Xu, Zhanhui Kang, Zhe Zhao, ZhenXiang Yan, Zheng Fang, Zhichao Hu, Zhongzhi Chen, Zhuoyu Li, Zongwei Li, Alex Yan, Ande Liang, Baitong Liu, Beiping Pan, Bin Xing, Binghong Wu, Bingxin Qu, Bolin Ni, Boyu Wu, Chen Li, Cheng Jiang , Cheng Zhang, Chengjun Liu, Chengxu Yang, Chengzhong Xu, Chiyu Wang, Chong Zha, Daisy Yi, Di Wang, Fanyang Lu, Fei Chen, Feifei Liu, Feng Zheng, Guanghua Yu, Guiyang Li, Guohua Wang, Haisheng Lin, Han Liu, Han Wang, Hao Fei, Hao Lu, Haoqing Jiang, Haoran Sun, Haotian Zhu, Huangjin Dai, Huankui Chen, Huawen Feng, Huihui Cai, Huxin Peng, Jackson Lv, Jiacheng Shi, Jiahao Bu, Jianbo Li, Jianglu Hu, Jiangtao Guan, Jianing Xu, Jianwei Cai, Jiarong Zhang, Jiawei Song, Jie Jiang, Jie Liu, Jieneng Yang, Jihong Zhang, Jin lv, Jing Zhao, Jinjian Li, Jinxing Liu, Jun Zhao, Juntao Guo, Kai Wang, Kan Wu, Lei Fu, Lei He, Lei Wang, Li Liu, Liang Dong, Liya Zhan, Long Cheng, Long Xu, Mao Zheng, Meng Liu, Mengkang Hu, Nanli Chen, Peirui Chen, Peng He, Pengju Pan, Pengzhi Wei, Qi Yang, Qi Yi, Roberts Wang, Rongpeng Chen, Rui Sun, Rui Yang, Ruibin Chen, Ruixu Zhou, Shaofeng Zhang, Sheng Zhang, Shihao Xu, Shuaishuai Chang, Shulin Liu, SiQi Wang, Songjia Feng, Songling Yuan, Tao Zhang, Tianjiao Lang, Tongkai Li, Wei Deng, Wei Li, Weichao Wang, Weigang Zhang, Weixuan Sun, Wen Ouyang, Wenxiang Jiao, Wenzhi Sun, Wenzhuo Jia, Xiang Zhang, Xiangyu He, Xianshun Ren, XiaoYing Zhu, Xiaolong Guo, Xiaoxue Li, Xiaoyu Ma, Xican Lu, Xinhua Feng, Xinting Huang, Xinyu Guan, Xirui Li, Xu Zhang, Xudong Gao, Xun Luo, Xuxiang Qi, Yangkun Chen, Yangyu Tao, Yanling Xiao, Yantao Mai, Yanze Chen, Yao Ding, Yeting Yang, YiFan Song, Yifan Yang, Yijiao Zhu, Yinhe Wu, Yixian Liu, Yong Yang, Yuanjun Cai, Yuanlin Tu, Yue Zhang, Yufei Huang, Yuhang Zhou, Yuhao Jiang, Yuhong Liu, Yuhui Hu, Yujin Lin, Yun Yang, Yunhao Wang, Yusong Zhang, Zekun Wu, Zelong Zhang, Zhan Yu, Zhaoliang Yang, Zhe Zhao, Zheng Li, Zhenyu Huang, Zhiguang Liu, Zhijiang Xu, Zhiqing Kui, Zhiyin Zeng, Zhiyuan Xiong, Zhuo Han, Zifan Wu, Zigang Geng, Zilong Zhao, Ziyan Tang, Ziyuan Zhu, Zonglei Zhu, Zhijiang Xu",API access,"https://web.archive.org/web/20250408105622/https://www.dapingtime.com/article/2171.html

https://medium.com/data-science-in-your-pocket/tencent-hunyuan-turbo-s-the-fastest-reasoning-llm-d64a02bed5c8

https://arxiv.org/abs/2505.15431",,"Tencent HunYuan Turbo S: The fastest reasoning LLM
At par with DeepSeek, Claude 3.5 and GPT-4o",2025-03-11,Tencent,560000000000.0," the model scales to 56B activated
parameters and 560B total parameters",,,Unspecified unreleased,,16000000000000.0,pre-trained on 16T high-quality tokens,,,,Confident,"As Large Language Models (LLMs) rapidly advance, we introduce Hunyuan-TurboS, a novel large hybrid Transformer-Mamba Mixture of Experts (MoE) model. It synergistically combines Mamba's long-sequence processing efficiency with Transformer's superior contextual understanding. Hunyuan-TurboS features an adaptive long-short chain-of-thought (CoT) mechanism, dynamically switching between rapid responses for simple queries and deep ""thinking"" modes for complex problems, optimizing computational resources. Architecturally, this 56B activated (560B total) parameter model employs 128 layers (Mamba2, Attention, FFN) with an innovative AMF/MF block pattern. Faster Mamba2 ensures linear complexity, Grouped-Query Attention minimizes KV cache, and FFNs use an MoE structure. Pre-trained on 16T high-quality tokens, it supports a 256K context length and is the first industry-deployed large-scale Mamba model. Our comprehensive post-training strategy enhances capabilities via Supervised Fine-Tuning (3M instructions), a novel Adaptive Long-short CoT Fusion method, Multi-round Deliberation Learning for iterative improvement, and a two-stage Large-scale Reinforcement Learning process targeting STEM and general instruction-following. Evaluations show strong performance: overall top 7 rank on LMSYS Chatbot Arena with a score of 1356, outperforming leading models like Gemini-2.0-Flash-001 (1352) and o4-mini-2025-04-16 (1345). TurboS also achieves an average of 77.9% across 23 automated benchmarks. Hunyuan-TurboS balances high performance and efficiency, offering substantial capabilities at lower inference costs than many reasoning models, establishing a new paradigm for efficient large-scale pre-trained models.",China,,,,,,Unreleased,API via Tencent Cloud,,,
Seedream 2.0,Image generation,"Image generation,Text-to-image","Lixue Gong, Xiaoxia Hou, Fanshi Li, Liang Li, Xiaochen Lian, Fei Liu, Liyang Liu, Wei Liu, Wei Lu, Yichun Shi, Shiqi Sun, Yu Tian, Zhi Tian, Peng Wang, Xun Wang, Ye Wang, Guofeng Wu, Jie Wu, Xin Xia, Xuefeng Xiao, Linjie Yang, Zhonghua Zhai, Xinyu Zhang, Qi Zhang, Yuwei Zhang, Shijia Zhao, Jianchao Yang, Weilin Huang",API access,https://arxiv.org/abs/2503.07703,,Seedream 2.0: A Native Chinese-English Bilingual Image Generation Foundation Model,2025-03-10,ByteDance,,,,,Unspecified unreleased,,,,,,,Unknown,"Rapid advancement of diffusion models has catalyzed remarkable progress in the field of image generation. However, prevalent models such as Flux, SD3.5 and Midjourney, still grapple with issues like model bias, limited text rendering capabilities, and insufficient understanding of Chinese cultural nuances. To address these limitations, we present Seedream 2.0, a native Chinese-English bilingual image generation foundation model that excels across diverse dimensions, which adeptly manages text prompt in both Chinese and English, supporting bilingual image generation and text rendering. We develop a powerful data system that facilitates knowledge integration, and a caption system that balances the accuracy and richness for image description. Particularly, Seedream is integrated with a self-developed bilingual large language model as a text encoder, allowing it to learn native knowledge directly from massive data. This enable it to generate high-fidelity images with accurate cultural nuances and aesthetic expressions described in either Chinese or English. Beside, Glyph-Aligned ByT5 is applied for flexible character-level text rendering, while a Scaled ROPE generalizes well to untrained resolutions. Multi-phase post-training optimizations, including SFT and RLHF iterations, further improve the overall capability. Through extensive experimentation, we demonstrate that Seedream 2.0 achieves state-of-the-art performance across multiple aspects, including prompt-following, aesthetics, text rendering, and structural correctness. Furthermore, Seedream 2.0 has been optimized through multiple RLHF iterations to closely align its output with human preferences, as revealed by its outstanding ELO score. In addition, it can be readily adapted to an instruction-based image editing model, such as SeedEdit, with strong editing capability that balances instruction-following and image consistency.",China,,,,,,Unreleased,,,,
"Ling-lite-1.5 (""Bailing"")",Language,"Language modeling/generation,Question answering,Code generation","Ling Team, Binwei Zeng, Chao Huang, Chao Zhang, Changxin Tian, Cong Chen, Dingnan Jin, Feng Yu, Feng Zhu, Feng Yuan, Fakang Wang, Gangshan Wang, Guangyao Zhai, Haitao Zhang, Huizhong Li, Jun Zhou, Jia Liu, Junpeng Fang, Junjie Ou, Jun Hu, Ji Luo, Ji Zhang, Jian Liu, Jian Sha, Jianxue Qian, Jiewei Wu, Junping Zhao, Jianguo Li, Jubao Feng, Jingchao Di, Junming Xu, Jinghua Yao, Kuan Xu, Kewei Du, Longfei Li, Lei Liang, Lu Yu, Li Tang, Lin Ju, Peng Xu, Qing Cui, Song Liu, Shicheng Li, Shun Song, Song Yan, Tengwei Cai, Tianyi Chen, Ting Guo, Ting Huang, Tao Feng, Tao Wu, Wei Wu, Xiaolu Zhang, Xueming Yang, Xin Zhao, Xiaobo Hu, Xin Lin, Yao Zhao, Yilong Wang, Yongzhen Guo, Yuanyuan Wang, Yue Yang, Yang Cao, Yuhao Fu, Yi Xiong, Yanzhe Li, Zhe Li, Zhiqiang Zhang, Ziqi Liu, Zhaoxin Huan, Zujie Wen, Zhenhang Sun, Zhuoxuan Du, Zhengyu He",Open weights (unrestricted),https://arxiv.org/abs/2503.05139,,Every FLOP Counts: Scaling a 300B Mixture-of-Experts LING LLM without Premium GPUs,2025-03-10,Ant Group,16800000000.0,16.8 billion parameters with 2.75 billion activated parameters,1.485e+23,6 FLOP / parameter / token * 2.75 * 10^9 active parameters * 9 * 10^12 tokens = 1.485e+23 FLOP,Unspecified unreleased,,9000000000000.0,"""e. To date, we have constructed a high-quality corpus consisting of approximately 9 trillion tokens, distributed across 1 trillion tokens in Chinese, 5.5 trillion in English, and 2.5 trillion in code.""",,,,Confident,"In this technical report, we tackle the challenges of training large-scale Mixture of Experts (MoE) models, focusing on overcoming cost inefficiency and resource limitations prevalent in such systems. To address these issues, we present two differently sized MoE large language models (LLMs), namely Ling-Lite and Ling-Plus (referred to as ""Bailing"" in Chinese, spelled BÇŽilÃ­ng in Pinyin). Ling-Lite contains 16.8 billion parameters with 2.75 billion activated parameters, while Ling-Plus boasts 290 billion parameters with 28.8 billion activated parameters. Both models exhibit comparable performance to leading industry benchmarks. This report offers actionable insights to improve the efficiency and accessibility of AI development in resource-constrained settings, promoting more scalable and sustainable technologies. Specifically, to reduce training costs for large-scale MoE models, we propose innovative methods for (1) optimization of model architecture and training processes, (2) refinement of training anomaly handling, and (3) enhancement of model evaluation efficiency. Additionally, leveraging high-quality data generated from knowledge graphs, our models demonstrate superior capabilities in tool use compared to other models. Ultimately, our experimental findings demonstrate that a 300B MoE LLM can be effectively trained on lower-performance devices while achieving comparable performance to models of a similar scale, including dense and MoE models. Compared to high-performance devices, utilizing a lower-specification hardware system during the pre-training phase demonstrates significant cost savings, reducing computing costs by approximately 20%. The models can be accessed at this https URL.",China,,,,,,Unreleased,"MIT license
https://huggingface.co/inclusionAI/Ling-lite-1.5",,,
"Ling-Plus (""Bailing"")",Language,"Language modeling/generation,Question answering,Code generation","Ling Team, Binwei Zeng, Chao Huang, Chao Zhang, Changxin Tian, Cong Chen, Dingnan Jin, Feng Yu, Feng Zhu, Feng Yuan, Fakang Wang, Gangshan Wang, Guangyao Zhai, Haitao Zhang, Huizhong Li, Jun Zhou, Jia Liu, Junpeng Fang, Junjie Ou, Jun Hu, Ji Luo, Ji Zhang, Jian Liu, Jian Sha, Jianxue Qian, Jiewei Wu, Junping Zhao, Jianguo Li, Jubao Feng, Jingchao Di, Junming Xu, Jinghua Yao, Kuan Xu, Kewei Du, Longfei Li, Lei Liang, Lu Yu, Li Tang, Lin Ju, Peng Xu, Qing Cui, Song Liu, Shicheng Li, Shun Song, Song Yan, Tengwei Cai, Tianyi Chen, Ting Guo, Ting Huang, Tao Feng, Tao Wu, Wei Wu, Xiaolu Zhang, Xueming Yang, Xin Zhao, Xiaobo Hu, Xin Lin, Yao Zhao, Yilong Wang, Yongzhen Guo, Yuanyuan Wang, Yue Yang, Yang Cao, Yuhao Fu, Yi Xiong, Yanzhe Li, Zhe Li, Zhiqiang Zhang, Ziqi Liu, Zhaoxin Huan, Zujie Wen, Zhenhang Sun, Zhuoxuan Du, Zhengyu He",Open weights (unrestricted),https://arxiv.org/abs/2503.05139,,Every FLOP Counts: Scaling a 300B Mixture-of-Experts LING LLM without Premium GPUs,2025-03-10,Ant Group,290000000000.0,290 billion parameters with 28.8 billion activated parameters,1.5552e+24,6 FLOP / parameter / token * 28.8 * 10^9 active parameters * 9 * 10^12 tokens = 1.5552e+24 FLOP,Unspecified unreleased,,9000000000000.0,"""e. To date, we have constructed a high-quality corpus consisting of approximately 9 trillion tokens, distributed across 1 trillion tokens in Chinese, 5.5 trillion in English, and 2.5 trillion in code.""",,,,Confident,"In this technical report, we tackle the challenges of training large-scale Mixture of Experts (MoE) models, focusing on overcoming cost inefficiency and resource limitations prevalent in such systems. To address these issues, we present two differently sized MoE large language models (LLMs), namely Ling-Lite and Ling-Plus (referred to as ""Bailing"" in Chinese, spelled BÇŽilÃ­ng in Pinyin). Ling-Lite contains 16.8 billion parameters with 2.75 billion activated parameters, while Ling-Plus boasts 290 billion parameters with 28.8 billion activated parameters. Both models exhibit comparable performance to leading industry benchmarks. This report offers actionable insights to improve the efficiency and accessibility of AI development in resource-constrained settings, promoting more scalable and sustainable technologies. Specifically, to reduce training costs for large-scale MoE models, we propose innovative methods for (1) optimization of model architecture and training processes, (2) refinement of training anomaly handling, and (3) enhancement of model evaluation efficiency. Additionally, leveraging high-quality data generated from knowledge graphs, our models demonstrate superior capabilities in tool use compared to other models. Ultimately, our experimental findings demonstrate that a 300B MoE LLM can be effectively trained on lower-performance devices while achieving comparable performance to models of a similar scale, including dense and MoE models. Compared to high-performance devices, utilizing a lower-specification hardware system during the pre-training phase demonstrates significant cost savings, reducing computing costs by approximately 20%. The models can be accessed at this https URL.",China,,,,,,Unreleased,"MIT license
https://huggingface.co/inclusionAI/Ling-plus",,,
Spark-X1,"Language,Mathematics","Language modeling/generation,Chat,Mathematical reasoning",,,"https://en.tmtpost.com/news/7477906
https://news.cgtn.com/news/2025-01-15/China-releases-Spark-X1-deep-reasoning-model-that-packs-a-punch-1AbIq8PzzEI/p.html
https://wallstreetcn.com/articles/3740575",,iFlytek Upgrades Spark X1 Model With Domestic Computing Power,2025-03-03,iFlytek,70000000000.0,,,"Similar size to Llama-3 70B, which was close to 1e25 FLOP, so plausibly Spark might be over.",,,,,,,Huawei Ascend 910B,Confident,,China,,,,10000.0,,,,,,
GPT-4.5,"Language,Vision,Multimodal","Language modeling/generation,Question answering,Quantitative reasoning,Translation,Visual question answering,Code generation,Instruction interpretation","Foundational contributors
Alex Paino, Ali Kamali, Amin Tootoonchian, Andrew Tulloch, Ben Sokolowsky, Clemens Winter, Colin Wei, Daniel Kappler, Daniel Levy, Felipe Petroski Such, Geoff Salmon, Ian Oâ€™Connell, Jason Teplitz, Kai Chen, Nik Tezak, Prafulla Dhariwal, Rapha Gontijo Lopes, Sam Schoenholz, Youlong Cheng, Yujia Jin, Yunxing Dai

Research
Core contributors

Aiden Low, Alec Radford, Alex Carney, Alex Nichol, Alexis Conneau, Ananya Kumar, Ben Wang, Charlotte Cole , Elizabeth Yang, Gabriel Goh, Hadi Salman, Haitang Hu, Heewoo Jun, Ian Sohl, Ishaan Gulrajani, Jacob Coxon, James Betker, Jamie Kiros, Jessica Landon, Kyle Luther, Lia Guy, Lukas Kondraciuk, Lyric Doshi, Mikhail Pavlov, Qiming Yuan, Reimar Leike, Rowan Zellers, Sean Metzger, Shengjia Zhao, Spencer Papay, Tao Wang

Contributors

Adam Lerer, Aidan McLaughlin, Alexander Prokofiev, Alexandra Barr, Allan Jabri, Ananya Kumar, Andrew Gibiansky, Andrew Schmidt, Casey Chu, Chak Li, Chelsea Voss, Chris Hallacy, Chris Koch, Christine McLeavey, David Mely, Dimitris Tsipras, Eric Sigler, Erin Kavanaugh, Farzad Khorasani, Huiwen Chang, Ilya Kostrikov, Ishaan Singal, Ji Lin, Jiahui Yu, Jing Yu Zhang, John Rizzo, Jong Wook Kim, Joyce Lee, Juntang Zhuang, Leo Liu, Li Jing, Long Ouyang, Louis Feuvrier, Mo Bavarian, Nick Stathas, Nitish Keskar, Oleg Murk, Preston Bowman, Scottie Yan, SQ Mah, Tao Xu, Taylor Gordon, Valerie Qi, Wenda Zhou, Yu Zhang

Scaling
Core contributors

Adam Goucher, Alex Chow, Alex Renzin, Aleksandra Spyra, Avi Nayak, Ben Leimberger, Christopher Hesse, Duc Phong Nguyen, Dinghua Li, Eric Peterson, Francis Zhang, Gene Oden, Kai Fricke, Kai Hayashi, Larry Lv, Leqi Zou, Lin Yang, Madeleine Thompson, Michael Petrov, Miguel Castro, Natalia Gimelshein, Phil Tillet, Reza Zamani, Ryan Cheu Stanley Hsieh, Steve Lee, Stewart Hall, Thomas Raoux, Tianhao Zheng, Vishal Kuo, Yongjik Kim, Yuchen Zhang, Zhuoran Liu

Contributors

Alvin Wan, Andrew Cann, Antoine Pelisse, Anuj Kalia, Aaron Hurst, Avital Oliver, Brad Barnes, Brian Hsu, Chen Ding, Chen Shen, Cheng Chang, Christian Gibson, Duncan Findlay, Fan Wang, Fangyuan Li, Gianluca Borello, Heather Schmidt, Henrique Ponde de Oliveira Pinto, Ikai Lan, Jiayi Weng, James Crooks, Jos Kraaijeveld, Junru Shao, Kenny Hsu, Kenny Nguyen, Kevin King, Leah Burkhardt, Leo Chen, Linden Li, Lu Zhang, Mahmoud Eariby, Marat Dukhan, Mateusz Litwin, Miki Habryn, Natan LaFontaine, Pavel Belov, Peng Su, Prasad Chakka, Rachel Lim, Rajkumar Samuel, Renaud Gaubert, Rory Carmichael, Sarah Dong, Shantanu Jain, Stephen Logsdon, Todd Underwood, Weixing Zhang, Will Sheu, Weiyi Zheng, Yinghai Lu, Yunqiao Zhang

Safety Systems
Andrea Vallone, Andy Applebaum, Cameron Raymond, Chong Zhang, Dan Mossing, Elizabeth Proehl, Eric Wallace, Evan Mays, Grace Zhao, Ian Kivlichan, Irina Kofman, Joel Parish, Kevin Liu, Keren Gu-Lemberg, Kristen Ying, Lama Ahmad, Lilian Weng , Leon Maksin, Leyton Ho, Meghan Shah, Michael Lampe, Michele Wang, Miles Wang, Olivia Watkins, Phillip Guo, Samuel Miserendino, Sam Toizer, Sandhini Agarwal, Tejal Patwardhan, Tom DuprÃ© la Tour, Tong Mu, Tyna Eloundou, Yunyun Wang

Deployment
Adam Brandon, Adam Perelman, Adele Li, Akshay Nathan, Alan Hayes, Alfred Xue, Alison Ben, Alec Gorge, Alex Guziel, Alex Iftimie, Ally Bennett, Andrew Chen, Andy Wang, Andy Wood, Angad Singh, Anoop Kotha, Antonia Woodford, Anuj Saharan, Ashley Tyra, Atty Eleti, Ben Schneider, Bessie Ji, Beth Hoover, Bill Chen, Blake Samic, Britney Smith, Brian Yu, Caleb Wang, Cary Bassin, Cary Hudson, Charlie Jatt, Chengdu Huang, Chris Beaumont, Christina Huang, Cristina Scheau, Dana Palmie, Daniel Levine, Daryl Neubieser, Dave Cummings, David Sasaki, Dibya Bhattacharjee, Dylan Hunn, Edwin Arbus, Elaine Ya Le, Enis Sert, Eric Kramer, Fred von Lohmann, Gaby Janatpour, Garrett McGrath, Garrett Ollinger, Gary Yang, Hao Sheng, Harold Hotelling, Janardhanan Vembunarayanan, Jeff Harris, Jeffrey Sabin Matsumoto, Jennifer Robinson, Jessica Liang, Jessica Shieh, Jiacheng Yang, Joel Morris, Joseph Florencio, Josh Kaplan, Kan Wu, Karan Sharma, Karen Li, Katie Pypes, Kendal Simon, Kendra Rimbach, Kevin Park, Kevin Rao, Laurance Fauconnet, Lauren Workman, Leher Pathak, Liang Wu, Liang Xiong, Lien Mamitsuka, Lindsay McCallum, Lukas Gross, Manoli Liodakis, Matt Nichols, Michelle Fradin, Minal Khan, Mingxuan Wang, Nacho Soto, Natalie Staudacher, Nikunj Handa, Niko Felix, Ning Liu, Olivier Godement, Oona Gleeson, Philip Pronin, Raymond Li, Reah Miyara, Rohan Nuttall, R.J. Marsan, Sara Culver, Scott Ethersmith, Sean Fitzgerald, Shamez Hemani, Sherwin Wu, Shiao Lee, Shuyang Cheng, Siyuan Fu, Spug Golden, Steve Coffey, Steven Heidel, Sundeep Tirumalareddy, Tabarak Khan, Thomas Degry, Thomas Dimson, Tom Stasi, Tomo Hiratsuka, Trevor Creech, Uzair Navid Iftikhar, Victoria Chernova, Victoria Spiegel, Wanning Jiang, Wenlei Xie, Yaming Lin, Yara Khakbaz, Yilei Qian, Yilong Qin, Yo Shavit, Zhi Bie

Executive Leadership
Bob McGrew, Greg Brockman, Hannah Wong, Jakub Pachocki, Johannes Heidecke, Joanne Jang, Kate Rouch, Kevin Weil, Lauren Itow, Liam Fedus, Mark Chen, Mia Glaese, Mira Murati, Nick Ryder, Sam Altman, Srinivas Narayanan, Tal Broda",API access,https://openai.com/index/introducing-gpt-4-5/,,Introducing GPT-4.5,2025-02-27,OpenAI,,,3.8e+26,"Analysis of GPT-4.5's training cluster yields a median estimate of 187M H100-hours of training. The utilization assumptions we used for the Grok 3 estimate (probably worth revisiting) were 20 to 40% under the H100 FP8 spec of 2000 teraflop/s. This leads to an estimate of 2.7e26 to 5.4e26 FLOP, or a geomean of 3.8e26

Alternatively, using a plausible range of 20 to 50% utilization, given the possibility of FP8 training, yields a median estimate of ~2e25 FLOP. See notebook below for details.

https://colab.research.google.com/drive/1QBmVPm64Ti0xucN0EsZTgSz_I7Mj9hAZ#scrollTo=NYH1ABJuLJlw 

This is consistent with OpenAI's statement that GPT-4.5 was a â€œnew order of magnitude in computeâ€ compared to previous models (e.g. GPT-4, which was ~2e25), suggesting around 2e26 FLOP. But they could have meant this somewhat loosely. 

In the ""Pretraining GPT-4.5"" interview, they state they used multi-cluster training: https://youtu.be/6nJZopACRuQ?si=FFJC-gEmGPZjvoPM&t=617 ",Unspecified unreleased,"""GPT-4.5 was pre-trained and post-trained on diverse datasets, including a mix of publicly available
data, proprietary data from data partnerships, and custom datasets developed in-house, which
collectively contribute to the modelâ€™s robust conversational capabilities and world knowledge.""
This model seems to also be known as GPT-4.5 Preview, which has a knowledge cutoff date of October 2023 - which I assume means October 1, 2023 - according to https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models?tabs=global-standard%2Cstandard-chat-completions#computer-use-preview.",,,,,,Likely,"We advance AI capabilities by scaling two complementary paradigms: unsupervised learning and reasoning. These represent two axes of intelligence.

Unsupervised learning increases world model accuracy and intuition. Models like GPTâ€‘3.5, GPTâ€‘4, and GPTâ€‘4.5 advance this paradigm.
Scaling reasoningâ , on the other hand, teaches models to think and produce a chain of thought before they respond, allowing them to tackle complex STEM or logic problems. Models like OpenAI o1 and OpenAI o3â€‘mini advance this paradigm.
GPTâ€‘4.5 is an example of scaling unsupervised learning by scaling up compute and data, along with architecture and optimization innovations. GPTâ€‘4.5 was trained on Microsoft Azure AI supercomputers. The result is a model that has broader knowledge and a deeper understanding of the world, leading to reduced hallucinations and more reliability across a wide range of topics.",United States of America,,,,,,Unreleased,,,,
Eleven Scribe,Speech,"Speech-to-text,Speech recognition (ASR)","Flavio Schneider, Tim von KÃ¤nel, Maximiliano Levi, Johan Nordberg, Piotr Dabkowski, Austin Malerba, Hristo Stoychev, Alex George",API access,https://elevenlabs.io/blog/meet-scribe,,State-of-the-art speech recognition model,2025-02-26,ElevenLabs,,,,,Unspecified unreleased,,,,,,,Unknown,"Scribe, our first Speech to Text model, is the worldâ€™s most accurate transcription model. Built to handle the unpredictability of real-world audio, Scribe transcribes speech in 99 languages, featuring word-level timestamps, speaker diarization, and audio-event taggingâ€”all delivered in a structured response for seamless integration.
Scribe is engineered for precision. In FLEURS & Common Voice benchmark tests across 99 languages, it consistently outperforms leading models like Gemini 2.0 Flash, Whisper Large V3 and Deepgram Nova-3. Whether itâ€™s meeting summaries, movie subtitles, or even song lyrics, Scribe delivers the lowest automated transcription word error rate in Italian (98.7%), English (96.7%) and 97 other languages.

Scribe makes ASR universally accessibleâ€”dramatically reducing errors in traditionally underserved languages such as Serbian, Cantonese, and Malayalam, where competing models often exceed 40% word error rates.",United States of America,,,,,,Unreleased,,,,
Wan 2.1 14B I2V,"Video,Vision","Video generation,Image-to-video,Text-to-video","Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, Jiayu Wang, Jingfeng Zhang, Jingren Zhou, Jinkai Wang, Jixuan Chen, Kai Zhu, Kang Zhao, Keyu Yan, Lianghua Huang, Mengyang Feng, Ningyi Zhang, Pandeng Li, Pingyu Wu, Ruihang Chu, Ruili Feng, Shiwei Zhang, Siyang Sun, Tao Fang, Tianxing Wang, Tianyi Gui, Tingyu Weng, Tong Shen, Wei Lin, Wei Wang, Wei Wang, Wenmeng Zhou, Wente Wang, Wenting Shen, Wenyuan Yu, Xianzhong Shi, Xiaoming Huang, Xin Xu, Yan Kou, Yangyu Lv, Yifei Li, Yijing Liu, Yiming Wang, Yingya Zhang, Yitong Huang, Yong Li, You Wu, Yu Liu, Yulin Pan, Yun Zheng, Yuntao Hong, Yupeng Shi, Yutong Feng, Zeyinzi Jiang, Zhen Han, Zhi-Fan Wu, Ziyu Liu",Open weights (unrestricted),"https://arxiv.org/abs/2503.20314
",,Wan 2.1 by Wan AI :best cost efficient video generation model Now Available,2025-02-25,Alibaba,14000000000.0,14B,2.5e+23,"""Through extensive experimentation, the model is validated at scale, reaching 14 billion parameters. Subsequently, Wan has seen large-scale data comprising billions of images and videos, amounting to O(1) trillions of tokens in total.""

So likely between 1T and 10T tokens. Assume 3T.

Transformer architecture, so 6ND should be a decent approximation.

6ND = 6 * 14e9 * 3e12 ~= 2.5e+23 FLOP",Unspecified unreleased,"""We curated and deduplicated a candidate dataset sourced from both internal copyrighted sources and publicly accessible data. I""",3000000000000.0,"""Wan has seen large-scale data comprising billions of images and videos, amounting to O(1) trillions of tokens in total.""

with ""Likely"" confidence, assuming ~3 trillion",,,,Likely,"SOTA Performance: Wan2.1 consistently outperforms existing open-source models and state-of-the-art commercial solutions across multiple benchmarks.
Multiple Tasks: Wan2.1 excels in Text-to-Video, Image-to-Video, Video Editing, Text-to-Image, and Video-to-Audio, advancing the field of video generation.
Visual Text Generation: Wan2.1 is the first video model capable of generating both Chinese and English text, featuring robust text generation that enhances its practical applications.
Powerful Video VAE: Wan-VAE delivers exceptional efficiency and performance, encoding and decoding 1080P videos of any length while preserving temporal information, making it an ideal foundation for video and image generation.",China,,,,,,Unreleased,"Apache 2.0
https://huggingface.co/Wan-AI/Wan2.1-I2V-14B-720P

Inference code:
https://github.com/Wan-Video/Wan2.1",,,
Bailing-Pro-20250225,Language,"Language modeling/generation,Question answering",Ant Group,Unreleased,https://rank.opencompass.org.cn/leaderboard-llm/?m=25-04,,,2025-02-25,Ant Group,,,,,Unspecified unreleased,,,,,,,Unknown,Mentioned by the OpenCompass benchmark but further open-source information is not available. ,China,,,,,,Unreleased,,,,
YandexGPT 5 Lite,Language,"Language modeling/generation,Question answering,Code generation,Mathematical reasoning,Retrieval-augmented generation",,Open weights (restricted use),"https://ya.ru/ai/gpt

https://habr.com/ru/companies/yandex/articles/885218/",,"The new generation: better at addressing user and business needs, solving problems, and writing code.",2025-02-25,Yandex,8000000000.0,8B,7.3536e+23,6 FLOP / token / parameter * 8 * 10^9 parameters * 15.32 * 10^12 tokens [see dataset size notes] = 7.3536e+23 FLOP,Unspecified unreleased,"""The dataset composition: 60% are web pages, 15% are code, 10% are mathematics, the rest are other specific data, including synthetics generated using our models and datasets from our services, such as Yandex Translate and the Search fact base.""",15320000000000.0,"""At the first stage, the model was trained mainly on Russian-language and English-language texts with a total volume of 15T tokens with a context length of up to 8k tokens.""

""In the second stage, which we called Powerup, the model was trained on high-quality data of 320B tokens.""",,,,Confident,"Introducing a new generation of Yandex generative text models. They handle answers better. On a stream that combines user questions and complex tasks in demand in the business sphere, YandexGPT 5 Pro outperforms a similar model of the previous generation in 67% of cases. In some types of tasks - for example, in writing and summarizing texts - the new model is not inferior to GPT-4o by OpenAI and other world leaders. The fifth generation has two models with a context length of 32 thousand tokens: the more powerful Pro and the lightweight Lite.",Russia,,,,,,Unreleased,Custom lisense - requires contract agreement for users with more than 10 million output tokens per month https://huggingface.co/yandex/YandexGPT-5-Lite-8B-pretrain,,,
Claude 3.7 Sonnet,"Language,Vision,Multimodal","Language modeling/generation,Question answering,Code generation,Quantitative reasoning,Translation,Instruction interpretation,Visual question answering",,API access,https://www.anthropic.com/news/claude-3-7-sonnet,,Claude 3.7 Sonnet,2025-02-24,Anthropic,,,3.35e+25,https://docs.google.com/spreadsheets/d/10bhwdVrfHI8tysVIz62ZxtvQ30L-HojYvmU18_b-WIM/edit?gid=0#gid=0,Unspecified unreleased,"""Claude 3.7 Sonnet is trained on a proprietary mix of publicly available information on the Internet, as well as non-public data from third parties, data provided by data labeling services and paid contractors, and data we
generate internally. While trained on publicly available information on the internet through November 2024, Claude 3.7 Sonnetâ€™s knowledge cut-off date is the end of October 2024. This means the modelâ€™s knowledge base is most extensive and reliable on information and events up to October 2024.""",,,,,,Likely,"Today, weâ€™re announcing Claude 3.7 Sonnet1, our most intelligent model to date and the first hybrid reasoning model on the market. Claude 3.7 Sonnet can produce near-instant responses or extended, step-by-step thinking that is made visible to the user. API users also have fine-grained control over how long the model can think for.

Claude 3.7 Sonnet shows particularly strong improvements in coding and front-end web development. ",United States of America,,,,,,Unreleased,,,,
Step-Video-T2V,Video,"Video generation,Text-to-video","Guoqing Ma, Haoyang Huang, Kun Yan, Liangyu Chen, Nan Duan, Shengming Yin, Changyi Wan, Ranchen Ming, Xiaoniu Song, Xing Chen, Yu Zhou, Deshan Sun, Deyu Zhou, Jian Zhou, Kaijun Tan, Kang An, Mei Chen, Wei Ji, Qiling Wu, Wen Sun, Xin Han, Yanan Wei, Zheng Ge, Aojie Li, Bin Wang, Bizhu Huang, Bo Wang, Brian Li, Changxing Miao, Chen Xu, Chenfei Wu, Chenguang Yu, Dapeng Shi, Dingyuan Hu, Enle Liu, Gang Yu, Ge Yang, Guanzhe Huang, Gulin Yan, Haiyang Feng, Hao Nie, Haonan Jia, Hanpeng Hu, Hanqi Chen, Haolong Yan, Heng Wang, Hongcheng Guo, Huilin Xiong, Huixin Xiong, Jiahao Gong, Jianchang Wu, Jiaoren Wu, Jie Wu, Jie Yang, Jiashuai Liu, Jiashuo Li, Jingyang Zhang, Junjing Guo, Junzhe Lin, Kaixiang Li, Lei Liu, Lei Xia, Liang Zhao, Liguo Tan, Liwen Huang, Liying Shi, Ming Li, Mingliang Li, Muhua Cheng, Na Wang, Qiaohui Chen, Qinglin He, Qiuyan Liang, Quan Sun, Ran Sun, Rui Wang, Shaoliang Pang, Shiliang Yang, Sitong Liu, Siqi Liu, Shuli Gao, Tiancheng Cao, Tianyu Wang, Weipeng Ming, Wenqing He, Xu Zhao, Xuelin Zhang, Xianfang Zeng, Xiaojia Liu, Xuan Yang, Yaqi Dai, Yanbo Yu, Yang Li, Yineng Deng, Yingming Wang, Yilei Wang, Yuanwei Lu, Yu Chen, Yu Luo, Yuchu Luo et al. (15 additional authors not shown)",Open weights (unrestricted),https://arxiv.org/abs/2502.10248,,"Step-Video-T2V Technical Report: The Practice, Challenges, and Future of Video Foundation Model",2025-02-24,StepFun,30000000000.0,30B,4.1015808e+24,"""We have constructed a datacenter comprising thousands of NVIDIA H800 GPUs""
""we have achieved 99% effective GPU
training time over more than one month.""

989000000000000 FLOP / GPU / sec [bf16 assumed] * 720 hours * 3600 sec / hour * 5000 GPUs [assumption -> ""Likely"" confidence] * 0.32 [reported utilization] = 4.1015808e+24 FLOP",Unspecified unreleased,,,"""We constructed a large-scale video dataset comprising 2B video-text pairs and 3.8B image-text pairs""
they are supposedly using only a subset of it (see Table 6):
 3.8B image-text pairs
 644M low resolution video-text pairs
+Post-filtering SFT dataset: 30M high-quality video-text pairs",720.0,"""we have achieved 99% effective GPU
training time over more than one month.""
one month ~720 hours",NVIDIA H800 SXM5,Likely,"We present Step-Video-T2V, a state-of-the-art text-to-video pre-trained model with 30B parameters and the ability to generate videos up to 204 frames in length. A deep compression Variational Autoencoder, Video-VAE, is designed for video generation tasks, achieving 16x16 spatial and 8x temporal compression ratios, while maintaining exceptional video reconstruction quality. User prompts are encoded using two bilingual text encoders to handle both English and Chinese. A DiT with 3D full attention is trained using Flow Matching and is employed to denoise input noise into latent frames. A video-based DPO approach, Video-DPO, is applied to reduce artifacts and improve the visual quality of the generated videos. We also detail our training strategies and share key observations and insights. Step-Video-T2V's performance is evaluated on a novel video generation benchmark, Step-Video-T2V-Eval, demonstrating its state-of-the-art text-to-video quality when compared with both open-source and commercial engines. Additionally, we discuss the limitations of current diffusion-based model paradigm and outline future directions for video foundation models. We make both Step-Video-T2V and Step-Video-T2V-Eval available at this https URL. The online version can be accessed from this https URL as well. Our goal is to accelerate the innovation of video foundation models and empower video content creators.",China,,,,5000.0,0.32,Unreleased,"MIT license

https://huggingface.co/stepfun-ai/stepvideo-t2v",,,
Evo 2 40B,Biology,Protein or nucleotide language model (pLM/nLM),"Garyk Brixi, Matthew G. Durrant, Jerome Ku, Michael Poli, Greg Brockman, Daniel Chang, Gabriel A. Gonzalez, Samuel H. King, David B. Li, Aditi T. Merchant, Mohsen Naghipourfar, Eric Nguyen, Chiara Ricci-Tam, David W. Romero, Gwanggyu Sun, Ali Taghibakshi, Anton Vorontsov, Brandon Yang, Myra Deng, Liv Gorton, Nam Nguyen, Nicholas K. Wang, Etowah Adams, Stephen A. Baccus, Steven Dillmann, Stefano Ermon, Daniel Guo, Rajesh Ilango, Ken Janik, Amy X. Lu, Reshma Mehta, Mohammad R.K. Mofrad, Madelena Y. Ng, Jaspreet Pannu, Christopher RÃ©, Jonathan C. Schmok, John St. John, Jeremy Sullivan, Kevin Zhu, Greg Zynda, Daniel Balsam, Patrick Collison, Anthony B. Costa, Tina Hernandez-Boussard, Eric Ho, Ming-Yu Liu, Thomas McGrath, Kimberly Powell, Dave P. Burke, Hani Goodarzi, Patrick D. Hsu, Brian L. Hie",Open weights (unrestricted),https://arcinstitute.org/manuscripts/Evo2,,Genome modeling and design across all domains of life with Evo 2,2025-02-19,"Arc Institute,Stanford University,NVIDIA,Liquid,University of California (UC) Berkeley,Goodfire,Columbia University,University of California San Francisco",40300000000.0,Table 1 lists 40.3B parameters as model size.,2.25e+24,"40.3e9 parameters * 9.3e12 training datapoints * 6 = 2.25e24.
Same FLOPS estimate given by authors in Table 1.",OpenGenome 2,,9300000000000.0,"""We trained two versions of Evo 2: a smaller version at 7B parameters trained on 2.4 trillion tokens and a full version at 40B parameters trained on 9.3 trillion tokens.""",,,,Confident,"All of life encodes information with DNA. While tools for sequencing, synthesis, and editing of genomic code have transformed biological research, intelligently composing new biological systems would also require a deep understanding of the immense complexity encoded by genomes. We introduce Evo 2, a biological foundation model trained on 9.3 trillion DNA base pairs from a highly curated genomic atlas spanning all domains of life. We train Evo 2 with 7B and 40B parameters to have an unprecedented 1 million token context window with single-nucleotide resolution. Evo 2 learns from DNA sequence alone to accurately predict the functional impacts of genetic variationâ€”from noncoding pathogenic mutations to clinically significant BRCA1 variantsâ€”without task-specific finetuning. Applying mechanistic interpretability analyses, we reveal that Evo 2 autonomously learns a breadth of biological features, including exonâ€“intron boundaries, transcription factor binding sites, protein structural elements, and prophage genomic regions. Beyond its predictive capabilities, Evo 2 generates mitochondrial, prokaryotic, and eukaryotic sequences at genome scale with greater naturalness and coherence than previous methods. Guiding Evo 2 via inference-time search enables controllable generation of epigenomic structure, for which we demonstrate the first inference-time scaling results in biology. We make Evo 2 fully open, including model parameters, training code, inference code, and the OpenGenome2 dataset, to accelerate the exploration and design of biological complexity.","United States of America,United States of America,United States of America,United States of America,United States of America,United States of America,United States of America",,,,,,Open source,"Apache 2.0 (weigths)
https://huggingface.co/arcinstitute/evo2_40b_base

Apache 2.0 (code)
https://github.com/ArcInstitute/evo2?tab=readme-ov-file",,,
Evo 2 7B,Biology,Protein or nucleotide language model (pLM/nLM),"Garyk Brixi, Matthew G. Durrant, Jerome Ku, Michael Poli, Greg Brockman, Daniel Chang, Gabriel A. Gonzalez, Samuel H. King, David B. Li, Aditi T. Merchant, Mohsen Naghipourfar, Eric Nguyen, Chiara Ricci-Tam, David W. Romero, Gwanggyu Sun, Ali Taghibakshi, Anton Vorontsov, Brandon Yang, Myra Deng, Liv Gorton, Nam Nguyen, Nicholas K. Wang, Etowah Adams, Stephen A. Baccus, Steven Dillmann, Stefano Ermon, Daniel Guo, Rajesh Ilango, Ken Janik, Amy X. Lu, Reshma Mehta, Mohammad R.K. Mofrad, Madelena Y. Ng, Jaspreet Pannu, Christopher RÃ©, Jonathan C. Schmok, John St. John, Jeremy Sullivan, Kevin Zhu, Greg Zynda, Daniel Balsam, Patrick Collison, Anthony B. Costa, Tina Hernandez-Boussard, Eric Ho, Ming-Yu Liu, Thomas McGrath, Kimberly Powell, Dave P. Burke, Hani Goodarzi, Patrick D. Hsu, Brian L. Hie",Open weights (unrestricted),https://arcinstitute.org/manuscripts/Evo2,,Genome modeling and design across all domains of life with Evo 2,2025-02-19,"Arc Institute,Stanford University,NVIDIA,Liquid,University of California (UC) Berkeley,Goodfire,Columbia University,University of California San Francisco",7000000000.0,Table 1,1.008e+23,7e9 parameters *2.4e12 training datapoints*6=1.008e23 FLOP ,OpenGenome 2,,2400000000000.0,"""We trained two versions of Evo 2: a smaller version at 7B parameters trained on 2.4 trillion tokens and a full version at 40B parameters trained on 9.3 trillion tokens.""",,,,Confident,"All of life encodes information with DNA. While tools for sequencing, synthesis, and editing of genomic code have transformed biological research, intelligently composing new biological systems would also require a deep understanding of the immense complexity encoded by genomes. We introduce Evo 2, a biological foundation model trained on 9.3 trillion DNA base pairs from a highly curated genomic atlas spanning all domains of life. We train Evo 2 with 7B and 40B parameters to have an unprecedented 1 million token context window with single-nucleotide resolution. Evo 2 learns from DNA sequence alone to accurately predict the functional impacts of genetic variationâ€”from noncoding pathogenic mutations to clinically significant BRCA1 variantsâ€”without task-specific finetuning. Applying mechanistic interpretability analyses, we reveal that Evo 2 autonomously learns a breadth of biological features, including exonâ€“intron boundaries, transcription factor binding sites, protein structural elements, and prophage genomic regions. Beyond its predictive capabilities, Evo 2 generates mitochondrial, prokaryotic, and eukaryotic sequences at genome scale with greater naturalness and coherence than previous methods. Guiding Evo 2 via inference-time search enables controllable generation of epigenomic structure, for which we demonstrate the first inference-time scaling results in biology. We make Evo 2 fully open, including model parameters, training code, inference code, and the OpenGenome2 dataset, to accelerate the exploration and design of biological complexity.","United States of America,United States of America,United States of America,United States of America,United States of America,United States of America,United States of America",,,,,,Open source,"Apache 2.0 (weights)
https://huggingface.co/arcinstitute/evo2_7b

Apache 2.0 (code)
https://github.com/ArcInstitute/evo2?tab=readme-ov-file
",,,
Qwen2.5-VL-72B,"Language,Vision,Multimodal","Visual question answering,Video description,Language modeling/generation,Translation,Question answering,Character recognition (OCR),Quantitative reasoning","Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, Junyang Lin",Open weights (restricted use),https://arxiv.org/abs/2502.13923,,Qwen2.5-VL Technical Report,2025-02-19,Alibaba,72000000000.0,The model is initialized with pre-trained weights from the Qwen2.5 LLM,9.5712e+24,7.8e+24 FLOP [base LLM compute] + 1.7712e+24 FLOP [VL training compute] = 9.5712e+24 FLOP,Unspecified unreleased,,4100000000000.0,"Table 2
4.1T total training tokens:",,,,Confident,"We introduce Qwen2.5-VL, the latest flagship model of Qwen vision-language series, which demonstrates significant advancements in both foundational capabilities and innovative functionalities. Qwen2.5-VL achieves a major leap forward in understanding and interacting with the world through enhanced visual recognition, precise object localization, robust document parsing, and long-video comprehension. A standout feature of Qwen2.5-VL is its ability to localize objects using bounding boxes or points accurately. It provides robust structured data extraction from invoices, forms, and tables, as well as detailed analysis of charts, diagrams, and layouts. To handle complex inputs, Qwen2.5-VL introduces dynamic resolution processing and absolute time encoding, enabling it to process images of varying sizes and videos of extended durations (up to hours) with second-level event localization. This allows the model to natively perceive spatial scales and temporal dynamics without relying on traditional normalization techniques. By training a native dynamic-resolution Vision Transformer (ViT) from scratch and incorporating Window Attention, we reduce computational overhead while maintaining native resolution. As a result, Qwen2.5-VL excels not only in static image and document understanding but also as an interactive visual agent capable of reasoning, tool usage, and task execution in real-world scenarios such as operating computers and mobile devices. Qwen2.5-VL is available in three sizes, addressing diverse use cases from edge AI to high-performance computing. The flagship Qwen2.5-VL-72B model matches state-of-the-art models like GPT-4o and Claude 3.5 Sonnet, particularly excelling in document and diagram understanding. Additionally, Qwen2.5-VL maintains robust linguistic performance, preserving the core language competencies of the Qwen2.5 LLM.",China,Qwen2.5-72B,1.7712e+24,6 FLOP / parameter / token * 72 * 10^9 parameters * 4.1 * 10^12 tokens = 1.7712e+24 FLOP,,,Unreleased,"https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct

Qwen license (restriction on >100m monthly users)",,,
Qwen2.5-VL-7B ,"Language,Vision,Multimodal","Visual question answering,Video description,Language modeling/generation,Translation,Question answering,Character recognition (OCR),Quantitative reasoning","Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, Junyang Lin",Open weights (unrestricted),https://arxiv.org/abs/2502.13923,,Qwen2.5-VL Technical Report,2025-02-19,Alibaba,7000000000.0,The model is initialized with pre-trained weights from the Qwen2.5 LLM,9.9408e+23,8.2188e+23 FLOP [base LLM compute] + 1.722e+23 FLOP [VL training compute] = 9.9408e+23 FLOP,Unspecified unreleased,,4100000000000.0,"Table 2
4.1T total training tokens:",,,,Confident,"We introduce Qwen2.5-VL, the latest flagship model of Qwen vision-language series, which demonstrates significant advancements in both foundational capabilities and innovative functionalities. Qwen2.5-VL achieves a major leap forward in understanding and interacting with the world through enhanced visual recognition, precise object localization, robust document parsing, and long-video comprehension. A standout feature of Qwen2.5-VL is its ability to localize objects using bounding boxes or points accurately. It provides robust structured data extraction from invoices, forms, and tables, as well as detailed analysis of charts, diagrams, and layouts. To handle complex inputs, Qwen2.5-VL introduces dynamic resolution processing and absolute time encoding, enabling it to process images of varying sizes and videos of extended durations (up to hours) with second-level event localization. This allows the model to natively perceive spatial scales and temporal dynamics without relying on traditional normalization techniques. By training a native dynamic-resolution Vision Transformer (ViT) from scratch and incorporating Window Attention, we reduce computational overhead while maintaining native resolution. As a result, Qwen2.5-VL excels not only in static image and document understanding but also as an interactive visual agent capable of reasoning, tool usage, and task execution in real-world scenarios such as operating computers and mobile devices. Qwen2.5-VL is available in three sizes, addressing diverse use cases from edge AI to high-performance computing. The flagship Qwen2.5-VL-72B model matches state-of-the-art models like GPT-4o and Claude 3.5 Sonnet, particularly excelling in document and diagram understanding. Additionally, Qwen2.5-VL maintains robust linguistic performance, preserving the core language competencies of the Qwen2.5 LLM.",China,Qwen2.5-7B,1.722e+23,6 FLOP / parameter / token * 7 * 10^9 parameters * 4.1 * 10^12 tokens = 1.722e+23 FLOP,,,Unreleased,"Apache 2.0
https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct",,,
Step-1,Language,"Language modeling/generation,Question answering","Ailin Huang, Boyong Wu, Bruce Wang, Chao Yan, Chen Hu, Chengli Feng, Fei Tian, Feiyu Shen, Jingbei Li, Mingrui Chen, Peng Liu, Ruihang Miao, Wang You, Xi Chen, Xuerui Yang, Yechang Huang, Yuxiang Zhang, Zheng Gong, Zixin Zhang, Hongyu Zhou, Jianjian Sun, Brian Li, Chengting Feng, Changyi Wan, Hanpeng Hu, Jianchang Wu, Jiangjie Zhen, Ranchen Ming, Song Yuan, Xuelin Zhang, Yu Zhou, Bingxin Li, Buyun Ma, Hongyuan Wang, Kang An, Wei Ji, Wen Li, Xuan Wen, Xiangwen Kong, Yuankai Ma, Yuanwei Liang, Yun Mou, Bahtiyar Ahmidi, Bin Wang, Bo Li, Changxin Miao, Chen Xu, Chenrun Wang, Dapeng Shi, Deshan Sun, Dingyuan Hu, Dula Sai, Enle Liu, Guanzhe Huang, Gulin Yan, Heng Wang, Haonan Jia, Haoyang Zhang, Jiahao Gong, Junjing Guo, Jiashuai Liu, Jiahong Liu, Jie Feng, Jie Wu, Jiaoren Wu, Jie Yang, Jinguo Wang, Jingyang Zhang, Junzhe Lin, Kaixiang Li, Lei Xia, Li Zhou, Liang Zhao, Longlong Gu, Mei Chen, Menglin Wu, Ming Li, Mingxiao Li, Mingliang Li, Mingyao Liang, Na Wang, Nie Hao, Qiling Wu, Qinyuan Tan, Ran Sun, Shuai Shuai, Shaoliang Pang, Shiliang Yang, Shuli Gao, Shanshan Yuan, Siqi Liu, Shihong Deng, Shilei Jiang, Sitong Liu, Tiancheng Cao, Tianyu Wang, Wenjin Deng, Wuxun Xie, Weipeng Ming, Wenqing He et al. (45 additional authors not shown)",Unreleased,https://arxiv.org/abs/2502.11946,,Step-Audio: Unified Understanding and Generation in Intelligent Speech Interaction,2025-02-18,StepFun,130000000000.0,130B,6.24e+23,"6 FLOP / token / parameter * 130 * 10^9 parameters * 800 * 10^9 tokens = 6.24e+23 FLOP

[1 epoch assumed]",Unspecified unreleased,,800000000000.0,". The text data, amounting to 800 billion tokens, encompasses
web documents, books, code, and proprietary materials.",,,,Confident,"Real-time speech interaction, serving as a fundamental interface for human-machine collaboration, holds immense potential. However, current open-source models face limitations such as high costs in voice data collection, weakness in dynamic control, and limited intelligence. To address these challenges, this paper introduces Step-Audio, the first production-ready open-source solution. Key contributions include: 1) a 130B-parameter unified speech-text multi-modal model that achieves unified understanding and generation, with the Step-Audio-Chat version open-sourced; 2) a generative speech data engine that establishes an affordable voice cloning framework and produces the open-sourced lightweight Step-Audio-TTS-3B model through distillation; 3) an instruction-driven fine control system enabling dynamic adjustments across dialects, emotions, singing, and RAP; 4) an enhanced cognitive architecture augmented with tool calling and role-playing abilities to manage complex tasks effectively. Based on our new StepEval-Audio-360 evaluation benchmark, Step-Audio achieves state-of-the-art performance in human evaluations, especially in terms of instruction following. On open-source benchmarks like LLaMA Question, shows 9.3% average performance improvement, demonstrating our commitment to advancing the development of open-source multi-modal language technologies. Our code and models are available at this https URL.",China,,,,,,Unreleased,,,,
Grok 3,"Language,Vision,Multimodal","Chat,Language modeling/generation,Question answering,Code generation,Visual question answering",,API access,https://x.ai/blog/grok-3,,Grok 3 Beta â€” The Age of Reasoning Agents,2025-02-17,xAI,3000000000000.0,,3.5e+26,"Estimate based on a cluster of 80,000 H100s per the xai website and an estimated training time of approximately three months.

Full estimate here: https://docs.google.com/document/d/1MIUFviULJ3YI_XjyzL8cwG0cBRANKNxVEB4DrUcFiNs/edit?usp=sharing",Unspecified unreleased,"Knowledge cutoff date is November 17, 2024, according to https://docs.x.ai/docs/models#models-and-pricing. ",,,2160.0,"Estimated to be approximately 3 months. See compute estimate notes for more details.
",NVIDIA H100 SXM5 80GB,Likely,"We are pleased to introduce Grok 3, our most advanced model yet: blending strong reasoning with extensive pretraining knowledge. Trained on our Colossus supercluster with 10x the compute of previous state-of-the-art models, Grok 3 displays significant improvements in reasoning, mathematics, coding, world knowledge, and instruction-following tasks. Grok 3's reasoning capabilities, refined through large scale reinforcement learning, allow it to think for seconds to minutes, correcting errors, exploring alternatives, and delivering accurate answers. Grok 3 has leading performance across both academic benchmarks and real-world user preferences, achieving an Elo score of 1402 in the Chatbot Arena. Alongside it, weâ€™re unveiling Grok 3 mini, which represents a new frontier in cost-efficient reasoning. Both models are still in training and will evolve rapidly with your feedback. We are rolling out Grok 3 to users in the coming days, along with an early preview of its reasoning capabilities.",United States of America,,,,80000.0,,Unreleased,,,849737163.9529365,4446013316.520631
YAYI-Ultra,"Multimodal,Language,Vision","Language modeling/generation,Code generation,Question answering,Visual question answering,Character recognition (OCR)",,Hosted access (no API),https://web.archive.org/web/20250627090629/https://blog.csdn.net/qq_19841021/article/details/145658088,,,2025-02-15,Yayi (Wenge),,,,,Unspecified unreleased,,,,,,,Unknown,,China,,,,,,Unreleased,,,,
o3-mini,Language,"Language modeling/generation,Question answering,Quantitative reasoning,Code generation","Training
Brian Zhang, Eric Mitchell, Hongyu Ren, Kevin Lu, Max Schwarzer, Michelle Pokrass, Shengjia Zhao, Ted Sanders

Eval
Adam Kalai, Alex Tachard Passos, Ben Sokolowsky, Elaine Ya Le, Erik Ritter, Hao Sheng, Hanson Wang, Ilya Kostrikov, James Lee, Johannes Ferstad, Michael Lampe, Prashanth Radhakrishnan, Sean Fitzgerald, Sebastien Bubeck, Yann Dubois, Yu Bai

Frontier Evals & Preparedness
Andy Applebaum, Elizabeth Proehl, Evan Mays, Joel Parish, Kevin Liu, Leon Maksin, Leyton Ho, Miles Wang, Michele Wang, Olivia Watkins, Patrick Chao, Samuel Miserendino, Tejal Patwardhan

Engineering
Adam Walker, Akshay Nathan, Alyssa Huang, Andy Wang, Ankit Gohel, Ben Eggers, Brian Yu, Bryan Ashley, Chengdu Huang, Christian Hoareau, Davin Bogan, Emily Sokolova, Eric Horacek, Eric Jiang, Felipe Petroski Such, Jonah Cohen, Josh Gross, Justin Becker, Kan Wu, Kevin Whinnery, Larry Lv, Lee Byron, Manoli Liodakis, Max Johnson, Mike Trpcic, Murat Yesildal, Rasmus Rygaard, RJ Marsan, Rohit Ramchandani, Rohan Kshirsagar, Roman Huet, Sara Conlon, Shuaiqi (Tony) Xia, Siyuan Fu, Srinivas Narayanan, Sulman Choudhry, Tomer Kaftan, Trevor Creech

Search
Adam Fry, Adam Perelman, Brandon Wang, Cristina Scheau, Philip Pronin, Sundeep Tirumalareddy, Will Ellsworth, Zewei Chu

Product
Antonia Woodford, Beth Hoover, Jake Brill, Kelly Stirman, Minnia Feng, Neel Ajjarapu, Nick Turley, Nikunj Handa, Olivier Godement

Safety
Andrea Vallone, Andrew Duberstein, Enis Sert, Eric Wallace, Grace Zhao, Irina Kofman, Jieqi Yu, Joaquin Quinonero Candela, Madelaine Boyd, Mehmet Yatbaz, Mike McClay, Mingxuan Wang, Saachi Jain, Sandhini Agarwal, Sam Toizer, Santiago HernÃ¡ndez, Steve Mostovoy, Young Cha, Tao Li, Yunyun Wang

External Redteaming
Lama Ahmad, Troy Peterson


Research Program Managers
Carpus Chang, Kristen Ying

Leadership
Aidan Clark, Dane Stuckey, Jerry Tworek, Jakub Pachocki, Johannes Heidecke, Kevin Weil, Liam Fedus, Mark Chen, Sam Altman, Wojciech Zaremba",API access,https://openai.com/index/openai-o3-mini/,,Pushing the frontier of cost-effective reasoning.,2025-01-31,OpenAI,,"Can't get an exact estimate, but we suspect total parameter count around 60B-120B, active parameters around 10B-30B. 

Given these models are served at 150-200 tok/s, at $4.40/Mtok output, inference economics (https://epoch.ai/blog/inference-economics-of-language-models) suggests total parameter count around 60-120B parameters, with mixture-of-experts active parameters around 10-30B. MoEs make a given model roughly comparable to a ~50% smaller dense model (https://epoch.ai/gradient-updates/moe-vs-dense-models-inference), which lines up decently with Magistral Small pricing (24B dense, served at a similar speed for the cheaper $1.50/Mtok). ",,"We canâ€™t make a precise estimate, but seems unlikely to exceed 10^25 FLOP. We think active parameter count is 10-30B. This would require >55T tokens to reach 10^25 FLOP at the large size, i.e. well beyond 10x overtraining relative to Chinchilla.",Unspecified unreleased,"This has a knowledge cutoff date of October 2023 - which I assume means October 1, 2023 - according to https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models?tabs=global-standard%2Cstandard-chat-completions#o-series-models.",,,,,,Unknown,"Weâ€™re releasing OpenAI o3-mini, the newest, most cost-efficient model in our reasoning series, available in both ChatGPT and the API today. Previewed in December 2024â , this powerful and fast model advances the boundaries of what small models can achieve, delivering exceptional STEM capabilitiesâ€”with particular strength in science, math, and codingâ€”all while maintaining the low cost and reduced latency of OpenAI o1-mini.

OpenAI o3-mini is our first small reasoning model that supports highly requested developer features including function callingâ (opens in a new window), Structured Outputsâ (opens in a new window), and developer messagesâ (opens in a new window), making it production-ready out of the gate. Like OpenAI o1-mini and OpenAI o1-preview, o3-mini will support streamingâ (opens in a new window). Also, developers can choose between three reasoning effortâ (opens in a new window) optionsâ€”low, medium, and highâ€”to optimize for their specific use cases. This flexibility allows o3-mini to â€œthink harderâ€ when tackling complex challenges or prioritize speed when latency is a concern. o3-mini does not support vision capabilities, so developers should continue using OpenAI o1 for visual reasoning tasks. o3-mini is rolling out in the Chat Completions API, Assistants API, and Batch API starting today to select developers in API usage tiers 3-5â ",United States of America,,,,,,Unreleased,,,,
Mistral Small 3,Language,"Language modeling/generation,Question answering,Quantitative reasoning,Code generation,Translation",,Open weights (unrestricted),https://mistral.ai/news/mistral-small-3/,,"Mistral Small 3, a latency-optimized 24B-parameter model released under the Apache 2.0 license.",2025-01-30,Mistral AI,24000000000.0,24B,1.152e+24,6ND = 6*8T tokens * 24B parameters = 1.152e+24 FLOP,Unspecified unreleased,"""Notably, Mistral Small 3 was developed without reinforcement learning or synthetic training data, techniques commonly used by competitors. Lample said this â€œrawâ€ approach helps avoid embedding unwanted biases that could be difficult to detect later.""",8000000000000.0,"8 trillion tokens

Source: https://venturebeat.com/ai/mistral-small-3-brings-open-source-ai-to-the-masses-smaller-faster-and-cheaper/",,,,Confident,"Mistral Small 3 is competitive with larger models such as Llama 3.3 70B or Qwen 32B, and is an excellent open replacement for opaque proprietary models like GPT4o-mini. Mistral Small 3 is on par with Llama 3.3 70B instruct, while being more than 3x faster on the same hardware.

Mistral Small 3 is a pre-trained and instructed model catered to the â€˜80%â€™ of generative AI tasksâ€”those that require robust language and instruction following performance, with very low latency.

We designed this new model to saturate performance at a size suitable for local deployment. Particularly, Mistral Small 3 has far fewer layers than competing models, substantially reducing the time per forward pass. At over 81% accuracy on MMLU and 150 tokens/s latency, Mistral Small is currently the most efficient model of its category.

Weâ€™re releasing both a pretrained and instruction-tuned checkpoint under Apache 2.0. The checkpoints can serve as a powerful base for accelerating progress. Note that Mistral Small 3 is neither trained with RL nor synthetic data, so is earlier in the model production pipeline than models like Deepseek R1 (a great and complementary piece of open-source technology!). It can serve as a great base model for building accrued reasoning capacities. We look forward to seeing how the open-source community adopts and customizes it.",France,,,,,,Unreleased,"Apache 2.0

https://huggingface.co/mistralai/Mistral-Small-24B-Instruct-2501",,,
Qwen2.5-Max,Language,"Language modeling/generation,Question answering,Code generation",Qwen Team,API access,https://qwenlm.github.io/blog/qwen2.5-max/,,Qwen2.5-Max: Exploring the Intelligence of Large-scale MoE Model,2025-01-28,Alibaba,,,,,Unspecified unreleased,,20000000000000.0,"""Concurrently, we are developing Qwen2.5-Max, a large-scale MoE model that has been pretrained on over 20 trillion tokens and further post-trained with curated Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF) methodologies. """,,,,Unknown,"It is widely recognized that continuously scaling both data size and model size can lead to significant improvements in model intelligence. However, the research and industry community has limited experience in effectively scaling extremely large models, whether they are dense or Mixture-of-Expert (MoE) models. Many critical details regarding this scaling process were only disclosed with the recent release of DeepSeek V3. Concurrently, we are developing Qwen2.5-Max, a large-scale MoE model that has been pretrained on over 20 trillion tokens and further post-trained with curated Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF) methodologies. Today, we are excited to share the performance results of Qwen2.5-Max and announce the availability of its API through Alibaba Cloud. We also invite you to explore Qwen2.5-Max on Qwen Chat!",China,,,,,,Unreleased,,,,
Doubao-1.5-pro,Language,Language generation,,Hosted access (no API),https://team.doubao.com/zh/special/doubao_1_5_pro,,Doubao-1.5-pro,2025-01-22,ByteDance,,"Not directly reported. We are told it is a MoE model, and that it matches the performance of a dense model trained on the same data, while using 1/7th of the activated parameters. Additionally they say ""The number of parameters of the Doubao dense model is also much smaller than that of Llama3.1-405B"", which suggests that the number of activated parameters on the forward pass is ""much less"" than 405B/7 = 58B parameters.",,"The model appears to have been trained on 9T tokens; since we believe the MoE model uses ""much less"" than 58B parameters (see parameter notes), training compute is likely to be less than 6 * 9T * 58B = 3.132e24

It is possible the 9T token training run was for comparison sake against the dense model (they label it as ""doubao-MoE"", not doubao-1.5-pro), and that they continued training beyond this. They would need to train for at least 29T tokens to ",,,9000000000000.0,9T tokens,,,,Confident,The model uses the MoE architecture and explores the ultimate balance between model performance and reasoning performance through integrated training-thinking design. Doubao-1.5-pro can use only a smaller activation parameter to exceed the performance of a first-class super-large pre-training model and achieve excellent results on multiple evaluation benchmarks.,China,,,,,,Unreleased,,,,
DeepSeek-R1,Language,"Language modeling/generation,Code generation,Quantitative reasoning,Question answering",,Open weights (unrestricted),https://api-docs.deepseek.com/news/news250120,,DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning,2025-01-20,DeepSeek,671000000000.0,"671B total
37B activated
https://github.com/deepseek-ai/DeepSeek-R1/tree/main",3.5e+24,"3.29e24 + 1.8e23 = 3.47e24, round to 3.5e24 due to lack of sigfigs",Unspecified unreleased,"RL + SFT

When reasoning-oriented RL converges, we utilize the resulting checkpoint to collect SFT (Supervised Fine-Tuning) data for the subsequent round. Unlike the initial cold-start data, which primarily focuses on reasoning, this stage incorporates data from other domains to enhance the modelâ€™s capabilities in writing, role-playing, and other general-purpose tasks.",,,,,,Confident,"We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities.
Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors. However, it encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.",China,DeepSeek-V3,1.83e+23,"H800 throughput is 1.5e15 FLOPs, and implied MFU is 23%
147k H800 GPU-hours is 147k * 3600 s/hr * 1.5e15 * 0.23 = 1.83e23 FLOP",,,Unreleased,"MIT licensed
https://huggingface.co/deepseek-ai/DeepSeek-R1",,,
MiniMax Speech-01-HD (T2A-01-HD),Speech,"Text-to-speech (TTS),Speech synthesis",,API access,"https://www.minimaxi.com/news/t2a-01-hd-%E5%8F%91%E5%B8%83

https://platform.minimaxi.com/document/T2A%20V2?key=66719005a427f0c8a5701643",,"speech-01-hd: Upgraded new architecture, ultra-high similarity, ultra-clear sound quality",2025-01-15,MiniMax,,,,,Unspecified unreleased,,,,,,,Unknown,"T2A-01-HD is an audio generator optimized for speech. T2A-01-HD can generate a synthetic voice with adjustable cadence, tone, and tenor in around 17 different languages, including English and Chinese, and clone a voice from just 10 seconds of an audio recording.",China,,,,,,Unreleased,,,,
MiniMax-Text-01,Language,"Language modeling/generation,Question answering","MiniMax, Aonian Li, Bangwei Gong, Bo Yang, Boji Shan, Chang Liu, Cheng Zhu, Chunhao Zhang, Congchao Guo, Da Chen, Dong Li, Enwei Jiao, Gengxin Li, Guojun Zhang, Haohai Sun, Houze Dong, Jiadai Zhu, Jiaqi Zhuang, Jiayuan Song, Jin Zhu, Jingtao Han, Jingyang Li, Junbin Xie, Junhao Xu, Junjie Yan, Kaishun Zhang, Kecheng Xiao, Kexi Kang, Le Han, Leyang Wang, Lianfei Yu, Liheng Feng, Lin Zheng, Linbo Chai, Long Xing, Meizhi Ju, Mingyuan Chi, Mozhi Zhang, Peikai Huang, Pengcheng Niu, Pengfei Li, Pengyu Zhao, Qi Yang, Qidi Xu, Qiexiang Wang, Qin Wang, Qiuhui Li, Ruitao Leng, Shengmin Shi, Shuqi Yu, Sichen Li, Songquan Zhu, Tao Huang, Tianrun Liang, Weigao Sun, Weixuan Sun, Weiyu Cheng, Wenkai Li, Xiangjun Song, Xiao Su, Xiaodong Han, Xinjie Zhang, Xinzhu Hou, Xu Min, Xun Zou, Xuyang Shen, Yan Gong, Yingjie Zhu, Yipeng Zhou, Yiran Zhong, Yongyi Hu, Yuanxiang Fan, Yue Yu, Yufeng Yang, Yuhao Li, Yunan Huang, Yunji Li, Yunpeng Huang, Yunzhi Xu, Yuxin Mao, Zehan Li, Zekang Li, Zewei Tao, Zewen Ying, Zhaoyang Cong, Zhen Qin, Zhenhua Fan, Zhihang Yu, Zhuo Jiang, Zijia Wu",Open weights (restricted use),https://arxiv.org/abs/2501.08313,,MiniMax-01: Scaling Foundation Models with Lightning Attention,2025-01-14,MiniMax,456000000000.0,"Total Parameters: 456B
Activated Parameters per Token: 45.9B
Number Layers: 80
Hybrid Attention: a softmax attention is positioned after every 7 lightning attention.
Number of attention heads: 64
Attention head dimension: 128
Mixture of Experts:
Number of experts: 32
Expert hidden dimension: 9216
Top-2 routing strategy
Positional Encoding: Rotary Position Embedding (RoPE) applied to half of the attention head dimension with a base frequency of 10,000,000
Hidden Size: 6144
Vocab Size: 200,064",3.1417632e+24,"6 FLOP / token / parameter * 45.9 * 10^9 activated parameters * 1.1408e+13 tokens = 1.98288e+24 FLOP

""Likely"" confidence because the model is MoE (formula might not be that accurate) 

data trained on is  1.1408e+13 = ((16 million * 500)+7.2 trillion + 3.2 trillion + 1 trillion)",Unspecified unreleased,,7200000000000.0,7.2T tokens,,,NVIDIA H800 SXM5,Likely,"We introduce MiniMax-01 series, including MiniMax-Text-01 and MiniMax-VL-01, which are comparable to top-tier models while offering superior capabilities in processing longer contexts. The core lies in lightning attention and its efficient scaling. To maximize computational capacity, we integrate it with Mixture of Experts (MoE), creating a model with 32 experts and 456 billion total parameters, of which 45.9 billion are activated for each token. We develop an optimized parallel strategy and highly efficient computation-communication overlap techniques for MoE and lightning attention. This approach enables us to conduct efficient training and inference on models with hundreds of billions of parameters across contexts spanning millions of tokens. The context window of MiniMax-Text-01 can reach up to 1 million tokens during training and extrapolate to 4 million tokens during inference at an affordable cost. Our vision-language model, MiniMax-VL-01 is built through continued training with 512 billion vision-language tokens. Experiments on both standard and in-house benchmarks show that our models match the performance of state-of-the-art models like GPT-4o and Claude-3.5-Sonnet while offering 20-32 times longer context window. We publicly release MiniMax-01 at this https URL.",China,,,,,,Unreleased,"https://huggingface.co/MiniMaxAI/MiniMax-Text-01

""MiniMax may terminate this Agreement if you are in breach of any term or condition of this Agreement.""

code seems to be just inference code: 
https://github.com/MiniMax-AI/MiniMax-01",,,
MiniMax-VL-01,"Vision,Language,Multimodal","Visual question answering,Language modeling/generation,Question answering","MiniMax, Aonian Li, Bangwei Gong, Bo Yang, Boji Shan, Chang Liu, Cheng Zhu, Chunhao Zhang, Congchao Guo, Da Chen, Dong Li, Enwei Jiao, Gengxin Li, Guojun Zhang, Haohai Sun, Houze Dong, Jiadai Zhu, Jiaqi Zhuang, Jiayuan Song, Jin Zhu, Jingtao Han, Jingyang Li, Junbin Xie, Junhao Xu, Junjie Yan, Kaishun Zhang, Kecheng Xiao, Kexi Kang, Le Han, Leyang Wang, Lianfei Yu, Liheng Feng, Lin Zheng, Linbo Chai, Long Xing, Meizhi Ju, Mingyuan Chi, Mozhi Zhang, Peikai Huang, Pengcheng Niu, Pengfei Li, Pengyu Zhao, Qi Yang, Qidi Xu, Qiexiang Wang, Qin Wang, Qiuhui Li, Ruitao Leng, Shengmin Shi, Shuqi Yu, Sichen Li, Songquan Zhu, Tao Huang, Tianrun Liang, Weigao Sun, Weixuan Sun, Weiyu Cheng, Wenkai Li, Xiangjun Song, Xiao Su, Xiaodong Han, Xinjie Zhang, Xinzhu Hou, Xu Min, Xun Zou, Xuyang Shen, Yan Gong, Yingjie Zhu, Yipeng Zhou, Yiran Zhong, Yongyi Hu, Yuanxiang Fan, Yue Yu, Yufeng Yang, Yuhao Li, Yunan Huang, Yunji Li, Yunpeng Huang, Yunzhi Xu, Yuxin Mao, Zehan Li, Zekang Li, Zewei Tao, Zewen Ying, Zhaoyang Cong, Zhen Qin, Zhenhua Fan, Zhihang Yu, Zhuo Jiang, Zijia Wu",Open weights (restricted use),https://arxiv.org/abs/2501.08313,,MiniMax-01: Scaling Foundation Models with Lightning Attention,2025-01-14,MiniMax,,,2.1238848e+24,1.98288e+24 FLOP [base model compute] + 1.410048e+23 FLOP [addtional vision-language finetune compute] = 2.1238848e+24 FLOP,Unspecified unreleased,,512000000000.0,"""MiniMax-VL-01 undergoes additional training with 512 billion vision-language tokens""",,,NVIDIA H800 SXM5,Likely,"We introduce MiniMax-01 series, including MiniMax-Text-01 and MiniMax-VL-01, which are comparable to top-tier models while offering superior capabilities in processing longer contexts. The core lies in lightning attention and its efficient scaling. To maximize computational capacity, we integrate it with Mixture of Experts (MoE), creating a model with 32 experts and 456 billion total parameters, of which 45.9 billion are activated for each token. We develop an optimized parallel strategy and highly efficient computation-communication overlap techniques for MoE and lightning attention. This approach enables us to conduct efficient training and inference on models with hundreds of billions of parameters across contexts spanning millions of tokens. The context window of MiniMax-Text-01 can reach up to 1 million tokens during training and extrapolate to 4 million tokens during inference at an affordable cost. Our vision-language model, MiniMax-VL-01 is built through continued training with 512 billion vision-language tokens. Experiments on both standard and in-house benchmarks show that our models match the performance of state-of-the-art models like GPT-4o and Claude-3.5-Sonnet while offering 20-32 times longer context window. We publicly release MiniMax-01 at this https URL.",China,MiniMax-Text-01,1.410048e+23,"Assuming same amount of activated parameters (45.9 * 10^9) as for the base model:

6 FLOP / parameter / token * 45.9 * 10^9 activated parameters * 512 * 10^9 tokens = 1.410048e+23 FLOP",,,Unreleased,"https://huggingface.co/MiniMaxAI/MiniMax-VL-01

""MiniMax may terminate this Agreement if you are in breach of any term or condition of this Agreement.""

code seems to be just inference code: 
https://github.com/MiniMax-AI/MiniMax-01",,,
SenseNova Unified Large Model,Language,,SenseTime,,https://www.sensetime.com/en/news-detail/51169317?categoryId=1072,,"SenseTime Launches the SenseNova Unified Large Model, Clinches Top Rankings Across Key Model Benchmark Evaluations",2025-01-13,SenseTime,,,,,,,,,,,,Unknown,"SenseTime has officially launched the SenseNova Unified Large Model, leading the way in natively integrated modalities. This model, which unifies in-depth reasoning and multimodal information processing capabilities, clinched first place on two comprehensive benchmarks, one on testing language capabilities and the other on visual content understanding.

According to the latest â€œ2024 Chinese Large Model Benchmark Reportâ€ by SuperCLUE, a leading model evaluation institute, the SenseNova Unified Large Model achieved an outstanding score of 68.3, ranking first place in China together with DeepSeek V3.",Hong Kong,,,,,,,,,,
"Cosmos-1.0-
Diffusion-14B Video2World","Robotics,Vision,Video","Robotic manipulation,Self-driving car,Video generation","NVIDIA: Niket Agarwal, Arslan Ali, Maciej Bala, Yogesh Balaji, Erik Barker, Tiffany Cai, Prithvijit Chattopadhyay, Yongxin Chen, Yin Cui, Yifan Ding, Daniel Dworakowski, Jiaojiao Fan, Michele Fenzi, Francesco Ferroni, Sanja Fidler, Dieter Fox, Songwei Ge, Yunhao Ge, Jinwei Gu, Siddharth Gururani, Ethan He, Jiahui Huang, Jacob Huffman, Pooya Jannaty, Jingyi Jin, Seung Wook Kim, Gergely KlÃ¡r, Grace Lam, Shiyi Lan, Laura Leal-Taixe, Anqi Li, Zhaoshuo Li, Chen-Hsuan Lin, Tsung-Yi Lin, Huan Ling, Ming-Yu Liu, Xian Liu, Alice Luo, Qianli Ma, Hanzi Mao, Kaichun Mo, Arsalan Mousavian, Seungjun Nah, Sriharsha Niverty, David Page, Despoina Paschalidou, Zeeshan Patel, Lindsey Pavao, Morteza Ramezanali, Fitsum Reda, Xiaowei Ren, Vasanth Rao Naik Sabavat, Ed Schmerling, Stella Shi, Bartosz Stefaniak, Shitao Tang, Lyne Tchapmi, Przemek Tredak, Wei-Cheng Tseng, Jibin Varghese, Hao Wang, Haoxiang Wang, Heng Wang, Ting-Chun Wang, Fangyin Wei, Xinyue Wei, Jay Zhangjie Wu, Jiashu Xu, Wei Yang, Lin Yen-Chen, Xiaohui Zeng, Yu Zeng, Jing Zhang, Qinsheng Zhang, Yuxuan Zhang, Qingqing Zhao, Artur Zolkowski",Open weights (restricted use),https://arxiv.org/abs/2501.03575,,Cosmos World Foundation Model Platform for Physical AI,2025-01-07,NVIDIA,14000000000.0,14B,2.8e+24,"""We train all of the WFM models reported in the paper using a cluster of 10,000 NVIDIA H100 GPUs in a time span of three months.""

I assign the FLOPs from this cluster proportional to the parameter size of the model trained. There are a total of 76B parameters between the 8 models. Therefore, assuming 20% utilization (starting with 33% but then accounting for time between experiments), we get
(10k H100s)*(90 days)*(24*60*60)*(979e12)*(0.2 utilization)*(14/76) = 2.8e24 FLOPs
",Unspecified unreleased,,,"""Suite of first-generation video models trained on 9,000 trillion tokens, including 20 million hours of robotics and driving data - generating high-quality videos from multimodal inputs like images, text, or video."" - https://www.nvidia.com/en-us/ai/cosmos/ ",,"""We train all of the WFM models reported in the paper using a cluster of 10,000 NVIDIA H100 GPUs in a time span of three months.""",NVIDIA H100 SXM5 80GB,Likely,"Physical AI needs to be trained digitally first. It needs a digital twin of itself, the policy model, and a digital twin of the world, the world model. In this paper, we present the Cosmos World Foundation Model Platform to help developers build customized world models for their Physical AI setups. We position a world foundation model as a general-purpose world model that can be fine-tuned into customized world models for downstream applications. Our platform covers a video curation pipeline, pre-trained world foundation models, examples of post-training of pre-trained world foundation models, and video tokenizers. To help Physical AI builders solve the most critical problems of our society, we make our platform open-source and our models open-weight with permissive licenses available via this https URL.",United States of America,,,,10000.0,,Unreleased,"NVIDIA Open Model License Agreement
Under the NVIDIA Open Model License, NVIDIA confirms:

Models are commercially usable.
You are free to create and distribute Derivative Models.
NVIDIA does not claim ownership to any outputs generated using the Models or Derivative Models.

Important Note: If you bypass, disable, reduce the efficacy of, or circumvent any technical limitation, safety guardrail or associated safety guardrail hyperparameter, encryption, security, digital rights management, or authentication mechanism contained in the Model, your rights under NVIDIA Open Model License Agreement will automatically terminate.

https://huggingface.co/nvidia/Cosmos-1.0-Diffusion-14B-Video2World",,,
Cosmos-Predict1-7b-Video2World,"Video,Vision,Robotics","Robotic manipulation,System control,Video generation","NVIDIA: Niket Agarwal, Arslan Ali, Maciej Bala, Yogesh Balaji, Erik Barker, Tiffany Cai, Prithvijit Chattopadhyay, Yongxin Chen, Yin Cui, Yifan Ding, Daniel Dworakowski, Jiaojiao Fan, Michele Fenzi, Francesco Ferroni, Sanja Fidler, Dieter Fox, Songwei Ge, Yunhao Ge, Jinwei Gu, Siddharth Gururani, Ethan He, Jiahui Huang, Jacob Huffman, Pooya Jannaty, Jingyi Jin, Seung Wook Kim, Gergely KlÃ¡r, Grace Lam, Shiyi Lan, Laura Leal-Taixe, Anqi Li, Zhaoshuo Li, Chen-Hsuan Lin, Tsung-Yi Lin, Huan Ling, Ming-Yu Liu, Xian Liu, Alice Luo, Qianli Ma, Hanzi Mao, Kaichun Mo, Arsalan Mousavian, Seungjun Nah, Sriharsha Niverty, David Page, Despoina Paschalidou, Zeeshan Patel, Lindsey Pavao, Morteza Ramezanali, Fitsum Reda, Xiaowei Ren, Vasanth Rao Naik Sabavat, Ed Schmerling, Stella Shi, Bartosz Stefaniak, Shitao Tang, Lyne Tchapmi, Przemek Tredak, Wei-Cheng Tseng, Jibin Varghese, Hao Wang, Haoxiang Wang, Heng Wang, Ting-Chun Wang, Fangyin Wei, Xinyue Wei, Jay Zhangjie Wu, Jiashu Xu, Wei Yang, Lin Yen-Chen, Xiaohui Zeng, Yu Zeng, Jing Zhang, Qinsheng Zhang, Yuxuan Zhang, Qingqing Zhao, Artur Zolkowski",Open weights (restricted use),https://arxiv.org/abs/2501.03575,,Cosmos World Foundation Model Platform for Physical AI,2025-01-07,NVIDIA,7000000000.0,7B,1.4e+24,"""We train all of the WFM models reported in the paper using a cluster of 10,000 NVIDIA H100 GPUs in a time span of three months.""

I assign the FLOPs from this cluster proportional to the parameter size of the model trained. There are a total of 76B parameters between the 8 models. Therefore, assuming 20% utilization (starting with 33% but then accounting for time between experiments), we get
(10k H100s)*(90 days)*(24*60*60)*(979e12)*(0.2 utilization)*(7/76) = 1.4e+24 FLOPs
",Unspecified unreleased,,,,,"""We train all of the WFM models reported in the paper using a cluster of 10,000 NVIDIA H100 GPUs in a time span of three months""",NVIDIA H100 SXM5 80GB,Likely,"Physical AI needs to be trained digitally first. It needs a digital twin of itself, the policy model, and a digital twin of the world, the world model. In this paper, we present the Cosmos World Foundation Model Platform to help developers build customized world models for their Physical AI setups. We position a world foundation model as a general-purpose world model that can be fine-tuned into customized world models for downstream applications. Our platform covers a video curation pipeline, pre-trained world foundation models, examples of post-training of pre-trained world foundation models, and video tokenizers. To help Physical AI builders solve the most critical problems of our society, we make Cosmos open-source and our models open-weight with permissive licenses available via https://github.com/nvidia-cosmos/cosmos-predict1.",United States of America,,,,10000.0,,Unreleased,"https://huggingface.co/nvidia/Cosmos-Predict1-7B-Video2World

NVIDIA Open Model License Agreement
Under the NVIDIA Open Model License, NVIDIA confirms:

Models are commercially usable.
You are free to create and distribute Derivative Models.
NVIDIA does not claim ownership to any outputs generated using the Models or Derivative Models.

Important Note: If you bypass, disable, reduce the efficacy of, or circumvent any technical limitation, safety guardrail or associated safety guardrail hyperparameter, encryption, security, digital rights management, or authentication mechanism contained in the Model, your rights under NVIDIA Open Model License Agreement will automatically terminate.",,,
Cosmos-Predict1-14b-Video2World,"Video,Vision,Robotics","Robotic manipulation,System control,Video generation","NVIDIA: Niket Agarwal, Arslan Ali, Maciej Bala, Yogesh Balaji, Erik Barker, Tiffany Cai, Prithvijit Chattopadhyay, Yongxin Chen, Yin Cui, Yifan Ding, Daniel Dworakowski, Jiaojiao Fan, Michele Fenzi, Francesco Ferroni, Sanja Fidler, Dieter Fox, Songwei Ge, Yunhao Ge, Jinwei Gu, Siddharth Gururani, Ethan He, Jiahui Huang, Jacob Huffman, Pooya Jannaty, Jingyi Jin, Seung Wook Kim, Gergely KlÃ¡r, Grace Lam, Shiyi Lan, Laura Leal-Taixe, Anqi Li, Zhaoshuo Li, Chen-Hsuan Lin, Tsung-Yi Lin, Huan Ling, Ming-Yu Liu, Xian Liu, Alice Luo, Qianli Ma, Hanzi Mao, Kaichun Mo, Arsalan Mousavian, Seungjun Nah, Sriharsha Niverty, David Page, Despoina Paschalidou, Zeeshan Patel, Lindsey Pavao, Morteza Ramezanali, Fitsum Reda, Xiaowei Ren, Vasanth Rao Naik Sabavat, Ed Schmerling, Stella Shi, Bartosz Stefaniak, Shitao Tang, Lyne Tchapmi, Przemek Tredak, Wei-Cheng Tseng, Jibin Varghese, Hao Wang, Haoxiang Wang, Heng Wang, Ting-Chun Wang, Fangyin Wei, Xinyue Wei, Jay Zhangjie Wu, Jiashu Xu, Wei Yang, Lin Yen-Chen, Xiaohui Zeng, Yu Zeng, Jing Zhang, Qinsheng Zhang, Yuxuan Zhang, Qingqing Zhao, Artur Zolkowski",Open weights (restricted use),https://arxiv.org/abs/2501.03575,,Cosmos World Foundation Model Platform for Physical AI,2025-01-07,NVIDIA,14000000000.0,14B,2.8e+24,"""We train all of the WFM models reported in the paper using a cluster of 10,000 NVIDIA H100 GPUs in a time span of three months.""

I assign the FLOPs from this cluster proportional to the parameter size of the model trained. There are a total of 76B parameters between the 8 models. Therefore, assuming 20% utilization (starting with 33% but then accounting for time between experiments), we get
(10k H100s)*(90 days)*(24*60*60)*(979e12)*(0.2 utilization)*(14/76) = 2.8e24 FLOPs
",Unspecified unreleased,,,,,"""We train all of the WFM models reported in the paper using a cluster of 10,000 NVIDIA H100 GPUs in a time span of three months""",NVIDIA H100 SXM5 80GB,Likely,"Physical AI needs to be trained digitally first. It needs a digital twin of itself, the policy model, and a digital twin of the world, the world model. In this paper, we present the Cosmos World Foundation Model Platform to help developers build customized world models for their Physical AI setups. We position a world foundation model as a general-purpose world model that can be fine-tuned into customized world models for downstream applications. Our platform covers a video curation pipeline, pre-trained world foundation models, examples of post-training of pre-trained world foundation models, and video tokenizers. To help Physical AI builders solve the most critical problems of our society, we make Cosmos open-source and our models open-weight with permissive licenses available via https://github.com/nvidia-cosmos/cosmos-predict1.",United States of America,,,,10000.0,,Unreleased,"https://huggingface.co/nvidia/Cosmos-Predict1-14B-Video2World

NVIDIA Open Model License Agreement
Under the NVIDIA Open Model License, NVIDIA confirms:

Models are commercially usable.
You are free to create and distribute Derivative Models.
NVIDIA does not claim ownership to any outputs generated using the Models or Derivative Models.

Important Note: If you bypass, disable, reduce the efficacy of, or circumvent any technical limitation, safety guardrail or associated safety guardrail hyperparameter, encryption, security, digital rights management, or authentication mechanism contained in the Model, your rights under NVIDIA Open Model License Agreement will automatically terminate.",,,
OLMo 2 Furious 7B,Language,"Language modeling/generation,Question answering","Team OLMo, Pete Walsh, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Shane Arora, Akshita Bhagia, Yuling Gu, Shengyi Huang, Matt Jordan, Nathan Lambert, Dustin Schwenk, Oyvind Tafjord, Taira Anderson, David Atkinson, Faeze Brahman, Christopher Clark, Pradeep Dasigi, Nouha Dziri, Michal Guerquin, Hamish Ivison, Pang Wei Koh, Jiacheng Liu, Saumya Malik, William Merrill, Lester James V. Miranda, Jacob Morrison, Tyler Murray, Crystal Nam, Valentina Pyatkin, Aman Rangapur, Michael Schmitz, Sam Skjonsberg, David Wadden, Christopher Wilhelm, Michael Wilson, Luke Zettlemoyer, Ali Farhadi, Noah A. Smith, Hannaneh Hajishirzi",Open weights (unrestricted),https://arxiv.org/abs/2501.00656,,2 OLMo 2 Furious,2024-12-31,"Allen Institute for AI,University of Washington,New York University (NYU)",7000000000.0,7B,1.8e+23,1.8*10^23 FLOPs (Table 6 - developers calculated using 6ND formula),"OLMo-Mix-1124,Dolmino-Mix-1124,Tulu 3 70B",,4000000000000.0,"Pretraining Stage 1
(OLMo-Mix-1124)	4 trillion tokens (= 1 epoch)	

Pretraining Stage 2
(Dolmino-Mix-1124)	50B tokens (3 runs)
merged	

Post-training
(Tulu 3 SFT OLMo mix)	SFT + DPO + PPO
(preference mix)	",,,NVIDIA H100 SXM5 80GB,Confident,"We present OLMo 2, the next generation of our fully open language models. OLMo 2 includes dense autoregressive models with improved architecture and training recipe, pretraining data mixtures, and instruction tuning recipes. Our modified model architecture and training recipe achieve both better training stability and improved per-token efficiency. Our updated pretraining data mixture introduces a new, specialized data mix called Dolmino Mix 1124, which significantly improves model capabilities across many downstream task benchmarks when introduced via late-stage curriculum training (i.e. specialized data during the annealing phase of pretraining). Finally, we incorporate best practices from TÃ¼lu 3 to develop OLMo 2-Instruct, focusing on permissive data and extending our final-stage reinforcement learning with verifiable rewards (RLVR). Our OLMo 2 base models sit at the Pareto frontier of performance to compute, often matching or outperforming open-weight only models like Llama 3.1 and Qwen 2.5 while using fewer FLOPs and with fully transparent training data, code, and recipe. Our fully open OLMo 2-Instruct models are competitive with or surpassing open-weight only models of comparable size, including Qwen 2.5, Llama 3.1 and Gemma 2. We release all OLMo 2 artifacts openly -- models at 7B and 13B scales, both pretrained and post-trained, including their full training data, training code and recipes, training logs and thousands of intermediate checkpoints. The final instruction model is available on the Ai2 Playground as a free research demo.","United States of America,United States of America,United States of America",,,,,,Open source,"apache 2
https://huggingface.co/allenai/OLMo-2-1124-7B
https://github.com/allenai/OLMo",,,
OLMo 2 Furious 13B,Language,"Language modeling/generation,Question answering","Team OLMo, Pete Walsh, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Shane Arora, Akshita Bhagia, Yuling Gu, Shengyi Huang, Matt Jordan, Nathan Lambert, Dustin Schwenk, Oyvind Tafjord, Taira Anderson, David Atkinson, Faeze Brahman, Christopher Clark, Pradeep Dasigi, Nouha Dziri, Michal Guerquin, Hamish Ivison, Pang Wei Koh, Jiacheng Liu, Saumya Malik, William Merrill, Lester James V. Miranda, Jacob Morrison, Tyler Murray, Crystal Nam, Valentina Pyatkin, Aman Rangapur, Michael Schmitz, Sam Skjonsberg, David Wadden, Christopher Wilhelm, Michael Wilson, Luke Zettlemoyer, Ali Farhadi, Noah A. Smith, Hannaneh Hajishirzi",Open weights (unrestricted),https://arxiv.org/abs/2501.00656,,2 OLMo 2 Furious,2024-12-31,"Allen Institute for AI,University of Washington,New York University (NYU)",13000000000.0,13B,4.6e+23,"4.6*10^23 FLOPs (Table 6 - developers calculated using 6ND formula)
","OLMo-Mix-1124,Dolmino-Mix-1124,Tulu 3 70B",,4000000000000.0,"Pretraining Stage 1
(OLMo-Mix-1124)	5 trillion tokens ( = 1.2 epochs)
Pretraining Stage 2
(Dolmino-Mix-1124) 100B tokens (3 runs)
300B tokens (1 run)
merged
Post-training
(Tulu 3 SFT OLMo mix)	SFT + DPO + PPO
(preference mix)	",,,NVIDIA H100 SXM5 80GB,Confident,"We present OLMo 2, the next generation of our fully open language models. OLMo 2 includes dense autoregressive models with improved architecture and training recipe, pretraining data mixtures, and instruction tuning recipes. Our modified model architecture and training recipe achieve both better training stability and improved per-token efficiency. Our updated pretraining data mixture introduces a new, specialized data mix called Dolmino Mix 1124, which significantly improves model capabilities across many downstream task benchmarks when introduced via late-stage curriculum training (i.e. specialized data during the annealing phase of pretraining). Finally, we incorporate best practices from TÃ¼lu 3 to develop OLMo 2-Instruct, focusing on permissive data and extending our final-stage reinforcement learning with verifiable rewards (RLVR). Our OLMo 2 base models sit at the Pareto frontier of performance to compute, often matching or outperforming open-weight only models like Llama 3.1 and Qwen 2.5 while using fewer FLOPs and with fully transparent training data, code, and recipe. Our fully open OLMo 2-Instruct models are competitive with or surpassing open-weight only models of comparable size, including Qwen 2.5, Llama 3.1 and Gemma 2. We release all OLMo 2 artifacts openly -- models at 7B and 13B scales, both pretrained and post-trained, including their full training data, training code and recipes, training logs and thousands of intermediate checkpoints. The final instruction model is available on the Ai2 Playground as a free research demo.","United States of America,United States of America,United States of America",,,,,,Open source,"apache 2
https://huggingface.co/allenai/OLMo-2-1124-13B
https://github.com/allenai/OLMo",,,
DeepSeek-V3,Language,"Language modeling/generation,Code generation,Quantitative reasoning,Question answering","DeepSeek-AI, Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Haowei Zhang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Li, Hui Qu, J.L. Cai, Jian Liang, Jianzhong Guo, Jiaqi Ni, Jiashi Li, Jiawei Wang, Jin Chen, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, Junxiao Song, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Lei Xu, Leyi Xia, Liang Zhao, Litong Wang, Liyue Zhang, Meng Li, Miaojun Wang, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Mingming Li, Ning Tian, Panpan Huang, Peiyi Wang, Peng Zhang, Qiancheng Wang, Qihao Zhu, Qinyu Chen, Qiushi Du, R.J. Chen, R.L. Jin, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, Runxin Xu, Ruoyu Zhang, Ruyi Chen, S.S. Li, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shaoqing Wu, Shengfeng Ye, Shengfeng Ye, Shirong Ma, Shiyu Wang, Shuang Zhou, Shuiping Yu, Shunfeng Zhou, Shuting Pan, T. Wang, Tao Yun, Tian Pei, Tianyu Sun, W.L. Xiao, Wangding Zeng et al. (100 additional authors not shown)",Open weights (restricted use),https://arxiv.org/abs/2412.19437,,DeepSeek-V3 Technical Report,2024-12-24,DeepSeek,671000000000.0,Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token.,3.4078e+24,"""At an economical cost of only 2.664M H800 GPU hours, we complete the pre-training of DeepSeek-V3 on 14.8T tokens, producing the currently strongest open-source base model. The subsequent training stages after pre-training require only 0.1M GPU hours.""

6 * 37B (active params) * 14.8T = 3.2856e24 for pretraining.

We know they trained in FP8. H800s get 1.513e15 FLOP/s in FP8:
2.688M * 3600 * 1.513e15 * MFU = 3.2856e24

Suggests a MFU of 0.2244 in pre-training. If we assume MFU was the same in post-training, that adds an additional:

0.1M * 3600 * 1.513e15 * 0.2244 = 1.222e23 FLOP from post-training

Total: 3.2856e24 + 1.222e23 = 3.4078e24 FLOP",,,14800000000000.0,"""We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities""",,"""DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training""",NVIDIA H800 SXM5,Confident,"We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. To achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms other open-source models and achieves performance comparable to leading closed-source models. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training. In addition, its training process is remarkably stable. Throughout the entire training process, we did not experience any irrecoverable loss spikes or perform any rollbacks.",China,,,,2048.0,0.1947,Unreleased,"MIT and deepseek license
https://github.com/deepseek-ai/DeepSeek-V3?tab=readme-ov-file

I cannot see training code in this repo https://github.com/deepseek-ai/DeepSeek-V3?tab=readme-ov-file",,,
o3,"Language,Vision,Multimodal","Language modeling/generation,Question answering,Quantitative reasoning,Code generation,Visual question answering,Search,Instruction interpretation,Visual puzzles",,API access,https://openai.com/index/introducing-o3-and-o4-mini/,,"Our most powerful reasoning model with leading performance on coding, math, science, and vision",2024-12-20,OpenAI,,,,,Unspecified unreleased,"""The OpenAI o-series models are trained with large-scale reinforcement learning on chains of thought.""

""OpenAI o3 and o4-mini were trained on diverse datasets,
including information that is publicly available on the internet, information that we partner with third parties to access, and information that our users or human trainers and researchers provide or generate. Our data processing pipeline includes rigorous filtering to maintain data quality and mitigate potential risks. We use advanced data filtering processes to reduce personal information
from training data. We also employ a combination of our Moderation API and safety classifiers to help prevent the use of harmful or sensitive content, including explicit materials such as sexual content involving a minor.""

This has a knowledge cutoff date of May 31, 2024, according to https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models?tabs=global-standard%2Cstandard-chat-completions#o-series-models. ",,,,,,Unknown,"Today, weâ€™re releasing OpenAI o3 and o4-mini, the latest in our o-series of models trained to think for longer before responding. These are the smartest models weâ€™ve released to date, representing a step change in ChatGPT's capabilities for everyone from curious users to advanced researchers. For the first time, our reasoning models can agentically use and combine every tool within ChatGPTâ€”this includes searching the web, analyzing uploaded files and other data with Python, reasoning deeply about visual inputs, and even generating images. Critically, these models are trained to reason about when and how to use tools to produce detailed and thoughtful answers in the right output formats, typically in under a minute, to solve more complex problems. This allows them to tackle multi-faceted questions more effectively, a step toward a more agentic ChatGPT that can independently execute tasks on your behalf. The combined power of state-of-the-art reasoning with full tool access translates into significantly stronger performance across academic benchmarks and real-world tasks, setting a new standard in both intelligence and usefulness.
<..>
OpenAI o3 is our most powerful reasoning model that pushes the frontier across coding, math, science, visual perception, and more. It sets a new SOTA on benchmarks including Codeforces, SWE-bench (without building a custom model-specific scaffold), and MMMU. Itâ€™s ideal for complex queries requiring multi-faceted analysis and whose answers may not be immediately obvious. It performs especially strongly at visual tasks like analyzing images, charts, and graphics. In evaluations by external experts, o3 makes 20 percent fewer major errors than OpenAI o1 on difficult, real-world tasksâ€”especially excelling in areas like programming, business/consulting, and creative ideation. Early testers highlighted its analytical rigor as a thought partner and emphasized its ability to generate and critically evaluate novel hypothesesâ€”particularly within biology, math, and engineering contexts.

________
model was announced 2024/12/20 
from ARS technica: ""On Friday, during Day 12 of its ""12 days of OpenAI,"" OpenAI CEO Sam Altman announced its latest AI ""reasoning"" models, o3 and o3-mini, which build upon the o1 models launched earlier this year. The company is not releasing them yet but will make these models available for public safety testing and research access today.""

https://arstechnica.com/information-technology/2024/12/openai-announces-o3-and-o3-mini-its-next-simulated-reasoning-models/

model was released 2025/04/16",United States of America,,,,,,Unreleased,"""Both o3 and o4-mini are also available to developers today via the Chat Completions API and Responses API (some developers will need to verify their organizationsâ (opens in a new window) to access these models)""",,,
Kling 1.6 Pro,"Video,Vision","Video generation,Text-to-video,Image-to-video",,API access,https://ir.kuaishou.com/news-releases/news-release-details/kuaishou-kling-ai-unveils-multi-image-reference-feature-further/,,,2024-12-19,Kuaishou Technology,,,,"Hard to bound the training compute. It performs similarly to Veo-2, so plausibly has similar training budget. Kuaishou, the developer, spent $1.7B on R&D in 2024, so plausibly could have afforded the compute for e.g. a 1e25 FLOP training run (https://kr-asia.com/kuaishou-turns-to-ai-and-kling-to-stay-sticky-in-a-flatlining-market).",Unspecified unreleased,,,,,,,Unknown,"We're excited to introduce the KLING AI 1.6 Model: With this update, we have significantly improved the response to prompt, the visual aesthetics, and the physical actions, hoping to bring more consistent and vivid results!
What's new:
1. New KLING AI 1.6 Model:
-Improved prompt adherence, more consistent and dynamic results
-Supports Standard & Professional Modes, achieving a 195% overall improvement compared with KLING 1.5 Model

In December 2024, the video generation model Kling AI 1.6 was officially launched, building upon the foundation model and elevating its capabilities significantly. Kling AI 1.6 introduced major advancements in text responsiveness, increasing its ability to interpret text descriptions related to motion, temporal action, and camera movement. Additionally, the upgraded model enhanced motion quality with smoother motion and more natural expressions. Kling AI 1.6 also delivered comprehensive upgrades in color accuracy, lighting dynamics, and detailed rendering, resulting in a striking improvement in overall video quality. ",China,,,,,,Unreleased,,,,
Granite 3.1 2B,Language,"Language modeling/generation,Question answering,Translation",,Open weights (unrestricted),https://huggingface.co/ibm-granite/granite-3.1-2b-base,,,2024-12-18,IBM,2500000000.0,"2.5B
Model Architecture: Granite-3.1-2B-Base is based on a decoder-only dense transformer architecture. Core components of this architecture are: GQA and RoPE, MLP with SwiGLU, RMSNorm, and shared input/output embeddings.",1.8e+23,6 FLOP / token / parameter * 2.5 * 10^9 parameters * 12*10^12 tokens = 1.8e+23 FLOP,Unspecified unreleased,"Training Data: This model is trained on a mix of open source and proprietary data following a three-stage training strategy.

Stage 1 data: The data for stage 1 is sourced from diverse domains, such as: web, code, academic sources, books, and math data.
Stage 2 data: The data for stage 2 comprises a curated mix of high-quality data from the same domains, plus multilingual and instruction data. The goal of this second training phase is to enhance the modelâ€™s performance on specific tasks.
Stage 3 data: The data for stage 3 consists of original stage-2 pretraining data with additional synthetic long-context data in form of QA/summary pairs where the answer contains a recitation of the related paragraph before the answer.",12000000000000.0,12T,,,NVIDIA H100 SXM5 80GB,Confident,Model Summary: Granite-3.1-2B-Base extends the context length of Granite-3.0-2B-Base from 4K to 128K using a progressive training strategy by increasing the supported context length in increments while adjusting RoPE theta until the model has successfully adapted to desired length of 128K. This long-context pre-training stage was performed using approximately 500B tokens.,United States of America,,,,,,Unreleased,"https://huggingface.co/ibm-granite/granite-3.1-2b-base
Apache 2.0",,,
Granite 3.1 8B,Language,"Language modeling/generation,Question answering,Translation",,Open weights (unrestricted),https://huggingface.co/ibm-granite/granite-3.1-8b-base,,,2024-12-18,IBM,8100000000.0,"8.1B
Model Architecture: Granite-3.1-8B-Base is based on a decoder-only dense transformer architecture. Core components of this architecture are: GQA and RoPE, MLP with SwiGLU, RMSNorm, and shared input/output embeddings.",5.832e+23,6ND = 6 FLOP / parameter / token * 8.1*10^9 parameters * 12*10^12 tokens = 5.832e+23 FLOP,Unspecified unreleased,"**Training Data:** This model is trained on a mix of open source and proprietary data following a three-stage training strategy. * Stage 1 data: The data for stage 1 is sourced from diverse domains, such as: web, code, academic sources, books, and math data. * Stage 2 data: The data for stage 2 comprises a curated mix of high-quality data from the same domains, plus multilingual and instruction data. The goal of this second training phase is to enhance the modelâ€™s performance on specific tasks. * Stage 3 data: The data for stage 3 consists of original stage-2 pretraining data with additional synthetic long-context data in form of QA/summary pairs where the answer contains a recitation of the related paragraph before the answer.",12000000000000.0,12T,,,NVIDIA H100 SXM5 80GB,Confident,Model Summary: Granite-3.1-8B-Base extends the context length of Granite-3.0-8B-Base from 4K to 128K using a progressive training strategy by increasing the supported context length in increments while adjusting RoPE theta until the model has successfully adapted to desired length of 128K. This long-context pre-training stage was performed using approximately 500B tokens.,United States of America,,,,,,Unreleased,"https://huggingface.co/ibm-granite/granite-3.1-8b-base
Apache 2.0",,,
Falcon3-7B,Language,"Language modeling/generation,Question answering,Code generation",,Open weights (unrestricted),https://huggingface.co/blog/falcon3,,Welcome to the Falcon 3 Family of Open Models!,2024-12-17,Technology Innovation Institute,7000000000.0,,5.88e+23,6 FLOP / parameter / token * 7 * 10^9 parameters * 14 * 10^12 tokens = 5.88e+23 FLOP,Unspecified unreleased,,14000000000000.0,"""Pretrained on 14 Teratokens of datasets comprising of web, code, STEM, high quality and mutlilingual data using 1024 H100 GPU chips""",,,NVIDIA H100 SXM5 80GB,Confident,"We introduce Falcon3, a family of decoder-only large language models under 10 billion parameters, developed by Technology Innovation Institute (TII) in Abu Dhabi. By pushing the boundaries of performance and training efficiency, this release reflects our ongoing commitment to advancing open and accessible large foundation models.
Falcon3 represents a natural evolution from previous releases, emphasizing expanding the models' science, math, and code capabilities.",United Arab Emirates,,,,1024.0,,Unreleased,"License: TII Falcon-LLM License 2.0
https://huggingface.co/tiiuae/Falcon3-7B-Base",,,
Veo 2,"Video,Vision","Video generation,Text-to-video,Image-to-video","Agrim Gupta, Ali Razavi, Ankush Gupta, Dumitru Erhan, Eric Lau, Frank Belletti, Gabe Barth-Maron, Hakan Erdogan, Hakim Sidahmed, Henna Nandwani, Hernan Moraldo, Hyunjik Kim, Jeff Donahue, JosÃ© Lezama, Kory Mathewson, Kurtis David, Marc van Zee, Medhini Narasimhan, Miaosen Wang, Mohammad Babaeizadeh, Nelly Papalampidi, Nick Pezzotti, Nilpa Jha, Parker Barnes, Pieter-Jan Kindermans, Rachel Hornung, Ruben Villegas, Ryan Poplin, Salah Zaiem, Sander Dieleman, Sayna Ebrahimi, Scott Wisdom, Serena Zhang, Shlomi Fruchter, Weizhe Hua, Xinchen Yan, Yuqing Du and Yutian Chen.",API access,https://deepmind.google/technologies/veo/veo-2/,,Our state-of-the-art video generation model,2024-12-16,Google DeepMind,,,,,Unspecified unreleased,,,,,,,Unknown,,"United States of America,United Kingdom of Great Britain and Northern Ireland",,,,,,Unreleased,https://cloud.google.com/vertex-ai/generative-ai/docs/models/veo/2-0-generate-001,,,
Phi-4,Language,"Language modeling/generation,Question answering,Code generation,Quantitative reasoning","Marah Abdin, Jyoti Aneja, Harkirat Behl, SÃ©bastien Bubeck, Ronen Eldan, Suriya Gunasekar, Michael Harrison, Russell J. Hewett, Mojan Javaheripi, Piero Kauffmann, James R. Lee, Yin Tat Lee, Yuanzhi Li, Weishung Liu, Caio C. T. Mendes, Anh Nguyen, Eric Price, Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Xin Wang, Rachel Ward, Yue Wu, Dingli Yu, Cyril Zhang, Yi Zhang",Open weights (unrestricted),https://arxiv.org/abs/2412.08905,,Phi-4 Technical Report,2024-12-12,Microsoft Research,14000000000.0,"14B parameters, dense decoder-only Transformer model",9.3202015e+23,"6ND = 6* 14*10^9 parameters * 10*10^12 tokens = 8.4e+23 FLOP

989500000000000 FLOP / sec [assumed bf16 precision] * 1920 GPUs * 504 hours * 3600 sec / hour * 0.3 [assumed utilization] = 1.0341209e+24 FLOP

geometric mean
sqrt(8.4e+23 * 1.0341209e+24) = 9.3202015e+23",Unspecified unreleased,"""The pretraining phase of phi-4 relies heavily on synthetic datasets generated through a variety of techniques. ""
""We collected a wide variety of high-quality organic data sources
for phi-4, prioritizing reasoning-dense and nuanced material (e.g., academic papers, educational forums,
and programming tutorials).""
""Our post-training data is composed of:
â€¢ Supervised Fine-Tuning (SFT) Datasets
â€¢ Direct Preference Optimization (DPO)",10000000000000.0,"""The model was pretrained for approximately 10T tokens using linear warm-up and decay schedules with peak learning rate of 0.0003, constant weight decay of 0.1, and global batch size of 5760. ""

Table 5:
Web 15% 1.3T unique tokens 1.2 epochs
Web rewrites 15% 290B unique tokens 5.2 epochs
Synthetic 40% 290B unique tokens 13.8 epochs
Code data 20% 820B unique tokens 2.4 epochs
Acquired sources 10% 580B unique tokens  1.7 epochs
",504.0,"https://huggingface.co/microsoft/phi-4
21 days * 24 hours / day = 504 hours",NVIDIA H100 SXM5 80GB,Confident,"We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process. While previous models in the Phi family largely distill the capabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model on STEM-focused QA capabilities, giving evidence that our data-generation and post-training techniques go beyond distillation. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong performance relative to its size -- especially on reasoning-focused benchmarks -- due to improved data, training curriculum, and innovations in the post-training scheme.",United States of America,,,,1920.0,,Unreleased,"""Phi-4 is currently available on Azure AI Foundry under a Microsoft Research License Agreement (MSRLA) and will be available on Hugging Face next week.  ""
Hugging Face: MIT license

https://huggingface.co/microsoft/phi-4",,,
Gemini 2.0 Flash,"Language,Vision,Audio,Speech,Video,Multimodal","Language modeling/generation,Question answering,Visual question answering,Speech recognition (ASR),Code generation,Quantitative reasoning,Video description,Translation,Chat,Table tasks,Search,Text summarization",Gemini Team,API access,https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,,Introducing Gemini 2.0: our new AI model for the agentic era,2024-12-11,"Google DeepMind,Google",,,,"""We used Trillium TPUs to train the new Gemini 2.0, Googleâ€™s most capable AI model yet"" according to https://cloud.google.com/blog/products/compute/trillium-tpu-is-ga",Unspecified unreleased,"Knowledge cutoff June 2024, according to https://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/2-0-flash. ",,,,,Google TPU v6e Trillium,Unknown,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.","United States of America,United Kingdom of Great Britain and Northern Ireland,United States of America",,,,,,Unreleased,"Availability	
Google AI Studio
Gemini API
Vertex AI
Gemini App",,,
Gemini 2.0 Pro,"Language,Multimodal,Vision,Video,Audio","Code generation,Language modeling/generation,Question answering,Visual question answering,Speech recognition (ASR),Video description",Gemini Team,Hosted access (no API),https://deepmind.google/technologies/gemini/pro/,,Our best model yet for coding performance and complex prompts,2024-12-11,Google DeepMind,,,,Flagship model from a leading developer in early 2025; very likely it used >1e25 FLOP.,Unspecified unreleased,"May have been renamed to Gemini 2.5 Pro, which has a knowledge cutoff date of January 2025, according to https://ai.google.dev/gemini-api/docs/models#gemini-2.5-pro-preview-05-06. If that's not the case, then Gemini 2.0 Pro may have a knowledge cutoff date of August 2024, according to https://docsbot.ai/models/gemini-2-0-pro, which seems in line with other Gemini 2.0 models, which have a knowledge cutoff date of June 2024.",,,,,,Unknown,"Today, weâ€™re releasing an experimental version of Gemini 2.0 Pro that responds to that feedback. It has the strongest coding performance and ability to handle complex prompts, with better understanding and reasoning of world knowledge, than any model weâ€™ve released so far. It comes with our largest context window at 2 million tokens, which enables it to comprehensively analyze and understand vast amounts of information, as well as the ability to call tools like Google Search and code execution.","United States of America,United Kingdom of Great Britain and Northern Ireland",,,,,,Unreleased,,,,
Sora Turbo,"Video,Vision","Video generation,Text-to-video",,Hosted access (no API),https://openai.com/index/sora-is-here/,,Sora is here,2024-12-09,OpenAI,,,,,Unspecified unreleased,"Sora was trained on diverse datasets, including a mix of publicly available data, proprietary data accessed through partnerships, and custom datasets developed in-house. These consist of:

Select publicly available data, mostly collected from industry-standard machine learning datasets and web crawls.

Proprietary data from data partnerships. We form partnerships to access non-publicly available data. For example, we partnered with Shutterstockâ  Pond5 on building and delivering AI-generated images. We also partner to commission and create datasets fit for our needs.

Human data: Feedback from AI trainers, red teamers, and employees. ",,,,,,Unknown,"Our video generation model is rolling out at sora.comâ .

Earlier this year, we introduced Soraâ , our model that can create realistic videos from text, and shared our initial research progressâ  on world simulation. Sora serves as a foundation for AI that understands and simulates realityâ€”an important step towards developing models that can interact with the physical world.

We developed a new version of Soraâ€”Sora Turboâ€”that is significantly faster than the model we previewed in February. Weâ€™re releasing it today as a standalone product at Sora.com to ChatGPT Plus and Pro users.",United States of America,,,,,,Unreleased,,,,
EXAONE 3.5 7.8B,Language,"Language modeling/generation,Question answering,Translation","Soyoung An, Kyunghoon Bae, Eunbi Choi, Kibong Choi, Stanley Jungkyu Choi, Seokhee Hong, Junwon Hwang, Hyojin Jeon, Gerrard Jeongwon Jo, Hyunjik Jo, Jiyeon Jung, Yountae Jung, Hyosang Kim, Joonkee Kim, Seonghwan Kim, Soyeon Kim, Sunkyoung Kim, Yireun Kim, Yongil Kim, Youchul Kim, Edward Hwayoung Lee, Haeju Lee, Honglak Lee, Jinsik Lee, Kyungmin Lee, Woohyung Lim, Sangha Park, Sooyoun Park, Yongmin Park, Sihoon Yang, Heuiyeen Yeen, Hyeongu Yun",Open weights (non-commercial),https://arxiv.org/abs/2412.04862,,EXAONE 3.5: Series of Large Language Models for Real-world Use Cases,2024-12-09,LG AI Research,7800000000.0,7.8B,4.21e+23,4.21 Ã— 10^23 (Table 2),Unspecified unreleased,,9000000000000.0,9T tokens (Table 2),,,,Confident,"This technical report introduces the EXAONE 3.5 instruction-tuned language models, developed and released by LG AI Research. The EXAONE 3.5 language models are offered in three configurations: 32B, 7.8B, and 2.4B. These models feature several standout capabilities: 1) exceptional instruction following capabilities in real-world scenarios, achieving the highest scores across seven benchmarks, 2) outstanding long-context comprehension, attaining the top performance in four benchmarks, and 3) competitive results compared to state-of-the-art open models of similar sizes across nine general benchmarks. The EXAONE 3.5 language models are open to anyone for research purposes and can be downloaded from this https URL. For commercial use, please reach out to the official contact point of LG AI Research: contact_us@lgresearch.ai.",Korea (Republic of),,,,,,Unreleased,Exaone license (allows only non-commercial usage),,,
EXAONE 3.5 32B,Language,"Language modeling/generation,Question answering,Translation","Soyoung An, Kyunghoon Bae, Eunbi Choi, Kibong Choi, Stanley Jungkyu Choi, Seokhee Hong, Junwon Hwang, Hyojin Jeon, Gerrard Jeongwon Jo, Hyunjik Jo, Jiyeon Jung, Yountae Jung, Hyosang Kim, Joonkee Kim, Seonghwan Kim, Soyeon Kim, Sunkyoung Kim, Yireun Kim, Yongil Kim, Youchul Kim, Edward Hwayoung Lee, Haeju Lee, Honglak Lee, Jinsik Lee, Kyungmin Lee, Woohyung Lim, Sangha Park, Sooyoun Park, Yongmin Park, Sihoon Yang, Heuiyeen Yeen, Hyeongu Yun",Open weights (non-commercial),https://arxiv.org/abs/2412.04862,,EXAONE 3.5: Series of Large Language Models for Real-world Use Cases,2024-12-09,LG AI Research,32000000000.0,32B,1.25e+24,1.25 Ã— 10^24 (Table 2) ,Unspecified unreleased,,6500000000000.0,6.5T tokens (Table 2),,,,Confident,"This technical report introduces the EXAONE 3.5 instruction-tuned language models, developed and released by LG AI Research. The EXAONE 3.5 language models are offered in three configurations: 32B, 7.8B, and 2.4B. These models feature several standout capabilities: 1) exceptional instruction following capabilities in real-world scenarios, achieving the highest scores across seven benchmarks, 2) outstanding long-context comprehension, attaining the top performance in four benchmarks, and 3) competitive results compared to state-of-the-art open models of similar sizes across nine general benchmarks. The EXAONE 3.5 language models are open to anyone for research purposes and can be downloaded from this https URL. For commercial use, please reach out to the official contact point of LG AI Research: contact_us@lgresearch.ai.",Korea (Republic of),,,,,,Unreleased,Exaone license (allows only non-commercial usage),,,
Llama 3.3 70B,Language,"Language modeling/generation,Question answering,Translation,Code generation",,Open weights (restricted use),https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_3/,,Meta Llama 3.3 multilingual large language model (LLM) ,2024-12-06,Meta AI,70000000000.0,70B,6.8649768e+24,"6ND = 6 FLOP / parameter / token * 70*10^9 parameters * 15*10^12 tokens = 6.3e+24 FLOP

7000000 GPU-hours * 3600 sec / hour * 989500000000000 FLOP / second * 0.3 [assumed utilization]= 7.48062e+24 FLOP

sqrt(7.48062e+24*6.3e+24) = 6.8649768e+24",Unspecified unreleased,"""A new mix of publicly available online data.""
Knowledge cutoff date is December 2023, according to https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct",15000000000000.0,"""Overview: Llama 3.3 was pretrained on ~15 trillion tokens of data from publicly available sources. The fine-tuning data includes publicly available instruction datasets, as well as over 25M synthetically generated examples.

Data Freshness: The pretraining data has a cutoff of December 2023.""",,"""Training utilized a cumulative of 39.3M GPU hours of computation on H100-80GB (TDP of 700W) type hardware, per the table below. Training time is the total GPU time required for training each model and power consumption is the peak power capacity per GPU device used, adjusted for power usage efficiency.""

Llama 3.3 70B: Training Time (GPU hours): 7M
",NVIDIA H100 SXM5 80GB,Confident,"The Meta Llama 3.3 multilingual large language model (LLM) is a pretrained and instruction tuned generative model in 70B (text in/text out). The Llama 3.3 instruction tuned text only model is optimized for multilingual dialogue use cases and outperforms many of the available open source and closed chat models on common industry benchmarks.

Model developer: Meta

Model Architecture: Llama 3.3 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety.",United States of America,,,,,,Unreleased,"License A custom commercial license, the Llama 3.3 Community License Agreement, is available at: https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/LICENSE

""Llama 3.3 is intended for commercial and research use in multiple languages.""",,,
o1,"Language,Mathematics,Multimodal","Code generation,Language modeling/generation,Quantitative reasoning,Chat,Question answering,Translation,Mathematical reasoning",,API access,https://openai.com/index/introducing-chatgpt-pro/,,Introducing ChatGPT Pro: Broadening usage of frontier AI.,2024-12-05,OpenAI,,,,,Unspecified unreleased,"""Both models were trained on a variety of publicly available datasets, including web data and open-source datasets. Key components include reasoning data and scientific literature. <..> To further enhance the capabilities of o1-preview and o1-mini, we formed partnerships to access high-value non-public datasets. These proprietary data sources include paywalled content, specialized archives, and other domain-specific datasets that provide deeper insights into industry-specific knowledge and use cases.""

This has a knowledge cutoff date of October 2023 - which I assume means October 1, 2023 - according to https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models?tabs=global-standard%2Cstandard-chat-completions#o-series-models.",,,,,,Unknown,"We've developed a new series of AI models designed to spend more time thinking before they respond. They can reason through complex tasks and solve harder problems than previous models in science, coding, and math.

Today, we are releasing the first of this series in ChatGPT and our API. This is a preview and we expect regular updates and improvements. Alongside this release, weâ€™re also including evaluations for the next update, currently in development.",United States of America,,,,,,Unreleased,,,,
Amazon Nova Pro,"Multimodal,Language,Video,Vision","Language modeling/generation,Retrieval-augmented generation,Visual question answering,Image captioning,Video description,Character recognition (OCR),Code generation,Translation",,API access,"https://aws.amazon.com/es/blogs/aws/introducing-amazon-nova-frontier-intelligence-and-industry-leading-price-performance/

https://assets.amazon.science/96/7d/0d3e59514abf8fdcfafcdc574300/nova-tech-report-20250317-0810.pdf",,Introducing Amazon Nova foundation models: Frontier intelligence and industry leading price performance,2024-12-03,Amazon,,,6.000010000000001e+24,"""probably just below 1e25 stemming from the Llama 70B serving speed.  If Llama 70B is trained proportionally to 405B, then it's at ~ 6.6e24. Nova Pro is served at 100tk/s, while Llama 70B is served at 70tk/s on average, and 100tk/s by together.ai at FP8. So Nova Pro would be >1e25 if they roughly 2x the amount of training compared to Llama 70B which [seems unlikely]""",Unspecified unreleased,"""Each model went through a series of training processes that began with pretraining using a mixture of large amounts of multilingual and multimodal data. Our models were trained on data from a variety of sources, including licensed data, proprietary data, open source datasets, and publicly available data where appropriate. We curated data from over 200 languages, with particular emphasis on Arabic, Dutch, English, French, German, Hebrew, Hindi, Italian, Japanese, Korean, Portuguese, Russian, Simplified Chinese, Spanish, and Turkish.""",,,,,"Amazon Trainium1,NVIDIA A100,NVIDIA H100 SXM5 80GB",Speculative," A highly capable multimodal model with the best combination of accuracy, speed, and cost for a wide range of tasks. Amazon Nova Pro is capable of processing up to 300K input tokens and sets new standards in multimodal intelligence and agentic workflows that require calling APIs and tools to complete complex workflows. It achieves state-of-the-art performance on key benchmarks including visual question answering (TextVQA) and video understanding (VATEX). Amazon Nova Pro demonstrates strong capabilities in processing both visual and textual information and excels at analyzing financial documents. With an input context of 300K tokens, it can process code bases with over fifteen thousand lines of code. Amazon Nova Pro also serves as a teacher model to distill custom variants of Amazon Nova Micro and Lite.",United States of America,,,,,,Unreleased,,,,
Hunyuan Video,Video,"Video generation,Text-to-video","Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, Kathrina Wu, Qin Lin, Junkun Yuan, Yanxin Long, Aladdin Wang, Andong Wang, Changlin Li, Duojun Huang, Fang Yang, Hao Tan, Hongmei Wang, Jacob Song, Jiawang Bai, Jianbing Wu, Jinbao Xue, Joey Wang, Kai Wang, Mengyang Liu, Pengyu Li, Shuai Li, Weiyan Wang, Wenqing Yu, Xinchi Deng, Yang Li, Yi Chen, Yutao Cui, Yuanbo Peng, Zhentao Yu, Zhiyu He, Zhiyong Xu, Zixiang Zhou, Zunnan Xu, Yangyu Tao, Qinglin Lu, Songtao Liu, Daquan Zhou, Hongfa Wang, Yong Yang, Di Wang, Yuhong Liu, Jie Jiang, Caesar Zhong",Open weights (restricted use),https://www.arxiv.org/abs/2412.03603,,HunyuanVideo: A Systematic Framework For Large Video Generative Models,2024-12-03,Tencent,13000000000.0,13b,1.4814815e+23,"from Figure 10:

the optimal model has 13b parameters, 5.8e+07PF (image training) + 7.0e+07PF (video training) of compute and 740B (image tokens) + 928B (video tokens) 

5.8e+07PF + 7.0e+07PF = 12.8e+07PF = 12.8*10^7*10^20/(24*3600) = 1.4814815e+23 FLOPs

6ND = 6*13*10^9*(740+928)*10^9 = 1.30104e+23
",Unspecified unreleased,"""We employ various filters for data filtering and progressively increase their thresholds to build 4 training datasets, i.e., 256p, 360p, 540p, and
720p, while the final SFT dataset is built through manual annotation.""",,,,,,Confident,"Recent advancements in video generation have significantly impacted daily life for both individuals and industries. However, the leading video generation models remain closed-source, resulting in a notable performance gap between industry capabilities and those available to the public. In this report, we introduce HunyuanVideo, an innovative open-source video foundation model that demonstrates performance in video generation comparable to, or even surpassing, that of leading closed-source models. HunyuanVideo encompasses a comprehensive framework that integrates several key elements, including data curation, advanced architectural design, progressive model scaling and training, and an efficient infrastructure tailored for large-scale model training and inference. As a result, we successfully trained a video generative model with over 13 billion parameters, making it the largest among all open-source models. We conducted extensive experiments and implemented a series of targeted designs to ensure high visual quality, motion dynamics, text-video alignment, and advanced filming techniques. According to evaluations by professionals, HunyuanVideo outperforms previous state-of-the-art models, including Runway Gen-3, Luma 1.6, and three top-performing Chinese video generative models. By releasing the code for the foundation model and its applications, we aim to bridge the gap between closed-source and open-source communities. This initiative will empower individuals within the community to experiment with their ideas, fostering a more dynamic and vibrant video generation ecosystem. The code is publicly available at this https URL. https://github.com/Tencent/HunyuanVideo.",China,,,,,,Unreleased,"""THIS LICENSE AGREEMENT DOES NOT APPLY IN THE EUROPEAN UNION, UNITED KINGDOM AND SOUTH KOREA"" 

also requires additional licensing in case of massive commercial use

https://huggingface.co/tencent/HunyuanVideo/blob/main/LICENSE

the code seems to be just inference code not training code
",,,
Amazon Nova Canvas,"Image generation,Vision","Image generation,Text-to-image",,API access,"https://docs.aws.amazon.com/ai/responsible-ai/nova-canvas/overview.html

https://assets.amazon.science/96/7d/0d3e59514abf8fdcfafcdc574300/nova-tech-report-20250317-0810.pdf",,,2024-12-03,Amazon,,,,,Unspecified unreleased,,,,,,"Amazon Trainium1,NVIDIA A100,NVIDIA H100 SXM5 80GB",Unknown,"an image generation model that creates professional grade images from text and image inputs. Amazon Nova Canvas is ideal for a wide range of applications such as advertising, marketing, and entertainment.

â€¢ Text-to-image generation: Amazon Nova Canvas can generate images with various resolutions (from 512 up to
2K horizontal resolution) and aspect ratios (any aspect ratio between 1:4 and 4:1 with a maximum of 4.2M
pixels). Customers can provide reference images to guide the model to generate outputs in a specific style or
color palette, or to generate variations of an image.
â€¢ Image editing: Amazon Nova Canvas allows precise image editing operations like inpainting and outpainting
through natural language mask prompts. These mask prompts describe the specific area of the input image that
needs to be repainted. The user can also easily change a background with the background removal feature,
leaving the subject of the image unchanged.
",United States of America,,,,,,Unreleased,,,,
360gpt2-pro,Language,Language modeling,360 Zhinao,,,,,2024-11-29,360 Security Technology,,"""Hundreds of billions of parameters""",,"Developer earns billions of dollars in annual revenue, so plausibly could afford a large training run.",,,,,,,,Unknown,,China,,,,,,,,,,
abab7,Language,Language modeling/generation,,API access,https://www.minimaxi.com/en/news/abab65-series,,,2024-11-29,MiniMax,,,,"Developer raised hundreds of millions of dollars (https://www.reuters.com/technology/china-ai-startup-minimax-raising-over-250-mln-tencent-backed-entity-others-2023-06-01/), so could plausibly afford a 1e25 FLOP scale training run. ",Unspecified unreleased,,,,,,,Unknown,,China,,,,,,Unreleased,,,,
bailing-pro-1120,Language,"Language modeling/generation,Question answering",Ant Group,Unreleased,,,,2024-11-15,Ant Group,,,,"Trained on ""trillions of tokens"".",Unspecified unreleased,,,,,,,Unknown,,China,,,,,,Unreleased,,,,
Qwen2.5-Coder (32B),Language,"Language modeling/generation,Code generation","Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Keming Lu, Kai Dang, Yang Fan, Yichang Zhang, An Yang, Rui Men, Fei Huang, Bo Zheng, Yibo Miao, Shanghaoran Quan, Yunlong Feng, Xingzhang Ren, Xuancheng Ren, Jingren Zhou, Junyang Lin",Open weights (unrestricted),https://arxiv.org/abs/2409.12186,,Qwen2.5-Coder Technical Report,2024-11-12,Alibaba,32500000000.0,32.5B (31B - non emb),1.0725e+24,"Assuming 1 epoch

6ND = 6*32.5 parameters *10^9*5.5*10^12 tokens = 1.0725e+24","GitHub,Common Crawl,Unspecified unreleased","""We collected public repositories from GitHub created before February 2024""

""We curated a large-scale and high-quality text-code mixed
dataset from Common Crawl, which includes code-related documentation, tutorials, blogs,
and more""

""We used CodeQwen1.5, the predecessor of Qwen2.5-Coder, to generate large-scale synthetic datasets.""

Knowledge cutoff date is March 2024, according to https://llm-stats.com/models/qwen-2.5-coder-32b-instruct. ",5500000000000.0,"""As a code-specific model, Qwen2.5-Coder is built upon the Qwen2.5 architecture and continues pretrained on a vast corpus of over 5.5 trillion tokens.""",,,,Confident,"In this report, we introduce the Qwen2.5-Coder series, a significant upgrade from its predecessor, CodeQwen1.5. This series includes six models: Qwen2.5-Coder-(0.5B/1.5B/3B/7B/14B/32B). As a code-specific model, Qwen2.5-Coder is built upon the Qwen2.5 architecture and continues pretrained on a vast corpus of over 5.5 trillion tokens. Through meticulous data cleaning, scalable synthetic data generation, and balanced data mixing, Qwen2.5-Coder demonstrates impressive code generation capabilities while retaining general and math skills. These models have been evaluated on a wide range of code-related tasks, achieving state-of-the-art (SOTA) performance across more than 10 benchmarks, including code generation, completion, reasoning, and repair, consistently outperforming larger models of the same model size. We believe that the release of the Qwen2.5-Coder series will advance research in code intelligence and, with its permissive licensing, support wider adoption by developers in real-world applications.",China,,,,,,Unreleased,"Apache 2.0
https://huggingface.co/Qwen/Qwen2.5-Coder-32B-Instruct

though they have apache 2.0 github repository it seems to be inference code rather than training code",,,
Hunyuan-Large,Language,"Language modeling/generation,Question answering,Code generation,Translation","Xingwu Sun, Yanfeng Chen, Yiqing Huang, Ruobing Xie, Jiaqi Zhu, Kai Zhang, Shuaipeng Li, Zhen Yang, Jonny Han, Xiaobo Shu, Jiahao Bu, Zhongzhi Chen, Xuemeng Huang, Fengzong Lian,
Saiyong Yang, Jianfeng Yan, Yuyuan Zeng, Xiaoqin Ren, Chao Yu, Lulu Wu, Yue Mao, Jun Xia, Tao Yang, Suncong Zheng, Kan Wu, Dian Jiao, Jinbao Xue, Xipeng Zhang, Decheng Wu, Kai Liu, Dengpeng Wu, Guanghui Xu, Shaohua Chen, Shuang Chen, Xiao Feng, Yigeng Hong, Junqiang Zheng, Chengcheng Xu, Zongwei Li, Xiong Kuang, Jianglu Hu, Yiqi Chen, Yuchi Deng, Guiyang Li, Ao Liu, Chenchen Zhang, Shihui Hu, Zilong Zhao, Zifan Wu, Yao Ding, Weichao Wang, Han Liu, Roberts Wang, Hao Fei, Peijie Yu, Ze Zhao, Xun Cao, Hai Wang, Fusheng Xiang, Mengyuan Huang, Zhiyuan Xiong, Bin Hu, Xuebin Hou, Lei Jiang, Jianqiang Ma, Jiajia Wu, Yaping Deng, Yi Shen, Qian Wang, Weijie Liu, Jie Liu, Meng Chen, Liang Dong, Weiwen Jia, Hu Chen, Feifei Liu, Rui Yuan, Huilin Xu, Zhenxiang Yan, Tengfei Cao, Zhichao Hu, Xinhua Feng, Dong Du, Tinghao Yu, Yangyu Tao, Feng Zhang, Jianchen Zhu, Chengzhong Xu, Xirui Li, Chong Zha, Wen Ouyang, Yinben Xia, Xiang Li, Zekun He, Rongpeng Chen, Jiawei Song, Ruibin Chen, Fan Jiang, Chongqing Zhao, Bo Wang, Hao Gong, Rong Gan, Winston Hu, Zhanhui Kang, Yong Yang, Yuhong Liu, Di Wang, and Jie Jiang.
",Open weights (restricted use),https://arxiv.org/abs/2411.02265,,Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated Parameters by Tencent,2024-11-06,Tencent,389000000000.0,"""a total of 389 billion parameters and 52 billion activation parameters""",3.49237e+24,"52B activated parameters

6ND = 6*52*10^9*7*10^12 = 2.184 Ã— 10^24

They also suggest more precise formula to calculate MoE compute budget:

9.59ND + 2.3 Ã— 10^8D = 9.59*52*10^9*7*10^12 + 2.3 Ã— 10^8 Ã—  7*10^12 = 3.49237Ã—10^24

which seems closer to projected compute on Figure 3",Unspecified unreleased,,7000000000000.0,"""# Trained Tokens 7T""  Table 1",,,,Confident,"In this paper, we introduce Hunyuan-Large, which is currently the largest open-source Transformer-based mixture of experts model, with a total of 389 billion parameters and 52 billion activation parameters, capable of handling up to 256K tokens. We conduct a thorough evaluation of Hunyuan-Large's superior performance across various benchmarks including language understanding and generation, logical reasoning, mathematical problem-solving, coding, long-context, and aggregated tasks, where it outperforms LLama3.1-70B and exhibits comparable performance when compared to the significantly larger LLama3.1-405B model. Key practice of Hunyuan-Large include large-scale synthetic data that is orders larger than in previous literature, a mixed expert routing strategy, a key-value cache compression technique, and an expert-specific learning rate strategy. Additionally, we also investigate the scaling laws and learning rate schedule of mixture of experts models, providing valuable insights and guidances for future model development and optimization. The code and checkpoints of Hunyuan-Large are released to facilitate future innovations and applications.",China,,,,,,Open (restricted use),"the license doesn't regulate usage in the EU
also requires additional licensing in case of massive commercial use",,,
Stable Diffusion 3.5 Medium,Image generation,"Image generation,Text-to-image",,Open weights (restricted use),https://stability.ai/news/introducing-stable-diffusion-3-5,,Introducing Stable Diffusion 3.5,2024-10-29,Stability AI,2500000000.0,2.5B,,,Unspecified unreleased,"""This model was trained on a wide variety of data, including synthetic data and filtered publicly available data.""",,,,,,Confident,"At 2.5 billion parameters, with improved MMDiT-X architecture and training methods, this model is designed to run â€œout of the boxâ€ on consumer hardware, striking a balance between quality and ease of customization. It is capable of generating images ranging between 0.25 and 2 megapixel resolution. ",United Kingdom of Great Britain and Northern Ireland,,,,,,Unreleased,"Stability AI Community License: Free for research, non-commercial, and commercial use for organisations or individuals with less than $1M annual revenue. You only need a paid Enterprise license if your yearly revenues exceed USD$1M and you use Stability AI models in commercial products or services.

https://huggingface.co/stabilityai/stable-diffusion-3.5-medium

MIT license
https://github.com/Stability-AI/sd3.5",,,
Doubao-pro,Language,"Language modeling/generation,Question answering,Text summarization,Text classification",,API access,https://www.volcengine.com/docs/6360/1264663,,Doubao General Model Pro (Doubao-pro),2024-10-28,ByteDance,500000000000.0,"[Speculative] Doubao's large language model has scaled up from 35 billion parameters to 800 billion, with 500 billion and 800 billion parameter models currently under training.
https://xueqiu.com/9637001584/309910396?md5__1038=7qmx2DyDuie4cDBqDTQEWqDtMvO4iTphD
",2.505e+25,6ND = 6 * 500*10^9 * 8350*10^9 = 2.505e+25,Unspecified unreleased,"Translation of report: The Doubao model's pre-training data volume is approximately 500TB, with the actual fed amount accounting for about 1/10.

Assuming 167M token per GB, this is 8.4T tokens.

Doubao's data sources primarily rely on proprietary business data, accounting for 50-60%; externally sourced data comprises 15-20%; and synthetic data has been used since June of this year, although Doubao is cautious in feeding synthetic data due to its uncertain quality.",8350000000000.0,"[Speculative] Doubao's pre-training data volume is approximately 500TB, with only about 10% of this data actually used for training. The current version employs a non-Mixture-of-Experts (MoE) architecture. In the future, MoE architecture may be introduced to increase parameter count and performance, while also integrating multimodal data solutions.

So this model is dense, and the training data is probably all text tokens, not multimodal.

50TB * 167M tokens/GB ~= 8.35 trillion tokens
",,,,Likely,"A professional-grade, self-developed LLM supporting up to 128k tokens, enabling fine-tuning across the entire series. ",China,,,,,,Unreleased,,,,
Claude 3.5 Haiku,Language,"Chat,Code generation,Language modeling/generation,Question answering",,API access,https://www.anthropic.com/news/3-5-models-and-computer-use,,"Our fastest model, delivering advanced coding, tool use, and reasoning at an accessible price",2024-10-22,Anthropic,,,,,Unspecified unreleased,Training data cutoff July 2024,,,,,,Unknown,"Claude 3.5 Haiku is the next generation of our fastest model. For a similar speed to Claude 3 Haiku, Claude 3.5 Haiku improves across every skill set and surpasses even Claude 3 Opus, the largest model in our previous generation, on many intelligence benchmarks. Claude 3.5 Haiku is particularly strong on coding tasks. For example, it scores 40.6% on SWE-bench Verified, outperforming many agents using publicly available state-of-the-art modelsâ€”including the original Claude 3.5 Sonnet and GPT-4o.

With low latency, improved instruction following, and more accurate tool use, Claude 3.5 Haiku is well suited for user-facing products, specialized sub-agent tasks, and generating personalized experiences from huge volumes of dataâ€”like purchase history, pricing, or inventory records.

Claude 3.5 Haiku will be made available later this month across our first-party API, Amazon Bedrock, and Google Cloudâ€™s Vertex AIâ€”initially as a text-only model and with image input to follow.",United States of America,,,,,,Unreleased,,,,
Stable Diffusion 3.5 Large,Image generation,"Image generation,Text-to-image",,Open weights (restricted use),https://stability.ai/news/introducing-stable-diffusion-3-5,,Introducing Stable Diffusion 3.5,2024-10-22,Stability AI,8100000000.0,8.1B,,,Unspecified unreleased,"""This model was trained on a wide variety of data, including synthetic data and filtered publicly available data.""",,,,,,Confident,"Stable Diffusion 3.5 Large: At 8.1 billion parameters, with superior quality and prompt adherence, this base model is the most powerful in the Stable Diffusion family. This model is ideal for professional use cases at 1 megapixel resolution.",United Kingdom of Great Britain and Northern Ireland,,,,,,Unreleased,"Stability AI Community License: Free for research, non-commercial, and commercial use for organisations or individuals with less than $1M annual revenue. You only need a paid Enterprise license if your yearly revenues exceed USD$1M and you use Stability AI models in commercial products or services.

https://huggingface.co/stabilityai/stable-diffusion-3.5-large/blob/main/LICENSE.md

MIT license
https://github.com/Stability-AI/sd3.5",,,
Granite 3.0 8B,Language,"Language modeling/generation,Question answering,Translation,Text summarization,Text classification,Code generation",Granite Team IBM,Open weights (unrestricted),https://github.com/ibm-granite/granite-3.0-language-models/tree/main,,Granite 3.0 Language Models,2024-10-21,IBM,8100000000.0,8.1B,5.832e+23,"6ND = 6 FLOP / parameter / token * 8.1*10^9 parameters * 12*10^12 tokens = 5.832e+23 FLOP

""All our Granite 3.0 models are trained using a
compute budget of 8.35 Ã— 10^23 FLOPS.""

8.35 Ã— 10^23 * 757.0 (model's power consumption) / (174.6+757.0+64.5+121.2) = 5.6573436e+23

hardware estimation:
832102 GPU-hours * 3600 sec / hour * 989500000000000 FLOP / GPU / sec * 0.3 [assumed utilization] = 8.8923412e+23",Unspecified unreleased,"Granite 3.0 language models are trained using data from various sources such as unstructured natural language text and code data from theWeb curated by IBM, a collection of synthetic datasets generated by IBM, and publicly available high-quality datasets with permissible licenses.",12000000000000.0,12T tokens,,Table 7,NVIDIA H100 SXM5 80GB,Confident,"This report presents Granite 3.0, a new set of lightweight, state-of-the-art, open foundation models ranging in scale from 400 million to 8 billion active parameters.
Equipped with native support of multilingual, coding, function 
calling, and strong safety performance, these models target enterprise use cases, including on-premise and on-device settings. Evaluations on a comprehensive set of tasks demonstrate that our models consistently reach state-of-the-art performance for their size (as shown in Figure 1 and 2). This report also discloses technical details of pre-training and post-training that may help the research community accelerate the collective efforts to develop open foundation models. We publicly release pre-trained and post-trained versions of all our Granite 3.0 models under a standard permissive Apache 2.0 license allowing both research and commercial use. With support from the open source community, the Granite 3.0 models have been integrated with a
range of existing tools for quantization, fine-tuning, and deployment.",United States of America,,,,256.0,,Unreleased,"Apache 2.0 license
https://huggingface.co/ibm-granite/granite-3.0-8b-instruct",,,
Granite 3.0 2B,Language,"Language modeling/generation,Question answering,Translation,Text summarization,Text classification,Code generation",Granite Team IBM,Open weights (unrestricted),https://github.com/ibm-granite/granite-3.0-language-models/tree/main,,Granite 3.0 Language Models,2024-10-21,IBM,2500000000.0,2.5B,1.8e+23,"6ND = 6 FLOP / token / parameter * 2.5*10^9 parameters * 12*10^12 tokens = 1.8e+23 FLOP

""""All our Granite 3.0 models are trained using a
compute budget of 8.35 Ã— 10^23 FLOPS.""

8.35 Ã— 10^23 * 174.6 (model's power consumption) / (174.6+757.0+64.5+121.2) =1.304851e+23

hardware estimation:
192030 GPU-hours * 3600 sec / hour *989500000000000 FLOP / GPU / sec * 0.3 [assumed utilization] = 2.0521478e+23 FLOP",Unspecified unreleased,"Granite 3.0 language models are trained using data from various sources such as unstructured natural language text and code data from theWeb curated by IBM, a collection of synthetic datasets generated by IBM, and publicly available high-quality datasets with permissible licenses.",12000000000000.0,12T tokens,,Table 7,NVIDIA H100 SXM5 80GB,Confident,"This report presents Granite 3.0, a new set of lightweight, state-of-the-art, open foundation models ranging in scale from 400 million to 8 billion active parameters.
Equipped with native support of multilingual, coding, function 
calling, and strong safety performance, these models target enterprise use cases, including on-premise and on-device settings. Evaluations on a comprehensive set of tasks demonstrate that our models consistently reach state-of-the-art performance for their size (as shown in Figure 1 and 2). This report also discloses technical details of pre-training and post-training that may help the research community accelerate the collective efforts to develop open foundation models. We publicly release pre-trained and post-trained versions of all our Granite 3.0 models under a standard permissive Apache 2.0 license allowing both research and commercial use. With support from the open source community, the Granite 3.0 models have been integrated with a
range of existing tools for quantization, fine-tuning, and deployment.",United States of America,,,,768.0,,Unreleased,"Apache 2.0 license
https://huggingface.co/ibm-granite/granite-3.0-2b-instruct",,,
Yi-Lightning,Language,Language modeling/generation,,API access,https://www.lingyiwanwu.com/en https://platform.lingyiwanwu.com/,,Yi-Lightning,2024-10-18,01.AI,,,1.5e+24,"The CEO of 01.AI tweeted that Yi-Lightning was trained for 1 month on 2000 H100s: https://x.com/kaifulee/status/1846310645849047524
Assuming this is accurate:
(9.9e14 * 2000) FLOP/s * 1 month * 30.5 days/month * 24hr/day * 3600 s/hr * 0.3 utilization assumption = 1.565e24",Unspecified unreleased,,,,720.0,"https://x.com/kaifulee/status/1846310645849047524
""it was trained on 2000 H100s for 1 month""",NVIDIA H100 SXM5 80GB,Confident,,China,,,,2000.0,,Unreleased,https://platform.lingyiwanwu.com/,,,
TeleChat2-35B,Language,"Language modeling/generation,Question answering,Quantitative reasoning,Chat,Text summarization,Code generation",,Open weights (restricted use),https://github.com/Tele-AI/TeleChat2,,TeleChat2,2024-10-18,China Telecom,35000000000.0,,,,,,,,,,,Confident,,China,,,,,,Open (restricted use),"https://modelscope.cn/models/TeleAI/TeleChat2-35B-32K

""The community must comply with the TeleChat Model Community License Agreement when using the TeleChat model. The TeleChat model supports commercial use. If you plan to use the TeleChat model or its derivatives for commercial purposes, you need to contact us via the following email""

no clear license but same disclaimer as above
https://github.com/Tele-AI/TeleChat2/

this is seems to be pre-training code:
https://github.com/Tele-AI/TeleChat2/tree/main/deepspeed",,,
Ministral 8B,Language,"Language modeling/generation,Question answering,Code generation,Quantitative reasoning","Albert Jiang, Alexandre Abou Chahine, Alexandre Sablayrolles, Alexis Tacnet, Alodie Boissonnet, Alok Kothari, AmÃ©lie HÃ©liou, Andy Lo, Anna Peronnin, Antoine Meunier, Antoine Roux, Antonin Faure, Aritra Paul, Arthur Darcet, Arthur Mensch, Audrey Herblin-Stoop, Augustin Garreau, Austin Birky, Avinash Sooriyarachchi, Baptiste RoziÃ¨re, Barry Conklin, Bastien Bouillon, Blanche Savary de Beauregard, Carole Rambaud, Caroline Feldman, Charles de Freminville, Charline Mauro, Chih-Kuan Yeh, Chris Bamford, Clement Auguy, Corentin Heintz, Cyriaque Dubois, Devendra Singh Chaplot, Diego Las Casas, Diogo Costa, ElÃ©onore Arcelin, Emma Bou Hanna, Etienne Metzger, Fanny Olivier Autran, Francois Lesage, Garance Gourdel, Gaspard Blanchet, Gaspard Donada Vidal, Gianna Maria Lengyel, Guillaume Bour, Guillaume Lample, Gustave Denis, Harizo Rajaona, Himanshu Jaju, Ian Mack, Ian Mathew, Jean-Malo Delignon, Jeremy Facchetti, Jessica Chudnovsky, Joachim Studnia, Justus Murke, Kartik Khandelwal, Kenneth Chiu, Kevin Riera, Leonard Blier, Leonard Suslian, Leonardo Deschaseaux, Louis Martin, Louis Ternon, Lucile Saulnier, LÃ©lio Renard Lavaud, Sophia Yang, Margaret Jennings, Marie Pellat, Marie Torelli, Marjorie Janiewicz, Mathis Felardos, Maxime Darrin, Michael Hoff, MickaÃ«l Seznec, Misha Jessel Kenyon, Nayef Derwiche, Nicolas Carmont Zaragoza, Nicolas Faurie, Nicolas Moreau, Nicolas Schuhl, Nikhil Raghuraman, Niklas Muhs, Olivier de Garrigues, Patricia RozÃ©, Patricia Wang, Patrick von Platen, Paul Jacob, Pauline Buche, Pavankumar Reddy Muddireddy, Perry Savas, Pierre Stock, Pravesh Agrawal, Renaud de Peretti, Romain Sauvestre, Romain Sinthe, Roman Soletskyi, Sagar Vaze, Sandeep Subramanian, Saurabh Garg, Soham Ghosh, Sylvain Regnier, Szymon Antoniak, Teven Le Scao, Theophile Gervet, Thibault Schueller, Thibaut Lavril, Thomas Wang, TimothÃ©e Lacroix, Valeriia Nemychnikova, Wendy Shang, William El Sayed, William Marshall",Open weights (non-commercial),https://mistral.ai/news/ministraux/,,"Un Ministral, des Ministraux
Introducing the worldâ€™s best edge models.",2024-10-16,Mistral AI,8000000000.0,"Architecture	Dense Transformer
Parameters	8,019,808,256
Layers	36
Heads	32",,,Unspecified unreleased,Trained on a large proportion of multilingual and code data,,,,,,Confident,"On the first anniversary of the release of Mistral 7B, the model that revolutionized independent frontier AI innovation for millions, we are proud to introduce two new state-of-the-art models for on-device computing and at-the-edge use cases. We call them les Ministraux: Ministral 3B and Ministral 8B.

These models set a new frontier in knowledge, commonsense, reasoning, function-calling, and efficiency in the sub-10B category, and can be used or tuned to a variety of uses, from orchestrating agentic workflows to creating specialist task workers. Both models support up to 128k context length (currently 32k on vLLM) and Ministral 8B has a special interleaved sliding-window attention pattern for faster and memory-efficient inference.",France,,,,,,Unreleased,"Mistral Commercial License
Mistral Research License

For self-deployed use, please reach out to us for commercial licenses. We will also assist you in lossless quantization of the models for your specific use-cases to derive maximum performance.

The model weights for Ministral 8B Instruct are available for research use. Both models will be available from our cloud partners shortly.

https://huggingface.co/mistralai/Ministral-8B-Instruct-2410",,,
Firefly Video,"Video,Vision","Video generation,Text-to-video,Image-to-video",Adobe,Hosted access (no API),https://news.adobe.com/news/2024/10/101424-adobe-launches-firefly-video-model,,"Adobe Launches Firefly Video Model and Enhances Image, Vector and Design Models",2024-10-14,Adobe,,,,,Unspecified unreleased,,,,,,,Unknown,,United States of America,,,,,,Unreleased,,,,
Chirp 2 Speech-to-Text,Speech,"Speech recognition (ASR),Speech-to-text,Translation",,API access,https://cloud.google.com/speech-to-text/v2/docs/chirp_2-model,,Chirp 2: Enhanced multilingual accuracy,2024-10-09,Google,,,,,Unspecified unreleased,,,,,,,Unknown,"Chirp 2 is the latest generation of Google's multilingual ASR-specific models, designed to meet user needs based on feedback and experience. It improves on the original Chirp model in accuracy and speed, as well as expanding into key new features like word-level timestamps, model adaptation, and speech translation.",United States of America,,,,,,Unreleased,,,,
CogVideoX,Video,"Video generation,Text-to-video","Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, Da Yin, Xiaotao Gu, Yuxuan Zhang, Weihan Wang, Yean Cheng, Ting Liu, Bin Xu, Yuxiao Dong, Jie Tang",Open weights (restricted use),https://arxiv.org/abs/2408.06072,,CogVideoX: Text-to-Video Diffusion Models with An Expert Transformer,2024-10-08,"Z.ai (Zhipu AI),Tsinghua University",5000000000.0,5 billion,,,"LAION,COYO-700M","""We construct a collection of relatively high-quality video clips with text descriptions with video filters and recaption models. After filtering, approximately 35M single-shot clips remain, with each clip averaging about 6 seconds. We additionally use 2B images filtered with aesthetics score from LAION-5B (Schuhmann et al., 2022) and COYO-700M (Byeon et al., 2022) datasets to assist training.""",,"""35M single-shot clips remain, with each clip averaging about 6 seconds""
""During training, we first train a 3D VAE at 256 Ã— 256 resolution and 17 frames to save computation. Randomly select 8 or 16 fps to enhance the modelâ€™s robustness""
""we conduct a two-stage training process by first training on a video of 17 frames and finetuning by context parallel on videos of 161 frames.""

They provide many training details for smaller models but not this one",,,,Confident,"We present CogVideoX, a large-scale text-to-video generation model based on diffusion transformer, which can generate 10-second continuous videos aligned with text prompt, with a frame rate of 16 fps and resolution of 768 * 1360 pixels. Previous video generation models often had limited movement and short durations, and is difficult to generate videos with coherent narratives based on text. We propose several designs to address these issues. First, we propose a 3D Variational Autoencoder (VAE) to compress videos along both spatial and temporal dimensions, to improve both compression rate and video fidelity. Second, to improve the text-video alignment, we propose an expert transformer with the expert adaptive LayerNorm to facilitate the deep fusion between the two modalities. Third, by employing a progressive training and multi-resolution frame pack technique, CogVideoX is adept at producing coherent, long-duration, different shape videos characterized by significant motions. In addition, we develop an effective text-video data processing pipeline that includes various data preprocessing strategies and a video captioning method, greatly contributing to the generation quality and semantic alignment. Results show that CogVideoX demonstrates state-of-the-art performance across both multiple machine metrics and human evaluations. The model weight of both 3D Causal VAE, Video caption model and CogVideoX are publicly available at this https URL.","China,China",,,,,,Open source,"CogVideoX License (commercial usage allowed for companies with up to 1 million visits per month)
https://huggingface.co/THUDM/CogVideoX-5b

https://github.com/THUDM/CogVideo
Apache 2.0 for code",,,
Aria,"Multimodal,Language,Video,Vision","Language modeling/generation,Visual question answering,Image captioning,Question answering,Code generation,Video description,Character recognition (OCR)","Dongxu Li, Yudong Liu, Haoning Wu, Yue Wang, Zhiqi Shen, Bowen Qu, Xinyao Niu, Fan Zhou, Chengen Huang, Yanpeng Li, Chongyan Zhu, Xiaoyi Ren, Chao Li, Yifan Ye, Peng Liu, Lihuan Zhang, Hanshu Yan, Guoyin Wang, Bei Chen, Junnan Li",Open weights (unrestricted),https://arxiv.org/abs/2410.05993,114.0,"ARIA : An Open Multimodal Native
Mixture-of-Experts Model",2024-10-08,Rhymes AI,24900000000.0,ARIA MoE activates 3.5B parameters per text token and has a total of 24.9B parameter,1.428e+23,"6 FLOP / parameter / token * 3,5 * 10^9 active parameters * 6,8 * 10^12 tokens = 1.428e+23 FLOP",Unspecified unreleased,"Data. ARIA is pre-trained on 6.4T language tokens and 400B multimodal tokens. We develop a rigorous procedure to curate high-quality data from a diverse set of sources. The multimodal pre-train data includes four major categories: interleaved image-text sequence from common crawl, synthetic image captions, documents transcriptions and question-answering pairs, synthetic video captions and questionanswering pairs",6800000000000.0,Data. ARIA is pre-trained on 6.4T language tokens and 400B multimodal tokens. ,,,,Confident,"Information comes in diverse modalities. Multimodal native AI models are essential to integrate real-world information and deliver comprehensive understanding. While proprietary multimodal native models exist, their lack of openness imposes obstacles for adoptions, let alone adaptations. To fill this gap, we introduce Aria, an open multimodal native model with best-in-class performance across a wide range of multimodal, language, and coding tasks. Aria is a mixture-of-expert model with 3.9B and 3.5B activated parameters per visual token and text token, respectively. It outperforms Pixtral-12B and Llama3.2-11B, and is competitive against the best proprietary models on various multimodal tasks. We pre-train Aria from scratch following a 4-stage pipeline, which progressively equips the model with strong capabilities in language understanding, multimodal understanding, long context window, and instruction following. We open-source the model weights along with a codebase that facilitates easy adoptions and adaptations of Aria in real-world applications.",United States of America,,,,,,Unreleased,"Apache 2.0
https://github.com/rhymes-ai/Aria
https://huggingface.co/rhymes-ai/Aria",,,
Movie Gen Video,"Video,Vision","Video generation,Text-to-video,Image-to-video","Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen
Shi, Chih-Yao Ma, Ching-Yao Chuang, David Yan, Dhruv Choudhary, Dingkang Wang, Geet Sethi, Guan
Pang, Haoyu Ma, Ishan Misra, Ji Hou, Jialiang Wang, Kiran Jagadeesh, Kunpeng Li, Luxin Zhang, Mannat
Singh, Mary Williamson, Matt Le, Mitesh Kumar Singh, Peizhao Zhang, Peter Vajda, Quentin Duval, Rohit
Girdhar, Roshan Sumbaly, Sai Saketh Rambhatla, Sam Tsai, Samaneh Azadi, Samyak Datta, Sanyuan Chen,
Sean Bell, Sharadh Ramaswamy, Shelly Sheynin, Siddharth Bhattacharya, Tao Xu, Tingbo Hou, Wei-Ning
Hsu, Xi Yin, Xiaoliang Dai, Yaniv Taigman, Yaqiao Luo, Yen-Cheng Liu, Yi-Chiao Wu, Yue Zhao, Yuval
Kirstain, Zecheng He, Zijian He",Unreleased,https://ai.meta.com/static-resource/movie-gen-research-paper,,Movie Gen: A Cast of Media Foundation Models,2024-10-04,Meta AI,30000000000.0,30B,1.65e+24,"Model size = 30B
Broken down by training stage (table 3):
256px T2I: samples seen = 1.94E9; sample token length = 256; flops = 6ND = 8.94E22
256px T2I/V: samples seen = 3.95E8; sample token length = 8192; flops = 6ND = 5.82E23
768px T2I/V: samples seen = 7.38E7; sample token length = 73,728; flops = 6ND = 9.79E23
Total flops = 1.65E24",,,26600000000.0,"O(1B) images
O(100M) videos, each with 256 frames ~= 25M images",331.0,"54 hours for 256px T2I
128 hours for 256px T2I/V
149 hours for 768px T2I/V",NVIDIA H100 SXM5 80GB,Confident,"We present Movie Gen, a cast of foundation models that generates high-quality, 1080p HD videos with different aspect ratios and synchronized audio. We also show additional capabilities such as precise instruction-based video editing and generation of personalized videos based on a userâ€™s image.
Our models set a new state-of-the-art on multiple tasks: text-to-video synthesis, video personalization, video editing, video-to-audio generation, and text-to-audio generation. Our largest video generation model is a 30B parameter transformer trained with a maximum context length of 73K video tokens, corresponding to a generated video of 16 seconds at 16 frames-per-second. We show multiple technical innovations and simplifications on the 
 architecture, latent spaces, training objectives and recipes, data curation, evaluation protocols, parallelization techniques, and inference optimizations that allow us to reap the benefits of scaling pre-training data, model size, and training compute for training large scale media generation models. We hope this paper helps the research community to accelerate progress and innovation in media generation models.
All videos from this paper are available at https://go.fb.me/MovieGenResearchVideos.",United States of America,,,,6144.0,,Unreleased,,,,
Movie Gen Audio,Audio,Audio generation,"Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen
Shi, Chih-Yao Ma, Ching-Yao Chuang, David Yan, Dhruv Choudhary, Dingkang Wang, Geet Sethi, Guan
Pang, Haoyu Ma, Ishan Misra, Ji Hou, Jialiang Wang, Kiran Jagadeesh, Kunpeng Li, Luxin Zhang, Mannat
Singh, Mary Williamson, Matt Le, Mitesh Kumar Singh, Peizhao Zhang, Peter Vajda, Quentin Duval, Rohit
Girdhar, Roshan Sumbaly, Sai Saketh Rambhatla, Sam Tsai, Samaneh Azadi, Samyak Datta, Sanyuan Chen,
Sean Bell, Sharadh Ramaswamy, Shelly Sheynin, Siddharth Bhattacharya, Tao Xu, Tingbo Hou, Wei-Ning
Hsu, Xi Yin, Xiaoliang Dai, Yaniv Taigman, Yaqiao Luo, Yen-Cheng Liu, Yi-Chiao Wu, Yue Zhao, Yuval
Kirstain, Zecheng He, Zijian He",Unreleased,https://ai.meta.com/static-resource/movie-gen-research-paper,,Movie Gen: A Cast of Media Foundation Models,2024-10-04,Meta AI,13000000000.0,13B,1.4e+23,Pre trained for 14 days on 384 H100s. I assumed a 0.3 utilization rate,,,,It was trained on O(1k) hours of audio,360.0,"14 days of pretraining, 1 day of fine tuning",NVIDIA H100 SXM5 80GB,Confident,"We present Movie Gen, a cast of foundation models that generates high-quality, 1080p HD videos
with different aspect ratios and synchronized audio. We also show additional capabilities such as
precise instruction-based video editing and generation of personalized videos based on a userâ€™s image.
Our models set a new state-of-the-art on multiple tasks: text-to-video synthesis, video personalization,
video editing, video-to-audio generation, and text-to-audio generation. Our largest video generation
model is a 30B parameter transformer trained with a maximum context length of 73K video tokens,
corresponding to a generated video of 16 seconds at 16 frames-per-second. We show multiple technical
innovations and simplifications on the architecture, latent spaces, training objectives and recipes, data
curation, evaluation protocols, parallelization techniques, and inference optimizations that allow us to
reap the benefits of scaling pre-training data, model size, and training compute for training large scale
media generation models. We hope this paper helps the research community to accelerate progress
and innovation in media generation models.
All videos from this paper are available at https://go.fb.me/MovieGenResearchVideos.",United States of America,,,,384.0,,Unreleased,,,,
Llama 3.2 11B,"Multimodal,Vision,Language","Visual question answering,Image captioning,Object detection",Meta AI,Open weights (restricted use),https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/,,"Llama 3.2: Revolutionizing edge AI and vision with open, customizable models",2024-09-24,Meta AI,10600000000.0,https://huggingface.co/meta-llama/Llama-3.2-11B-Vision,5.79e+23,"Tensor type is BF16 (https://huggingface.co/meta-llama/Llama-3.2-11B-Vision-Instruct).

â€œTraining utilized a cumulative of 2.02M GPU hours of computation on H100-80GB (TDP of 700W) type hardware, per the table below. Training time is the total GPU time required for training each model and power consumption is the peak power capacity per GPU device used, adjusted for power usage efficiencyâ€¦ Training time: Stage 1 pretraining: 147K H100 hours Stage 2 annealing: 98K H100 hours SFT: 896 H100 hours RLHF: 224 H100 hours.â€ (https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD_VISION.md#hardware-and-software).

The Nvidia H100 80GB is the H100 SXM. BFLOAT16 Tensor Core peak FLOPS with sparsity is 1,979 teraFLOPS (https://resources.nvidia.com/en-us-tensor-core/nvidia-tensor-core-gpu-datasheet?ncid=no-ncid).

Assuming 33% utilization rate,
Training compute
~= 0.33 * ( 147000 + 98000 + 896 + 224 ) hours * 3600 s / hour * 1979e12 FLOPS / GPU
~= 5.79e23 FLOPS",Unspecified unreleased,"""Llama 3.2-Vision was pretrained on 6B image and text pairs. The instruction tuning data includes publicly available vision instruction datasets, as well as over 3M synthetically generated examples... The pretraining data has a cutoff of December 2023."" (https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD_VISION.md#training-data).",6000000000.0,"Since the model can also be used for image captioning, I assume the dataset size is measured in numbers of image-caption pairs (https://docs.google.com/document/d/1XWLyMzcVfDv4eFQX3yPgM8MZ3_Q1phtIFz9GKv4_KaM/edit?tab=t.0#heading=h.ny4fw3njk98p). """"Llama 3.2-Vision was pretrained on 6B image and text pairs"" (https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD_VISION.md#training-data).",,"""Stage 1 pretraining: 147K H100 hours Stage 2 annealing: 98K H100 hours SFT: 896 H100 hours RLHF: 224 H100 hours""

https://huggingface.co/meta-llama/Llama-3.2-11B-Vision",NVIDIA H100 SXM5 80GB,Confident,"Today, weâ€™re releasing Llama 3.2, which includes small and medium-sized vision LLMs (11B and 90B), and lightweight, text-only models (1B and 3B) that fit onto edge and mobile devices, including pre-trained and instruction-tuned versions.
Supported by a broad ecosystem, the Llama 3.2 11B and 90B vision models are drop-in replacements for their corresponding text model equivalents, while exceeding on image understanding tasks compared to closed models, such as Claude 3 Haiku. Unlike other open multimodal models, both pre-trained and aligned models are available to be fine-tuned for custom applications using torchtune and deployed locally using torchchat. Theyâ€™re also available to try using our smart assistant, Meta AI.
Weâ€™re sharing the first official Llama Stack distributions, which will greatly simplify the way developers work with Llama models in different environments, including single-node, on-prem, cloud, and on-device, enabling turnkey deployment of retrieval-augmented generation (RAG) and tooling-enabled applications with integrated safety.
Weâ€™ve been working closely with partners like AWS, Databricks, Dell Technologies, Fireworks, Infosys, and Together AI to build Llama Stack distributions for their downstream enterprise clients. On-device distribution is via PyTorch ExecuTorch, and single-node distribution is via Ollama.
We continue to share our work because we believe openness drives innovation and is good for developers, Meta, and the world. Llama is already leading the way on openness, modifiability, and cost efficiencyâ€”enabling more people to have creative, useful, and life-changing breakthroughs using generative AI.
Weâ€™re making Llama 3.2 models available for download on llama.com and Hugging Face, as well as available for immediate development on our broad ecosystem of partner platforms, including AMD, AWS, Databricks, Dell, Google Cloud, Groq, IBM, Intel, Microsoft Azure, NVIDIA, Oracle Cloud, Snowflake, and more.",United States of America,Llama 3.1-8B,3.50010000000001e+23,147000+98000+896+224 GPU-hours => 246120 GPU-hours * 60 * 60 * 989e12 FLOP * 0.4 (utilization) = 3.5e23 FLOP,,,Unreleased,"LLAMA 3.2 COMMUNITY LICENSE AGREEMENT

https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/LICENSE",,,
Llama 3.2 90B,"Multimodal,Vision,Language","Visual question answering,Image captioning,Object detection",,Open weights (restricted use),https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/,,"Llama 3.2: Revolutionizing edge AI and vision with open, customizable models",2024-09-24,Meta AI,88600000000.0,https://huggingface.co/meta-llama/Llama-3.2-90B-Vision,,,Unspecified unreleased,"Knowledge cutoff date is December 2023, according to https://huggingface.co/meta-llama/Llama-3.2-11B-Vision.",6000000000.0,"""6B (image, text) pairs""",,"""Stage 1 pretraining: 885K H100 hours Stage 2 annealing: 885K H100 hours SFT: 3072 H100 hours RLHF: 2048 H100 hours""

https://huggingface.co/meta-llama/Llama-3.2-90B-Vision",NVIDIA H100 SXM5 80GB,Confident,"Today, weâ€™re releasing Llama 3.2, which includes small and medium-sized vision LLMs (11B and 90B), and lightweight, text-only models (1B and 3B) that fit onto edge and mobile devices, including pre-trained and instruction-tuned versions.
Supported by a broad ecosystem, the Llama 3.2 11B and 90B vision models are drop-in replacements for their corresponding text model equivalents, while exceeding on image understanding tasks compared to closed models, such as Claude 3 Haiku. Unlike other open multimodal models, both pre-trained and aligned models are available to be fine-tuned for custom applications using torchtune and deployed locally using torchchat. Theyâ€™re also available to try using our smart assistant, Meta AI.
Weâ€™re sharing the first official Llama Stack distributions, which will greatly simplify the way developers work with Llama models in different environments, including single-node, on-prem, cloud, and on-device, enabling turnkey deployment of retrieval-augmented generation (RAG) and tooling-enabled applications with integrated safety.
Weâ€™ve been working closely with partners like AWS, Databricks, Dell Technologies, Fireworks, Infosys, and Together AI to build Llama Stack distributions for their downstream enterprise clients. On-device distribution is via PyTorch ExecuTorch, and single-node distribution is via Ollama.
We continue to share our work because we believe openness drives innovation and is good for developers, Meta, and the world. Llama is already leading the way on openness, modifiability, and cost efficiencyâ€”enabling more people to have creative, useful, and life-changing breakthroughs using generative AI.
Weâ€™re making Llama 3.2 models available for download on llama.com and Hugging Face, as well as available for immediate development on our broad ecosystem of partner platforms, including AMD, AWS, Databricks, Dell, Google Cloud, Groq, IBM, Intel, Microsoft Azure, NVIDIA, Oracle Cloud, Snowflake, and more.",United States of America,Llama 3.1-70B,2.500010000000001e+24,885000Ã—2+3072+2048 GPU-hours => 1775120 GPU-hours * 60 * 60 * 989e12 FLOP * 0.4 (utilization) = 2.5e24 FLOP,,,Unreleased,"LLAMA 3.2 COMMUNITY LICENSE AGREEMENT

https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/LICENSE",,,
Llama 3.2 3B,Language,"Language modeling/generation,Text summarization,Question answering,Quantitative reasoning,Translation",,Open weights (restricted use),https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/,,"Llama 3.2: Revolutionizing edge AI and vision with open, customizable models",2024-09-24,Meta AI,3210000000.0,https://huggingface.co/meta-llama/Llama-3.2-1B,1.7334e+23,"6ND = 6*3210000000.00*9000000000000 = 1.7334e+23

460000 hours * 3600 s * 133800000000000 FLOPS/s* 0.3 = 6.647184e+22",Unspecified unreleased,"Knowledge cutoff date is December 2023, according to, https://huggingface.co/meta-llama/Llama-3.2-1B.",9000000000000.0,"""Llama 3.2 was pretrained on up to 9 trillion tokens of data from publicly available sources.""",,https://huggingface.co/meta-llama/Llama-3.2-1B,NVIDIA H100 SXM5 80GB,Confident,"Today, weâ€™re releasing Llama 3.2, which includes small and medium-sized vision LLMs (11B and 90B), and lightweight, text-only models (1B and 3B) that fit onto edge and mobile devices, including pre-trained and instruction-tuned versions.
The Llama 3.2 1B and 3B models support context length of 128K tokens and are state-of-the-art in their class for on-device use cases like summarization, instruction following, and rewriting tasks running locally at the edge. These models are enabled on day one for Qualcomm and MediaTek hardware and optimized for Arm processors.
Weâ€™re sharing the first official Llama Stack distributions, which will greatly simplify the way developers work with Llama models in different environments, including single-node, on-prem, cloud, and on-device, enabling turnkey deployment of retrieval-augmented generation (RAG) and tooling-enabled applications with integrated safety.
Weâ€™ve been working closely with partners like AWS, Databricks, Dell Technologies, Fireworks, Infosys, and Together AI to build Llama Stack distributions for their downstream enterprise clients. On-device distribution is via PyTorch ExecuTorch, and single-node distribution is via Ollama.
We continue to share our work because we believe openness drives innovation and is good for developers, Meta, and the world. Llama is already leading the way on openness, modifiability, and cost efficiencyâ€”enabling more people to have creative, useful, and life-changing breakthroughs using generative AI.
Weâ€™re making Llama 3.2 models available for download on llama.com and Hugging Face, as well as available for immediate development on our broad ecosystem of partner platforms, including AMD, AWS, Databricks, Dell, Google Cloud, Groq, IBM, Intel, Microsoft Azure, NVIDIA, Oracle Cloud, Snowflake, and more.",United States of America,,,,,,Unreleased,"LLAMA 3.2 COMMUNITY LICENSE AGREEMENT

https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/LICENSE",,,
PixelDance,"Video,Vision","Video generation,Text-to-video,Image-to-video",ByteDance,Hosted access (no API),https://pixeldance.io/ (LINK BROKEN),,PixelDance AI - Leading AI Video Generation Platform,2024-09-24,ByteDance,,,,,,,,,,,,Unknown,"PixelDance V1.4 is a video generation model developed by the ByteDance Research team, using the DiT structure. It supports both text-to-video and image-to-video generation, capable of producing impressive 10-second video clips. The model has excellent semantic understanding, capable of handling complex narratives and subtle emotional expressions. It can perform sequential multi-shot actions, support complex interactions between multiple subjects, and offer a variety of camera effects. Compatible with multiple styles and proportions, it can quickly generate high-quality video clips, empowering film creation, advertising, short videos, live streaming, and e-commerce.",China,,,,,,Unreleased,,,,
Spark 4.0,Language,Language modeling,iFlytek,Unreleased,https://www.iflytek.com/en/news-events/news/218.html,,"""SPARK"" Ignites a New Engine for Manufacturing Development: iFLYTEK Shines at 2024 World Manufacturing Convention",2024-09-23,iFlytek,,,,"Developer could afford a 1e25 FLOP scale training run, so plausibly they might have done this.",,,,,,,,Unknown,"From September 20 to 23, the 2024 World Manufacturing Convention, themed ""Intelligent Manufacturing for a Better Future"", was held at the Hefei Binhu International Convention & Exhibition Center. iFLYTEK SPARK Large Model V4.0 (hereinafter referred to as "" iFLYTEK SPARK""), along with humanoid robot equipped with iFLYTEK SPARK, the Anhui Provincial Imaging Cloud Platform, and other products and solutions, made a grand appearance at the exhibition. Antelope Industrial Internet Co., Ltd. unveiled the Antelope Industrial Large Model V2.0 and a series of industrial application scenarios, demonstrating in a comprehensive manner the diverse ecosystem of general artificial intelligence empowering various industries.",China,,,,,,,,,,
Kling 1.5 Pro,"Video,Vision","Video generation,Text-to-video,Image-to-video",,API access,https://www.zeniteq.com/blog/kling-1-5-ai-video-generator-is-finally-here-with-major-upgrades,,"Kling 1.5 AI Video Generator is Finally Here With Major Upgrades
",2024-09-22,Kuaishou Technology,,,,"Plausibly used more compute than Meta Movie Gen, as it had higher preference scores against Veo-2 than MMG did.",Unspecified unreleased,,,,,,,Unknown,"Kling AI, one of the most popular AI video generators and a strong competitor to Runway Gen-3 and OpenAIâ€™s Sora, has just released Kling AI 1.5 Pro. This update brings some major improvements: a new and improved video model, the introduction of a motion brush feature, and much more.

Developed by Kuaishou, a Beijing-based company that competes directly with TikTok, Kling AI allows users to create videos up to 10 seconds long with 30 frames per second at 1080p resolution. It also supports flexible aspect ratios, making it a versatile tool for content creators.",China,,,,,,Unreleased,,,,
Telechat2-115B,Language,"Language modeling/generation,Question answering,Quantitative reasoning,Chat,Text summarization,Code generation",Zihan Wang and Xinzhang Liu and Shixuan Liu and Yitong Yao and Yuyao Huang and Zhongjiang He and Xuelong Li and Yongxiang Li and Zhonghao Che and Zhaoxi Zhang and Yan Wang and Xin Wang and Luwen Pu and Huihan Xu and Ruiyu Fang and Yu Zhao and Jie Zhang and Xiaomeng Huang and Zhilong Lu and Jiaxin Peng and Wenjun Zheng and Shiquan Wang and Bingkai Yang and Xuewei he and Zhuoru Jiang and Qiyi Xie and Yanhan Zhang and Zhongqiu Li and Lingling Shi and Weiwei Fu and Yin Zhang and Zilu Huang and Sishi Xiong and Yuxiang Zhang and Chao Wang and Shuangyong Song,Open weights (restricted use),https://huggingface.co/Tele-AI/TeleChat2-115B,,TeleChat Technical Report,2024-09-20,China Telecom,115000000000.0,,6.9e+24,6ND: 6 * 115B * 10T = 6.9e24,,,10000000000000.0,The open source TeleChat2-115B model is trained using 10 trillion tokens of high-quality Chinese and English corpus,,,,Confident,,China,,,,,,Open (restricted use),"Apache 2.0
https://huggingface.co/Tele-AI/TeleChat2-115B

""TeleChat model supports commercial use if you plan to treat TeleChat The model or its derivatives are used for commercial purposes, you need to contact the mailbox below tele_ai@chinatelecom.cn""

no clear license but same disclaimer as above
https://github.com/Tele-AI/TeleChat2/

this is seems to be pre-training code:
https://github.com/Tele-AI/TeleChat2/tree/main/deepspeed",,,
Qwen2.5-72B,Language,"Language modeling/generation,Question answering,Quantitative reasoning",Qwen Team,Open weights (unrestricted),https://qwenlm.github.io/blog/qwen2.5/,,Qwen2.5: A Party of Foundation Models!,2024-09-19,Alibaba,72700000000.0,72.7B,7.8e+24,"Training dataset size was 18 trillion

6ND = 6 * 72.7 billion parameters * 18 trillion tokens = 7.8e24",Unspecified unreleased,"""In terms of Qwen2.5, the language models, all models are pretrained on our latest large-scale dataset, encompassing up to 18 trillion tokens.""",18000000000000.0,"""In terms of Qwen2.5, the language models, all models are pretrained on our latest large-scale dataset, encompassing up to 18 trillion tokens""",,,,Confident,"In the past three months since Qwen2â€™s release, numerous developers have built new models on the Qwen2 language models, providing us with valuable feedback. During this period, we have focused on creating smarter and more knowledgeable language models. Today, we are excited to introduce the latest addition to the Qwen family: Qwen2.5. We are announcing what might be the largest opensource release in history! Letâ€™s get the party started!",China,,,,,,Unreleased,"license: allows commercial. weights only
https://huggingface.co/Qwen/Qwen2.5-72B/blob/main/LICENSE ",,,
Qwen2.5-3B,Language,"Language modeling/generation,Question answering,Quantitative reasoning",Qwen Team,Open weights (non-commercial),https://qwenlm.github.io/blog/qwen2.5-llm/,,Qwen2.5: A Party of Foundation Models!,2024-09-19,Alibaba,3090000000.0,3.09B,3.3372e+23,"Training dataset size was 18 trillion

6 FLOP/parameter/token * 3090000000 parameters * 18000000000000 tokens = 3.3372e+23 FLOP",Unspecified unreleased,"""In terms of Qwen2.5, the language models, all models are pretrained on our latest large-scale dataset, encompassing up to 18 trillion tokens.""",18000000000000.0,"""In terms of Qwen2.5, the language models, all models are pretrained on our latest large-scale dataset, encompassing up to 18 trillion tokens""",,,,Confident,"In the past three months since Qwen2â€™s release, numerous developers have built new models on the Qwen2 language models, providing us with valuable feedback. During this period, we have focused on creating smarter and more knowledgeable language models. Today, we are excited to introduce the latest addition to the Qwen family: Qwen2.5. We are announcing what might be the largest opensource release in history! Letâ€™s get the party started!",China,,,,,,Unreleased,Qwen Research license,,,
Qwen2.5-7B,Language,"Language modeling/generation,Question answering,Quantitative reasoning",Qwen Team,Open weights (unrestricted),https://qwenlm.github.io/blog/qwen2.5/,,Qwen2.5: A Party of Foundation Models!,2024-09-19,Alibaba,7610000000.0,7.61B,8.2188e+23,"Training dataset size was 18 trillion

6ND = 6 * 7.61 billion parameters * 18 trillion tokens = 8.2188e+23",Unspecified unreleased,"""In terms of Qwen2.5, the language models, all models are pretrained on our latest large-scale dataset, encompassing up to 18 trillion tokens.""",18000000000000.0,"""In terms of Qwen2.5, the language models, all models are pretrained on our latest large-scale dataset, encompassing up to 18 trillion tokens""",,,,Confident,"In the past three months since Qwen2â€™s release, numerous developers have built new models on the Qwen2 language models, providing us with valuable feedback. During this period, we have focused on creating smarter and more knowledgeable language models. Today, we are excited to introduce the latest addition to the Qwen family: Qwen2.5. We are announcing what might be the largest opensource release in history! Letâ€™s get the party started!

The Qwen2.5-7B model surpasses its predecessors and counterparts in numerous benchmarks, despite having fewer non-embedding parameters. It demonstrates significant improvements across various tasks, achieving 74.2 on general benchmarks like MMLU, 49.8 on math challenges such as MATH, and 57.9 on coding tasks like HumanEval.",China,,,,,,Unreleased,Apache 2.0,,,
Qwen2.5-1.5B,Language,"Language modeling/generation,Question answering,Quantitative reasoning",Qwen Team,Open weights (unrestricted),https://qwenlm.github.io/blog/qwen2.5-llm/,,Qwen2.5-LLM: Extending the boundary of LLMs,2024-09-19,Alibaba,1540000000.0,1.54B,1.6632e+23,"Training dataset size was 18 trillion

6ND = 6 * 1.54B billion parameters * 18 trillion tokens = 1.6632e+23",Unspecified unreleased,"""In terms of Qwen2.5, the language models, all models are pretrained on our latest large-scale dataset, encompassing up to 18 trillion tokens.""",18000000000000.0,"""In terms of Qwen2.5, the language models, all models are pretrained on our latest large-scale dataset, encompassing up to 18 trillion tokens""",,,,Confident,"In the past three months since Qwen2â€™s release, numerous developers have built new models on the Qwen2 language models, providing us with valuable feedback. During this period, we have focused on creating smarter and more knowledgeable language models. Today, we are excited to introduce the latest addition to the Qwen family: Qwen2.5. We are announcing what might be the largest opensource release in history! Letâ€™s get the party started!",China,,,,,,Unreleased,Apache 2.0,,,
Qwen2.5-14B,Language,"Language modeling/generation,Question answering,Quantitative reasoning",Qwen Team,Open weights (unrestricted),https://qwenlm.github.io/blog/qwen2.5/,,Qwen2.5: A Party of Foundation Models!,2024-09-19,Alibaba,14700000000.0,"14.7B according to the model card
https://qwenlm.github.io/blog/qwen2.5-llm/",1.58760000000001e+24,"Training dataset size was 18 trillion

6ND = 6 * 14.7 billion parameters * 18 trillion tokens = 1.59e24",Unspecified unreleased,"""In terms of Qwen2.5, the language models, all models are pretrained on our latest large-scale dataset, encompassing up to 18 trillion tokens.""",18000000000000.0,"""In terms of Qwen2.5, the language models, all models are pretrained on our latest large-scale dataset, encompassing up to 18 trillion tokens""",,,,Confident,"In the past three months since Qwen2â€™s release, numerous developers have built new models on the Qwen2 language models, providing us with valuable feedback. During this period, we have focused on creating smarter and more knowledgeable language models. Today, we are excited to introduce the latest addition to the Qwen family: Qwen2.5. We are announcing what might be the largest opensource release in history! Letâ€™s get the party started!

The Qwen2.5-7B model surpasses its predecessors and counterparts in numerous benchmarks, despite having fewer non-embedding parameters. It demonstrates significant improvements across various tasks, achieving 74.2 on general benchmarks like MMLU, 49.8 on math challenges such as MATH, and 57.9 on coding tasks like HumanEval.",China,,,,,,Unreleased,Apache 2.0,,,
Qwen2-VL-72B,"Language,Vision,Multimodal","Visual question answering,Video description,Language modeling/generation,Translation,Question answering,Character recognition (OCR),Quantitative reasoning","Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, Junyang Lin",Open weights (restricted use),https://arxiv.org/abs/2409.12191,,Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution,2024-09-18,Alibaba,72000000000.0,72 billion (language model) and 675M (vision encoder),6.048e+23,6ND = 6 FLOP / token / parameter Ã— 1.4Ã—10^12 tokens Ã— 7.2Ã—10^10 parameters = 6.048e+23 FLOP,Unspecified unreleased,"""The model is pre-trained on a diverse dataset that includes image-text pairs, optical character recognition (OCR) data, interleaved image-text articles, visual question answering datasets, video dialogues, and image knowledge datasets. Our data sources primarily comprise cleaned web pages, open-source datasets, and synthetic data. The cutoff date for our data knowledge is June 2023.""",1400000000000.0,"""Throughout the pre-training stages, Qwen2-VL processes a cumulative total of 1.4 trillion tokens. Specifically, these tokens encompass not only text tokens but also image tokens""",,,,Confident,"We present the Qwen2-VL Series, an advanced upgrade of the previous Qwen-VL models that redefines the conventional predetermined-resolution approach in visual processing. Qwen2-VL introduces the Naive Dynamic Resolution mechanism, which enables the model to dynamically process images of varying resolutions into different numbers of visual tokens. This approach allows the model to generate more efficient and accurate visual representations, closely aligning with human perceptual processes. The model also integrates Multimodal Rotary Position Embedding (M-RoPE), facilitating the effective fusion of positional information across text, images, and videos. We employ a unified paradigm for processing both images and videos, enhancing the model's visual perception capabilities. To explore the potential of large multimodal models, Qwen2-VL investigates the scaling laws for large vision-language models (LVLMs). By scaling both the model size-with versions at 2B, 8B, and 72B parameters-and the amount of training data, the Qwen2-VL Series achieves highly competitive performance. Notably, the Qwen2-VL-72B model achieves results comparable to leading models such as GPT-4o and Claude3.5-Sonnet across various multimodal benchmarks, outperforming other generalist models. Code is available at \url{this https URL}.",China,,,,,,Unreleased,"tongyi-qianwen (<100M MAU)
https://huggingface.co/Qwen/Qwen2-VL-72B-Instruct",,,
Qwen2.5-Coder (7B),Language,"Code generation,Code autocompletion,Quantitative reasoning,Question answering,Language modeling/generation","Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Kai Dang, An Yang, Rui Men, Fei Huang, Xingzhang Ren, Xuancheng Ren, Jingren Zhou, Junyang Lin",Open weights (unrestricted),https://arxiv.org/abs/2409.12186,,Qwen2.5-Coder Technical Report,2024-09-18,Alibaba,7610000000.0,Number of Parameters: 7.61B,2.5113e+23,6ND = 6 FLOP / token / parameter *7610000000 parameters *5.5T tokens =2.5113e+23 FLOP,"GitHub,Common Crawl","""we constructed a dataset named Qwen2.5-Coder-Data. This dataset comprises five key data types: Source Code Data, Text-Code Grounding Data, Synthetic Data, Math Data, and Text Data.""",5500000000000.0,,,,,Confident,"In this report, we introduce the Qwen2.5-Coder series, a significant upgrade from its predecessor, CodeQwen1.5. This series includes two models: Qwen2.5-Coder-1.5B and Qwen2.5-Coder-7B. As a code-specific model, Qwen2.5-Coder is built upon the Qwen2.5 architecture and continues pretrained on a vast corpus of over 5.5 trillion tokens. Through meticulous data cleaning, scalable synthetic data generation, and balanced data mixing, Qwen2.5-Coder demonstrates impressive code generation capabilities while retaining general versatility. The model has been evaluated on a wide range of code-related tasks, achieving state-of-the-art (SOTA) performance across more than 10 benchmarks, including code generation, completion, reasoning, and repair, consistently outperforming larger models of the same model size. We believe that the release of the Qwen2.5-Coder series will not only push the boundaries of research in code intelligence but also, through its permissive licensing, encourage broader adoption by developers in real-world applications.",China,,,,,,Unreleased,"Apache 2.0 
https://huggingface.co/Qwen/Qwen2.5-Coder-7B",,,
Pixtral 12B,"Vision,Language,Multimodal","Language modeling/generation,Question answering,Visual question answering,Code generation",Mistral AI Team,Open weights (unrestricted),https://mistral.ai/news/pixtral-12b/,,Pixtral 12B - the first-ever multimodal Mistral model. Apache 2.0.,2024-09-17,Mistral AI,12400000000.0,"""New 400M parameter vision encoder trained from scratch""
+
""12B parameter multimodal decoder based on Mistral Nemo""",,,Unspecified unreleased,,,"""We simply pass images through the vision encoder at their native resolution and aspect ratio, converting them into image tokens for each 16x16 patch in the image""",,,,Confident,"Natively multimodal, trained with interleaved image and text data
Strong performance on multimodal tasks, excels in instruction following
Maintains state-of-the-art performance on text-only benchmarks
New 400M parameter vision encoder trained from scratch
12B parameter multimodal decoder based on Mistral Nemo
Supports variable image sizes and aspect ratios
Supports multiple images in the long context window of 128k tokens
License: Apache 2.0
Pixtral is trained to understand both natural images and documents, achieving 52.5% on the MMMU reasoning benchmark, surpassing a number of larger models. The model shows strong abilities in tasks such as chart and figure understanding, document question answering, multimodal reasoning and instruction following. Pixtral is able to ingest images at their natural resolution and aspect ratio, giving the user flexibility on the number of tokens used to process an image. Pixtral is also able to process any number of images in its long context window of 128K tokens. Unlike previous open-source models, Pixtral does not compromise on text benchmark performance to excel in multimodal tasks.",France,,,,,,Unreleased,Apache 2.0,,,
Qwen2.5-32B,Language,"Language modeling/generation,Question answering,Quantitative reasoning",Qwen Team,Open weights (unrestricted),https://qwenlm.github.io/blog/qwen2.5/ ,,Qwen2.5: A Party of Foundation Models!,2024-09-17,Alibaba,32500000000.0,32.5B,3.51e+24,6 FLOP / parameter / token * 32.5B parameters * 18 trillion tokens = 3.51 Ã— 10^24 FLOP,Unspecified unreleased,"""In terms of Qwen2.5, the language models, all models are pretrained on our latest large-scale dataset, encompassing up to 18 trillion tokens.""",18000000000000.0,"""In terms of Qwen2.5, the language models, all models are pretrained on our latest large-scale dataset, encompassing up to 18 trillion tokens""",,,,Confident,"In the past three months since Qwen2â€™s release, numerous developers have built new models on the Qwen2 language models, providing us with valuable feedback. During this period, we have focused on creating smarter and more knowledgeable language models. Today, we are excited to introduce the latest addition to the Qwen family: Qwen2.5. We are announcing what might be the largest opensource release in history! Letâ€™s get the party started!

The Qwen2.5-7B model surpasses its predecessors and counterparts in numerous benchmarks, despite having fewer non-embedding parameters. It demonstrates significant improvements across various tasks, achieving 74.2 on general benchmarks like MMLU, 49.8 on math challenges such as MATH, and 57.9 on coding tasks like HumanEval.",China,,,,,,Unreleased,"Apache 2.0
https://huggingface.co/Qwen/Qwen2.5-32B",,,
o1-mini,Language,"Code generation,Language modeling/generation,Quantitative reasoning,Chat,Question answering,Translation",,API access,https://openai.com/index/learning-to-reason-with-llms/,,Learning to reason with LLMs,2024-09-12,OpenAI,,"Can't get an exact estimate, but we suspect total parameter count around 60B-120B, active parameters around 10B-30B. 

Given these models are served at 150-200 tok/s, at $4.40/Mtok output, inference economics (https://epoch.ai/blog/inference-economics-of-language-models) suggests total parameter count around 60-120B parameters, with mixture-of-experts active parameters around 10-30B. MoEs make a given model roughly comparable to a ~50% smaller dense model (https://epoch.ai/gradient-updates/moe-vs-dense-models-inference), which lines up decently with Magistral Small pricing (24B dense, served at a similar speed for the cheaper $1.50/Mtok). ",,"We canâ€™t make a precise estimate, but seems unlikely to exceed 10^25 FLOP. We think active parameter count is 10-30B. This would require >55T tokens to reach 10^25 FLOP at the large size, i.e. well beyond 10x overtraining relative to Chinchilla.",Unspecified unreleased,"""Both models were trained on a variety of publicly available datasets, including web data and open-source datasets. Key components include reasoning data and scientific literature. <..> To further enhance the capabilities of o1-preview and o1-mini, we formed partnerships to access high-value non-public datasets. These proprietary data sources include paywalled content, specialized archives, and other domain-specific datasets that provide deeper insights into industry-specific knowledge and use cases.""

This has a knowledge cutoff date of October 2023 - which I assume means October 1, 2023 - according to https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models?tabs=global-standard%2Cstandard-chat-completions#o-series-models.",,,,,,Unknown,"We've developed a new series of AI models designed to spend more time thinking before they respond. They can reason through complex tasks and solve harder problems than previous models in science, coding, and math.

...

Weâ€™re also releasing OpenAI o1-mini, a faster, cheaper reasoning model that is particularly effective at coding. As a smaller model, o1-mini is 80% cheaper than o1-preview, making it a powerful, cost-effective model for applications that require reasoning but not broad world knowledge.",United States of America,,,,,,Unreleased,,,,
o1-preview,"Language,Mathematics,Biology","Code generation,Language modeling/generation,Quantitative reasoning,Chat,Question answering,Translation",,API access,https://openai.com/index/introducing-openai-o1-preview/,,A new series of reasoning models for solving hard problems.,2024-09-12,OpenAI,,,,,Unspecified unreleased,"""Both models were trained on a variety of publicly available datasets, including web data and open-source datasets. Key components include reasoning data and scientific literature. <..> To further enhance the capabilities of o1-preview and o1-mini, we formed partnerships to access high-value non-public datasets. These proprietary data sources include paywalled content, specialized archives, and other domain-specific datasets that provide deeper insights into industry-specific knowledge and use cases.""

This has a knowledge cutoff date of October 2023 - which I assume means October 1, 2023 - according to https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models?tabs=global-standard%2Cstandard-chat-completions#o-series-models.",,,,,,Unknown,"We've developed a new series of AI models designed to spend more time thinking before they respond. They can reason through complex tasks and solve harder problems than previous models in science, coding, and math.

Today, we are releasing the first of this series in ChatGPT and our API. This is a preview and we expect regular updates and improvements. Alongside this release, weâ€™re also including evaluations for the next update, currently in development.",United States of America,,,,,,Unreleased,,,,
DeepSeek-V2.5,Language,"Language modeling/generation,Chat,Code generation","DeepSeek-AI, Aixin Liu, Bei Feng, Bin Wang, Bingxuan Wang, Bo Liu, Chenggang Zhao, Chengqi Dengr, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Hanwei Xu, Hao Yang, Haowei Zhang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Li, Hui Qu, J.L. Cai, Jian Liang, Jianzhong Guo, Jiaqi Ni, Jiashi Li, Jin Chen, Jingyang Yuan, Junjie Qiu, Junxiao Song, Kai Dong, Kaige Gao, Kang Guan, Lean Wang, Lecong Zhang, Lei Xu, Leyi Xia, Liang Zhao, Liyue Zhang, Meng Li, Miaojun Wang, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Mingming Li, Ning Tian, Panpan Huang, Peiyi Wang, Peng Zhang, Qihao Zhu, Qinyu Chen, Qiushi Du, R.J. Chen, R.L. Jin, Ruiqi Ge, Ruizhe Pan, Runxin Xu, Ruyi Chen, S.S. Li, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shaoqing Wu, Shengfeng Ye, Shirong Ma, Shiyu Wang, Shuang Zhou, Shuiping Yu, Shunfeng Zhou, Size Zheng, T. Wang, Tian Pei, Tian Yuan, Tianyu Sun, W.L. Xiao, Wangding Zeng, Wei An, Wen Liu, Wenfeng Liang, Wenjun Gao, Wentao Zhang, X.Q. Li, Xiangyue Jin, Xianzu Wang, Xiao Bi, Xiaodong Liu, Xiaohan Wang, Xiaojin Shen, Xiaokang Chen, Xiaosha Chen, Xiaotao Nie, Xiaowen Sun",Open weights (restricted use),https://huggingface.co/deepseek-ai/DeepSeek-V2.5,,DeepSeek-V2.5,2024-09-06,DeepSeek,236000000000.0,"21B active params, 236B total",1.7892e+24,"V2.5 is a merge of V2-coder and V2-chat
V2-coder is trained for 6T additional tokens from an intermediate checkpoint of V2, which had been trained for 4.2T tokens. Total: 10.2T
V2-chat is fine-tuned from V2, saw 8.2T tokens in pre-training
Unique steps: 8.2T + 6T = 14.2T
FLOPs: 6 * 21B * 14.2T = 1.7892e24","GitHub,Common Crawl",,,"The original V2 had a dataset of 8.1T unique tokens, and coder-V2 added an additional 1.391T unique tokens of code and math. But it appears no additional training was done to combine them into this model.",,,,Confident,,China,,,,,,Unreleased,,,,
Moonshot-v1,Language,Language modeling/generation,Moonshot,,,,,2024-09-01,Moonshot,,,,"Developer raised $1B in early 2024, so plausibly could have done a 1e25 scale training run. (https://www.maginative.com/article/chinese-ai-startup-moonshot-raises-1-billion-with-2-5-billion-valuation/)",,,,,,,,Unknown,,China,,,,,,,,,,
GLM-4-Plus,Language,Language modeling,Zhipu AI,API access,https://bigmodel.cn/dev/howuse/glm-4,,GLM-4-Plus,2024-08-29,Z.ai (Zhipu AI),,,,Estimated to be 3.6e+25 FLOP using benchmark imputation. https://colab.research.google.com/drive/1r3pUMhB7Kh0Gls9eG-v_XefWrye9fVQR?usp=sharing,,,,,,,,Speculative,"At the KDD International Conference on Data Mining and Knowledge Discovery, the Zhipu GLM team unveiled the new generation of base large modelâ€”GLM-4-Plus. As the latest version of Zhipuâ€™s fully self-developed GLM large model, GLM-4-Plus signifies Zhipu AIâ€™s continuous dedication in the field of general artificial intelligence, advancing the independent and autonomous innovation of large model technology.",China,,,,,,Unreleased,,,,
Pharia-1-LLM-7B,Language,"Language modeling/generation,Translation,Question answering",,Open weights (non-commercial),https://huggingface.co/Aleph-Alpha/Pharia-1-LLM-7B-control,,Introducing Pharia-1-LLM: transparent and compliant,2024-08-26,Aleph Alpha,7041544704.0,,4.43e+23,"reported by the authors: 2.75*10^23 + 1.68*10^23 = 4.43*10^23 FLOP

https://huggingface.co/Aleph-Alpha/Pharia-1-LLM-7B-control#compute--training-efficiency",Common Crawl,"The training data of our models comprises two components: web-crawled data and structured datasets with a total size of 7.7T, with a cutoff date 04/2023. We performed some additional web scraping to augment these datasets.

Web-crawled data was obtained by filtering and deduplicating data available in public datasets, derived from Common Crawl, in the following languages: English, French, German, Italian, Spanish, Dutch, Portuguese.

To deduplicate the data, we applied a Bloomfilter for exact document deduplication in English, French, German, Italian and Spanish. Portuguese and Dutch data was deduplicated using both URLs and fuzzy-deduplication with MinHashLSH.",7700000000000.0,4.7T + 3T = 7.7T tokens,,,"NVIDIA A100 SXM4 80 GB,NVIDIA H100 SXM5 80GB",Confident,"We are pleased to announce our new foundation model family that includes Pharia-1-LLM-7B-control and Pharia-1-LLM-7B-control-aligned, now publicly available under the Open Aleph License, which explicitly allows for non-commercial research and educational use. Pharia-1-LLM-7B-control is engineered to deliver concise, length-controlled responses that match the performance of leading open-source models in the 7B to 8B parameter range and is culturally and linguistically optimized for German, French, and Spanish by being trained on a multilingual base corpus. Pharia-1-LLM-7B-control is trained on carefully curated data in compliance with applicable EU and national regulations, including copyright and data privacy laws. With improved token efficiency, Pharia-1-LLM-7B-control excels in domain-specific applications, particularly in the automotive and engineering industries, and can be aligned to user preferences, making it suitable for critical applications without the risk of shutdown behavior. As such, it serves as a valuable addition to the communityâ€™s selection of weight-available foundation models. Pharia-1-LLM-7B-control-aligned has been added with additional safety guardrails via alignment methods.",Germany,,,,256.0,,Open (non-commercial),"https://huggingface.co/Aleph-Alpha/Pharia-1-LLM-7B-control

training framework is released here: https://github.com/Aleph-Alpha-Research/scaling",,,
Jamba 1.5-Large,Language,"Language modeling/generation,Chat,Translation,Question answering","Barak Lenz, Alan Arazi, Amir Bergman, Avshalom Manevich, Barak Peleg, Ben Aviram, Chen Almagor, Clara Fridman, Dan Padnos, Daniel Gissin, Daniel Jannai, Dor Muhlgay, Dor Zimberg, Edden M Gerber, Elad Dolev, Eran Krakovsky, Erez Safahi, Erez Schwartz, Gal Cohen, Gal Shachaf, Haim Rozenblum, Hofit Bata, Ido Blass, Inbal Magar, Itay Dalmedigos, Jhonathan Osin, Julie Fadlon, Maria Rozman, Matan Danos, Michael Gokhman, Mor Zusman, Naama Gidron, Nir Ratner, Noam Gat, Noam Rozen, Oded Fried, Ohad Leshno, Omer Antverg, Omri Abend, Opher Lieber, Or Dagan, Orit Cohavi, Raz Alon, Ro'i Belson, Roi Cohen, Rom Gilad, Roman Glozman, Shahar Lev, Shaked Meirom, Tal Delbari, Tal Ness, Tomer Asida, Tom Ben Gal, Tom Braude, Uriya Pumerantz, Yehoshua Cohen, Yonatan Belinkov, Yuval Globerson, Yuval Peleg Levy, Yoav Shoham",Open weights (restricted use),"https://arxiv.org/abs/2408.12570
https://www.ai21.com/blog/announcing-jamba-model-family
https://huggingface.co/ai21labs/AI21-Jamba-1.5-Large",,Jamba-1.5: Hybrid Transformer-Mamba Models at Scale,2024-08-22,AI21 Labs,398000000000.0,94B active/398B total,,,Unspecified unreleased,,,,,,NVIDIA H100 SXM5 80GB,Confident,"We present Jamba-1.5, new instruction-tuned large language models based on our Jamba architecture. Jamba is a hybrid Transformer-Mamba mixture of experts architecture, providing high throughput and low memory usage across context lengths, while retaining the same or better quality as Transformer models. We release two model sizes: Jamba-1.5-Large, with 94B active parameters, and Jamba-1.5-Mini, with 12B active parameters. Both models are fine-tuned for a variety of conversational and instruction-following capabilties, and have an effective context length of 256K tokens, the largest amongst open-weight models. To support cost-effective inference, we introduce ExpertsInt8, a novel quantization technique that allows fitting Jamba-1.5-Large on a machine with 8 80GB GPUs when processing 256K-token contexts without loss of quality. When evaluated on a battery of academic and chatbot benchmarks, Jamba-1.5 models achieve excellent results while providing high throughput and outperforming other open-weight models on long-context benchmarks. The model weights for both sizes are publicly available under the Jamba Open Model License and we release ExpertsInt8 as open source.",Israel,,,,,,Unreleased,Commercial use allowed up to $50M USD annual revenue.,,,
Gen-3 Alpha Turbo,"Video,Vision","Video generation,Image-to-video",,API access,"https://runwayml.com/news/introducing-the-runway-api

https://help.runwayml.com/hc/en-us/articles/30266515017875-Creating-with-Text-Image-to-Video-on-Gen-3-Alpha-and-Turbo",,Gen-3 Alpha Turbo is a faster model in the Gen-3 Alpha family that generates at a lower cost. The Turbo model is available on all plan levels and requires an input image.,2024-08-15,Runway,,,,,Unspecified unreleased,,,,,,,Unknown,"Gen-3 Alpha Turbo Image to Video is now available and can generate 7x faster for half the price of the original Gen-3 Alpha. All while still matching performance across many use cases. Turbo is available for all plans, including trial for free users.

More improvements to the model, control mechanisms and possibilities for real-time interactivity to come.",United States of America,,,,,,Unreleased,,,,
Grok-2,"Language,Vision,Multimodal","Chat,Language modeling/generation,Question answering,Code generation,Visual question answering",,API access,https://x.ai/blog/grok-2,,Grok-2 Beta Release,2024-08-13,xAI,,,2.96e+25,Estimate based on xAI statements comparing Grok-2 compute to GPT-4 and Grok-3. Full estimate here: https://docs.google.com/document/d/1C_dABuZrAqYE_ui4_GZ4bRLtq3TBjIGoBSktaPElhEU/edit?usp=sharing,Unspecified unreleased,"Knowledge cutoff date is August 2024, according to https://llm-stats.com/models/grok-2.",,,,,NVIDIA H100 SXM5 80GB,Confident,Grok-2 is our frontier language model with state-of-the-art reasoning capabilities. This release includes two members of the Grok family: Grok-2 and Grok-2 mini. Both models are now being released to Grok users on the ð• platform.,United States of America,,,,,,Unreleased,,,,
Falcon Mamba,Language,"Language modeling/generation,Question answering,Code generation,Quantitative reasoning","Jingwei Zuo, Maksim Velikanov, Dhia Eddine Rhaiem,  Ilyas Chahed,  Younes Belkada, Guillaume Kunsch, Hakim Hacid",Open weights (restricted use),https://huggingface.co/tiiuae/falcon-mamba-7b,,Falcon Mamba: The First Competitive Attention-free 7B Language Model,2024-08-12,Technology Innovation Institute,7000000000.0,,3.9391101e+23,989400000000000 FLOP / GPU / sec [bf16 assumed] * 1440 hours [see training time notes] * 3600 sec / hour * 256 GPUs * 0.3 [assumed utilization] = 3.9391101e+23 FLOP,RefinedWeb,,,"""Falcon Mamba was trained with ~ 5500GT of data, mainly composed of RefinedWeb data with addition of high-quality technical data and code data from public sources.""

""Batch size	2048""
""Sequence length	8192	During the last training stages""",1440.0,"""Falcon-Mamba-7B was trained on 256 H100 80GB GPUs for the majority of the training, using a 3D parallelism strategy (TP=1, PP=1, DP=256) combined with ZeRO.""

""The model training took roughly two months.""

30*2*24=1440 hours",NVIDIA H100 SXM5 80GB,Confident,Falcon Mamba is a new model by Technology Innovation Institute (TII) in Abu Dhabi released under the TII Falcon Mamba 7B License 1.0. The model is open access and available within the Hugging Face ecosystem here for anyone to use for their research or application purposes.,United Arab Emirates,,,,256.0,,Unreleased,"""The model is open access and available within the Hugging Face ecosystem here for anyone to use for their research or application purposes.""
TII Falcon-Mamba License 2.0
https://huggingface.co/tiiuae/falcon-mamba-7b",,,
EXAONE 3.0,Language,"Language modeling/generation,Code generation,Question answering","LG AI Research: Soyoung An, Kyunghoon Bae, Eunbi Choi, Stanley Jungkyu Choi, Yemuk Choi, Seokhee Hong, Yeonjung Hong, Junwon Hwang, Hyojin Jeon, Gerrard Jeongwon Jo, Hyunjik Jo, Jiyeon Jung, Yountae Jung, Euisoon Kim, Hyosang Kim, Joonkee Kim, Seonghwan Kim, Soyeon Kim, Sunkyoung Kim, Yireun Kim, Youchul Kim, Edward Hwayoung Lee, Haeju Lee, Honglak Lee, Jinsik Lee, Kyungmin Lee, Moontae Lee, Seungjun Lee, Woohyung Lim, Sangha Park, Sooyoun Park, Yongmin Park, Boseong Seo, Sihoon Yang, Heuiyeen Yeen, Kyungjae Yoo, Hyeongu Yun",Open weights (non-commercial),https://arxiv.org/abs/2408.03541,,EXAONE 3.0 7.8B Instruction Tuned Language Model,2024-08-07,LG AI Research,7820000000.0,7.8B,4.0000000000000003e+23,"6ND = 6*7.8B parameters *8T tokens  = 3.744e+23 FLOP

""EXAONE language models were trained using Google Cloud Platform and a cluster powered by NVIDIA H100 GPUs and NVIDIA NeMo Framework. Then, they were optimized by NVIDIA TensorRT-LLM. The total amount of computation used for model training was about 4 Ã— 1023 FLOPS""",Unspecified unreleased,"8T training data (tokens)

the token per word ration is 2.46 and given in the paper",8000000000000.0,,,,NVIDIA H100 SXM5 80GB,Confident,"We introduce EXAONE 3.0 instruction-tuned language model, the first open model in the family of Large Language Models (LLMs) developed by LG AI Research. Among different model sizes, we publicly release the 7.8B instruction-tuned model to promote open research and innovations. Through extensive evaluations across a wide range of public and in-house benchmarks, EXAONE 3.0 demonstrates highly competitive real-world performance with instruction-following capability against other state-of-the-art open models of similar size. Our comparative analysis shows that EXAONE 3.0 excels particularly in Korean, while achieving compelling performance across general tasks and complex reasoning. With its strong real-world effectiveness and bilingual proficiency, we hope that EXAONE keeps contributing to advancements in Expert AI. Our EXAONE 3.0 instruction-tuned model is available at this https URL",Korea (Republic of),,,,,,Unreleased,"https://huggingface.co/LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct

Exaone license (allows only non-commercial usage)",,,
Jais-70B ,Language,"Language modeling/generation,Question answering",,Open weights (restricted use),https://www.g42.ai/resources/news/g42-launches-jais-70b-and-20-other-ai-models-champion-arabic-natural-language-processing,,G42 launches JAIS 70B and 20 other AI Models to Champion Arabic Natural Language Processing,2024-08-05,"G42,Inception G42",70000000000.0,70B,9.654e+23,8.1e+23 FLOP (base model training compute) + 1.554e+23 FLOP (finetune compute) = 9.654e+23 FLOP,Unspecified unreleased,,370000000000.0,"""JAIS 70B was developed using continuous training, a process of fine-tuning a pre-trained model, on 370 billion tokens of which 330 billion were Arabic tokens, the largest Arabic dataset ever used to train an open-source foundational model.""

""Batch size	960
Context Length	4096
Steps	94316""",,,Cerebras CS-2,Confident,"The latest JAIS large language model (LLM), JAIS 70B was released today by Inception, a G42 company specializing in the development of advanced AI models and applications, all provided as a service. A 70 billion parameter model, JAIS 70B is built for developers of Arabic-based natural-language processing (NLP) solutions and promises to accelerate the integration of Generative AI services across various industries, enhancing capabilities in areas such as customer service, content creation, and data analysis.  

JAIS 70B delivers Arabic-English bilingual capabilities at an unprecedented size and scale for the open-source community. As a 70 billion parameter model, it has increased ability to handle complicated and nuanced tasks, as well as better capability to process complex datasets. JAIS 70B was developed using continuous training, a process of fine-tuning a pre-trained model, on 370 billion tokens of which 330 billion were Arabic tokens, the largest Arabic dataset ever used to train an open-source foundational model.","United Arab Emirates,United Arab Emirates",Llama 2-70B,1.554e+23,6 FLOP / parameter / token * 70 * 10^9 parameters * 370 * 10^9 tokens = 1.554e+23 FLOP,64.0,,Unreleased,"apache 2.0 (I believe Llama license restrictions still apply)
https://huggingface.co/inceptionai/jais-adapted-70b",,,
Midjourney V6.1,Image generation,Image generation,,Hosted access (no API),https://updates.midjourney.com/version-6-1/,,,2024-07-30,Midjourney,,,,,Unspecified unreleased,,,,,,,Unknown,"More coherent images (arms, legs, hands, bodies, plants, animals, etc)
Much better image quality (reduced pixel artifacts, enhanced textures, skin, etc)
More precise, detailed, and correct small image features (eyes, small faces, far away hands, etc)
New 2x upscalers with much better image / texture quality
Roughly 25% faster for standard image jobs
Improved text accuracy (when drawing words via â€œquotationsâ€ in prompts)
A new personalization model with improved nuance, surprise, and accuracy
Personalization code versioning (use any personalization code from old jobs to use the personalization model and data from that job)
A new --q 2 mode which takes 25% longer to (sometimes) add more texture at the cost of reduced image coherence
Things should look â€œgenerally more beautifulâ€ across the board",United States of America,,,,,,Unreleased,,,,
AFM-on-device,Language,Language modeling/generation,"Andy Narayanan, Aonan Zhang, Bowen Zhang, Chen Chen, Chong Wang, Chung-Cheng Chiu, David Qiu, Deepak Gopinath, Dian Ang Yap, Dong Yin, Feng Nan, Floris Weers, Guoli Yin, Haoshuo Huang, Jianyu Wang, Jiarui Lu, John Peebles, Ke Ye, Mark Lee, Nan Du, Qibin Chen, Quentin Keunebroek, Ruoming Pang, Sam Wiseman, Syd Evans, Tao Lei, Tom Gunter, Vivek Rathod, Xiang Kong, Xianzhi Du, Yanghao Li, Yongqiang Wang, Yuan Gao, Zaid Ahmed, Zhaoyang Xu, Zhiyun Lu, Zirui Wang, Al Rashid, Albin Madappally Jose, Alec Doane, Alfredo Bencomo, Allison Vanderby, Andrew Hansen, Ankur Jain, Anupama Mann Anupama, Areeba
Kamal, Bugu Wu, Carolina Brum, Charlie Maalouf, Chinguun Erdenebileg,
Chris Dulhanty, Dominik Moritz, Doug Kang, Eduardo Jimenez, Evan Ladd,
Fangping Shi, Felix Bai, Frank Chu, Fred Hohman, Hadas Kotek, Hannah
Gillis Coleman, Jane Li, Jeffrey Bigham, Jeffery Cao, Jeff Lai, Jessica Cheung, Jiulong Shan, Joe Zhou, John Li, Jun Qin, Karanjeet Singh, Karla Vega, Ke Ye, Kelvin Zou, Laura Heckman, Lauren Gardiner, Margit Bowler, Mark Lee, Maria Cordell, Meng Cao, Nicole Hay, Nilesh Shahdadpuri, Otto Godwin, Pranay Dighe, Pushyami Rachapudi, Ramsey Tantawi, Roman Frigg, Sam Davarnia, Sanskruti Shah, Saptarshi Guha, Sasha Sirovica, Shen Ma, Shuang Ma, Simon Wang, Sulgi Kim, Suma Jayaram, Vaishaal Shankar, Varsha Paidi, Vivek Kumar, Xiang Kong, Xin Wang, Xin Zheng, Walker Cheng, Yael Shrager, Yang Ye, Yasu Tanaka, Yihao Guo, Yunsong Meng, Zhao Tang Luo, Zhi Ouyang, Zhiyun Lu, Alp Aygar, Alvin Wan, Andrew Walkingshaw, Andy Narayanan, Antonie Lin, Arsalan Farooq, Brent Ramerth, Chong Wang, Colorado Reed, Chris Bartels, Chris Chaney, David Riazati, Eric Liang Yang, Erin Feldman, Gabriel Hochstrasser, Guillaume Seguin, Guoli Yin, Irina Belousova, Jianyu Wang, Joris Pelemans, Karen Yang, Keivan Alizadeh Vahid, Liangliang Cao, Mahyar Najibi , Marco Zuliani, Max Horton, Minsik Cho, Nikhil Bhendawade, Patrick Dong, Piotr Maj, Pulkit Agrawal, Qi Shan, Qibin Chen, Qichen Fu, Regan Poston, Sam Xu, Shuangning Liu, Sushma Rao, Tashweena Heeramun, Thomas Merth, Uday Rayala, Victor Cui, Vivek Rangarajan Sridhar, Vivek Rathod, Wencong Zhang, Wenqi Zhang, Wentao Wu, Xiang Kong, Xingyu Zhou, Xinwen Liu, Yang Zhao, Yin Xia, Zhile Ren, Zhongzheng Ren",Hosted access (no API),https://machinelearning.apple.com/research/apple-intelligence-foundation-language-models,,Apple Intelligence Foundation Language Models,2024-07-29,Apple,2730000000.0,"Table 1, sum of non-embedding and embedding parameters",4.5126e+23,"Model was initialized from a pruned version of a 6.4B parameter model trained using the same recipe as AFM-server. Assuming ""same recipe"" involves training for the full 6.3T tokens, this implies 6 * 6.3T * 6.4B = 2.42e23 FLOP. 

The pruning masks are learned by training over 188B tokens, which suggests 6 * 188B * 6.4B = 7.22e21 FLOPs.

Pretraining is then run over 6.3T tokens; however, labels are a convex combination of true labels and the predicted labels from the unpruned 6.4B model. Since this involves running the 6.3T tokens forward through both the 6.4B and the 2.73B model, but only calculating gradients for the smaller model, FLOPs here are equal to (6 * 6.3T * 2.73B) + (2 * 6.3T * 6.4B) = 1.84e23. 

Finally, there is a 1T ""continuation"" pretraining stage without distillation loss, for 6 * 1T * 2.73B = 1.64e22 FLOP, and a 100B context-lengthening stage for another 6 * 100B * 2.73B = 1.64e21 FLOP

In total: 2.42e23 + 7.22e21 + 1.84e23 + 1.64e22 + 1.64e21 = 4.51e23 FLOP",,"188B of tokens are used to train a pruning mask to reduce a 6.4B model to the 2.73B used for AFM-on-device. Main pre-training data is 6.3T tokens of web text, code, and math, plus another 1T in the second pre-training stage and 100B in the third. See section 3.1 for details. Post-training details do not give details on dataset size.",7588000000000.0,"Not explicitly mentioned, but I assume the 7.588T tokens do not involve multiple epochs.",,"Trained on ""one slice of 2048 TPUv5p chips""; wall-time not given.",Google TPU v5p,Confident,"We present foundation language models developed to power Apple Intelligence features, including a âˆ¼3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute [Apple, 2024b]. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",United States of America,,,,2048.0,0.52,Unreleased,,,,
AFM-server,Language,Language modeling/generation,"Andy Narayanan, Aonan Zhang, Bowen Zhang, Chen Chen, Chong Wang, Chung-Cheng Chiu, David Qiu, Deepak Gopinath, Dian Ang Yap, Dong Yin, Feng Nan, Floris Weers, Guoli Yin, Haoshuo Huang, Jianyu Wang, Jiarui Lu, John Peebles, Ke Ye, Mark Lee, Nan Du, Qibin Chen, Quentin Keunebroek, Ruoming Pang, Sam Wiseman, Syd Evans, Tao Lei, Tom Gunter, Vivek Rathod, Xiang Kong, Xianzhi Du, Yanghao Li, Yongqiang Wang, Yuan Gao, Zaid Ahmed, Zhaoyang Xu, Zhiyun Lu, Zirui Wang, Al Rashid, Albin Madappally Jose, Alec Doane, Alfredo Bencomo, Allison Vanderby, Andrew Hansen, Ankur Jain, Anupama Mann Anupama, Areeba
Kamal, Bugu Wu, Carolina Brum, Charlie Maalouf, Chinguun Erdenebileg,
Chris Dulhanty, Dominik Moritz, Doug Kang, Eduardo Jimenez, Evan Ladd,
Fangping Shi, Felix Bai, Frank Chu, Fred Hohman, Hadas Kotek, Hannah
Gillis Coleman, Jane Li, Jeffrey Bigham, Jeffery Cao, Jeff Lai, Jessica Cheung, Jiulong Shan, Joe Zhou, John Li, Jun Qin, Karanjeet Singh, Karla Vega, Ke Ye, Kelvin Zou, Laura Heckman, Lauren Gardiner, Margit Bowler, Mark Lee, Maria Cordell, Meng Cao, Nicole Hay, Nilesh Shahdadpuri, Otto Godwin, Pranay Dighe, Pushyami Rachapudi, Ramsey Tantawi, Roman Frigg, Sam Davarnia, Sanskruti Shah, Saptarshi Guha, Sasha Sirovica, Shen Ma, Shuang Ma, Simon Wang, Sulgi Kim, Suma Jayaram, Vaishaal Shankar, Varsha Paidi, Vivek Kumar, Xiang Kong, Xin Wang, Xin Zheng, Walker Cheng, Yael Shrager, Yang Ye, Yasu Tanaka, Yihao Guo, Yunsong Meng, Zhao Tang Luo, Zhi Ouyang, Zhiyun Lu, Alp Aygar, Alvin Wan, Andrew Walkingshaw, Andy Narayanan, Antonie Lin, Arsalan Farooq, Brent Ramerth, Chong Wang, Colorado Reed, Chris Bartels, Chris Chaney, David Riazati, Eric Liang Yang, Erin Feldman, Gabriel Hochstrasser, Guillaume Seguin, Guoli Yin, Irina Belousova, Jianyu Wang, Joris Pelemans, Karen Yang, Keivan Alizadeh Vahid, Liangliang Cao, Mahyar Najibi , Marco Zuliani, Max Horton, Minsik Cho, Nikhil Bhendawade, Patrick Dong, Piotr Maj, Pulkit Agrawal, Qi Shan, Qibin Chen, Qichen Fu, Regan Poston, Sam Xu, Shuangning Liu, Sushma Rao, Tashweena Heeramun, Thomas Merth, Uday Rayala, Victor Cui, Vivek Rangarajan Sridhar, Vivek Rathod, Wencong Zhang, Wenqi Zhang, Wentao Wu, Xiang Kong, Xingyu Zhou, Xinwen Liu, Yang Zhao, Yin Xia, Zhile Ren, Zhongzheng Ren",Hosted access (no API),https://machinelearning.apple.com/research/apple-intelligence-foundation-language-models,,Apple Intelligence Foundation Language Models,2024-07-29,Apple,,,4.3e+24,"""The AFM base models are dense decoder-only models that build on the
Transformer architecture""

""We train AFM-server from scratch for 6.3T tokens on 8192
TPUv4 chips, using a sequence length of 4096 and a batch-size of 4096 sequences.""

""For both models we perform continued pre-training at a sequence length of
8192, with another 1T tokens from a mixture that upweights math and code,
and down-weights the bulk web-crawl.""

""The sustained model-flop-utilization (MFU) for this training run was approximately 52%.""

Parameter count is not specified other than it being ""larger"" than 3 billion.

Counting FLOP: Chinchilla scaling laws would suggest 7.3T / 20 = 365B parameters. 

365B parameters * 7.3T tokens * 6 ~= 1.6e25 FLOP.

However, the attention to inference optimization in the technical report suggests a smaller size, even for this ""server"" model. One point of reference is Llama 3 70B being overtrained by a factor of 10. If this is true of AFM-server, the parameter count would be ~37B and training compute would be 1.6e24 FLOP.

GPU-time: assume a wall-clock training time of 30 days based on the current trend value for notable models.

8192 chips * 275e12 FLOP/s per chip * 0.52 utilization * 30 * 24 * 60 * 60 s ~= 3.0e24 FLOP

The geometric mean of these three estimates is 4.3e24 FLOP.",,"6.3T tokens of web text, code, and math, plus another 1T in the second stage and 100B in the third. See section 3.1 for details.",7400000000000.0,"Not explicitly mentioned, but I assume the 7.4T tokens do not involve multiple epochs.",,,Google TPU v4,Likely,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",United States of America,,,,8192.0,0.52,Unreleased,,,,
Mistral Large 2,Language,"Language modeling/generation,Translation,Code generation","Albert Jiang, Alexandre Sablayrolles, Alexis Tacnet, Alok Kothari, Antoine Roux, Arthur Mensch, Audrey Herblin-Stoop, Augustin Garreau, Austin Birky, Bam4d, Baptiste Bout, Baudouin de Monicault, Blanche Savary, Carole Rambaud, Caroline Feldman, Devendra Singh Chaplot, Diego de las Casas, Diogo Costa, Eleonore Arcelin, Emma Bou Hanna, Etienne Metzger, Gaspard Blanchet, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Harizo Rajaona, Henri Roussez, Hichem Sattouf, Ian Mack, Jean-Malo Delignon, Jessica Chudnovsky, Justus Murke, Kartik Khandelwal, Lawrence Stewart, Louis Martin, Louis Ternon, Lucile Saulnier, LÃ©lio Renard Lavaud, Margaret Jennings, Marie Pellat, Marie Torelli, Marie-Anne Lachaux, Marjorie Janiewicz, MickaÃ«l Seznec, Nicolas Schuhl, Niklas Muhs, Olivier de Garrigues, Patrick von Platen, Paul Jacob, Pauline Buche, Pavan Kumar Reddy, Perry Savas, Pierre Stock, Romain Sauvestre, Sagar Vaze, Sandeep Subramanian, Saurabh Garg, Sophia Yang, Szymon Antoniak, Teven Le Scao, Thibault Schueller, Thibaut Lavril, Thomas Wang, ThÃ©ophile Gervet, TimothÃ©e Lacroix, Valera Nemychnikova, Wendy Shang, William El Sayed, William Marshall",Open weights (non-commercial),https://mistral.ai/news/mistral-large-2407/,,"Top-tier reasoning for high-complexity tasks, for your most sophisticated needs.",2024-07-24,Mistral AI,123000000000.0,,2.13e+25,"Details are sparse, but we can hazard a guess based on evidence about the training cluster they may have used, the scale up in compute they likely would have used relative to Mistral Large 1, and from the model's MMLU score. Extended reasoning given here: https://docs.google.com/document/d/1I2ZWBLFMpRZYcdMMUfKAGZFJrOJpduNDS9ZeVFIHnd8/edit?usp=sharing",Unspecified unreleased,,,,,,,Likely,"Today, we are announcing Mistral Large 2, the new generation of our flagship model. Compared to its predecessor, Mistral Large 2 is significantly more capable in code generation, mathematics, and reasoning. It also provides a much stronger multilingual support, and advanced function calling capabilities.",France,,,,,,Unreleased,"""We are releasing Mistral Large 2 under the Mistral Research License, that allows usage and modification for research and non-commercial usages. For commercial usage of Mistral Large 2 requiring self-deployment, a Mistral Commercial License must be acquired by contacting us.""",,,
Llama 3.1-405B,Language,"Language modeling/generation,Question answering,Code generation,Mathematical reasoning","Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,
Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie
Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen
Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux,
Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang
Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle
Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino,
Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip
Radenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, Gregoire
Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov,
Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert,
Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong,
Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe
Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala,
Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer,
Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence
Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat,
Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas,
Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar
Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji,
Olivier Duchenne, Onur Ã‡elebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng,
Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong,
Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta
Raileanu, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor,
Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun
Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan,
Shruti Bhosale, Shun Zhang, Simon Vandenhende, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin
Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas
Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta,
Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vladan Petrovic, Weiwei
Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaoqing Ellen Tan,
Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen
Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, and Zoe
Papakipos.
(core contributors)",Open weights (restricted use),https://ai.meta.com/research/publications/the-llama-3-herd-of-models/,,The Llama 3 Herd of Models,2024-07-23,Meta AI,405000000000.0,405B,3.8e+25,"Stated in paper.

Also, 6 * 405B * 15.6T training tokens = 3.8e25",Llama 3 dataset,"Knowledge cutoff date is December 2023, according to https://huggingface.co/meta-llama/Llama-3.1-405B.",15600000000000.0,15.6T tokens,2142.0,"Trained on 30.84M GPU hours (https://huggingface.co/blog/llama31) and used ""up to 16K H100 GPU[s]"" so training took at least
30.84M / 16k = 1927.5 hours or ~80 days. 

Section 3.3.4 gives reliability details over a 54 day period during training, for which they had ""higher than 90% effective training time""
1927.5 / 0.9 = 2142 hours

Probably, full training time is somewhat longer, since it sounds like there were periods where not all 16k H100s were running.",NVIDIA H100 SXM5 80GB,Confident,"Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.",United States of America,,,,16384.0,0.4042,Open (restricted use),"Llama 3.1 model license:

https://huggingface.co/meta-llama/Meta-Llama-3.1-8B/blob/main/LICENSE 

must seek separate license if over 700m monthly users, acceptable use restrictions

training code here: https://github.com/meta-llama/llama-recipes/blob/main/src/llama_recipes/utils/train_utils.py#L70 
",,175966627.18329293,928433368.1802773
Llama 3.1-70B,Language,Language modeling/generation,"Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,
Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie
Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen
Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux,
Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang
Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle
Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino,
Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip
Radenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, Gregoire
Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov,
Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert,
Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong,
Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe
Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala,
Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer,
Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence
Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat,
Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas,
Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar
Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji,
Olivier Duchenne, Onur Ã‡elebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng,
Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong,
Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta
Raileanu, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor,
Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun
Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan,
Shruti Bhosale, Shun Zhang, Simon Vandenhende, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin
Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas
Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta,
Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vladan Petrovic, Weiwei
Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaoqing Ellen Tan,
Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen
Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, and Zoe
Papakipos.
(core contributors)",Open weights (restricted use),https://ai.meta.com/research/publications/the-llama-3-herd-of-models/,,The Llama 3 Herd of Models,2024-07-23,Meta AI,70000000000.0,70B,7.929e+24,"Huggingface page says 3.1-70B used 7.0M H100 hours and trained over 15T tokens. https://huggingface.co/meta-llama/Llama-3.1-70B
The paper also says that 3.1-405B got MFU of between 38-43%; presumably 70B was around the same or a bit higher. I'll assume utilization of 40%

6ND:
6 * 15T * 70B = 6.3e24 FLOPs

Hardware:
7M * 9.9e14 * 3600 * 0.4 = 9.98e24 FLOPs

Geometric mean: sqrt(6.3e24 * 9.98e24) = 7.929e24

Note that Llama 3-70B also said it used 15T tokens, but only 6.4M H100 hours. This suggests 3.1 might have used a bit more than 15T tokens.

Training compute upper bound: 7M H100-hours * 989 TFLOPS * 50% utilization = 1.25e25 FLOP",Llama 3 dataset,"Knowledge cutoff date is December 2023, according to https://huggingface.co/meta-llama/Llama-3.1-405B.",,,,,,Confident,"Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.",United States of America,,,,,,Open (restricted use),"Llama 3.1 license:

https://huggingface.co/meta-llama/Meta-Llama-3.1-8B/blob/main/LICENSE 

must seek separate license if over 700m monthly users, acceptable use restrictions

code here: https://github.com/meta-llama/llama3/tree/main 
",,,
Llama 3.1-8B,Language,Language modeling/generation,"Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux,
Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat,
Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar
Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Olivier Duchenne, Onur Ã‡elebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng,
Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta
Raileanu, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun
Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin
Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta,
Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaoqing Ellen Tan, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, and Zoe Papakipos.
(core contributors)",Open weights (restricted use),https://ai.meta.com/research/publications/the-llama-3-herd-of-models/,,The Llama 3 Herd of Models,2024-07-23,Meta AI,8000000000.0,8B,1.224e+24,"Huggingface page says 3.1-8B used 1.46M H100 hours and trained over 15T tokens. https://huggingface.co/meta-llama/Llama-3.1-70B
The paper also says that 3.1-405B got MFU of between 38-43%; presumably 8B was around the same or a bit higher. I'll assume utilization of 40%

6ND:
6 * 15T * 8B = 7.2e23 FLOPs

Hardware:
1.46M * 9.9e14 * 3600 * 0.4 = 2.08e24 FLOPs

Geometric mean: sqrt(7.2e23 * 2.08e24) = 1.224e24

Note that Llama 3-8B also said it used 15T tokens, but only 1.3M H100 hours. This suggests 3.1 might have used a bit more than 15T tokens.",Llama 3 dataset,"Knowledge cutoff date is December 2023, according to https://huggingface.co/meta-llama/Llama-3.1-405B.",,,,,,Likely,"Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.",United States of America,,,,,,Open (restricted use),"Llama 3.1 license:

https://huggingface.co/meta-llama/Meta-Llama-3.1-8B/blob/main/LICENSE 

must seek separate license if over 700m monthly users, acceptable use restrictions

code here: https://github.com/meta-llama/llama3/tree/main 
",,,
DCLM 7B,Language,"Language modeling/generation,Question answering,Quantitative reasoning",,Open weights (restricted use),https://huggingface.co/apple/DCLM-7B,,Model Card for DCLM-Baseline-7B,2024-07-20,Apple,7000000000.0,7B,1.05e+23,6 FLOP / parameter / token * 7 * 10^9 parameters * 2.5 * 10^12 tokens = 1.05e+23 FLOP,"StarCoder,Proofpile 2,DCLM-baseline","""To ensure our trained model is broadly useful, including for math and coding tasks, we combine our 3.8T DCLM-BASELINE with the StarCoder and ProofPile2 data to arrive at a 4.1T token dataset.""",2500000000000.0,Total Training Tokens: 2.5T,,,NVIDIA H100 SXM5 80GB,Confident,"DCLM-Baseline-7B is a 7 billion parameter language model trained on the DCLM-Baseline dataset, which was curated as part of the DataComp for Language Models (DCLM) benchmark. This model is designed to showcase the effectiveness of systematic data curation techniques for improving language model performance.",United States of America,,,,,,Unreleased,"Apple Sample Code license (no patent rights, copyright-only)
https://huggingface.co/apple/DCLM-7B",,,
Eleven Turbo v2.5,Speech,"Text-to-speech (TTS),Speech synthesis",,API access,https://elevenlabs.io/blog/introducing-turbo-v2-5,,"High quality, low latency text to speech in 32 languages",2024-07-19,ElevenLabs,,,,,Unspecified unreleased,,,,,,,Unknown,"Eleven Turbo v2.5 is our high-quality, low-latency model with a good balance of quality and speed.

This model is an ideal choice for all scenarios where youâ€™d use Flash v2.5, but where youâ€™re willing to trade off latency for higher quality voice generation.",United States of America,,,,,,Unreleased,,,,
GPT-4o mini,"Language,Multimodal,Vision","Chat,Language modeling/generation,Code generation,Visual question answering","Pre-training leads
Aidan Clark, Alex Paino, Jacob Menick

Post-training leads
Liam Fedus, Luke Metz

Architecture leads
Clemens Winter, Lia Guy

Optimization leads
Sam Schoenholz, Daniel Levy

Long-context lead
Nitish Keskar

Pre-training Data leads
Alex Carney, Alex Paino, Ian Sohl, Qiming Yuan

Tokenizer lead
Reimar Leike

Human data leads
Arka Dhar, Brydon Eastman, Mia Glaese

Eval lead
Ben Sokolowsky

Data flywheel lead
Andrew Kondrich

Inference lead
Felipe Petroski Such

Inference Productionization lead
Henrique Ponde de Oliveira Pinto

Post-training infrastructure leads
Jiayi Weng, Randall Lin, Youlong Cheng

Pre-training organization lead
Nick Ryder

Pre-training program lead
Lauren Itow

Post-training organization leads
Barret Zoph, John Schulman

Post-training program lead
Mianna Chen

Core contributors
Adam Lerer, Adam P. Goucher, Adam Perelman, Akila Welihinda, Alec Radford, Alex Borzunov, Alex Carney, Alex Chow, Alex Paino, Alex Renzin, Alex Tachard Passos, Alexi Christakis, Ali Kamali, Allison Moyer, Allison Tam, Amadou Crookes, Amin Tootoonchian, Ananya Kumar, Andrej Karpathy, Andrey Mishchenko, Andrew Cann, Andrew Kondrich, Andrew Tulloch, Angela Jiang, Antoine Pelisse, Antonia Woodford, Anuj Gosalia, Avi Nayak, Avital Oliver, Behrooz Ghorbani, Ben Leimberger, Ben Wang, Beth Hoover, Blake Samic, Brian Guarraci, Brydon Eastman, Camillo Lugaresi, Chak Li, Charlotte Barette, Chelsea Voss, Chen Ding, Chong Zhang, Chris Beaumont, Chris Hallacy, Chris Koch, Christian Gibson, Christine Choi, Christopher Hesse, Colin Wei, Daniel Kappler, Daniel Levin, Daniel Levy, David Farhi, David Mely, David Sasaki, Dimitris Tsipras, Doug Li, Duc Phong Nguyen, Duncan Findlay, Edmund Wong, Ehsan Asdar, Elizabeth Proehl, Elizabeth Yang, Eric Peterson, Eric Sigler, Eugene Brevdo, Farzad Khorasani, Francis Zhang, Gene Oden, Geoff Salmon, Hadi Salman, Haiming Bao, Heather Schmidt, Hongyu Ren, Hyung Won Chung, Ian Kivlichan, Ian O'Connell, Ian Osband, Ibrahim Okuyucu, Ilya Kostrikov, Ingmar Kanitscheider, Jacob Coxon, James Crooks, James Lennon, Jane Park, Jason Teplitz, Jason Wei, Jason Wolfe, Jay Chen, Jeff Harris, Jiayi Weng, Jie Tang, Joanne Jang, Jonathan Ward, Jonathan McKay, Jong Wook Kim, Josh Gross, Josh Kaplan, Joy Jiao, Joyce Lee, Juntang Zhuang, Kai Fricke, Kavin Karthik, Kenny Hsu, Kiel Howe, Kyle Luther, Larry Kai, Lauren Itow, Leo Chen, Lia Guy, Lien Mamitsuka, Lilian Weng, Long Ouyang, Louis Feuvrier, Lukas Kondraciuk, Lukasz Kaiser, Lyric Doshi, Mada Aflak, Maddie Simens, Madeleine Thompson, Marat Dukhan, Marvin Zhang, Mateusz Litwin, Matthew Zeng, Max Johnson, Mayank Gupta, Mia Glaese, Michael Janner, Michael Petrov, Michael Wu, Michelle Fradin, Michelle Pokrass, Miguel Oom Temudo de Castro, Mikhail Pavlov, Minal Khan, Mo Bavarian, Murat Yesildal, Natalia Gimelshein, Natalie Staudacher, Nick Stathas, Nik Tezak, Nithanth Kudige, Noel Bundick, Ofir Nachum, Oleg Boiko, Oleg Murk, Olivier Godement, Owen Campbell-Moore, Philip Pronin, Philippe Tillet, Rachel Lim, Rajan Troll, Randall Lin, Rapha gontijo lopes, Raul Puri, Reah Miyara, Reimar Leike, Renaud Gaubert, Reza Zamani, Rob Honsby, Rohit Ramchandani, Rory Carmichael, Ruslan Nigmatullin, Ryan Cheu, Sara Culver, Scott Gray, Sean Grove, Sean Metzger, Shantanu Jain, Shengjia Zhao, Sherwin Wu, Shuaiqi (Tony) Xia, Sonia Phene, Spencer Papay, Steve Coffey, Steve Lee, Steve Lee, Stewart Hall, Suchir Balaji, Tal Broda, Tal Stramer, Tarun Gogineni, Ted Sanders, Thomas Cunninghman, Thomas Dimson, Thomas Raoux, Tianhao Zheng, Christina Kim, Todd Underwood, Tristan Heywood, Valerie Qi, Vinnie Monaco, Vlad Fomenko, Weiyi Zheng, Wenda Zhou, Wojciech Zaremba, Yash Patil, Yilei, Qian, Yongjik Kim, Youlong Cheng, Yuchen He, Yuchen Zhang, Yujia Jin, Yunxing Dai, Yury Malkov",API access,https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/,,GPT-4o mini: advancing cost-efficient intelligence,2024-07-18,OpenAI,,,,"Training compute estimated to be 7.36001e+24 from benchmark scores. https://colab.research.google.com/drive/1r3pUMhB7Kh0Gls9eG-v_XefWrye9fVQR?usp=sharing

90% CI [3.23e+24, 2.05e+25]",Unspecified unreleased,"This has a knowledge cutoff date of October 2023 - which I assume means October 1, 2023 - according to https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models?tabs=global-standard%2Cstandard-chat-completions#gpt-4o-and-gpt-4-turbo.",,,,,,Speculative,"OpenAI is committed to making intelligence as broadly accessible as possible. Today, we're announcing GPT-4o mini, our most cost-efficient small model. We expect GPT-4o mini will significantly expand the range of applications built with AI by making intelligence much more affordable. GPT-4o mini scores 82% on MMLU and currently outperforms GPT-41 on chat preferences in LMSYS leaderboard(opens in a new window). It is priced at 15 cents per million input tokens and 60 cents per million output tokens, an order of magnitude more affordable than previous frontier models and more than 60% cheaper than GPT-3.5 Turbo.",United States of America,,,,,,Unreleased,,,,
MAP-Neo,Language,"Language modeling/generation,Question answering,Quantitative reasoning,Translation,Code generation","Ge Zhang, Scott Qu, Jiaheng Liu, Chenchen Zhang, Chenghua Lin, Chou Leuang Yu, Danny Pan, Esther Cheng, Jie Liu, Qunshu Lin, Raven Yuan, Tuney Zheng, Wei Pang, Xinrun Du, Yiming Liang, Yinghao Ma, Yizhi Li, Ziyang Ma, Bill Lin, Emmanouil Benetos, Huan Yang, Junting Zhou, Kaijing Ma, Minghao Liu, Morry Niu, Noah Wang, Quehry Que, Ruibo Liu, Sine Liu, Shawn Guo, Soren Gao, Wangchunshu Zhou, Xinyue Zhang, Yizhi Zhou, Yubo Wang, Yuelin Bai, Yuhan Zhang, Yuxiang Zhang, Zenith Wang, Zhenzhu Yang, Zijian Zhao, Jiajun Zhang, Wanli Ouyang, Wenhao Huang, Wenhu Chen",Open weights (unrestricted),https://arxiv.org/abs/2405.19327,,MAP-Neo: Highly Capable and Transparent Bilingual Large Language Model Series,2024-07-10,"University of Waterloo,01.AI,Wuhan University",7000000000.0,7B,1.89e+23,6 FLOP / parameter / token * 7 * 10^9 parameters * 4.5 * 10^12 tokens = 1.89e+23 FLOP,Matrix,"""we introduce Matrix, a bilingual pre-training corpus of 4.5T tokens. Upon its release, Matrix could be the largest transparent LLM pre-training corpus to our best knowledge""",4500000000000.0,4.5T high-quality tokens,,,NVIDIA H800 SXM5,Confident,"Large Language Models (LLMs) have made great strides in recent years to achieve unprecedented performance across different tasks. However, due to commercial interest, the most competitive models like GPT, Gemini, and Claude have been gated behind proprietary interfaces without disclosing the training details. Recently, many institutions have open-sourced several strong LLMs like LLaMA-3, comparable to existing closed-source LLMs. However, only the model's weights are provided with most details (e.g., intermediate checkpoints, pre-training corpus, and training code, etc.) being undisclosed. To improve the transparency of LLMs, the research community has formed to open-source truly open LLMs (e.g., Pythia, Amber, OLMo), where more details (e.g., pre-training corpus and training code) are being provided. These models have greatly advanced the scientific study of these large models including their strengths, weaknesses, biases and risks. However, we observe that the existing truly open LLMs on reasoning, knowledge, and coding tasks are still inferior to existing state-of-the-art LLMs with similar model sizes. To this end, we open-source MAP-Neo, a highly capable and transparent bilingual language model with 7B parameters trained from scratch on 4.5T high-quality tokens. Our MAP-Neo is the first fully open-sourced bilingual LLM with comparable performance compared to existing state-of-the-art LLMs. Moreover, we open-source all details to reproduce our MAP-Neo, where the cleaned pre-training corpus, data cleaning pipeline, checkpoints, and well-optimized training/evaluation framework are provided. Finally, we hope our MAP-Neo will enhance and strengthen the open research community and inspire more innovations and creativities to facilitate the further improvements of LLMs.","Canada,China,China",,,,512.0,,Open source,"Apache 2.0
https://huggingface.co/m-a-p/neo_7b

MIT license for code
https://github.com/multimodal-art-projection/MAP-NEO",,,
Ernie 4.0 Turbo,"Multimodal,Language,Vision","Vision-language generation,Language modeling/generation,Question answering,Chat,Visual question answering",Baidu,API access,https://www.reuters.com/technology/artificial-intelligence/baidu-launches-upgraded-ai-model-says-user-base-hits-300-mln-2024-06-28/,,"Baidu launches upgraded AI model, says Ernie Bot hits 300 mln users",2024-06-28,Baidu,,,,Unlikely to be >1e25 FLOP given ERNIE 4.5 was <1e25 FLOP.,,,,,,,,Unknown,"BEIJING, June 28 (Reuters) - Chinese search engine giant Baidu (9888.HK), opens new tab on Friday unveiled an upgraded version of its artificial intelligence (AI) model, Ernie 4.0 Turbo, as it seeks to maintain its position in China's competitive AI market.
The launch follows the October 2023 release of Ernie 4, which Baidu claimed rivaled OpenAI's GPT-4 in capabilities.
The new model will be accessible to the public via web and mobile app interfaces, with developers able to integrate the technology through Baidu's Qianfan AI platform, the company's Chief Technology Officer Wang Haifeng said at a corporate event.
Wang said its artificial intelligence chatbot Ernie Bot has reached 300 million users since its launch.
Baidu on Friday also announced an upgrade to its PaddlePaddle AI ecosystem, which it said now supports 14.65 million developers and serves 370,000 businesses and institutions.
OpenAI announced this week plans to block access to its API from China and other countries starting July 9. The decision affects numerous Chinese enterprises that have been using OpenAI's technology.
In response, Baidu, Alibaba (9988.HK), opens new tab and other domestic AI firms have launched initiatives to attract affected users, offering free migration services and incentives.",China,,,,,,Unreleased,,,,
ESM3 (98B),Biology,Protein generation,"Thomas Hayes, Roshan Rao, Halil Akin, Nicholas James Sofroniew, Deniz Oktay, Zeming Lin, Robert Verkuil, Vincent Quy Tran, Jonathan Deaton, Marius Wiggert, Rohil Badkundri, Irhum Shafkat, Jun Gong, Alexander Derry, Raul Santiago Molina, Neil Thomas, Yousuf Khan, Chetan Mishra, Carolyn Kim, Liam J Bartie, Patrick D Hsu, Tom Sercu, Salvatore Candido, Alexander Rives",Unreleased,https://www.evolutionaryscale.ai/blog/esm3-release ,,ESM3: Simulating 500 million years of evolution with a language model,2024-06-25,"EvolutionaryScale,University of California (UC) Berkeley",98500000000.0,98.5 billion (Table S1),1.07e+24,"""ESM3 at its largest scale was trained with 1.07Ã—10^24 FLOPs on 2.78 billion proteins and 771 billion unique tokens, and has 98 billion parameters.""

per Table 1, trained 98B model on 1.8T training tokens. 98 billion * 1800 billion * 6 = 1.06e24. Likely some rounding, so will go with developer's reported count.",ESM3 Dataset,,771000000000.0, 771 billion tokens,,,,Confident,"More than three billion years of evolution have
produced an image of biology encoded into the
space of natural proteins. Here we show that language models trained on tokens generated by evolution can act as evolutionary simulators to generate functional proteins that are far away from
known proteins. We present ESM3, a frontier
multimodal generative language model that reasons over the sequence, structure, and function
of proteins. ESM3 can follow complex prompts
combining its modalities and is highly responsive
to biological alignment. We have prompted ESM3
to generate fluorescent proteins with a chain of
thought. Among the generations that we synthesized, we found a bright fluorescent protein at far
distance (58% identity) from known fluorescent
proteins. Similarly distant natural fluorescent proteins are separated by over five hundred million
years of evolution,

(from paper preview: https://evolutionaryscale-public.s3.us-east-2.amazonaws.com/research/esm3.pdf )","United States of America,United States of America",,,,,,Unreleased,only small version released,,,
Gemma 2 9B,Language,"Language modeling/generation,Chat,Code generation,Question answering,Quantitative reasoning","Gemma Team, Google DeepMind",Open weights (restricted use),https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf,,"Gemma 2 offers best-in-class performance, runs at incredible speed across different hardware and easily integrates with other AI tools.",2024-06-24,Google DeepMind,9000000000.0,,4.32e+23,"""For the 9B model, we train on an 8x16x32 configuration of TPUv4, totaling 4096 chips""

6ND = 6 FLOP / token / parameter * 9000000000 parameters * 8000000000000 tokens = 4.32e+23 FLOP",Unspecified unreleased,"Web Documents: A diverse collection of web text ensures the model is exposed to a broad range of linguistic styles, topics, and vocabulary. Primarily English-language content.
Code: Exposing the model to code helps it to learn the syntax and patterns of programming languages, which improves its ability to generate code or understand code-related questions.
Mathematics: Training on mathematical text helps the model learn logical reasoning, symbolic representation, and to address mathematical queries.",8000000000000.0,"""the 9B model on 8 trillion tokens""",,,Google TPU v4,Confident,"Now weâ€™re officially releasing Gemma 2 to researchers and developers globally. Available in both 9 billion (9B) and 27 billion (27B) parameter sizes, Gemma 2 is higher-performing and more efficient at inference than the first generation, with significant safety advancements built in. In fact, at 27B, it offers competitive alternatives to models more than twice its size, delivering the kind of performance that was only possible with proprietary models as recently as December. And thatâ€™s now achievable on a single NVIDIA H100 Tensor Core GPU or TPU host, significantly reducing deployment costs.","United States of America,United Kingdom of Great Britain and Northern Ireland",,,,4096.0,,Unreleased,"Gemma 2 is available under our 
commercially-friendly Gemma license, giving developers and researchers the ability to share and commercialize their innovations.",,,
Gemma 2 27B,Language,"Language modeling/generation,Chat,Code generation,Question answering,Quantitative reasoning","Gemma Team, Google DeepMind",Open weights (restricted use),https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf,,"Gemma 2 offers best-in-class performance, runs at incredible speed across different hardware and easily integrates with other AI tools.",2024-06-24,Google DeepMind,27000000000.0,,2.106e+24,"""For the 27B model, we train on an 8x24x32 configuration of
TPUv5p, totaling 6144 chips""

trained on 13T tokens

6ND = 6*27000000000*13000000000000=2.106e+24",Unspecified unreleased,"Web Documents: A diverse collection of web text ensures the model is exposed to a broad range of linguistic styles, topics, and vocabulary. Primarily English-language content.
Code: Exposing the model to code helps it to learn the syntax and patterns of programming languages, which improves its ability to generate code or understand code-related questions.
Mathematics: Training on mathematical text helps the model learn logical reasoning, symbolic representation, and to address mathematical queries.",13000000000000.0,"""We train Gemma 2 27B on 13 trillion tokens of primarily-English data""",,,Google TPU v5p,Confident,"Now weâ€™re officially releasing Gemma 2 to researchers and developers globally. Available in both 9 billion (9B) and 27 billion (27B) parameter sizes, Gemma 2 is higher-performing and more efficient at inference than the first generation, with significant safety advancements built in. In fact, at 27B, it offers competitive alternatives to models more than twice its size, delivering the kind of performance that was only possible with proprietary models as recently as December. And thatâ€™s now achievable on a single NVIDIA H100 Tensor Core GPU or TPU host, significantly reducing deployment costs.","United States of America,United Kingdom of Great Britain and Northern Ireland",,,,6144.0,,Unreleased,"Gemma 2 is available under our 
commercially-friendly Gemma license, giving developers and researchers the ability to share and commercialize their innovations.",,,
Claude 3.5 Sonnet,"Multimodal,Language,Vision","Chat,Image captioning,Code generation,Language modeling/generation",,API access,https://www-cdn.anthropic.com/fed9cc193a14b84131812372d8d5857f8f304c52/Model_Card_Claude_3_Addendum.pdf,,Claude 3.5 Sonnet,2024-06-20,Anthropic,,,2.700000000000001e+25,"Blog post by Dario Amodei includes some info on 3.5 Sonnet compute: https://darioamodei.com/on-deepseek-and-export-controls
""Claude 3.5 Sonnet is a mid-sized model that cost a few $10M's to train (I won't give an exact number). Also, 3.5 Sonnet was not trained in any way that involved a larger or more expensive model (contrary to some rumors).""

Using assumptions about GPU pricing, this lets us estimate compute. https://docs.google.com/spreadsheets/d/1-p-ab6t6dkUM6T7GwnFp85ePTMpZMW7LFY7fW2t8POs/",Unspecified unreleased,Training data cutoff Apr 2024,,,,,,Speculative,"This addendum to our Claude 3 Model Card describes Claude 3.5 Sonnet, a new model which outperforms
our previous most capable model, Claude 3 Opus, while operating faster and at a lower cost. Claude 3.5
Sonnet offers improved capabilities, including better coding and visual processing. Since it is an evolution of
the Claude 3 model family, we are providing an addendum rather than a new model card. We provide updated
key evaluations and results from our safety testing",United States of America,,,,,,Unreleased,,,,
GLM-4V-9B,"Language,Multimodal,Vision","Language modeling/generation,Code generation,Question answering,Translation,Visual question answering","Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Diego Rojas, Guanyu Feng, Hanlin Zhao, Hanyu Lai, Hao Yu, Hongning Wang, Jiadai Sun, Jiajie Zhang, Jiale Cheng, Jiayi Gui, Jie Tang, Jing Zhang, Juanzi Li, Lei Zhao, Lindong Wu, Lucen Zhong, Mingdao Liu, Minlie Huang, Peng Zhang, Qinkai Zheng, Rui Lu, Shuaiqi Duan, Shudan Zhang, Shulin Cao, Shuxun Yang, Weng Lam Tam, Wenyi Zhao, Xiao Liu, Xiao Xia, Xiaohan Zhang, Xiaotao Gu, Xin Lv, Xinghan Liu, Xinyi Liu, Xinyue Yang, Xixuan Song, Xunkai Zhang, Yifan An, Yifan Xu, Yilin Niu, Yuantao Yang, Yueyan Li, Yushi Bai, Yuxiao Dong, Zehan Qi, Zhaoyu Wang, Zhen Yang, Zhengxiao Du, Zhenyu Hou, Zihan Wang",Open weights (non-commercial),https://arxiv.org/abs/2406.12793,,ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools,2024-06-18,"Z.ai (Zhipu AI),Tsinghua University",9000000000.0,9B,,"6ND = 6 FLOP / token / parameter * 9000000000 parameters * 10000000000000 tokens = 54*10^(9+13)=5.4*10^23 FLOP

""GLM-4-9B is pre-trained on approximately ten trillion tokens of multilingual corpus with a context length of 8192 (8K) and post-trained with the same pipeline and data used for GLM-4 (0520)"".
",,"""To date, the GLM-4 models are
pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage""",10000000000000.0,"""GLM-4-9B is pre-trained on approximately ten trillion tokens of multilingual corpus with a context length of 8192 (8K) and post-trained with the same pipeline and data used for GLM-4 (0520)"".",,,,Likely,"We introduce ChatGLM, an evolving family of large language models that we have been developing over time. This report primarily focuses on the GLM-4 language series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. They represent our most capable models that are trained with all the insights and lessons gained from the preceding three generations of ChatGLM. To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage. The high-quality alignment is achieved via a multi-stage post-training process, which involves supervised fine-tuning and learning from human feedback. Evaluations show that GLM-4 1) closely rivals or outperforms GPT-4 in terms of general metrics such as MMLU, GSM8K, MATH, BBH, GPQA, and HumanEval, 2) gets close to GPT-4-Turbo in instruction following as measured by IFEval, 3) matches GPT-4 Turbo (128K) and Claude 3 for long context tasks, and 4) outperforms GPT-4 in Chinese alignments as measured by AlignBench. The GLM-4 All Tools model is further aligned to understand user intent and autonomously decide when and which tool(s) touse -- including web browser, Python interpreter, text-to-image model, and user-defined functions -- to effectively complete complex tasks. In practical applications, it matches and even surpasses GPT-4 All Tools in tasks like accessing online information via web browsing and solving math problems using Python interpreter. Over the course, we have open-sourced a series of models, including ChatGLM-6B (three generations), GLM-4-9B (128K, 1M), GLM-4V-9B, WebGLM, and CodeGeeX, attracting over 10 million downloads on Hugging face in the year 2023 alone. The open models can be accessed through this https URL and this https URL.","China,China",,,,,,Unreleased,https://github.com/THUDM/GLM-4/blob/main/README_en.md,,,
Gen-3 Alpha,"Video,Vision","Video generation,Text-to-video,Image-to-video",,API access,"https://runwayml.com/research/introducing-gen-3-alpha

https://help.runwayml.com/hc/en-us/articles/30266515017875-Creating-with-Text-Image-to-Video-on-Gen-3-Alpha-and-Turbo",,"Gen-3 Alpha is the first of upcoming models that offers improvement in fidelity, consistency, motion and speed over previous generations of models. ",2024-06-17,Runway,,,,,Unspecified unreleased,,,,,,,Unknown,"Gen-3 Alpha is the first of the next generation of foundation models trained by Runway on a new infrastructure built for large-scale multimodal training. It is a major improvement in fidelity, consistency, and motion over Gen-2, and a step towards building General World Models.",United States of America,,,,,,Unreleased,,,,
JIUTIAN-139MoE,Language,"Language modeling/generation,Question answering,Code generation",,Open weights (unrestricted),https://gitee.com/CMCC-jiutian/JIUTIAN-139MoE?skip_mobile=true,,JIUTIAN-139MOE: TECHNICAL REPORT,2024-06-15,China Mobile,38800000000.0,"38.8B - total parameters
13B - activated parameters",4.3596131e+23,"6 FLOP / parameter / token * 13 * 10^9 activated parameters * 5 * 10^12 tokens = 3.9e+23 FLOP

Nvidia A800 and Ascend 910B chips were used

(376000000000000 FLOP / sec / GPU + 312000000000000 FLOP / sec / GPU) * 0.5 [assuming A800/910B 50/50] * 1464 hours [see training time notes] * 3600 sec / hour * 896 GPUs * 0.3 [assumed utilization] = 4.8733913e+23 FLOP

sqrt(3.9e+23 * 4.8733913e+23) = 4.3596131e+23 FLOP",Unspecified unreleased,"""We collect pretraining data from various types of data sources: web-pages, books, news articles, industrial knowledge materials, academic papers, etc. Most of the datasets are written in English and Chinese languages. We still have a small portion of other countriesâ€™ languages (e.g., Indonesian, Spanish, Arabic, Russian) and code data in Python, C++, or other programming languages""",5000000000000.0,"""After filtering, cleaning, deduplication and tokenization, finally we built a dataset encompassing 5 trillion tokens for the pretraining process.""",1464.0,"""The total training time is
approximately 2 months""

(30+31) * 24 = 1464 (hours)","NVIDIA A800 PCIe,Ascend (æ˜‡è…¾) 910B",Confident,"We report the development of JIUTIAN-139MoE, a 13-billion active parameter language model designed to be an efficient foundation model for industrial use. It adopts a decoder-only Transformerbased Mixture-of-Experts (MoE) architecture, employing a pair of large twin experts and six small experts to capture the intelligence associated with diverse industries. In terms of training, we support training with clusters of various GPUs and NPUs. We also support lossless switch between two
heterogeneous clusters. In addition, JIUTIAN-139MoE-Chat, a fine-tuned version of JIUTIAN139MoE, surpasses state-of-the-art large language models on both open and self-built industrystandard benchmarks. Specifically, it exhibits outstanding performances on 10 industrial benchmarks and leading performances on 10 benchmarks of general knowledge understanding, reasoning, math and coding capabilities. JIUTIAN-139MoE is released under the Apache 2.0 license and JIUTIAN Large Model Community License Agreement. It is publicly available at https://jiutian.10086.
cn/qdlake/qdh-web/#/model/detail/1070.",China,,,,896.0,,Unreleased,JIUTIAN-139MoE is released under the Apache 2.0 license and JIUTIAN Large Model Community License Agreement. It is publicly available at https://jiutian.10086.cn/qdlake/qdh-web/#/model/detail/1070,,,
Nemotron-4 340B,Language,"Language modeling/generation,Chat,Question answering","Bo Adler, Niket Agarwal, Ashwath Aithal, Dong H. Anh, Pallab Bhattacharya, Annika Brundyn, Jared Casper, Bryan Catanzaro, Sharon Clay, Jonathan Cohen, Sirshak Das, Ayush Dattagupta, Olivier Delalleau, Leon Derczynski, Yi Dong, Daniel Egert, Ellie Evans, Aleksander Ficek, Denys Fridman, Shaona Ghosh, Boris Ginsburg, Igor Gitman, Tomasz Grzegorzek, Robert Hero, Jining Huang, Vibhu Jawa, Joseph Jennings, Aastha Jhunjhunwala, John Kamalu, Sadaf Khan, Oleksii Kuchaiev, Patrick LeGresley, Hui Li, Jiwei Liu, Zihan Liu, Eileen Long, Ameya Sunil Mahabaleshwarkar, Somshubra Majumdar, James Maki, Miguel Martinez, Maer Rodrigues de Melo, Ivan Moshkov, Deepak Narayanan, Sean Narenthiran, Jesus Navarro, Phong Nguyen, Osvald Nitski, Vahid Noroozi, Guruprasad Nutheti, Christopher Parisien, Jupinder Parmar, Mostofa Patwary, Krzysztof Pawelec, Wei Ping, Shrimai Prabhumoye, Rajarshi Roy, Trisha Saar, Vasanth Rao Naik Sabavat, Sanjeev Satheesh, Jane Polak Scowcroft, Jason Sewall, Pavel Shamis, Gerald Shen, Mohammad Shoeybi, Dave Sizer, Misha Smelyanskiy, Felipe Soares, Makesh Narsimhan Sreedhar, Dan Su, Sandeep Subramanian, Shengyang Sun, Shubham Toshniwal, Hao Wang, Zhilin Wang, Jiaxuan You, Jiaqi Zeng, Jimmy Zhang, Jing Zhang, Vivienne Zhang, Yian Zhang, Chen Zhu",Open weights (unrestricted),https://blogs.nvidia.com/blog/nemotron-4-synthetic-data-generation-llm-training/ ,,NVIDIA Releases Open Synthetic Data Generation Pipeline for Training Large Language Models,2024-06-14,NVIDIA,340000000000.0,340B,1.8e+25,"9 trillion tokens for training
6 * 340B * 9T = 1.8E25

alternatively, can do a hardware estimate with a few extra steps:

According to the technical report, Nemotron-4 340B was trained using up to 6144 H100 GPUs. Helpfully, they also report the model FLOP utilization (MFU), which was 41-42% (Table 2). This is the ratio of the actual output of their GPUs, in FLOP used for training, relative to their theoretical max of 989 teraFLOP/s per GPU. 
Unfortunately, the report omits the last ingredient, which is the duration of the training run. However, in Table 2 they report some relevant data that we can use to infer the training time. 
Nemotron-4 was trained in several stages, but the largest stage used all 6144 GPUs with a batch size of 2304 and an iteration time (time per batch) of 8.0 seconds. This stage involved 7.6T tokens, so it makes up the majority of training. 
A batch size of 2304 means that each batch consists of 2304 sequences, and they report that the sequence length used for training was 4096 tokens. This means that each batch contained 4096 * 2304 = 9,437,184 tokens. 
So, during this stage, it took 8 seconds to train the model on 9.4m tokens. Extrapolating to the entire 9T token dataset, this implies the training run would have taken 7,659,574 seconds, or 89 days. (it actually took longer because they didn't use all their GPUs for the whole run) 
Multiplying 7,659,574 seconds by 41% MFU, 989 peak teraFLOP/s for each H100, and 6144 H100s, we get ~1.9e25 FLOP. This is very close to our first estimate. 
",Unspecified unreleased,"The technical report for the 340B model cites the report for the 15B version (https://arxiv.org/pdf/2402.16819 )

from that paper:

""We train Nemotron-4 15B on a pre-training dataset consisting of 8 trillion tokens. At a high-level,
the data blend is split into three different types of data: English natural language data (70%), multilingual
natural language data (15%), and source-code data (15%).
The English corpus consists of curated documents from a variety of sources and domains including web
documents, news articles, scientific papers, books, etc and the distribution used in our pre-training set is
highlighted in Figure 2. The code and multilingual data consists of a diverse set of natural and programming
languages. We find that appropriately sampling tokens from these languages is key to strong accuracies in
these domains. We share the distributions used for both code and multilingual tokens in our pre-training
dataset in Figure 3 and Figure 4 respectively.
In constructing the pre-training corpus, we remove any possible duplicates via document-level exact and
near-deduplication (Jennings et al., 2023). We additionally applied document-level quality filtering across
our corpus using a language-model based filtering approach similar to (Wenzek et al., 2019) in addition to a
series of heuristic filters as described in (Rae et al., 2022) and (Raffel et al., 2020).""",6750000000000.0,"9T training tokens.

They first train on an 8T token dataset and then an additional 1T tokens, it's slightly unclear if that's more data or a partial second epoch

6.75T words using 1 token = 0.75 words",2200.0,"see training compute notes, this is an inferred estimate",NVIDIA H100 SXM5 80GB,Confident,"We release the Nemotron-4 340B model family, including Nemotron-4-340B-Base, Nemotron-4-
340B-Instruct, and Nemotron-4-340B-Reward. Our models are open access under the NVIDIA Open
Model License Agreement, a permissive model license that allows distribution, modification, and use of
the models and its outputs. These models perform competitively to open access models on a wide range
of evaluation benchmarks, and were sized to fit on a single DGX H100 with 8 GPUs when deployed in
FP8 precision. We believe that the community can benefit from these models in various research studies
and commercial applications, especially for generating synthetic data to train smaller language models.
Notably, over 98% of data used in our model alignment process is synthetically generated, showcasing
the effectiveness of these models in generating synthetic data. To further support open research and
facilitate model development, we are also open-sourcing the synthetic data generation pipeline used in
our model alignment process.

(from technical report: https://d1qx31qr3h6wln.cloudfront.net/publications/Nemotron_4_340B_8T_0.pdf )",United States of America,,,,6144.0,0.410675,Unreleased,Permissive commercial license: https://developer.download.nvidia.com/licenses/nvidia-open-model-license-agreement-june-2024.pdf ,,67898317.22964796,348799800.27266073
PLaMo-100B,Language,Language modeling/generation,Preferred Elements (PFE),API access,https://tech.preferred.jp/ja/blog/plamo-100b/,,"Pre-training of the proprietary LLM ""PLaMo-100B"" with 100 billion parameters",2024-06-14,Preferred Networks Inc,100000000000.0,,1.2e+24,6*100B*2T=1.2e24,,,,"""The pre-trained model of PLaMo-100B developed this time was trained on a total of 2T tokens of both Japanese and English text data.""",,,,Confident,"Preferred Elements (PFE), a subsidiary of Preferred Networks (PFN), has been developing a 100 billion (100B) parameter LLM called ""PLaMo-100B"" since February. The pre-training part of the development of PLaMo-100B was completed in May, so in this article we will introduce the pre-training part of this model.",Japan,,,,,,Unreleased,,,,
Mamba2-Hybrid,Language,"Language modeling/generation,Question answering","Roger Waleffe, Wonmin Byeon, Duncan Riach, Brandon Norick, Vijay Korthikanti, Tri Dao, Albert Gu, Ali Hatamizadeh, Sudhakar Singh, Deepak Narayanan, Garvit Kulshreshtha, Vartika Singh, Jared Casper, Jan Kautz, Mohammad Shoeybi, Bryan Catanzaro",Open weights (unrestricted),https://arxiv.org/abs/2406.07887,,An Empirical Study of Mamba-based Language Models,2024-06-12,NVIDIA,8660000000.0,Table 6,1.8186e+23,"6ND = 6*8660000000.00 parameters * 3500000000000 tokens = 1.8186 Ã— 10^23

",Unspecified unreleased,"""We train the models discussed in this report on 1.1T and 3.5T token datasets. Both datasets are predecessors of the dataset used to train Nemotron-4 and are comprised of 70% English, 15% non-English, and 15% code.""

",3500000000000.0,,,,NVIDIA H100 SXM5 80GB,Likely,"Selective state-space models (SSMs) like Mamba overcome some of the shortcomings of Transformers, such as quadratic computational complexity with sequence length and large inference-time memory requirements from the key-value cache. Moreover, recent studies have shown that SSMs can match or exceed the language modeling capabilities of Transformers, making them an attractive alternative. In a controlled setting (e.g., same data), however, studies so far have only presented small scale experiments comparing SSMs to Transformers. To understand the strengths and weaknesses of these architectures at larger scales, we present a direct comparison between 8B-parameter Mamba, Mamba-2, and Transformer models trained on the same datasets of up to 3.5T tokens. We also compare these models to a hybrid architecture consisting of 43% Mamba-2, 7% attention, and 50% MLP layers (Mamba-2-Hybrid). Using a diverse set of tasks, we answer the question of whether Mamba models can match Transformers at larger training budgets. Our results show that while pure SSMs match or exceed Transformers on many tasks, they lag behind Transformers on tasks which require strong copying or in-context learning abilities (e.g., 5-shot MMLU, Phonebook) or long-context reasoning. In contrast, we find that the 8B Mamba-2-Hybrid exceeds the 8B Transformer on all 12 standard tasks we evaluated (+2.65 points on average) and is predicted to be up to 8x faster when generating tokens at inference time. To validate long-context capabilities, we provide additional experiments evaluating variants of the Mamba-2-Hybrid and Transformer extended to support 16K, 32K, and 128K sequences. On an additional 23 long-context tasks, the hybrid model continues to closely match or exceed the Transformer on average. To enable further study, we release the checkpoints as well as the code used to train our models as part of NVIDIA's Megatron-LM project.",United States of America,,,,1024.0,0.299,Open source,"https://github.com/NVIDIA/Megatron-LM/tree/ssm/examples/mamba
Apache 2.0

train script: https://github.com/NVIDIA/Megatron-LM/blob/ssm/examples/mamba/train.sh ",,,
Phoenix 1.0 Ultra,Image generation,"Text-to-image,Image generation",,API access,https://docs.leonardo.ai/docs/generate-images-using-leonardo-phoenix-model,,"Our first foundational model is here, changing everything you know about AI image generation.",2024-06-12,Leonardo AI,,,,"Developing organization is very small (seems to be under 50 employees), so it's unlikely they used a huge amount of compute.",,,,,,,,Unknown,"We are thrilled to introduce Phoenix, our very own foundational model, now in preview for ALL users! ðŸŒŸ

âœ¨ Key Features of Phoenix:

ðŸ‘‰ Improved Prompt Adherence
Experience exceptional accuracy with your prompts - and it's only going to get better from here!

ðŸ‘‰ Coherent Text in Images
Generate clear and accurate text within images, perfect for banners, posters, logos, and other text-heavy visuals.

ðŸ‘‰ Architectural Innovation
Our R&D team has pushed the boundaries to redefine what's possible in AI-generated imagery.

We're also rolling out Prompt Enhance to elevate shorter prompts, making it easier than ever to get exactly what you envision.

ðŸ“ How It Works:
Prompt with Ease: Whether you use a detailed instruction or a short description, we've got you covered.

Seamless Prompt Editing: Click the purple icon next to any previous Phoenix prompt to edit it effortlessly with the help of AI.

""Make it daytime"" - ""Add another character"" - whatever you want to change, just instruct.",Australia,,,,,,Unreleased,,,,
Kling,"Video,Vision","Video generation,Image-to-video,Text-to-video",,Hosted access (no API),"https://klingai.io/
https://ir.kuaishou.com/news-releases/news-release-details/kuaishou-unveils-comprehensive-ai-models-reshaping-content-and",,"Kling is a video generation model developed by the Kuaishou model team, which has powerful video generation capabilities and allows users to easily and efficiently complete artistic video creation.",2024-06-10,Kuaishou Technology,,,,,,,,,,,,Unknown,"Large-scale reasonable movement
Kling uses a 3D spatiotemporal joint attention mechanism to better model complex spatiotemporal movement, generate video content with large-scale movement, and conform to the laws of movement.

Video generation up to 2 minutes
Thanks to efficient training infrastructure, extreme reasoning optimization and scalable infrastructure, Kling's large model can generate videos up to 2 minutes long with a frame rate of 30fps.

Simulate physical world characteristics
Based on the powerful modeling capabilities inspired by the self-developed model architecture and Scaling Law, Kling can simulate the physical characteristics of the real world and generate videos that conform to the laws of physics.

Powerful concept combination capabilities
Based on a deep understanding of text-video semantics and the powerful capabilities of the Diffusion Transformer architecture, Kling can transform users' rich imagination into concrete pictures and fictional scenes that will not appear in the real world.

Movie-level image generation
Based on the self-developed 3D VAE, Keling can generate movie-level videos with 1080p resolution, which can vividly present both the vast and magnificent grand scenes and the delicate close-up shots.

Supports free output video aspect ratio
Keling adopts a variable resolution training strategy, which can output a variety of video aspect ratios for the same content during the inference process, meeting the needs of using video materials in richer scenes.

Expression and body drive
Based on the self-developed 3D face and body reconstruction technology, combined with background stability and redirection modules, the expression and body full drive technology is realized. With only a full-body photo, you can experience the vivid ""singing and dancing"" gameplay.",China,,,,,,Unreleased,"""Yes, KLING AI is accessible as a public demo in China, allowing users to experience its capabilities firsthand.""",,,
Qwen2-72B,Language,"Chat,Language modeling/generation,Question answering","An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jianxin Yang, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Xuejing Liu, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, Zhifang Guo, Zhihao Fan",Open weights (unrestricted),"https://qwenlm.github.io/blog/qwen2/ 
https://arxiv.org/abs/2407.10671 ",,Hello Qwen2,2024-06-07,Alibaba,72710000000.0,"72.71B parameters in total, of which 70.21B are non-embedding parameters",3.02e+24,"72 billion params, 7 trillion tokens

6 * 72 billion * 7 trillion ~= 3.02e24",Unspecified unreleased,"""All models were pre-trained on a high-quality, large-scale dataset comprising over 7 trillion tokens,
covering a wide range of domains and languages. Compared to previous editions of Qwen, Qwen2
includes a broader spectrum of linguistic data, enhancing the quantity and quality of code and mathematics content. """,7000000000000.0,"""All models were pre-trained on a high-quality, large-scale dataset comprising over 7 trillion tokens, covering a wide range of domains and languages.""",,,,Confident,"After months of efforts, we are pleased to announce the evolution from Qwen1.5 to Qwen2. This time, we bring to you:
- Pretrained and instruction-tuned models of 5 sizes, including Qwen2-0.5B, Qwen2-1.5B, Qwen2-7B, Qwen2-57B-A14B, and Qwen2-72B;
- Having been trained on data in 27 additional languages besides English and Chinese;
- State-of-the-art performance in a large number of benchmark evaluations;
- Significantly improved performance in coding and mathematics;
- Extended context length support up to 128K tokens with Qwen2-7B-Instruct and Qwen2-72B-Instruct.

(Technical report to follow)",China,,,,,,Unreleased,Apache 2.0,,,
Qwen2-7B,Language,"Chat,Language modeling/generation,Question answering","An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jianxin Yang, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Xuejing Liu, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, Zhifang Guo, Zhihao Fan",Open weights (unrestricted),"https://qwenlm.github.io/blog/qwen2/ 
https://arxiv.org/abs/2407.10671 ",,Hello Qwen2,2024-06-07,Alibaba,7000000000.0,7B parameters (table 1),2.9400000000001e+23,"7 billion params, 7 trillion tokens

6 FLOP * 7 billion * 7 trillion ~= 2.94e23 FLOP",Unspecified unreleased,"""All models were pre-trained on a high-quality, large-scale dataset comprising over 7 trillion tokens, covering a wide range of domains and languages. Compared to previous editions of Qwen, Qwen2 includes a broader spectrum of linguistic data, enhancing the quantity and quality of code and mathematics content.""",7000000000000.0,"""All models were pre-trained on a high-quality, large-scale dataset comprising over 7 trillion tokens, covering a wide range of domains and languages.""",,,,Confident,"This report introduces the Qwen2 series, the latest addition to our large language models and large multimodal models. We release a comprehensive suite of
foundational and instruction-tuned language models, encompassing a parameter
range from 0.5 to 72 billion, featuring dense models and a Mixture-of-Experts
model. Qwen2 surpasses most prior open-weight models, including its predecessor
Qwen1.5, and exhibits competitive performance relative to proprietary models
across diverse benchmarks on language understanding, generation, multilingual
proficiency, coding, mathematics, and reasoning.
The flagship model, Qwen2-72B, showcases remarkable performance: 84.2 on
MMLU, 37.9 on GPQA, 64.6 on HumanEval, 89.5 on GSM8K, and 82.4 on BBH as
a base language model. The instruction-tuned variant, Qwen2-72B-Instruct, attains
9.1 on MT-Bench, 48.1 on Arena-Hard, and 35.7 on LiveCodeBench. Moreover,
Qwen2 demonstrates robust multilingual capabilities, proficient in approximately
30 languages, spanning English, Chinese, Spanish, French, German, Arabic, Russian, Korean, Japanese, Thai, Vietnamese, and more, underscoring its versatility
and global reach.
To foster community innovation and accessibility, we have made the Qwen2 model
weights openly available on Hugging Face1 and ModelScope2, and the supplementary materials including example code on GitHub3. These platforms also include
resources for quantization, fine-tuning, and deployment, facilitating a wide range
of applications and research endeavors.",China,,,,,,Unreleased,Apache 2.0,,,
Qwen2-57B-A14B,Language,"Chat,Language modeling/generation,Question answering","An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jianxin Yang, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Xuejing Liu, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, Zhifang Guo, Zhihao Fan",Open weights (unrestricted),"https://qwenlm.github.io/blog/qwen2/ 
https://arxiv.org/abs/2407.10671 ",,Hello Qwen2,2024-06-07,Alibaba,57000000000.0,57B parameters (table 1),3.7800000000001e+23,"""For MoE models, 57B-A14B denotes that the model has 57B parameters in total and for each token 14B parameters are active"" (page 5)

C ~= 6 FLOP * 14e9 * 4.5e12 = 3.78e23",Unspecified unreleased,"""All models were pre-trained on a high-quality, large-scale dataset comprising over 7 trillion tokens, covering a wide range of domains and languages. Compared to previous editions of Qwen, Qwen2 includes a broader spectrum of linguistic data, enhancing the quantity and quality of code and mathematics content.""",4500000000000.0,"""All models were pre-trained on a high-quality, large-scale dataset comprising over 7 trillion tokens, covering a wide range of domains and languages.""

57B-A14B model was trained with a 4.5T subset of the 7T overall dataset. (table 1)",,,,Confident,"This report introduces the Qwen2 series, the latest addition to our large language models and large multimodal models. We release a comprehensive suite of
foundational and instruction-tuned language models, encompassing a parameter
range from 0.5 to 72 billion, featuring dense models and a Mixture-of-Experts
model. Qwen2 surpasses most prior open-weight models, including its predecessor
Qwen1.5, and exhibits competitive performance relative to proprietary models
across diverse benchmarks on language understanding, generation, multilingual
proficiency, coding, mathematics, and reasoning.
The flagship model, Qwen2-72B, showcases remarkable performance: 84.2 on
MMLU, 37.9 on GPQA, 64.6 on HumanEval, 89.5 on GSM8K, and 82.4 on BBH as
a base language model. The instruction-tuned variant, Qwen2-72B-Instruct, attains
9.1 on MT-Bench, 48.1 on Arena-Hard, and 35.7 on LiveCodeBench. Moreover,
Qwen2 demonstrates robust multilingual capabilities, proficient in approximately
30 languages, spanning English, Chinese, Spanish, French, German, Arabic, Russian, Korean, Japanese, Thai, Vietnamese, and more, underscoring its versatility
and global reach.
To foster community innovation and accessibility, we have made the Qwen2 model
weights openly available on Hugging Face1 and ModelScope2, and the supplementary materials including example code on GitHub3. These platforms also include
resources for quantization, fine-tuning, and deployment, facilitating a wide range
of applications and research endeavors.",China,,,,,,Unreleased,Apache 2.0,,,
Granite 20B,Language,Language modeling/generation,IBM Research,Open weights (unrestricted),https://www.ibm.com/downloads/documents/us-en/10a99803c92fdb35,,Granite Foundation Models,2024-05-31,IBM Research,20000000000.0,,3.0000000000001e+23,6*2500000000000*20000000000=3e+23,"Stack Exchange,Common Crawl,Wikimedia","""For English and code, we used Wikimedia, Stack Exchange, and commoncrawl. For multilingual data, we used portions of commoncrawl.""",2500000000000.0,"For pre-training, we used 0.5 trillion English, 0.4 trillion multilingual (es, fr, de, pt), and 1.6 trillion code tokens.",,,,Confident,"We introduce the Granite series of decoder-only foundation models for generative artificial intelligence (AI) tasks that are ready for enterprise use. We report on the architecture, capabilities, underlying data and data governance, training algorithms, compute infrastructure, energy and carbon footprint, testing and evaluation, socio-technical harms and mitigations, and usage policies.",United States of America,,,,,,Unreleased,"Apache 2.0 
https://huggingface.co/ibm-granite/granite-20b-code-base-8k

no pretraining code here, inference and fine-tuning only
https://github.com/ibm-granite/granite-code-models",,,
Codestral,Language,"Code generation,Code autocompletion","Albert Jiang, Alexandre Sablayrolles, Alexis Tacnet, Antoine Roux, Arthur Mensch, Audrey Herblin-Stoop, Baptiste Bout, Baudouin de Monicault, Blanche Savary, Bam4d, Caroline Feldman, Devendra Singh Chaplot, Diego de las Casas, Eleonore Arcelin, Emma Bou Hanna, Etienne Metzger, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Harizo Rajaona, Henri Roussez, Jean-Malo Delignon, Jia Li, Justus Murke, Kartik Khandelwal, Lawrence Stewart, Louis Martin, Louis Ternon, Lucile Saulnier, LÃ©lio Renard Lavaud, Margaret Jennings, Marie Pellat, Marie Torelli, Marie-Anne Lachaux, Marjorie Janiewicz, Mickael Seznec, Nicolas Schuhl, Patrick von Platen, Romain Sauvestre, Pierre Stock, Sandeep Subramanian, Saurabh Garg, Sophia Yang, Szymon Antoniak, Teven Le Scao, Thibaut Lavril, Thibault Schueller, TimothÃ©e Lacroix, ThÃ©ophile Gervet, Thomas Wang, Valera Nemychnikova, Wendy Shang, William El Sayed, William Marshall",Open weights (non-commercial),https://mistral.ai/news/codestral/,,Empowering developers and democratising coding with Mistral AI.,2024-05-29,Mistral AI,22200000000.0,22.2B from hugging face model card,,,Unspecified unreleased,"Codestral is trained on a diverse dataset of 80+ programming languages, including the most popular ones, such as Python, Java, C, C++, JavaScript, and Bash. It also performs well on more specific ones like Swift and Fortran. ",,,,,,Confident,"We introduce Codestral, our first-ever code model. Codestral is an open-weight generative AI model explicitly designed for code generation tasks. It helps developers write and interact with code through a shared instruction and completion API endpoint. As it masters code and English, it can be used to design advanced AI applications for software developers.

A model fluent in 80+ programming languages
Codestral is trained on a diverse dataset of 80+ programming languages, including the most popular ones, such as Python, Java, C, C++, JavaScript, and Bash. It also performs well on more specific ones like Swift and Fortran. This broad language base ensures Codestral can assist developers in various coding environments and projects.

Codestral saves developers time and effort: it can complete coding functions, write tests, and complete any partial code using a fill-in-the-middle mechanism. Interacting with Codestral will help level up the developerâ€™s coding game and reduce the risk of errors and bugs.",France,,,,,,Unreleased,"Codestral is a 22B open-weight model licensed under the new Mistral AI Non-Production License, which means that you can use it for research and testing purposes. Codestral can be downloaded on HuggingFace.

https://huggingface.co/mistralai/Codestral-22B-v0.1",,,
Nanbeige2-16B-Chat,Language,"Chat,Question answering",Nanbeige Lab,Open weights (restricted use),https://huggingface.co/Nanbeige/Nanbeige2-16B-Chat,,Nanbeige2-16B-Chat,2024-05-28,Nanbeige LLM Lab,15800000000.0,https://huggingface.co/Nanbeige/Nanbeige2-16B-Chat,4.05e+23,"The model has 15.8B parameters and was trained on 4.5T tokens during the training phrase (https://huggingface.co/Nanbeige/Nanbeige2-16B-Chat). It appears to be entirely transformer-based (https://huggingface.co/Nanbeige/Nanbeige2-16B-Chat/blob/main/modeling_nanbeige.py).

Assuming training was done for 1 epoch, the 6ND approximation yields
Training compute
= # of active parameters / forward pass * # of tokens * 6 FLOPS / token
= 1.5e10 parameters * 4.5e12 tokens * 6 FLOPS / token
= 4.05e23 FLOPS",,,,The model was trained on 4.5T tokens during the training phrase (https://huggingface.co/Nanbeige/Nanbeige2-16B-Chat).,,,,Confident,"The Nanbeige2-16B-Chat is the latest 16B model developed by the Nanbeige Lab, which utilized 4.5T tokens of high-quality training data during the training phase. During the alignment phase, we initially trained our model using 1 million samples through Supervised Fine-Tuning (SFT). We then engaged in curriculum learning with 400,000 high-quality samples that presented a greater level of difficulty. Subsequently, we incorporated human feedback through the Direct Preference Optimization (DPO), culminating in the development of Nanbeige2-16B-Chat. Nanbeige2-16B-Chat has achieved superior performance across various authoritative benchmark datasets.",China,,,,,,Unreleased,"Apache 2.0 though you need to ask to use commercially
""If you intend to use the Nanbeige Models or its derivatives for commercial purposes, please submit application materials to meet the requirements of the Nanbeige Models Community License Agreement by contacting nanbeige@126.com""

https://huggingface.co/Nanbeige/Nanbeige2-16B-Chat

no training code here

https://github.com/Nanbeige/Nanbeige/blob/main/README_EN.md",,,
360Zhinao-7B,Language,"Question answering,Language modeling/generation",360zhinao,Open weights (unrestricted),https://arxiv.org/abs/2405.13386,0.0,360Zhinao Technical Report,2024-05-22,360 Security Technology,7000000000.0,,1.428e+23,6*3400000000000*7000000000=1.428e+23,,,3400000000000.0,3.4 trillion tokens,,,,Confident,"We present 360Zhinao models with 7B parameter size and context lengths spanning 4K, 32K and 360K, all available at this https URL. For rapid development in pretraining, we establish a stable and sensitive ablation environment to evaluate and compare experiment runs with minimal model size. Under such guidance, we perfect our data cleaning and composition strategies to pretrain 360Zhinao-7B-Base on 3.4T tokens. We also mainly emphasize data during alignment, where we strive to balance quantity and quality with filtering and reformatting. With tailored data, 360Zhinao-7B's context window is easily extended to 32K and 360K. RMs and RLHF are trained following SFT and credibly applied to specific tasks. All together these contributions lead to 360Zhinao-7B's competitive performance among models of similar size.",China,,,,,,Open source,"Apache 2.0
https://huggingface.co/qihoo360/360Zhinao-7B-Base

https://github.com/Qihoo360/360zhinao?tab=readme-ov-file",,,
ALLaM 34B,Language,"Language modeling/generation,Translation,Question answering",Saudi Data and Artificial Intelligence Authority,Unreleased,https://arxiv.org/abs/2407.15390,42.0,AI Models for Arabic and English,2024-05-21,Saudi Data and Artificial Intelligence Authority,34000000000.0,,1.0608e+24,6*34000000000*5200000000000=1.060800e+24,,,5200000000000.0,"3,431,217,579(4.3B) total documents, with a total of 4,587,781,981,546(4.5T) words, and 5.2T tokens.",,,NVIDIA A100,Confident,"We present ALLaM: Arabic Large Language Model, a series of large language models to support the ecosystem of Arabic Language Technologies (ALT). ALLaM is carefully trained considering the values of language alignment and knowledge transfer at scale. Our autoregressive decoder-only architecture models demonstrate how second-language acquisition via vocabulary expansion and pretraining on a mixture of Arabic and English text can steer a model towards a new language (Arabic) without any catastrophic forgetting in the original language (English). Furthermore, we highlight the effectiveness of using parallel/translated data to aid the process of knowledge alignment between languages. Finally, we show that extensive alignment with human preferences can significantly enhance the performance of a language model compared to models of a larger scale with lower quality alignment. ALLaM achieves state-of-the-art performance in various Arabic benchmarks, including MMLU Arabic, ACVA, and Arabic Exams. Our aligned models improve both in Arabic and English from their base aligned models.",Saudi Arabia,,,,,,Unreleased,"34b is not in the HF repo yet
https://huggingface.co/ALLaM-AI",,,
Chameleon-34B,"Multimodal,Image generation,Language,Vision","Language modeling/generation,Vision-language generation,Visual question answering,Text-to-image","Srinivasan Iyer, Bernie Huang, Lili Yu, Arun Babu, Chunting Zhou, Kushal Tirumala, Xi Victoria Lin, Hu Xu, Xian Li, Akshat Shrivastava, Omer Levy, Armen Aghajanyan, Ram Pasunuru, Andrew Cohen, Aram H. Markosyan, Koustuv Sinha, Xiaoqing Ellen Tan, Ivan Evtimov, Ping Yu, Tianlu Wang, Olga Golovneva, Asli Celikyilmaz, Pedro Rodriguez, Leonid Shamis, Vasu Sharma, Christine Jou, Karthik Padthe, Ching-Feng Yeh, Mingda Chen, Bapi Akula, Jacob Kahn, Daniel Li, Scott Yih, Barlas Oguz, Morteza Behrooz, Benjamin Muller, Carleigh Wood, Mary Williamson, Ramya Raghavendra, Barbara Usher, William Ngan, Nikolay Bashlykov, Lukas Blecher, Sony Theakanath, Ammar Rizvi, Gargi Ghosh, Luke Zettlemoyer",Open weights (non-commercial),https://arxiv.org/abs/2405.09818v1,,Chameleon: Mixed-Modal Early-Fusion Foundation Models,2024-05-16,Facebook AI Research,34000000000.0,,1.6453571041e+24,"GPU method:
Table 2 shows that 34B model pre-training uses 4282407 GPU-hours, trained across 3072 A100s.
3.12e14 * 4282407 * 3600 * 0.3 =  1.44e24

Parameter-token method:
Pre-training goes over 9.2T tokens, post-training only goes over 1.1B tokens (sum of tokens column in Table 3).
6 * 34B *  9.2T = 1.88e24

Geometric mean: sqrt(1.44e24 * 1.88e24) = 1.65e24",Unspecified unreleased,"Pre-training:
- 2.9 trillion tokens of pure text
- 1.4 billion text-image pairs, which produces 1.5 trillion text-image tokens
	- Since each image is 1024 tokens, implies 1.43 trillion image tokens and 0.07 trillion text tokens
- 400 billion tokens of image-text interleaved documents
	- Difficult to estimate image-to-text ratio, but references OBELIKS paper which had 141 million web pages, 353 million associated images, and 115 billion text tokens.
	- 353 million * 1024 = 361.5 billion image tokens, so 75.9% of the tokens would be from images
	- Implies 303.5 billion image tokens and 96.5 text tokens
- There is a second stage of ""higher quality"" pre-training tokens which are not described in detail


Post-training:
- 940 million tokens of text
- 1.1 million tokens of code
- 19.4 million tokens of visual chats
	- Featuring 16.7 thousand images, so 17.1M image tokens and 2.3M text tokens
- 68 million tokens of image generations
	- Featuring 64.3 thousand images, so 65.8M image tokens and 2.2M text tokens
- 35.8 million tokens of interleaved generations
	- Featuring 30.7 thousand images, so 31.4M image tokens and 4.4M text tokens
- 38.6 million tokens of safety training
	- Featuring 1.6 thousand images, so 1.6M image tokens and 37M text tokens",4400000000000.0,"Slightly conflicting info. Pre-training data details describe different types of data that sum to 4.8 trillion tokens, but Table 1 indicates 4.4T. Using table values as this agrees with other statements about epochs and total tokens seen.",1394.0,"34B model pre-training uses 4282407 GPU-hours, trained across 3072 A100s
4282407 / 3072 = 1394",NVIDIA A100 SXM4 80 GB,Confident,"We present Chameleon, a family of early-fusion token-based mixed-modal models capable of understanding and generating images and text in any arbitrary sequence. We outline a stable training approach from inception, an alignment recipe, and an architectural parameterization tailored for the early-fusion, token-based, mixed-modal setting. The models are evaluated on a comprehensive range of tasks, including visual question answering, image captioning, text generation, image generation, and long-form mixed modal generation. Chameleon demonstrates broad and general capabilities, including state-of-the-art performance in image captioning tasks, outperforms Llama-2 in text-only tasks while being competitive with models such as Mixtral 8x7B and Gemini-Pro, and performs non-trivial image generation, all in a single model. It also matches or exceeds the performance of much larger models, including Gemini Pro and GPT-4V, according to human judgments on a new long-form mixed-modal generation evaluation, where either the prompt or outputs contain mixed sequences of both images and text. Chameleon marks a significant step forward in a unified modeling of full multimodal documents.","United States of America,France",,,"Not enough info to estimate. GPU time given for pretraining, and while we know # of fine-tuning tokens we don't know # of epochs.",3072.0,,Unreleased,"https://ai.meta.com/resources/models-and-libraries/chameleon-downloads/?gk_enable=chameleon_web_flow_is_live

""The models weâ€™re releasing today were safety tuned and support mixed-modal inputs and text-only output to be used for research purposes. While weâ€™ve taken steps to develop these models responsibly, we recognize that risks remain. At this time, we are not releasing the Chameleon image generation model.""",,,
FragLlama: Next-fragment prediction for molecular design,"Multimodal,Image generation,Vision,Language","Language modeling/generation,Vision-language generation,Visual question answering,Text-to-image","Srinivasan Iyer, Bernie Huang, Lili Yu, Arun Babu, Chunting Zhou, Kushal Tirumala, Xi Victoria Lin, Hu Xu, Xian Li, Akshat Shrivastava, Omer Levy, Armen Aghajanyan, Ram Pasunuru, Andrew Cohen, Aram H. Markosyan, Koustuv Sinha, Xiaoqing Ellen Tan, Ivan Evtimov, Ping Yu, Tianlu Wang, Olga Golovneva, Asli Celikyilmaz, Pedro Rodriguez, Leonid Shamis, Vasu Sharma, Christine Jou, Karthik Padthe, Ching-Feng Yeh, Mingda Chen, Bapi Akula, Jacob Kahn, Daniel Li, Scott Yih, Barlas Oguz, Morteza Behrooz, Benjamin Muller, Carleigh Wood, Mary Williamson, Ramya Raghavendra, Barbara Usher, William Ngan, Nikolay Bashlykov, Lukas Blecher, Sony Theakanath, Ammar Rizvi, Gargi Ghosh, Luke Zettlemoyer",Open weights (non-commercial),https://arxiv.org/abs/2405.09818v1,,Chameleon: Mixed-Modal Early-Fusion Foundation Models,2024-05-16,Facebook AI Research,7000000000.0,,3.3399700602e+23,"GPU method:
Table 2 shows that 7B model pre-training uses 856481 GPU-hours, trained across 1024 A100s
3.12e14 * 856481 * 3600 * 0.3 =  2.89e23

Parameter-token method:
Pre-training goes over 9.2T tokens, post-training only goes over 1.1B tokens (sum of tokens column in Table 3)
6 * 7B *  9.2T = 3.86e23

Geometric mean: sqrt(2.89e23 * 3.86e23) = 3.34e23",Unspecified unreleased,"Pre-training:
- 2.9 trillion tokens of pure text
- 1.4 billion text-image pairs, which produces 1.5 trillion text-image tokens
	- Since each image is 1024 tokens, implies 1.43 trillion image tokens and 0.07 trillion text tokens
- 400 billion tokens of image-text interleaved documents
	- Difficult to estimate image-to-text ratio, but references OBELIKS paper which had 141 million web pages, 353 million associated images, and 115 billion text tokens.
	- 353 million * 1024 = 361.5 billion image tokens, so 75.9% of the tokens would be from images
	- Implies 303.5 billion image tokens and 96.5 text tokens
- There is a second stage of ""higher quality"" pre-training tokens which are not described in detail


Post-training:
- 940 million tokens of text
- 1.1 million tokens of code
- 19.4 million tokens of visual chats
	- Featuring 16.7 thousand images, so 17.1M image tokens and 2.3M text tokens
- 68 million tokens of image generations
	- Featuring 64.3 thousand images, so 65.8M image tokens and 2.2M text tokens
- 35.8 million tokens of interleaved generations
	- Featuring 30.7 thousand images, so 31.4M image tokens and 4.4M text tokens
- 38.6 million tokens of safety training
	- Featuring 1.6 thousand images, so 1.6M image tokens and 37M text tokens",4400000000000.0,"Slightly conflicting info. Pre-training data details describe different types of data that sum to 4.8 trillion tokens, but Table 1 indicates 4.4T. Using table values as this agrees with other statements about epochs and total tokens seen.",836.4,"34B model pre-training uses 856481 GPU-hours, trained across 1024 A100s
856481 / 1024 = 836.4",NVIDIA A100 SXM4 80 GB,Confident,"We present Chameleon, a family of early-fusion token-based mixed-modal models capable of understanding and generating images and text in any arbitrary sequence. We outline a stable training approach from inception, an alignment recipe, and an architectural parameterization tailored for the early-fusion, token-based, mixed-modal setting. The models are evaluated on a comprehensive range of tasks, including visual question answering, image captioning, text generation, image generation, and long-form mixed modal generation. Chameleon demonstrates broad and general capabilities, including state-of-the-art performance in image captioning tasks, outperforms Llama-2 in text-only tasks while being competitive with models such as Mixtral 8x7B and Gemini-Pro, and performs non-trivial image generation, all in a single model. It also matches or exceeds the performance of much larger models, including Gemini Pro and GPT-4V, according to human judgments on a new long-form mixed-modal generation evaluation, where either the prompt or outputs contain mixed sequences of both images and text. Chameleon marks a significant step forward in a unified modeling of full multimodal documents.","United States of America,France",,,"Not enough info to estimate. GPU time given for pretraining, and while we know # of fine-tuning tokens we don't know # of epochs.",,,Unreleased,"https://ai.meta.com/resources/models-and-libraries/chameleon-downloads/?gk_enable=chameleon_web_flow_is_live

""The models weâ€™re releasing today were safety tuned and support mixed-modal inputs and text-only output to be used for research purposes. While weâ€™ve taken steps to develop these models responsibly, we recognize that risks remain. At this time, we are not releasing the Chameleon image generation model.""",,,
Doubao-lite,Language,"Language modeling/generation,Question answering",,API access,https://www.volcengine.com/docs/6360/1264663,,Doubao General Model Lite (Doubao-lite),2024-05-15,ByteDance,,,,,Unspecified unreleased,,,,,,,Unknown,"A lightweight version of the LLM, offering lower token costs and latency compared to the Pro version. 
",China,,,,,,Unreleased,,,,
Imagen 3,Image generation,"Image generation,Text-to-image","Jason Baldridge, Jakob Bauer, Mukul Bhutani, Nicole Brichtova, Andrew Bunner, Kelvin Chan, Sergio GÃ³mez Colmenarejo, Sander Dieleman, Yuqing Du, Zach Eaton-Rosen, Hongliang Fei, Yilin Gao, Evgeny Gladchenko, Mandy Guo, Alex Haig, Will Hawkins, Hexiang (Frank) Hu, Huilian Huang, Tobenna Peter Igwe, Christos Kaplanis, Siavash Khodadadeh, Ksenia Konyushkova, Karol Langner, Eric Lau, Shixin Luo, SoÅˆa MokrÃ¡, Henna Nandwani, Yasumasa Onoe, AÃ¤ron van den Oord, Zarana Parekh, Jordi Pont-Tuset, Hang Qi, Rui Qian, Deepak Ramachandran, Ali Razavi, Robert Riachi, Hansa Srinivasan, Srivatsan Srinivasan, Robin Strudel, Benigno Uria, Oliver Wang, Su Wang, Austin Waters, Chris Wolff, Auriel Wright, Zhisheng Xiao, Keyang Xu, Marc van Zee, Junlin Zhang, Wenlei Zhou and Konrad Zoln.",Hosted access (no API),https://deepmind.google/technologies/imagen-3/,,Imagen 3: our highest quality text-to-image model,2024-05-14,Google DeepMind,,,,,Unspecified unreleased,"""Our model is trained on a large dataset comprising images, text and associated annotations. To ensure quality and safety standards, we employ a multi-stage filtering process. This process begins by removing unsafe, violent, or low-quality images. We then eliminate AI-generated images to prevent
the model from learning artifacts or biases commonly found in such images. Additionally, we use deduplication pipelines and down-weight similar images to minimize the risk of outputs overfitting particular elements of training data.""",,,,,"Google TPU v4,Google TPU v5e",Unknown,"Imagen 3 is our highest quality text-to-image model, capable of generating images with even better detail, richer lighting and fewer distracting artifacts than our previous models.","United States of America,United Kingdom of Great Britain and Northern Ireland",,,,,,Unreleased,,,,
Yi-Large,Language,"Chat,Language modeling/generation","Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, Kaidong Yu, Peng Liu, Qiang Liu, Shawn Yue, Senbin Yang, Shiming Yang, Tao Yu, Wen Xie, Wenhao Huang, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu, Pengcheng Nie, Yuchi Xu, Yudong Liu, Yue Wang, Yuxuan Cai, Zhenyu Gu, Zhiyuan Liu, Zonghong Dai",API access,,,,2024-05-13,01.AI,100000000000.0,"""Yi-Large is a software over-the-air-driven closed-source large model with a parameter of over 100 billion tokens."" from https://www.chinadaily.com.cn/a/202405/13/WS6641abd1a31082fc043c6ccd.html",1.8e+24,"6ND = 6*100000000000*3000000000000=1.8e+24

(speculative confidence because training dataset size is very uncertain)",,,3000000000000.0,"3T tokens for previous Yi models: ""Targeted as a bilingual language model and trained on 3T multilingual corpus, the Yi series models become one of the strongest LLM worldwide, showing promise in language understanding, commonsense reasoning, reading comprehension, and more.""
",,,,Speculative,,China,,,,,,Unreleased,,,,
GPT-4o (May 2024),"Multimodal,Language,Audio,Speech,Vision","Chat,Image generation,Audio generation,Vision-language generation,Table tasks,Language modeling/generation,Question answering,Speech recognition (ASR),Speech-to-text","Aidan Clark, Alex Paino, Jacob Menick, Liam Fedus, Luke Metz, Clemens Winter, Lia Guy, Sam Schoenholz, Daniel Levy, Nitish Keskar, Alex Carney, Alex Paino, Ian Sohl, Qiming Yuan, Reimar Leike, Arka Dhar, Brydon Eastman, Mia Glaese, Ben Sokolowsky, Andrew Kondrich, Felipe Petroski Such, Henrique Ponde de Oliveira Pinto, Jiayi Weng, Randall Lin, Youlong Cheng, Nick Ryder, Lauren Itow, Barret Zoph, John Schulman, Mianna Chen, Adam Lerer, Adam P. Goucher, Adam Perelman, Akila Welihinda, Alec Radford, Alex Borzunov, Alex Carney, Alex Chow, Alex Paino, Alex Renzin, Alex Tachard Passos, Alexi Christakis, Ali Kamali, Allison Moyer, Allison Tam, Amin Tootoonchian, Ananya Kumar, Andrej Karpathy, Andrey Mishchenko, Andrew Cann, Andrew Kondrich, Andrew Tulloch, Angela Jiang, Antoine Pelisse, Anuj Gosalia, Avi Nayak, Avital Oliver, Behrooz Ghorbani, Ben Leimberger, Ben Wang, Blake Samic, Brian Guarraci, Brydon Eastman, Camillo Lugaresi, Chak Li, Charlotte Barette, Chelsea Voss, Chong Zhang, Chris Beaumont, Chris Hallacy, Chris Koch, Christian Gibson, Christopher Hesse, Colin Wei, Daniel Kappler, Daniel Levin, Daniel Levy, David Farhi, David Mely, David Sasaki, Dimitris Tsipras, Doug Li, Duc Phong Nguyen, Duncan Findlay, Edmund Wong, Ehsan Asdar, Elizabeth Proehl, Elizabeth Yang, Eric Peterson, Eric Sigler, Eugene Brevdo, Farzad Khorasani, Francis Zhang, Gene Oden, Geoff Salmon, Hadi Salman, Haiming Bao, Heather Schmidt, Hongyu Ren, Hyung Won Chung, Ian Kivlichan, Ian O'Connell, Ian Osband, Ilya Kostrikov, Ingmar Kanitscheider, Jacob Coxon, James Crooks, James Lennon, Jason Teplitz, Jason Wei, Jason Wolfe, Jay Chen, Jeff Harris, Jiayi Weng, Jie Tang, Joanne Jang, Jonathan Ward, Jonathan McKay, Jong Wook Kim, Josh Gross, Josh Kaplan, Joy Jiao, Joyce Lee, Juntang Zhuang, Kai Fricke, Kavin Karthik, Kenny Hsu, Kiel Howe, Kyle Luther, Larry Kai, Lauren Itow, Leo Chen, Lia Guy, Lien Mamitsuka, Lilian Weng, Long Ouyang, Louis Feuvrier, Lukas Kondraciuk, Lukasz Kaiser, Lyric Doshi, Mada Aflak, Maddie Simens, Madeleine Thompson, Marat Dukhan, Marvin Zhang, Mateusz Litwin, Max Johnson, Mayank Gupta, Mia Glaese, Michael Janner, Michael Petrov, Michael Wu, Michelle Fradin, Michelle Pokrass, Miguel Oom Temudo de Castro, Mikhail Pavlov, Minal Khan, Mo Bavarian, Natalia Gimelshein, Natalie Staudacher, Nick Stathas, Nik Tezak, Nithanth Kudige, Noel Bundick, Ofir Nachum, Oleg Boiko, Oleg Murk, Olivier Godement, Owen Campbell-Moore, Philip Pronin, Philippe Tillet, Rachel Lim, Rajan Troll, Randall Lin, Rapha gontijo lopes, Raul Puri, Reah Miyara, Reimar Leike, Renaud Gaubert, Reza Zamani, Rob Honsby, Rohit Ramchandani, Rory Carmichael, Ruslan Nigmatullin, Ryan Cheu, Scott Gray, Sean Grove, Sean Metzger, Shantanu Jain, Shengjia Zhao, Sherwin Wu, Shuaiqi (Tony) Xia, Sonia Phene, Spencer Papay, Steve Coffey, Steve Lee, Steve Lee, Stewart Hall, Suchir Balaji, Tal Broda, Tal Stramer, Tarun Gogineni, Ted Sanders, Thomas Cunninghman, Thomas Dimson, Thomas Raoux, Tianhao Zheng, Tina Kim, Todd Underwood, Tristan Heywood, Valerie Qi, Vinnie Monaco, Vlad Fomenko, Weiyi Zheng, Wenda Zhou, Wojciech Zaremba, Yash Patil, Yilei, Qian, Yongjik Kim, Youlong Cheng, Yuchen He, Yuchen Zhang, Yujia Jin, Yunxing Dai, Yury Malkov",API access,"https://openai.com/index/hello-gpt-4o/ 
https://openai.com/index/gpt-4o-system-card/",,Hello GPT-4o,2024-05-13,OpenAI,,"Not known.

Inference costs in the API are 2x cheaper than GPT-4 Turbo",,Training compute estimated to be 3.8e25 FLOP from benchmark scores. https://colab.research.google.com/drive/1r3pUMhB7Kh0Gls9eG-v_XefWrye9fVQR?usp=sharing,Unspecified unreleased,"""With GPT-4o, we trained a single new model end-to-end across text, vision, and audio.""
This has a knowledge cutoff date of October 2023 - which I assume means October 1, 2023 - according to https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models?tabs=global-standard%2Cstandard-chat-completions#gpt-4o-and-gpt-4-turbo.",,,,,,Speculative,"Weâ€™re announcing GPT-4o, our new flagship model that can reason across audio, vision, and text in real time.

GPT-4o (â€œoâ€ for â€œomniâ€) is a step towards much more natural human-computer interactionâ€”it accepts as input any combination of text, audio, image, and video and generates any combination of text, audio, and image outputs. It can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time(opens in a new window) in a conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models.",United States of America,,,"Definitely a new model, not a GPT-4 finetune",,,Unreleased,,,,
Yi-1.5-34B,Language,"Chat,Language modeling/generation,Translation,Code generation",,Open weights (restricted use),https://huggingface.co/01-ai/Yi-1.5-34B,,"Yi-1.5 is an upgraded version of Yi, delivering stronger performance in coding, math, reasoning, and instruction-following capability.",2024-05-13,01.AI,34000000000.0,34b,7.344e+23,6 FLOP / parameter / token * 34*10^9 parameters * 3.6*10^12 tokens = 7.344e+23 FLOP,Unspecified unreleased,assuming same as Yi 34 - Chinese and English dataset,3600000000000.0,"3.6T
""Yi-1.5 is an upgraded version of Yi. It is continuously pre-trained on Yi with a high-quality corpus of 500B tokens and fine-tuned on 3M diverse fine-tuning samples.""

3.6T total pre-trained tokens ",,,,Confident,"Yi-1.5 is an upgraded version of Yi. It is continuously pre-trained on Yi with a high-quality corpus of 500B tokens and fine-tuned on 3M diverse fine-tuning samples.

Compared with Yi, Yi-1.5 delivers stronger performance in coding, math, reasoning, and instruction-following capability, while still maintaining excellent capabilities in language understanding, commonsense reasoning, and reading comprehension.

Yi-1.5 comes in 3 model sizes: 34B, 9B, and 6B. For model details and benchmarks, see Model Card.",China,,,,,,Unreleased,"no training code

the model https://huggingface.co/01-ai/Yi-1.5-34B Apache 2.0

""If you create derivative works based on this model, please include the following attribution in your derivative works:""",,,
Yi-1.5-9B,Language,"Language modeling/generation,Question answering,Chat",,Open weights (unrestricted),https://huggingface.co/01-ai/Yi-1.5-9B,,Yi-1.5 is an upgraded version of Yi.,2024-05-13,01.AI,8830000000.0,8.83B (safetensors),1.90728e+23,6 FLOP / parameter / token * 8.83*10^9 parameters * 3.6*10^12 tokens = 1.90728e+23 FLOP,Unspecified unreleased,,3600000000000.0,3.6T pre-trained tokens,,,,Confident,"Yi-1.5 is an upgraded version of Yi. It is continuously pre-trained on Yi with a high-quality corpus of 500B tokens and fine-tuned on 3M diverse fine-tuning samples.

Compared with Yi, Yi-1.5 delivers stronger performance in coding, math, reasoning, and instruction-following capability, while still maintaining excellent capabilities in language understanding, commonsense reasoning, and reading comprehension.",China,,,,,,Unreleased,"https://huggingface.co/01-ai/Yi-1.5-9B
Apache 2.0",,,
Falcon 2 11B,Language,"Language modeling/generation,Question answering",,Open weights (restricted use),https://huggingface.co/tiiuae/falcon-11B ,,Falcon2-11B,2024-05-09,Technology Innovation Institute,11000000000.0,11B,3.6e+23,"trained on 5.5T tokens

6 * 11B * 5.5T = 3.6e23",RefinedWeb,"""Falcon2-11B was trained over 5,000B tokens of RefinedWeb, a high-quality filtered and deduplicated web dataset which we enhanced with curated corpora. It followed a four stage training strategy. The first three stages were focused on increasing the context length, from to 2048 to 4096 and finally to 8192 tokens. The last stage aimed to further enhance performance using only high quality data.""

Possibly an updated version of RefinedWeb, which only had 3.5T tokens when Falcon 1 was released? not clear.",5500000000000.0,5.5T tokens: https://falconllm.tii.ae/falcon-2.html ,1400.0,"roughly two months: https://huggingface.co/tiiuae/falcon-11B 

so ~1400 days",NVIDIA A100 SXM4 40 GB,Confident,"Falcon2-11B is an 11B parameters causal decoder-only model built by TII and trained on over 5,000B tokens of RefinedWeb enhanced with curated corpora. The model is made available under the TII Falcon License 2.0, the permissive Apache 2.0-based software license which includes an acceptable use policy that promotes the responsible use of AI.",United Arab Emirates,,,,,,Unreleased,"Open but has an acceptable use policy: https://falconllm-staging.tii.ae/falcon-2-acceptable-use-policy.html 

https://huggingface.co/tiiuae/falcon-11B",,,
DeepSeek-V2 (MoE-236B),Language,"Language modeling/generation,Chat,Code generation","DeepSeek-AI, Aixin Liu, Bei Feng, Bin Wang, Bingxuan Wang, Bo Liu, Chenggang Zhao, Chengqi Dengr, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Hanwei Xu, Hao Yang, Haowei Zhang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Li, Hui Qu, J.L. Cai, Jian Liang, Jianzhong Guo, Jiaqi Ni, Jiashi Li, Jin Chen, Jingyang Yuan, Junjie Qiu, Junxiao Song, Kai Dong, Kaige Gao, Kang Guan, Lean Wang, Lecong Zhang, Lei Xu, Leyi Xia, Liang Zhao, Liyue Zhang, Meng Li, Miaojun Wang, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Mingming Li, Ning Tian, Panpan Huang, Peiyi Wang, Peng Zhang, Qihao Zhu, Qinyu Chen, Qiushi Du, R.J. Chen, R.L. Jin, Ruiqi Ge, Ruizhe Pan, Runxin Xu, Ruyi Chen, S.S. Li, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shaoqing Wu, Shengfeng Ye, Shirong Ma, Shiyu Wang, Shuang Zhou, Shuiping Yu, Shunfeng Zhou, Size Zheng, T. Wang, Tian Pei, Tian Yuan, Tianyu Sun, W.L. Xiao, Wangding Zeng, Wei An, Wen Liu, Wenfeng Liang, Wenjun Gao, Wentao Zhang, X.Q. Li, Xiangyue Jin, Xianzu Wang, Xiao Bi, Xiaodong Liu, Xiaohan Wang, Xiaojin Shen, Xiaokang Chen, Xiaosha Chen, Xiaotao Nie, Xiaowen Sun",Open weights (restricted use),"https://arxiv.org/abs/2405.04434 
https://github.com/deepseek-ai/DeepSeek-V2 ",,"DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model",2024-05-07,DeepSeek,236000000000.0,"21B active params, 236B total",1.02e+24,21b active params * 8.1 trillion * 6 = 1.02e24,Unspecified unreleased,"""We construct a high-quality and multi-source pre-training corpus consisting of 8.1T tokens.
Compared with the corpus used in DeepSeek 67B (our previous release) (DeepSeek-AI, 2024), this
corpus features an extended amount of data, especially Chinese data, and higher data quality. We
first pretrain DeepSeek-V2 on the full pre-training corpus""",8100000000000.0,8.1 Trillion,,"172.8K GPU hours, wall time not stated",NVIDIA H800 SXM5,Confident,"We present DeepSeek-V2, a strong Mixture-of-Experts (MoE) language model characterized by economical training and efficient inference. It comprises 236B total parameters, of which 21B are activated for each token, and supports a context length of 128K tokens. DeepSeek-V2 adopts innovative architectures including Multi-head Latent Attention (MLA) and DeepSeekMoE. MLA guarantees efficient inference through significantly compressing the Key-Value (KV) cache into a latent vector, while DeepSeekMoE enables training strong models at an economical cost through sparse computation. Compared with DeepSeek 67B, DeepSeek-V2 achieves significantly stronger performance, and meanwhile saves 42.5% of training costs, reduces the KV cache by 93.3%, and boosts the maximum generation throughput to 5.76 times. We pretrain DeepSeek-V2 on a high-quality and multi-source corpus consisting of 8.1T tokens, and further perform Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) to fully unlock its potential. Evaluation results show that, even with only 21B activated parameters, DeepSeek-V2 and its chat versions still achieve top-tier performance among open-source models.",China,,,,,,Unreleased,open weights with harmful use restrictions: https://github.com/deepseek-ai/DeepSeek-V2/blob/main/LICENSE-MODEL ,,,
Multi-Token Prediction 7B,Language,Code generation,"Fabian Gloeckle, Badr Youbi Idrissi, Baptiste RoziÃ¨re, David Lopez-Paz, Gabriel Synnaeve",Open weights (non-commercial),https://arxiv.org/abs/2404.19737,,Better & Faster Large Language Models via Multi-token Prediction,2024-04-30,Facebook AI Research,6700000000.0,6.7B (â€œ7Bâ€),3.841092e+23,"""training all models reported in the paper required around 500K GPU hours of computation on hardware of type A100-80GB and H100.""
A100-80 GB peak FLOP/s [assumed fp16 precision]: 77970000000000
H100 peak FLOP/s [assumed SXM5 TensorCore]: 989000000000000
assuming 50/50 usage:

(77970000000000+989000000000000)*0.5*500000hours*3600s*0.3=2.880819e+23 for ALL models in the paper

assuming this model has taken around 40% of all used compute (https://docs.google.com/spreadsheets/d/1Yc-HAdYgn6e9SUIliMaQtvQscIsEZomLdXbWyYl_jfQ/edit?usp=sharing)

then it's assumed compute is 3.841092e+23 FLOPs","CodeContests,Unspecified unreleased",,250000000000.0,1T total tokens over 4 epochs (Table 1),,,"NVIDIA A100 SXM4 80 GB,NVIDIA H100 SXM5 80GB",Likely,"Large language models such as GPT and Llama are trained with a next-token prediction loss. In this work, we suggest that training language models to predict multiple future tokens at once results in higher sample efficiency. More specifically, at each position in the training corpus, we ask the model to predict the following n tokens using n independent output heads, operating on top of a shared model trunk. Considering multi-token prediction as an auxiliary training task, we measure improved downstream capabilities with no overhead in training time for both code and natural language models. The method is increasingly useful for larger model sizes, and keeps its appeal when training for multiple epochs. Gains are especially pronounced on generative benchmarks like coding, where our models consistently outperform strong baselines by several percentage points. Our 13B parameter models solves 12 % more problems on HumanEval and 17 % more on MBPP than comparable next-token models. Experiments on small algorithmic tasks demonstrate that multi-token prediction is favorable for the development of induction heads and algorithmic reasoning capabilities. As an additional benefit, models trained with 4-token prediction are up to 3 times faster at inference, even with large batch sizes.","United States of America,France",,,,,,Unreleased,"https://huggingface.co/facebook/multi-token-prediction

""weâ€™re releasing the pre-trained models for code completion under a non-commercial/research-only license.""",,,
Multi-Token Prediction 13B,Language,Code generation,"Fabian Gloeckle, Badr Youbi Idrissi, Baptiste RoziÃ¨re, David Lopez-Paz, Gabriel Synnaeve",Unreleased,https://arxiv.org/abs/2404.19737,,Better & Faster Large Language Models via Multi-token Prediction,2024-04-30,Facebook AI Research,13000000000.0,13B (Figure 1),1.5364368e+23,"""training all models reported in the paper required around 500K GPU hours of computation on hardware of type A100-80GB and H100.""
A100-80 GB peak FLOP/s [assumed fp16 precision]: 77970000000000
H100 peak FLOP/s [assumed SXM5 TensorCore]: 989000000000000
assuming 50/50 usage:

(77970000000000+989000000000000)*0.5*500000hours*3600s*0.3=2.880819e+23 for ALL models in the paper

assuming this model has taken around 16% of all used compute (https://docs.google.com/spreadsheets/d/1Yc-HAdYgn6e9SUIliMaQtvQscIsEZomLdXbWyYl_jfQ/edit?usp=sharing)

then it's assumed compute is 1.5364368e+23 FLOPs",,,209700000000.0, 209.7B (Table S13),,,"NVIDIA A100 SXM4 80 GB,NVIDIA H100 SXM5 80GB",Likely,"Large language models such as GPT and Llama are trained with a next-token prediction loss. In this work, we suggest that training language models to predict multiple future tokens at once results in higher sample efficiency. More specifically, at each position in the training corpus, we ask the model to predict the following n tokens using n independent output heads, operating on top of a shared model trunk. Considering multi-token prediction as an auxiliary training task, we measure improved downstream capabilities with no overhead in training time for both code and natural language models. The method is increasingly useful for larger model sizes, and keeps its appeal when training for multiple epochs. Gains are especially pronounced on generative benchmarks like coding, where our models consistently outperform strong baselines by several percentage points. Our 13B parameter models solves 12 % more problems on HumanEval and 17 % more on MBPP than comparable next-token models. Experiments on small algorithmic tasks demonstrate that multi-token prediction is favorable for the development of induction heads and algorithmic reasoning capabilities. As an additional benefit, models trained with 4-token prediction are up to 3 times faster at inference, even with large batch sizes.","United States of America,France",,,,,,Unreleased,,,,
Qwen1.5-110B,Language,"Chat,Language modeling/generation,Quantitative reasoning,Code generation,Translation",Qwen Team,Open weights (unrestricted),https://qwenlm.github.io/blog/qwen1.5-110b/?ref=upstract.com,,Qwen1.5-110B: The First 100B+ Model of the Qwen1.5 Series,2024-04-25,Alibaba,110000000000.0,"110B
",,lower bound is taken from Qwen1.5 72B training compute estimation,Unspecified unreleased,"We pretrained the models with a large amount of data, and we post-trained the models with both supervised finetuning and direct preference optimization.",,"A Qwen developer gave token counts for other models in the series at this github issue: https://github.com/QwenLM/Qwen2/issues/97
110B was asked but got no response.
7B, 14B, and 72B got 4T, 4T, and 3T tokens respectively.

In another issue from Qwen2: ""We are not authorized to share the details right now but the rough number is over 3T tokens for Qwen1.5 and over 7T tokens for Qwen2."" https://github.com/QwenLM/Qwen2/issues/562",,,,Confident,"The Qwen1.5-110B is the largest model in the Qwen1.5 series, and it is also the first one with over 100 billion parameters in the series. It demonstrates competitive performance against the very recently released SOTA model Llama-3-70B and it is significantly better than the 72B model. This tells us that there is still a lot of room in model size scaling for better performance. While the releease of Llama-3 indicates the significance of data scaling to an extremely large scale, we believe we can get the best of both worlds by scaling both data and model size in our future release. Stay tuned for Qwen2!",China,,,,,,Unreleased,https://huggingface.co/Qwen/Qwen1.5-110B,,,
Arctic,Language,"Language modeling/generation,Question answering,Code generation,Quantitative reasoning",Snowflake AI Research,Open weights (unrestricted),https://www.snowflake.com/en/blog/arctic-open-efficient-foundation-language-models-snowflake/,,"Snowflake Arctic: The Best LLM for Enterprise AI â€” Efficiently Intelligent, Truly Open",2024-04-24,Snowflake,480000000000.0,""" It combines a 10B dense transformer model with a residual 128x3.66B MoE MLP resulting in 480B total and 17B active parameters chosen using a top-2 gating.""",3.8347175e+23,"from the graph:
1x - Arctic
1.9X - Llama 3 8B (7.2Ã—10^23) ~ = Yi 34B (6.1e23) -> x = 3.2105263e+23
3X - Code Llama 70B (1.26e+24) -> x = 4.2e+23
17.5X - Llama 3 70B (7.861e24) -> x =4.492e+23

= 3.7975893e+23

Operation counting (17B active parameters): 
6ND = 6 FLOP / parameter / token * 17*10^9 parameters * 3.5*10^12 tokens = 3.57e+23 FLOP

geometric mean:(3.2105263e+23*4.492e+23*4.2e+23*3.57e+23)^(1/4) = 3.8347175e+23",,,3500000000000.0,"""Arctic was trained with a three-stage curriculum each with a different data composition focusing on generic skills in the first phase (1T Tokens), and enterprise-focused skills in the latter two phases (1.5T and 1T tokens). ""

1+1.5+1 = 3.5",,"""less than 3K GPU weeks""",,Confident,"Built by Snowflake, Arctic is a family of enterprise-grade LLMs with leading performance in enterprise intelligence and breakthrough efficiency. Snowflake Arctic is a truly open, Apache 2.0 licensed model.",United States of America,,,,,,Open source,Apache 2.0 license with ungated access to weights and code paired with open data recipe and research insights.,,,
phi-3-medium 14B,Language,"Chat,Language modeling/generation","Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, Alon Benhaim, Misha Bilenko, Johan Bjorck, SÃ©bastien Bubeck, Martin Cai, Caio CÃ©sar Teodoro Mendes, Weizhu Chen, Vishrav Chaudhary, Parul Chopra, Allie Del Giorno, Gustavo de Rosa, Matthew Dixon, Ronen Eldan, Dan Iter, Amit Garg, Abhishek Goswami, Suriya Gunasekar, Emman Haider, Junheng Hao, Russell J. Hewett, Jamie Huynh, Mojan Javaheripi, Xin Jin, Piero Kauffmann, Nikos Karampatziakis, Dongwoo Kim, Mahoud Khademi, Lev Kurilenko, James R. Lee, Yin Tat Lee, Yuanzhi Li, Chen Liang, Weishung Liu, Eric Lin, Zeqi Lin, Piyush Madan, Arindam Mitra, Hardik Modi, Anh Nguyen, Brandon Norick, Barun Patra, Daniel Perez-Becker, Thomas Portet, Reid Pryzant, Heyang Qin, Marko Radmilac, Corby Rosset, Sambudha Roy, Olatunji Ruwase, Olli Saarikivi, Amin Saied, Adil Salim, Michael Santacroce, Shital Shah, Ning Shang, Hiteshi Sharma, Xia Song, Masahiro Tanaka, Xin Wang, Rachel Ward, Guanhua Wang, Philipp Witte, Michael Wyatt, Can Xu, Jiahang Xu, Sonali Yadav, Fan Yang, Ziyi Yang, Donghan Yu, Chengruidong Zhang, Cyril Zhang, Jianwen Zhang, Li Lyna Zhang, Yi Zhang, Yue Zhang, Yunan Zhang, Xiren Zhou",Open weights (unrestricted),https://arxiv.org/abs/2404.14219,,Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone,2024-04-23,Microsoft,14000000000.0,14B,4.032e+23,counting operations: 6Ã—4.8Ã—10^12 tokens Ã— 14Ã—10^9 parameters â‰ˆ 4.032Ã—10^23 FLOPS,Phi-3 Dataset,"we also trained phi-3-medium, a model with 14B parameters using the same tokenizer and architecture of phi-3-mini, and trained on the same data for slightly more epochs (4.8T tokens total as for phi-3-small)
Knowledge cutoff date is October 2023, according to https://console.cloud.google.com/vertex-ai/publishers/microsoft/model-garden/phi3?hl=ja&inv=1&invt=AbxQQg.",4800000000000.0,,,,,Likely,"We introduce phi-3-mini, a 3.8 billion parameter language model trained on 3.3 trillion tokens, whose overall performance, as measured by both academic benchmarks and internal testing, rivals that of models such as Mixtral 8x7B and GPT-3.5 (e.g., phi-3-mini achieves 69% on MMLU and 8.38 on MT-bench), despite being small enough to be deployed on a phone. The innovation lies entirely in our dataset for training, a scaled-up version of the one used for phi-2, composed of heavily filtered web data and synthetic data. The model is also further aligned for robustness, safety, and chat format. We also provide some initial parameter-scaling results with a 7B and 14B models trained for 4.8T tokens, called phi-3-small and phi-3-medium, both significantly more capable than phi-3-mini (e.g., respectively 75% and 78% on MMLU, and 8.7 and 8.9 on MT-bench).",United States of America,,,,,,Unreleased,"MIT license for weights:
https://huggingface.co/microsoft/Phi-3-medium-128k-instruct ",,,
SenseChat 5.0,Language,"Chat,Language modeling/generation",SenseNova Team,Hosted access (no API),https://zhidx.com/p/421866.html,,,2024-04-23,SenseTime,600000000000.0,"This article claims the model is a 600B MoE:
https://www.sensetime.com/cn/news-detail/51168158?categoryId=72",,,,,1670000000000.0,"Words per gygabyte for Mandarin Chinese: 167M

10000*167000000 = 1670000000000

""is trained based on more than 10TB of tokens, covers a large amount of synthetic data""

However, a later news article from SenseTime itself said: ""In terms of data, SenseChat V5 uses a new generation of data production pipelines to produce 10T tokens of high-quality training data."" (It also says ""10T tokens"" in the original untranslated version).",,,,Likely,"SenseTime has newly upgraded its ""SenseNova 5.0"" large model system , and its comprehensive capabilities are fully comparable to GPT-4 Turbo .",Hong Kong,,,,,,Unreleased,,,,
phi-3-small 7.4B,Language,"Chat,Language modeling/generation","Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, Alon Benhaim, Misha Bilenko, Johan Bjorck, SÃ©bastien Bubeck, Martin Cai, Caio CÃ©sar Teodoro Mendes, Weizhu Chen, Vishrav Chaudhary, Parul Chopra, Allie Del Giorno, Gustavo de Rosa, Matthew Dixon, Ronen Eldan, Dan Iter, Amit Garg, Abhishek Goswami, Suriya Gunasekar, Emman Haider, Junheng Hao, Russell J. Hewett, Jamie Huynh, Mojan Javaheripi, Xin Jin, Piero Kauffmann, Nikos Karampatziakis, Dongwoo Kim, Mahoud Khademi, Lev Kurilenko, James R. Lee, Yin Tat Lee, Yuanzhi Li, Chen Liang, Weishung Liu, Eric Lin, Zeqi Lin, Piyush Madan, Arindam Mitra, Hardik Modi, Anh Nguyen, Brandon Norick, Barun Patra, Daniel Perez-Becker, Thomas Portet, Reid Pryzant, Heyang Qin, Marko Radmilac, Corby Rosset, Sambudha Roy, Olatunji Ruwase, Olli Saarikivi, Amin Saied, Adil Salim, Michael Santacroce, Shital Shah, Ning Shang, Hiteshi Sharma, Xia Song, Masahiro Tanaka, Xin Wang, Rachel Ward, Guanhua Wang, Philipp Witte, Michael Wyatt, Can Xu, Jiahang Xu, Sonali Yadav, Fan Yang, Ziyi Yang, Donghan Yu, Chengruidong Zhang, Cyril Zhang, Jianwen Zhang, Li Lyna Zhang, Yi Zhang, Yue Zhang, Yunan Zhang, Xiren Zhou",Open weights (unrestricted),https://arxiv.org/abs/2404.14219,,Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone,2024-04-23,Microsoft,7400000000.0,7.4B,2.1312e+23,6ND = 6 FLOP / parameter / token * 7.4B parameters * 4.8T tokens = 2.1312e+23 FLOP,,"""4.8T tokens total as for phi-3-small""
Knowledge cutoff date is October 2023, according to https://console.cloud.google.com/vertex-ai/publishers/microsoft/model-garden/phi3?hl=ja&inv=1&invt=AbxQQg.",4800000000000.0,,,,,Confident,"We introduce phi-3-mini, a 3.8 billion parameter language model trained on 3.3 trillion tokens, whose overall performance, as measured by both academic benchmarks and internal testing, rivals that of models such as Mixtral 8x7B and GPT-3.5 (e.g., phi-3-mini achieves 69% on MMLU and 8.38 on MT-bench), despite being small enough to be deployed on a phone. The innovation lies entirely in our dataset for training, a scaled-up version of the one used for phi-2, composed of heavily filtered web data and synthetic data. The model is also further aligned for robustness, safety, and chat format. We also provide some initial parameter-scaling results with a 7B and 14B models trained for 4.8T tokens, called phi-3-small and phi-3-medium, both significantly more capable than phi-3-mini (e.g., respectively 75% and 78% on MMLU, and 8.7 and 8.9 on MT-bench).",United States of America,,,,,,Unreleased,"MIT license

https://huggingface.co/microsoft/Phi-3-small-128k-instruct",,,
Phi-3.5-MoE,Language,"Language modeling/generation,Translation,Question answering,Code generation,Quantitative reasoning","Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, Alon Benhaim, Misha Bilenko, Johan Bjorck, SÃ©bastien Bubeck, Martin Cai, Caio CÃ©sar Teodoro Mendes, Weizhu Chen, Vishrav Chaudhary, Parul Chopra, Allie Del Giorno, Gustavo de Rosa, Matthew Dixon, Ronen Eldan, Dan Iter, Amit Garg, Abhishek Goswami, Suriya Gunasekar, Emman Haider, Junheng Hao, Russell J. Hewett, Jamie Huynh, Mojan Javaheripi, Xin Jin, Piero Kauffmann, Nikos Karampatziakis, Dongwoo Kim, Mahoud Khademi, Lev Kurilenko, James R. Lee, Yin Tat Lee, Yuanzhi Li, Chen Liang, Weishung Liu, Eric Lin, Zeqi Lin, Piyush Madan, Arindam Mitra, Hardik Modi, Anh Nguyen, Brandon Norick, Barun Patra, Daniel Perez-Becker, Thomas Portet, Reid Pryzant, Heyang Qin, Marko Radmilac, Corby Rosset, Sambudha Roy, Olatunji Ruwase, Olli Saarikivi, Amin Saied, Adil Salim, Michael Santacroce, Shital Shah, Ning Shang, Hiteshi Sharma, Xia Song, Masahiro Tanaka, Xin Wang, Rachel Ward, Guanhua Wang, Philipp Witte, Michael Wyatt, Can Xu, Jiahang Xu, Sonali Yadav, Fan Yang, Ziyi Yang, Donghan Yu, Chengruidong Zhang, Cyril Zhang, Jianwen Zhang, Li Lyna Zhang, Yi Zhang, Yue Zhang, Yunan Zhang, Xiren Zhou",Open weights (unrestricted),https://arxiv.org/abs/2404.14219,,Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone,2024-04-23,Microsoft,60800000000.0,"""Phi-3.5-MoE has 16x3.8B parameters with 6.6B active parameters when using 2 experts. The model is a mixture-of-expert decoder-only Transformer model using the tokenizer with vocabulary size of 32,064.""",3.0202896e+23,"512 GPUs * 989500000000000 FLOP / sec * 552 hours * 3600 sec / hour * 0.3 [assumed utilization] = 3.0202896e+23 FLOP

6 FLOP / token / parameter * 4900000000000 tokens * 6.6*10^9 active parameters = 1.9404e+23 FLOP (slightly less confidence than hardware estimation since the 6ND formula is less accurate for MoE)",Unspecified unreleased,"""Our training data includes a wide variety of sources, totaling 4.9 trillion tokens (including 10% multilingual), and is a combination of

publicly available documents filtered rigorously for quality, selected high-quality educational data, and code;
newly created synthetic, â€œtextbook-likeâ€ data for the purpose of teaching math, coding, common sense reasoning, general knowledge of the world (science, daily activities, theory of mind, etc.);
high quality chat format supervised data covering various topics to reflect human preferences on different aspects such as instruct-following, truthfulness, honesty and helpfulness.
We are focusing on the quality of data that could potentially improve the reasoning ability for the""
Knowledge cutoff date is October 2023, according to https://console.cloud.google.com/vertex-ai/publishers/microsoft/model-garden/phi3?hl=ja&inv=1&invt=AbxQQg.",4900000000000.0,Training data: 4.9T tokens,552.0,"GPUs: 512 H100-80G
Training time: 23 days

23 days * 24 hours / day = 552 hours",NVIDIA H100 SXM5 80GB,Confident,"We introduce phi-3-mini, a 3.8 billion parameter language model trained on 3.3 trillion tokens, whose overall performance, as measured by both academic benchmarks and internal testing, rivals that of models such as Mixtral 8x7B and GPT-3.5 (e.g., phi-3-mini achieves 69% on MMLU and 8.38 on MT-bench), despite being small enough to be deployed on a phone. Our training dataset is a scaled-up version of the one used for phi-2, composed of heavily filtered publicly available web data and synthetic data. The model is also further aligned for robustness, safety, and chat format. We also provide parameter-scaling results with a 7B, 14B models trained for 4.8T tokens, called phi-3-small, phi-3-medium, both significantly more capable than phi-3-mini (e.g., respectively 75%, 78% on MMLU, and 8.7, 8.9 on MT-bench). To enhance multilingual, multimodal, and long-context capabilities, we introduce three models in the phi-3.5 series: phi-3.5-mini, phi-3.5-MoE, and phi-3.5-Vision. The phi-3.5-MoE, a 16 x 3.8B MoE model with 6.6 billion active parameters, achieves superior performance in language reasoning, math, and code tasks compared to other open-source models of similar scale, such as Llama 3.1 and the Mixtral series, and on par with Gemini-1.5-Flash and GPT-4o-mini. Meanwhile, phi-3.5-Vision, a 4.2 billion parameter model derived from phi-3.5-mini, excels in reasoning tasks and is adept at handling both single-image and text prompts, as well as multi-image and text prompts.",United States of America,,,,512.0,,Unreleased,"https://huggingface.co/microsoft/Phi-3.5-MoE-instruct

MIT license (instruct model)",,,
Llama 3-70B,Language,"Chat,Language modeling/generation,Code generation",Aaditya Singh; Aaron Grattafiori; Abhimanyu Dubey; Abhinav Jauhri; Abhinav Pandey; Abhishek Kadian; Adam Kelsey; Adi Gangidi; Ahmad Al-Dahle; Amit Sangani; Ahuva Goldstand; Aiesha Letman; Ajay Menon; Akhil Mathur; Alan Schelten; Alex Vaughan; Amy Yang; Andrei Lupu; Andres Alvarado; Andrew Gallagher; Andrew Gu; Andrew Ho; Andrew Poulton; Andrew Ryan; Angela Fan; Ankit Ramchandani; Anthony Hartshorn; Archi Mitra; Archie Sravankumar; Artem Korenev; Arun Rao; Ashley Gabriel; Ashwin Bharambe; Assaf Eisenman; Aston Zhang; Ash JJhaveri; Aurelien Rodriguez; Austen Gregerson; Ava Spataru; Baptiste Roziere; Ben Maurer; Benjamin Leonhardi; Bernie Huang; Bhargavi Paranjape; Bing Liu; Binh Tang; Bobbie Chern; Brani Stojkovic; Brian Fuller; Catalina Mejia Arenas; Chao Zhou; Charlotte Caucheteux; Chaya Nayak; Ching-Hsiang Chu; Chloe Bi; Chris Cai; Chris Cox; Chris Marra; Chris McConnell; Christian Keller; Christoph Feichtenhofer; Christophe Touret; Chunyang Wu; Corinne Wong; Cristian Canton Ferrer; Damien Allonsius; Daniel Kreymer; Daniel Haziza; Daniel Li; Danielle Pintz; Danny Livshits; Danny Wyatt; David Adkins; David Esiobu; David Xu; Davide Testuggine; Delia David; Devi Parikh; Dhruv Choudhary; Dhruv Mahajan; Diana Liskovich; Diego Garcia-Olano; Diego Perino; Dieuwke Hupkes; Dingkang Wang; Dustin Holland; Egor Lakomkin; Elina Lobanova; Xiaoqing Ellen Tan; Emily Dinan; Eric Smith; Erik Brinkman; Esteban Arcaute; Filip Radenovic; Firat Ozgenel; Francesco Caggioni; Frank Seide; Frank Zhang; Gabriel Synnaeve; Gabriella Schwarz; Gabrielle Lee; Gada Badeer; Georgia Anderson; Graeme Nail; Gregoire Mialon; Guan Pang; Guillem Cucurell; Hailey Nguyen; Hamid Shojanazeri; Hannah Korevaar; Hannah Wang; Haroun Habeeb; Harrison Rudolph; Henry Aspegren; Hu Xu; Hugo Touvron; Iga Kozlowska; Igor Molybog; Igor Tufanov; Iliyan Zarov; Imanol Arrieta Ibarra; Irina-Elena Veliche; Isabel Kloumann; Ishan Misra; Ivan Evtimov; Jacob Xu; Jade Copet; Jake Weissman; Jan Geffert; Jana Vranes; Japhet Asher; Jason Park; Jay Mahadeokar; Jean-Baptiste Gaya; Jeet Shah; Jelmer van der Linde; Jennifer Chan; Jenny Hong; Jenya Lee; Jeremy Fu; Jeremy Teboul; Jianfeng Chi; Jianyu Huang; Jie Wang; Jiecao Yu; Joanna Bitton; Joe Spisak; Joelle Pineau; Jon Carvill; Jongsoo Park; Joseph Rocca; Joshua Johnstun; Junteng Jia; Kalyan Vasuden Alwala; Kam Hou U; Kate Plawiak; Kartikeya Upasani; Kaushik Veeraraghavan; Ke Li; Kenneth Heafield; Kevin Stone; Khalid El-Arini; Krithika Iyer; Kshitiz Malik; Kuenley Chiu; Kunal Bhalla; Kyle Huang; Lakshya Garg; Lauren Rantala-Yeary; Laurens van der Maaten; Lawrence Chen; Leandro Silva; Lee Bell; Lei Zhang; Liang Tan; Louis Martin; Lovish Madaan; Luca Wehrstedt; Lukas Blecher; Luke de Oliveira; Madeline Muzzi; Madian Khabsa; Manav Avlani; Mannat Singh; Manohar Paluri; Mark Zuckerberg; Marcin Kardas; Martynas Mankus; Mathew Oldham; Mathieu Rita; Matthew Lennie; Maya Pavlova; Meghan Keneally; Melanie Kambadur; Mihir Patel; Mikayel Samvelyan; Mike Clark; Mike Lewis; Min Si; Mitesh Kumar Singh; Mo Metanat; Mona Hassan; Naman Goyal; Narjes Torabi; Nicolas Usunier; Nikolay Bashlykov; Nikolay Bogoychev; Niladri Chatterji; Ning Dong; Oliver Aobo Yang; Olivier Duchenne; Onur Celebi; Parth Parekh; Patrick Alrassy; Paul Saab; Pavan Balaji; Pedro Rittner; Pengchuan Zhang; Pengwei Li; Petar Vasic; Peter Weng; Polina Zvyagina; Prajjwal Bhargava; Pratik Dubal; Praveen Krishnan; Punit Singh Koura; Qing He; Rachel Rodriguez; Ragavan Srinivasan; Rahul Mitra; Ramon Calderer; Raymond Li; Robert Stojnic; Roberta Raileanu; Robin Battey; Rocky Wang; Rohit Girdhar; Rohit Patel; Romain Sauvestre; Ronnie Polidoro; Roshan Sumbaly; Ross Taylor; Ruan Silva; Rui Hou; Rui Wang; Russ Howes; Ruty Rinott; Saghar Hosseini; Sai Jayesh Bondu; Samyak Datta; Sanjay Singh; Sara Chugh; Sargun Dhillon; Satadru Pan; Sean Bell; Sergey Edunov; Shaoliang Nie; Sharan Narang; Sharath Raparthy; Shaun Lindsay; Sheng Feng; Sheng Shen; Shenghao Lin; Shiva Shankar; Shruti Bhosale; Shun Zhang; Simon Vandenhende; Sinong Wang; Seohyun Sonia Kim; Soumya Batra; Sten Sootla; Steve Kehoe; Suchin Gururangan; Sumit Gupta; Sunny Virk; Sydney Borodinsky; Tamar Glaser; Tamar Herman; Tamara Best; Tara Fowler; Thomas Georgiou; Thomas Scialom; Tianhe Li; Todor Mihaylov; Tong Xiao; Ujjwal Karn; Vedanuj Goswami; Vibhor Gupta; Vignesh Ramanathan; Viktor Kerkez; Vinay Satish Kumar; Vincent Gonguet; Vish Vogeti; Vlad Poenaru; Vlad Tiberiu Mihailescu; Vladan Petrovic; Vladimir Ivanov; Wei Li; Weiwei Chu; Wenhan Xiong; Wenyin Fu; Wes Bouaziz; Whitney Meers; Will Constable; Xavier Martinet; Xiaojian Wu; Xinbo Gao; Xinfeng Xie; Xuchao Jia; Yaelle Goldschlag; Yann LeCun; Yashesh Gaur; Yasmine Babaei; Ye Qi; Yenda Li; Yi Wen; Yiwen Song; Youngjin Nam; Yuchen Hao; Yuchen Zhang; Yun Wang; Yuning Mao; Yuzi He; Zacharie Delpierre Coudert; Zachary DeVito; Zahra Hankir; Zhaoduo Wen; Zheng Yan; Zhengxing Chen; Zhenyu Yang; Zoe Papakipos,Open weights (restricted use),https://ai.meta.com/blog/meta-llama-3/,,Introducing Meta Llama 3: The most capable openly available LLM to date,2024-04-18,Meta AI,70000000000.0,,7.861e+24,"Arithmetic calculation:
6 * 15T tokens * 70B parameters = 6.3e24

GPU calculation:
https://huggingface.co/meta-llama/Meta-Llama-3-70B indicates training took 6.4M GPU-hours
We also know their larger scale training runs for 405B were getting between 0.38-0.41 MFU. Presumably the 70B model gets at least 0.43 utilization (405B has to be split across two nodes, while 70B should fit on one).
990 TFLOPS per GPU * 6.4 million GPU hours * 3600s * 0.43 = 9.808e24

Geometric mean: sqrt(6.3e24 * 9.808e24) = 7.861e24",Llama 3 dataset,"""Llama 3 is pretrained on over 15T tokens that were all collected from publicly available sources. Our training dataset is seven times larger than that used for Llama 2, and it includes four times more code. To prepare for upcoming multilingual use cases, over 5% of the Llama 3 pretraining dataset consists of high-quality non-English data that covers over 30 languages.""
Knowledge cutoff date is December 2023, according to https://huggingface.co/meta-llama/Meta-Llama-3-70B. ",15000000000000.0,,,,NVIDIA H100 SXM5 80GB,Confident,,United States of America,,,,,,Unreleased,"https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md

License A custom commercial license is available at: https://llama.meta.com/llama3/license",,,
Llama 3-8B,Language,"Chat,Language modeling/generation,Code generation,Question answering",Aaditya Singh; Aaron Grattafiori; Abhimanyu Dubey; Abhinav Jauhri; Abhinav Pandey; Abhishek Kadian; Adam Kelsey; Adi Gangidi; Ahmad Al-Dahle; Amit Sangani; Ahuva Goldstand; Aiesha Letman; Ajay Menon; Akhil Mathur; Alan Schelten; Alex Vaughan; Amy Yang; Andrei Lupu; Andres Alvarado; Andrew Gallagher; Andrew Gu; Andrew Ho; Andrew Poulton; Andrew Ryan; Angela Fan; Ankit Ramchandani; Anthony Hartshorn; Archi Mitra; Archie Sravankumar; Artem Korenev; Arun Rao; Ashley Gabriel; Ashwin Bharambe; Assaf Eisenman; Aston Zhang; Ash JJhaveri; Aurelien Rodriguez; Austen Gregerson; Ava Spataru; Baptiste Roziere; Ben Maurer; Benjamin Leonhardi; Bernie Huang; Bhargavi Paranjape; Bing Liu; Binh Tang; Bobbie Chern; Brani Stojkovic; Brian Fuller; Catalina Mejia Arenas; Chao Zhou; Charlotte Caucheteux; Chaya Nayak; Ching-Hsiang Chu; Chloe Bi; Chris Cai; Chris Cox; Chris Marra; Chris McConnell; Christian Keller; Christoph Feichtenhofer; Christophe Touret; Chunyang Wu; Corinne Wong; Cristian Canton Ferrer; Damien Allonsius; Daniel Kreymer; Daniel Haziza; Daniel Li; Danielle Pintz; Danny Livshits; Danny Wyatt; David Adkins; David Esiobu; David Xu; Davide Testuggine; Delia David; Devi Parikh; Dhruv Choudhary; Dhruv Mahajan; Diana Liskovich; Diego Garcia-Olano; Diego Perino; Dieuwke Hupkes; Dingkang Wang; Dustin Holland; Egor Lakomkin; Elina Lobanova; Xiaoqing Ellen Tan; Emily Dinan; Eric Smith; Erik Brinkman; Esteban Arcaute; Filip Radenovic; Firat Ozgenel; Francesco Caggioni; Frank Seide; Frank Zhang; Gabriel Synnaeve; Gabriella Schwarz; Gabrielle Lee; Gada Badeer; Georgia Anderson; Graeme Nail; Gregoire Mialon; Guan Pang; Guillem Cucurell; Hailey Nguyen; Hamid Shojanazeri; Hannah Korevaar; Hannah Wang; Haroun Habeeb; Harrison Rudolph; Henry Aspegren; Hu Xu; Hugo Touvron; Iga Kozlowska; Igor Molybog; Igor Tufanov; Iliyan Zarov; Imanol Arrieta Ibarra; Irina-Elena Veliche; Isabel Kloumann; Ishan Misra; Ivan Evtimov; Jacob Xu; Jade Copet; Jake Weissman; Jan Geffert; Jana Vranes; Japhet Asher; Jason Park; Jay Mahadeokar; Jean-Baptiste Gaya; Jeet Shah; Jelmer van der Linde; Jennifer Chan; Jenny Hong; Jenya Lee; Jeremy Fu; Jeremy Teboul; Jianfeng Chi; Jianyu Huang; Jie Wang; Jiecao Yu; Joanna Bitton; Joe Spisak; Joelle Pineau; Jon Carvill; Jongsoo Park; Joseph Rocca; Joshua Johnstun; Junteng Jia; Kalyan Vasuden Alwala; Kam Hou U; Kate Plawiak; Kartikeya Upasani; Kaushik Veeraraghavan; Ke Li; Kenneth Heafield; Kevin Stone; Khalid El-Arini; Krithika Iyer; Kshitiz Malik; Kuenley Chiu; Kunal Bhalla; Kyle Huang; Lakshya Garg; Lauren Rantala-Yeary; Laurens van der Maaten; Lawrence Chen; Leandro Silva; Lee Bell; Lei Zhang; Liang Tan; Louis Martin; Lovish Madaan; Luca Wehrstedt; Lukas Blecher; Luke de Oliveira; Madeline Muzzi; Madian Khabsa; Manav Avlani; Mannat Singh; Manohar Paluri; Mark Zuckerberg; Marcin Kardas; Martynas Mankus; Mathew Oldham; Mathieu Rita; Matthew Lennie; Maya Pavlova; Meghan Keneally; Melanie Kambadur; Mihir Patel; Mikayel Samvelyan; Mike Clark; Mike Lewis; Min Si; Mitesh Kumar Singh; Mo Metanat; Mona Hassan; Naman Goyal; Narjes Torabi; Nicolas Usunier; Nikolay Bashlykov; Nikolay Bogoychev; Niladri Chatterji; Ning Dong; Oliver Aobo Yang; Olivier Duchenne; Onur Celebi; Parth Parekh; Patrick Alrassy; Paul Saab; Pavan Balaji; Pedro Rittner; Pengchuan Zhang; Pengwei Li; Petar Vasic; Peter Weng; Polina Zvyagina; Prajjwal Bhargava; Pratik Dubal; Praveen Krishnan; Punit Singh Koura; Qing He; Rachel Rodriguez; Ragavan Srinivasan; Rahul Mitra; Ramon Calderer; Raymond Li; Robert Stojnic; Roberta Raileanu; Robin Battey; Rocky Wang; Rohit Girdhar; Rohit Patel; Romain Sauvestre; Ronnie Polidoro; Roshan Sumbaly; Ross Taylor; Ruan Silva; Rui Hou; Rui Wang; Russ Howes; Ruty Rinott; Saghar Hosseini; Sai Jayesh Bondu; Samyak Datta; Sanjay Singh; Sara Chugh; Sargun Dhillon; Satadru Pan; Sean Bell; Sergey Edunov; Shaoliang Nie; Sharan Narang; Sharath Raparthy; Shaun Lindsay; Sheng Feng; Sheng Shen; Shenghao Lin; Shiva Shankar; Shruti Bhosale; Shun Zhang; Simon Vandenhende; Sinong Wang; Seohyun Sonia Kim; Soumya Batra; Sten Sootla; Steve Kehoe; Suchin Gururangan; Sumit Gupta; Sunny Virk; Sydney Borodinsky; Tamar Glaser; Tamar Herman; Tamara Best; Tara Fowler; Thomas Georgiou; Thomas Scialom; Tianhe Li; Todor Mihaylov; Tong Xiao; Ujjwal Karn; Vedanuj Goswami; Vibhor Gupta; Vignesh Ramanathan; Viktor Kerkez; Vinay Satish Kumar; Vincent Gonguet; Vish Vogeti; Vlad Poenaru; Vlad Tiberiu Mihailescu; Vladan Petrovic; Vladimir Ivanov; Wei Li; Weiwei Chu; Wenhan Xiong; Wenyin Fu; Wes Bouaziz; Whitney Meers; Will Constable; Xavier Martinet; Xiaojian Wu; Xinbo Gao; Xinfeng Xie; Xuchao Jia; Yaelle Goldschlag; Yann LeCun; Yashesh Gaur; Yasmine Babaei; Ye Qi; Yenda Li; Yi Wen; Yiwen Song; Youngjin Nam; Yuchen Hao; Yuchen Zhang; Yun Wang; Yuning Mao; Yuzi He; Zacharie Delpierre Coudert; Zachary DeVito; Zahra Hankir; Zhaoduo Wen; Zheng Yan; Zhengxing Chen; Zhenyu Yang; Zoe Papakipos,Open weights (restricted use),https://ai.meta.com/blog/meta-llama-3/,,Introducing Meta Llama 3: The most capable openly available LLM to date,2024-04-18,Meta AI,8000000000.0,,7.2e+23,"Counting operations
15000000000000 tokens*8000000000.00 parameters*6 FLOP / token / parameter = 7.2Ã—10^23 FLOP

GPU calculation
400 TFLOPS per GPU * 1.3M GPU hours * 3600s=1.872Ã—10^24 
(it is not confident that 400 TFLOPs applies to the Llama 3-8B training run)",Llama 3 dataset,"Knowledge cutoff date is March 2023, according to https://huggingface.co/meta-llama/Meta-Llama-3-70B. ",15000000000000.0,,,,NVIDIA H100 SXM5 80GB,Confident,"Today, weâ€™re introducing Meta Llama 3, the next generation of our state-of-the-art open source large language model.
Llama 3 models will soon be available on AWS, Databricks, Google Cloud, Hugging Face, Kaggle, IBM WatsonX, Microsoft Azure, NVIDIA NIM, and Snowflake, and with support from hardware platforms offered by AMD, AWS, Dell, Intel, NVIDIA, and Qualcomm.
Weâ€™re dedicated to developing Llama 3 in a responsible way, and weâ€™re offering various resources to help others use it responsibly as well. This includes introducing new trust and safety tools with Llama Guard 2, Code Shield, and CyberSec Eval 2.
In the coming months, we expect to introduce new capabilities, longer context windows, additional model sizes, and enhanced performance, and weâ€™ll share the Llama 3 research paper.
Meta AI, built with Llama 3 technology, is now one of the worldâ€™s leading AI assistants that can boost your intelligence and lighten your loadâ€”helping you learn, get things done, create content, and connect to make the most out of every moment. You can try Meta AI here.",United States of America,,,,,,Unreleased,"https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md

License A custom commercial license is available at: https://llama.meta.com/llama3/license",,,
Reka Edge,"Multimodal,Language,Vision,Video,Speech","Chat,Language modeling/generation,Image captioning,Code generation,Code autocompletion,Question answering,Visual question answering,Video description,Speech recognition (ASR),Speech-to-text","Aitor Ormazabal, Che Zheng, Cyprien de Masson d'Autume, Dani Yogatama, Deyu Fu, Donovan Ong, Eric Chen, Eugenie Lamprecht, Hai Pham, Isaac Ong, Kaloyan Aleksiev, Lei Li, Matthew Henderson, Max Bain, Mikel Artetxe, Nishant Relan, Piotr Padlewski, Qi Liu, Ren Chen, Samuel Phua, Yazheng Yang, Yi Tay, Yuqi Wang, Zhongkai Zhu, Zhihui Xie",API access,https://arxiv.org/abs/2404.12387,,"Reka Core, Flash, and Edge: A Series of Powerful Multimodal Language Models",2024-04-18,Reka AI,7000000000.0,7B dense,1.89e+23,"6 FLOP / parameter / token * 7 * 10^9 parameters * 4.5 * 10^12 text tokens = 1.89e+23 FLOP 

the calculation above doesn't account for multimodal training, total training compute could be 2-5 times higher

""Reka Flash and Edge were trained on several hundreds of H100s across a period of several weeks.""

989400000000000 FLOP / GPU / sec * 300 GPUs [assumption] * 3 weeks [assumption] * 168 hours / week * 3600 sec / hour * 0.3 [assumed utilization] = 1.6156506e+23 FLOP (same OOM)",Unspecified unreleased,"approximately 25% of our pretraining data is code related, and 30% are
STEM related. Approximately 25% of the data is web crawl. About 10% of our data has some relation to
math

Multimodal Data: The multimodal training data comprises large collections of images, videos, documents,
and webpages",,4.5T text tokens + unknown amount of multimodal data,,,"NVIDIA A100,NVIDIA H100 SXM5 80GB",Likely,"We introduce Reka Core, Flash, and Edge, a series of powerful multimodal language models trained from scratch by Reka. Reka models are able to process and reason with text, images, video, and audio inputs. This technical report discusses details of training some of these models and provides comprehensive evaluation results. We show that Reka Edge and Reka Flash are not only state-of-the-art but also outperform many much larger models, delivering outsized values for their respective compute class. Meanwhile, our most capable and largest model, Reka Core, approaches the best frontier models on both automatic evaluations and blind human evaluations. On image question answering benchmarks (e.g. MMMU, VQAv2), Core performs competitively to GPT4-V. Meanwhile, on multimodal chat, Core ranks as the second most preferred model under a blind third-party human evaluation setup, outperforming other models such as Claude 3 Opus. On text benchmarks, Core not only performs competitively to other frontier models on a set of well-established benchmarks (e.g. MMLU, GSM8K) but also outperforms GPT4-0613 on human evaluation. On video question answering (Perception-Test), Core outperforms Gemini Ultra. Models are shipped in production at this http URL . A showcase of non cherry picked qualitative examples can also be found at this http URL .",United States of America,,,,,,Unreleased,,,,
Mixtral 8x22B,Language,"Language modeling/generation,Code generation,Translation,Quantitative reasoning,Question answering","Albert Jiang, Alexandre Sablayrolles, Alexis Tacnet, Antoine Roux, Arthur Mensch, Audrey Herblin-Stoop, Baptiste Bout, Baudouin de Monicault,Blanche Savary, Bam4d, Caroline Feldman, Devendra Singh Chaplot, Diego de las Casas, Eleonore Arcelin, Emma Bou Hanna, Etienne Metzger, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Harizo Rajaona, Jean-Malo Delignon, Jia Li, Justus Murke, Louis Martin, Louis Ternon, Lucile Saulnier, LÃ©lio Renard Lavaud, Margaret Jennings, Marie Pellat, Marie Torelli, Marie-Anne Lachaux, Nicolas Schuhl, Patrick von Platen, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Thibaut Lavril, TimothÃ©e Lacroix, ThÃ©ophile Gervet, Thomas Wang, Valera Nemychnikova, William El Sayed, William Marshall",Open weights (unrestricted),https://mistral.ai/news/mixtral-8x22b/,,Mixtral 8x22B,2024-04-17,Mistral AI,141000000000.0,"141B params, 39B active: https://mistral.ai/news/mixtral-8x22b/ ",2.34e+24,"Assuming the model was trained on ~1-10 trillions of tokens (same OOM as the models from the comparison in Figure 1. Llama 2 was trained on 2T tokens) + Mistral Small 3 was trained on 8T of tokens, we can estimate training compute with ""speculative"" confidence:

6 FLOP / token / parameter * 39 * 10^9 active parameters * 10*10^12 tokens [speculatively] = 2.34e+24 FLOP",Unspecified unreleased,,,,,,,Speculative,"Mixtral 8x22B is our latest open model. It sets a new standard for performance and efficiency within the AI community. It is a sparse Mixture-of-Experts (SMoE) model that uses only 39B active parameters out of 141B, offering unparalleled cost efficiency for its size.

Mixtral 8x22B comes with the following strengths:

- It is fluent in English, French, Italian, German, and Spanish
- It has strong mathematics and coding capabilities
- It is natively capable of function calling; along with the constrained output mode implemented on la Plateforme, this enables application development and tech stack modernisation at scale
- Its 64K tokens context window allows precise information recall from large documents
",France,,,,,,Unreleased,Apache 2.0 license,,,
abab6.5,Language,Language modeling/generation,,API access,https://www.minimaxi.com/en/news/abab65-series,,The General Large Language Model abab6.5 Series,2024-04-17,MiniMax,1000000000000.0,,,,Unspecified unreleased,,,,,,,Confident,"The abab6.5 series includes two models: abab6.5 and abab6.5s. Abab6.5 features a trillion parameters and supports a context length of 200k tokens; abab6.5s uses the same training techniques and data as abab6.5 but is more efficient, supporting a 200k token context length and capable of processing nearly 30,000 words within a second. With significant cost advantages, an industry-leading context length and fast speed, MiniMax's abab6.5 series of LLMs offer unique value propositions.",China,,,,,,Unreleased,,,,
OLMo 1.7-7B ,Language,"Language modeling/generation,Chat,Question answering",,Open weights (unrestricted),https://allenai.org/blog/olmo-1-7-7b-a-24-point-improvement-on-mmlu-92b43f7d269d,,OLMo 1.7â€“7B: A 24 point improvement on MMLU,2024-04-17,Allen Institute for AI,7000000000.0,,,,Dolma 1.7,,2300000000000.0,The full Dolma 1.7 collection is 2.3 trillion tokens summing across all sources.,,,,,"Today, we've released an updated version of our 7 billion parameter Open Language Model, OLMo 1.7â€“7B. This model scores 52 on MMLU, sitting above Llama 2â€“7B and approaching Llama 2â€“13B, and outperforms Llama 2â€“13B on GSM8K (see below).

OLMo 1.7â€“7Bâ€Š-â€Šcreated on the path towards our upcoming 70 billion parameter modelâ€Š-â€Šshowcases a longer context length, up from 2048 to 4096 tokens. It exhibits higher benchmark performance due to a combination of improved data quality, a new two-stage training procedure, and architectural improvements. Get the model on Hugging Face, licensed with Apache 2.0. The training data, Dolma 1.7, is licensed under ODC-BY, as recently announced.",United States of America,,,,,,Open source,"License: The code and model are released under Apache 2.0.

Weights https://huggingface.co/allenai/OLMo-7B
Code https://github.com/allenai/OLMo",,,
Reka Core,"Multimodal,Language,Vision,Video,Speech","Chat,Language modeling/generation,Image captioning,Code generation,Code autocompletion,Question answering,Visual question answering,Video description,Speech recognition (ASR),Speech-to-text,Quantitative reasoning","Aitor Ormazabal, Che Zheng, Cyprien de Masson d'Autume, Dani Yogatama, Deyu Fu, Donovan Ong, Eric Chen, Eugenie Lamprecht, Hai Pham, Isaac Ong, Kaloyan Aleksiev, Lei Li, Matthew Henderson, Max Bain, Mikel Artetxe, Nishant Relan, Piotr Padlewski, Qi Liu, Ren Chen, Samuel Phua, Yazheng Yang, Yi Tay, Yuqi Wang, Zhongkai Zhu, Zhihui Xie",API access,https://arxiv.org/abs/2404.12387,,"Reka Core, Flash, and Edge: A Series of Powerful
Multimodal Language Models",2024-04-15,Reka AI,67000000000.0,,8.400010000000001e+24,"No direct information about Reka Core model (""Reka Core has not finished training and is still improving."")

The smaller dense model Reka Flash has 21B parameters and was trained on 5 trillion language tokens.

There is information about compute: ""Our setup comprises of clusters from a mixture of vendors with our peak compute being approximately
2.5K H100s and 2.5K A100s.""

If we assume 2 months of training with 2.5k H100s and 2.5k A100s at utilization 0.5 we get 8.4e24 FLOP (2500*9.9e14+2500*3.12e14)*60*60*24*60*0.5.","Wikipedia,Unspecified unreleased","The training data comprises a mixture of publicly available and proprietary/licensed datasets with a dataset knowledge cutoff of November 2023. The dataset ingested by our model comprises of text, images, videos, and audio clips. Reka Flash and Reka Edge were trained on approximately 5 trillion and 4.5 trillion extensively deduplicated and filtered language tokens, respectively. While the classification of corpora is not strictly defined to one class or category, approximately 25% of our pretraining data is code related, and 30% are STEM related. Approximately 25% of the data is web crawl. About 10% of our data has some relation to math.

Multimodal Data: The multimodal training data comprises large collections of images, videos, documents,
and webpages",,,,,"NVIDIA A100,NVIDIA H100 SXM5 80GB",Speculative,"We introduce Reka Core, Flash, and Edge, a series of powerful multimodal language models trained from scratch by Reka. Reka models are able to process and reason with text, images, video, and audio inputs. This technical report discusses details of training some of these models and provides comprehensive evaluation results. We show that Reka Edge and Reka Flash are not only state-of-the-art but also outperform many much larger models, delivering outsized values for their respective compute class. Meanwhile, our most capable and largest model, Reka Core, approaches the best frontier models on both automatic evaluations and blind human evaluations. On image question answering benchmarks (e.g. MMMU, VQAv2), Core performs competitively to GPT4-V. Meanwhile, on multimodal chat, Core ranks as the second most preferred model under a blind third-party human evaluation setup, outperforming other models such as Claude 3 Opus. On text benchmarks, Core not only performs competitively to other frontier models on a set of well-established benchmarks (e.g. MMLU, GSM8K) but also outperforms GPT4-0613 on human evaluation. On video question answering (Perception-Test), Core outperforms Gemini Ultra. Models are shipped in production at this http URL . A showcase of non cherry picked qualitative examples can also be found at this http URL .",United States of America,,,,,,Unreleased,,,,
Reka Flash,"Multimodal,Language,Vision,Video,Speech","Chat,Language modeling/generation,Image captioning,Code generation,Code autocompletion,Question answering,Visual question answering,Video description,Speech recognition (ASR),Speech-to-text","Aitor Ormazabal, Che Zheng, Cyprien de Masson d'Autume, Dani Yogatama, Deyu Fu, Donovan Ong, Eric Chen, Eugenie Lamprecht, Hai Pham, Isaac Ong, Kaloyan Aleksiev, Lei Li, Matthew Henderson, Max Bain, Mikel Artetxe, Nishant Relan, Piotr Padlewski, Qi Liu, Ren Chen, Samuel Phua, Yazheng Yang, Yi Tay, Yuqi Wang, Zhongkai Zhu, Zhihui Xie",API access,https://arxiv.org/abs/2404.12387,,"Reka Core, Flash, and Edge: A Series of Powerful
Multimodal Language Models",2024-04-15,Reka AI,21000000000.0,21B dense,6.3e+23,"Reka Flash has 21B parameters and was trained on 5 trillion language tokens (+ unknown amount of multimodal data)

6 FLOP / token / parameter * 21B parameters * 5 trillion text tokens = 6.3 Ã— 10^23 FLOP

OOM agrees with GPU details: ""Reka Flash and Edge were trained on several hundreds of H100s across a period of several weeks.""

3 weeks * 300 H100s * 7 day/week * 24 hour/day * 3600 s/day * 9.9e14 FLOP/GPU-s * 0.3 [assumed utilization]=  1.6156506e+23 FLOP

Not enough info to estimate SFT and RLHF post-training FLOPs.",Unspecified unreleased,"The training data comprises a mixture of publicly available and proprietary/licensed datasets with a dataset
knowledge cutoff of November 2023. The dataset ingested by our model comprises of text, images, videos, and
audio clips. Reka Flash and Reka Edge were trained on approximately 5 trillion and 4.5 trillion extensively
deduplicated and filtered language tokens, respectively. While the classification of corpora is not strictly
defined to one class or category, approximately 25% of our pretraining data is code related, and 30% are
STEM related. Approximately 25% of the data is web crawl. About 10% of our data has some relation to
math.

Multimodal Data: The multimodal training data comprises large collections of images, videos, documents,
and webpages",,5T text tokens + unknown amount of multimodal data,,,"NVIDIA A100,NVIDIA H100 SXM5 80GB",Likely,"We introduce Reka Core, Flash, and Edge, a series of powerful multimodal language models trained from scratch by Reka. Reka models are able to process and reason with text, images, video, and audio inputs. This technical report discusses details of training some of these models and provides comprehensive evaluation results. We show that Reka Edge and Reka Flash are not only state-of-the-art but also outperform many much larger models, delivering outsized values for their respective compute class. Meanwhile, our most capable and largest model, Reka Core, approaches the best frontier models on both automatic evaluations and blind human evaluations. On image question answering benchmarks (e.g. MMMU, VQAv2), Core performs competitively to GPT4-V. Meanwhile, on multimodal chat, Core ranks as the second most preferred model under a blind third-party human evaluation setup, outperforming other models such as Claude 3 Opus. On text benchmarks, Core not only performs competitively to other frontier models on a set of well-established benchmarks (e.g. MMLU, GSM8K) but also outperforms GPT4-0613 on human evaluation. On video question answering (Perception-Test), Core outperforms Gemini Ultra. Models are shipped in production at this http URL . A showcase of non cherry picked qualitative examples can also be found at this http URL .",United States of America,,,,,,Unreleased,,,,
Stable LM 2 12B,Language,"Language modeling/generation,Translation",,Open weights (restricted use),"https://stability.ai/news/introducing-stable-lm-2-12b
https://huggingface.co/stabilityai/stablelm-2-12b",,Introducing Stable LM 2 12B,2024-04-08,Stability AI,12143605760.0,Precise number given in HF model card,2.91e+23,"2* 12143605760 params * 3* 2T tokens * 2 epochs = 2.91e23. 
Trained on 384 H100s (AWS P5 instances).","RefinedWeb,RedPajama-Data,The Pile,StarCoder,CulturaX","The dataset is comprised of a filtered mixture of open-source large-scale datasets available on the HuggingFace Hub: Falcon RefinedWeb extract (Penedo et al., 2023), RedPajama-Data (Together Computer., 2023) and The Pile (Gao et al., 2020) both without the Books3 subset, and StarCoder (Li et al., 2023). We further supplement our training with multi-lingual data from CulturaX (Nguyen et al., 2023) and, in particular, from its OSCAR corpora, as well as restructured data in the style of Yuan & Liu (2022).",2000000000000.0,2T tokens,,,NVIDIA H100 SXM5 80GB,Confident,"Introducing the latest additions to our Stable LM 2 language model series: a 12 billion parameter base model and an instruction-tuned variant, trained on 2 trillion tokens in seven languages: English, Spanish, German, Italian, French, Portuguese, and Dutch. This medium-sized model balances strong performance, efficiency, memory requirements, and speed, following our established Stable LM 2 1.6B framework as detailed in our previously released technical report. With this release, weâ€™re extending our model range, offering a transparent and powerful tool for developers to innovate in AI language technology. Soon, we plan to introduce a long-context variant of these models which will be available on Hugging Face upon release.

From Hugging Face:
Stable LM 2 12B is a 12.1 billion parameter decoder-only language model pre-trained on 2 trillion tokens of diverse multilingual and code datasets for two epochs.",United Kingdom of Great Britain and Northern Ireland,,,,,,Open source,"Requires Stability AI Membership. Free for non-commercial use, $20/month for commercial use if less than $1M in annual revenue, $1M in institutional funding, and 1M monthly active users. 

Apache 2.0 license for repo, which includes detailed hyperparams and training details: https://github.com/Stability-AI/StableLM/blob/main/LICENSE  ",,,
Command R+,Language,"Language modeling/generation,Language generation,Translation,Code autocompletion",,Open weights (non-commercial),https://txt.cohere.com/command-r-plus-microsoft-azure/,,,2024-04-04,"Cohere,Cohere for AI",104000000000.0,,,,Unspecified unreleased,,,,,,,Confident,"C4AI Command R+ is an open weights research release of a 104B billion parameter model with highly advanced capabilities, this includes Retrieval Augmented Generation (RAG) and tool use to automate sophisticated tasks. The tool use in this model generation enables multi-step tool use which allows the model to combine multiple tools over multiple steps to accomplish difficult tasks. C4AI Command R+ is a multilingual model evaluated in 10 languages for performance: English, French, Spanish, Italian, German, Brazilian Portuguese, Japanese, Korean, Arabic, and Simplified Chinese. Command R+ is optimized for a variety of use cases including reasoning, summarization, and question answering.","Canada,Canada",,,,,,Unreleased,"cc-by-nc license, weights only: https://huggingface.co/CohereForAI/c4ai-command-r-plus ",,,
Viking,Language,"Language modeling/generation,Language generation,Translation",,Open weights (unrestricted),https://huggingface.co/LumiOpen/Viking-33B,,"Viking 33B is a 33B parameter decoder-only transformer pretrained on Finnish, English, Swedish, Danish, Norwegian, Icelandic and code. It is being trained on 2 trillion tokens (700B billion as of this release). Viking 33B is a fully open source model and is made available under the Apache 2.0 License.",2024-04-04,"Silo AI,University of Turku",33000000000.0,33B,2.574e+23,"Plan is to train on 2 trillion tokens, but most recent release is at 1.3T
6 * 33B * 1.3 trillion = 2.574E23",Unspecified unreleased,,2000000000000.0,"Viking is being trained on a 2 trillion token mixed dataset of English, Finnish, Swedish, Danish, Norwegian, Icelandic and code.",,,AMD Radeon Instinct MI250X,Confident,,"Finland,Finland",,,,1024.0,,Open source,code here: https://github.com/LumiOpen/Megatron-DeepSpeed/blob/main/pretrain_viking_33B.sh ,,,
Grok-1.5,Language,"Language modeling,Chat",,Hosted access (no API),https://x.ai/blog/grok-1.5,,"Introducing Grok-1.5, our latest model capable of long context understanding and advanced reasoning. Grok-1.5 will be available to our early testers and existing Grok users on the ð• platform in the coming days.",2024-03-28,xAI,,,9.26e+24,"Lower bound is taken from Grok-1 estimation
Upper bound is taken from Grok-2 estimation

geometric mean: 
sqrt(2.90000000001*29.6)*10^24 = 9.26e+24",Unspecified unreleased,,,,,,,Speculative,,United States of America,,,,,,Unreleased,"Musk noted that Grok-1.5 will power xAIâ€™s ChatGPT-challenging chatbot on the X platform, while Grok-2, the successor of the new model, is still in the training phase",,,
Grok-1.5V,"Multimodal,Language,Vision","Language modeling,Chat,Image captioning,Code autocompletion,Code generation,Visual question answering",,Hosted access (no API),https://x.ai/blog/grok-1.5v,,"Introducing Grok-1.5V, our first-generation multimodal model. In addition to its strong text capabilities, Grok can now process a wide variety of visual information, including documents, diagrams, charts, screenshots, and photographs. Grok-1.5V will be available soon to our early testers and existing Grok users.",2024-03-28,xAI,,,,Lower bound is taken from Grok-1 estimation,Unspecified unreleased,,,,,,,Speculative,,United States of America,,,,,,Unreleased,,,,
YandexGPT 3,Language,"Language modeling/generation,Chat,Question answering,Text summarization,Table tasks",,API access,https://ya.ru/ai/gpt-3,,,2024-03-28,Yandex,,,,,Unspecified unreleased,,,,,,,Unknown,,Russia,,,,,,Unreleased,,,,
DBRX,Language,"Chat,Code generation",Mosaic Research Team,Open weights (restricted use),https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm,,Introducing DBRX: A New State-of-the-Art Open LLM,2024-03-27,Databricks,132000000000.0,132B mixture of experts. 36B parameters active per inference,2.6e+24,"Mixture of Experts (MoE)

36 billion active params * 12 trillion tokens * 6 ~= 2.6e24
https://www.wolframalpha.com/input?i=6+FLOP+*+36+billion+*+12+trillion

also, it was trained on 3072 NVIDIA H100s, but with an unclear timeframe (end-end process was three months, including evals and red-teaming).",,"12T tokens, text and code

""It was pre-trained on 12T tokens of text and code data...

DBRX was pretrained on 12T tokens of carefully curated data and a maximum context length of 32k tokens. We estimate that this data is at least 2x better token-for-token than the data we used to pretrain the MPT family of models""

from HF: https://huggingface.co/databricks/dbrx-base

The training mix used for DBRX contains both natural-language and code examples. The vast majority of our training data is in the English language",9000000000000.0,"12T tokens is equivalent to 9T words. Though it includes code data, so not very literally 9T words",,,NVIDIA H100 SXM5 80GB,Confident,"Today, we are excited to introduce DBRX, an open, general-purpose LLM created by Databricks. Across a range of standard benchmarks, DBRX sets a new state-of-the-art for established open LLMs. Moreover, it provides the open community and enterprises building their own LLMs with capabilities that were previously limited to closed model APIs; according to our measurements, it surpasses GPT-3.5, and it is competitive with Gemini 1.0 Pro. It is an especially capable code model, surpassing specialized models like CodeLLaMA-70B on programming, in addition to its strength as a general-purpose LLM.",United States of America,,,,,,Unreleased,"license: https://www.databricks.com/legal/open-model-license
conditions based on monthly users",,,
Xinghai (æ˜Ÿæµ·),"Multimodal,Language,Vision","Recommender system,Image super-resolution",,,https://web.archive.org/web/20250627092753/https://www.sohu.com/a/764344251_115978,,,2024-03-15,Hisense,,,,,,,,,,,,Unknown,"Xinghai big model was trained based on Hisense's own tens of millions of high-quality language, image and other data, and ranked second on the authoritative evaluation list C-Eval and first in the TV industry.

In Feb 2025 - integration with DeepSeek R1",China,,,,,,,,,,
MM1-30B,"Multimodal,Language,Vision","Chat,Image captioning,Visual question answering","Brandon McKinzie, Zhe Gan, Jean-Philippe Fauconnier, Sam Dodge, Bowen Zhang, Philipp Dufter, Dhruti Shah, Xianzhi Du, Futang Peng, Floris Weers, Anton Belyi, Haotian Zhang, Karanjeet Singh, Doug Kang, Ankur Jain, Hongyu HÃ¨, Max Schwarzer, Tom Gunter, Xiang Kong, Aonan Zhang, Jianyu Wang, Chong Wang, Nan Du, Tao Lei, Sam Wiseman, Guoli Yin, Mark Lee, Zirui Wang, Ruoming Pang, Peter Grasch, Alexander Toshev, Yinfei Yang",Unreleased,https://arxiv.org/abs/2403.09611,243.0,"MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training",2024-03-14,Apple,30000000000.0,30B,4.86e+23,"Pre-trained on ~2B image-text pairs and 2T tokens (Table 2). Each image is 144 tokens, so the images are ~300B tokens.
Then additional multimodal training for 400B tokens, for a total of ~2.7T tokens.

This is the final training recipe: ""We initialize both the image encoder and the underlying LLM decoder weights for MM1 from in-house pre-trained models2. We then perform multimodal pre-training on the above data mix for 200k steps (approx. 400B tokens).""

Compute  = 6ND = 6 * 2.7 trillion * 30 billion = 4.86e23

maybe the size of the visual connector is relevant","Conceptual Captions (CC3M),Conceptual Captions 12M (CC12M),COYO-700M,Unspecified unreleased,OBELICS","Text, captioned images. See Table 2",1500000000000.0,at least 2T tokens,,,,Likely,"In this work, we discuss building performant Multimodal Large Language Models (MLLMs). In particular, we study the importance of various architecture components and data choices. Through careful and comprehensive ablations of the image encoder, the vision language connector, and various pre-training data choices, we identified several crucial design lessons. For example, we demonstrate that for large-scale multimodal pre-training using a careful mix of image-caption, interleaved image-text, and text-only data is crucial for achieving state-of-the-art (SOTA) few-shot results across multiple benchmarks, compared to other published pre-training results. Further, we show that the image encoder together with image resolution and the image token count has substantial impact, while the vision-language connector design is of comparatively negligible importance. By scaling up the presented recipe, we build MM1, a family of multimodal models up to 30B parameters, including both dense models and mixture-of-experts (MoE) variants, that are SOTA in pre-training metrics and achieve competitive performance after supervised fine-tuning on a range of established multimodal benchmarks. Thanks to large-scale pre-training, MM1 enjoys appealing properties such as enhanced in-context learning, and multi-image reasoning, enabling few-shot chain-of-thought prompting.",United States of America,,,,,,Unreleased,,,,
Inflection-2.5,Language,Chat,,Hosted access (no API),https://inflection.ai/inflection-2-5,,Inflection-2.5: meet the world's best personal AI,2024-03-07,Inflection AI,,,8.000001e+24,"""Inflection-1 used approximately 4% the training FLOPs of GPT-4 and, on average, performed at approximately 72% GPT-4 level on a diverse range of IQ-oriented tasks. Inflection-2.5, now powering Pi, achieves more than 94% the average performance of GPT-4 despite using only 40% the training FLOPs.""

This is a weird one - we estimated GPT-4 at 2.1e25 FLOP (which could be off somewhat, or Inflection could believe a different number). 40% of that is ~8e24. 
",,,,,,,NVIDIA H100 SXM5 80GB,Likely,"At Inflection, our mission is to create a personal AI for everyone. Last May, we released Piâ€”a personal AI, designed to be empathetic, helpful, and safe. In November we announced a new major foundation model, Inflection-2, the second best LLM in the world at the time.

Now we are adding IQ to Piâ€™s exceptional EQ.

We are launching Inflection-2.5, our upgraded in-house model that is competitive with all the world's leading LLMs like GPT-4 and Gemini. It couples raw capability with our signature personality and unique empathetic fine-tuning. Inflection-2.5 is available to all Pi's users today, at pi.ai, on iOS, on Android, or our new desktop app.",United States of America,,,,,,Unreleased,,,,
Claude 3 Haiku,Language,"Chat,Code generation,Language modeling/generation,Question answering",,API access,https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf,,"The Claude 3 Model Family: Opus, Sonnet, Haiku",2024-03-04,Anthropic,,,,,Unspecified unreleased,"Claude 3 models are trained on a proprietary mix of publicly available information on the Internet as of August 2023, as well as non-public data from third parties, data provided by data labeling services and paid contractors, and data we generate internally. We employ several data cleaning and filtering methods, including deduplication and classification. The Claude 3 suite of models have not been trained on any user prompt or output data submitted to us by users or customers, including free users, Claude Pro users, and API customers.
This has a knowledge cutoff date of August 2023, according to https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf, which I assume means August 1, 2023.",,,,"Like its predecessors, Claude 3 models employ various training methods, such as unsupervised learning and Constitutional AI [6]. These models were trained using hardware from Amazon Web Services (AWS) and Google Cloud Platform (GCP)",,Unknown,"We introduce Claude 3, a new family of large multimodal models â€“ Claude 3 Opus, our most capable offering, Claude 3 Sonnet, which provides a combination of skills and speed, and Claude 3 Haiku, our fastest and least expensive model. All new models have vision capabilities that enable them to process and analyze image data. The Claude 3 family demonstrates strong performance across benchmark evaluations and sets a new standard on
measures of reasoning, math, and coding. Claude 3 Opus achieves state-of-the-art results on evaluations like GPQA [1], MMLU [2], MMMU [3] and many more. Claude 3 Haiku performs as well or better than Claude 2 [4] on most pure-text tasks, while Sonnet and Opus significantly outperform it. Additionally, these models exhibit improved fluency in non-English languages, making them more versatile for a global audience. In this report, we provide an in-depth analysis of our evaluations, focusing on core capabilities, safety, societal impacts, and the catastrophic risk assessments we committed to in our Responsible Scaling Policy [5].
",United States of America,,,,,,Unreleased,,,,
Claude 3 Sonnet,"Multimodal,Language,Vision","Chat,Image captioning,Code generation,Language modeling/generation",,API access,https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf,,"The Claude 3 Model Family: Opus, Sonnet, Haiku",2024-03-04,Anthropic,,,,,Unspecified unreleased,"Claude 3 models are trained on a proprietary mix of publicly available information on the Internet as of August 2023, as well as non-public data from third parties, data provided by data labeling services and paid contractors, and data we generate internally. We employ several data cleaning and filtering methods, including deduplication and classification. The Claude 3 suite of models have not been trained on any user prompt or output data submitted to us by users or customers, including free users, Claude Pro users, and API customers.
This has a knowledge cutoff date of August 2023, according to https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf, which I assume means August 1, 2023.",,,,"Like its predecessors, Claude 3 models employ various training methods, such as unsupervised learning and Constitutional AI [6]. These models were trained using hardware from Amazon Web Services (AWS) and Google Cloud Platform (GCP)",,Unknown,"We introduce Claude 3, a new family of large multimodal models â€“ Claude 3 Opus, our most capable offering, Claude 3 Sonnet, which provides a combination of skills and speed, and Claude 3 Haiku, our fastest and least expensive model. All new models have vision capabilities that enable them to process and analyze image data. The Claude 3 family demonstrates strong performance across benchmark evaluations and sets a new standard on
measures of reasoning, math, and coding. Claude 3 Opus achieves state-of-the-art results on evaluations like GPQA [1], MMLU [2], MMMU [3] and many more. Claude 3 Haiku performs as well or better than Claude 2 [4] on most pure-text tasks, while Sonnet and Opus significantly outperform it. Additionally, these models exhibit improved fluency in non-English languages, making them more versatile for a global audience. In this report, we provide an in-depth analysis of our evaluations, focusing on core capabilities, safety, societal impacts, and the catastrophic risk assessments we committed to in our Responsible Scaling Policy [5].
",United States of America,,,,,,Unreleased,,,,
Claude 3 Opus,"Multimodal,Language,Vision","Chat,Image captioning,Code generation,Language modeling/generation",,API access,https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf,,"The Claude 3 Model Family: Opus, Sonnet, Haiku",2024-03-04,Anthropic,,,,Training compute estimated to be 1.64e25 FLOP from benchmark scores. https://colab.research.google.com/drive/1r3pUMhB7Kh0Gls9eG-v_XefWrye9fVQR?usp=sharing,Unspecified unreleased,"Claude 3 models are trained on a proprietary mix of publicly available information on the Internet as of August 2023, as well as non-public data from third parties, data provided by data labeling services and paid contractors, and data we generate internally. We employ several data cleaning and filtering methods, including deduplication and classification. The Claude 3 suite of models have not been trained on any user prompt or output data submitted to us by users or customers, including free users, Claude Pro users, and API customers.
This has a knowledge cutoff date of August 2023, according to https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf, which I assume means August 1, 2023.",,,,"Like its predecessors, Claude 3 models employ various training methods, such as unsupervised learning and Constitutional AI [6]. These models were trained using hardware from Amazon Web Services (AWS) and Google Cloud Platform (GCP)",,Speculative,"We introduce Claude 3, a new family of large multimodal models â€“ Claude 3 Opus, our most capable offering, Claude 3 Sonnet, which provides a combination of skills and speed, and Claude 3 Haiku, our fastest and least expensive model. All new models have vision capabilities that enable them to process and analyze image data. The Claude 3 family demonstrates strong performance across benchmark evaluations and sets a new standard on
measures of reasoning, math, and coding. Claude 3 Opus achieves state-of-the-art results on evaluations like GPQA [1], MMLU [2], MMMU [3] and many more. Claude 3 Haiku performs as well or better than Claude 2 [4] on most pure-text tasks, while Sonnet and Opus significantly outperform it. Additionally, these models exhibit improved fluency in non-English languages, making them more versatile for a global audience. In this report, we provide an in-depth analysis of our evaluations, focusing on core capabilities, safety, societal impacts, and the catastrophic risk assessments we committed to in our Responsible Scaling Policy [5].
",United States of America,,,,,,Unreleased,,,,
Aramco Metabrain AI,Language,Language modeling/generation,Saudi Aramco,Unreleased,https://www.offshore-technology.com/news/saudi-aramco-unveils-industry-first-generative-ai-model/,,Saudi Aramco unveils industryâ€™s first generative AI model,2024-03-04,Saudi Aramco,250000000000.0,"""It has 250 billion parameters that are adjustable during training to generate outputs or make predictions.""",1.05e+25,6*250B*7T=1.05e+25,,"""The AI was trained using seven trillion data points, collecting more than 90 years of company history.""",7000000000000.0,,,,,Likely,,Saudi Arabia,,,,,,Unreleased,,,,
Step-2,Language,"Language modeling/generation,Question answering",,API access,"https://www.sohu.com/a/791101272_114877
https://platform.stepfun.com/#step2",,"100å¤©åŽï¼Œé˜¶è·ƒæ˜Ÿè¾°äº¤å‡ºäº†ç¬¬äºŒä»½ç­”å·
",2024-03-01,StepFun,1000000000000.0,"""The company has already launched the Step series of foundation models, which includes Step-2, a cutting-edge trillion-parameter Mixture of Experts (MoE) language model"" according to the organization card on HF.
https://huggingface.co/stepfun-ai",,"Developer raised hundreds of millions of dollars in late 2024, plausibly might have done a 1e25 FLOP training run if previous funding was commensurate (https://www.techinasia.com/news/tencent-backed-chinese-ai-startup-raises-series-b-funding).",,,,,,,,Likely,"A new generation of MoE architecture large model, with parameters breaking through trillions. Model performance/sensory/planning capabilities are fully approaching the international mainstream model to meet the various needs of users in the Chinese/English field, reflecting the cutting-edge results of Scaling Laws.",China,,,,,,,,,,
StarCoder 2 15B,Language,"Code generation,Code autocompletion","Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, Tianyang Liu, Max Tian, Denis Kocetkov, Arthur Zucker, Younes Belkada, Zijian Wang, Qian Liu, Dmitry Abulkhanov, Indraneil Paul, Zhuang Li, Wen-Ding Li, Megan Risdal, Jia Li, Jian Zhu, Terry Yue Zhuo, Evgenii Zheltonozhskii, Nii Osae Osae Dade, Wenhao Yu, Lucas KrauÃŸ, Naman Jain, Yixuan Su, Xuanli He, Manan Dey, Edoardo Abati, Yekun Chai, Niklas Muennighoff, Xiangru Tang, Muhtasham Oblokulov, Christopher Akiki, Marc Marone, Chenghao Mou, Mayank Mishra, Alex Gu, Binyuan Hui, Tri Dao, Armel Zebaze, Olivier Dehaene, Nicolas Patry, Canwen Xu, Julian McAuley, Han Hu, Torsten Scholak, Sebastien Paquet, Jennifer Robinson, Carolyn Jane Anderson, Nicolas Chapados, Mostofa Patwary, Nima Tajbakhsh, Yacine Jernite, Carlos MuÃ±oz Ferrandis, Lingming Zhang, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, Harm de Vries",Open weights (restricted use),https://arxiv.org/abs/2402.19173,,StarCoder 2 and The Stack v2: The Next Generation,2024-02-29,"Hugging Face,ServiceNow,NVIDIA,BigCode",15000000000.0,15B,3.87e+23,estimation is given in Table 6 ,The Stack v2,See Table 4. The Stack V2 plus some extras. Created from repositorites from Github with permissive licences.,913230000000.0,from Table 4,,,,Confident,"The BigCode project, an open-scientific collaboration focused on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder2. In partnership with Software Heritage (SWH), we build The Stack v2 on top of the digital commons of their source code archive. Alongside the SWH repositories spanning 619 programming languages, we carefully select other high-quality data sources, such as GitHub pull requests, Kaggle notebooks, and code documentation. This results in a training set that is 4x larger than the first StarCoder dataset. We train StarCoder2 models with 3B, 7B, and 15B parameters on 3.3 to 4.3 trillion tokens and thoroughly evaluate them on a comprehensive set of Code LLM benchmarks. We find that our small model, StarCoder2-3B, outperforms other Code LLMs of similar size on most benchmarks, and also outperforms StarCoderBase-15B. Our large model, StarCoder2- 15B, significantly outperforms other models of comparable size. In addition, it matches or outperforms CodeLlama-34B, a model more than twice its size. Although DeepSeekCoder- 33B is the best-performing model at code completion for high-resource languages, we find that StarCoder2-15B outperforms it on math and code reasoning benchmarks, as well as several low-resource languages. We make the model weights available under an OpenRAIL license and ensure full transparency regarding the training data by releasing the SoftWare Heritage persistent IDentifiers (SWHIDs) of the source code data. ","United States of America,United States of America,United States of America",,,,,,Unreleased,"commercial use allowed, but various use cases restricted: https://huggingface.co/spaces/bigcode/bigcode-model-license-agreement

code is fine-tune only: https://github.com/bigcode-project/starcoder2?tab=readme-ov-file#fine-tuning ",,,
StarCoder 2 7B,Language,"Code generation,Code autocompletion","Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, Tianyang Liu, Max Tian, Denis Kocetkov, Arthur Zucker, Younes Belkada, Zijian Wang, Qian Liu, Dmitry Abulkhanov, Indraneil Paul, Zhuang Li, Wen-Ding Li, Megan Risdal, Jia Li, Jian Zhu, Terry Yue Zhuo, Evgenii Zheltonozhskii, Nii Osae Osae Dade, Wenhao Yu, Lucas KrauÃŸ, Naman Jain, Yixuan Su, Xuanli He, Manan Dey, Edoardo Abati, Yekun Chai, Niklas Muennighoff, Xiangru Tang, Muhtasham Oblokulov, Christopher Akiki, Marc Marone, Chenghao Mou, Mayank Mishra, Alex Gu, Binyuan Hui, Tri Dao, Armel Zebaze, Olivier Dehaene, Nicolas Patry, Canwen Xu, Julian McAuley, Han Hu, Torsten Scholak, Sebastien Paquet, Jennifer Robinson, Carolyn Jane Anderson, Nicolas Chapados, Mostofa Patwary, Nima Tajbakhsh, Yacine Jernite, Carlos MuÃ±oz Ferrandis, Lingming Zhang, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, Harm de Vries",Open weights (restricted use),https://arxiv.org/abs/2402.19173,,StarCoder 2 and The Stack v2: The Next Generation,2024-02-29,"Hugging Face,ServiceNow,NVIDIA,BigCode",7000000000.0,7B,1.55e+23,estimation is given in Table 6 ,The Stack v2,See Table 4. The Stack V2 plus some extras. Created from repositorites from Github with permissive licences.,658580000000.0,from Table 4,,"""A cumulative of 145,152 hours of computation was performed on hardware of type H100""",NVIDIA H100 SXM5 80GB,Confident,"The BigCode project, an open-scientific collaboration focused on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder2. In partnership with Software Heritage (SWH), we build The Stack v2 on top of the digital commons of their source code archive. Alongside the SWH repositories spanning 619 programming languages, we carefully select other high-quality data sources, such as GitHub pull requests, Kaggle notebooks, and code documentation. This results in a training set that is 4x larger than the first StarCoder dataset. We train StarCoder2 models with 3B, 7B, and 15B parameters on 3.3 to 4.3 trillion tokens and thoroughly evaluate them on a comprehensive set of Code LLM benchmarks. We find that our small model, StarCoder2-3B, outperforms other Code LLMs of similar size on most benchmarks, and also outperforms StarCoderBase-15B. Our large model, StarCoder2- 15B, significantly outperforms other models of comparable size. In addition, it matches or outperforms CodeLlama-34B, a model more than twice its size. Although DeepSeekCoder- 33B is the best-performing model at code completion for high-resource languages, we find that StarCoder2-15B outperforms it on math and code reasoning benchmarks, as well as several low-resource languages. We make the model weights available under an OpenRAIL license and ensure full transparency regarding the training data by releasing the SoftWare Heritage persistent IDentifiers (SWHIDs) of the source code data. ","United States of America,United States of America,United States of America",,,,,,Unreleased,"https://huggingface.co/spaces/bigcode/bigcode-model-license-agreement

https://huggingface.co/bigcode/starcoder2-7b

The Responsible AI License allows users to take advantage of the model in a wide range of settings (including free use and redistribution) as long as they respect the specific use case restrictions outlined, which correspond to model applications the licensor deems ill-suited for the model or are likely to cause harm

only finetune code here
https://github.com/bigcode-project/starcoder2",,,
Ideogram 1.0,Image generation,"Image generation,Text-to-image",,Hosted access (no API),https://about.ideogram.ai/1.0,,Ideogram 1.0,2024-02-28,Ideogram,,,,"Unlikely to exceed 10^25 FLOP as the developer raised $80M (https://betakit.com/midjourney-competitor-ideogram-closes-80-million-series-a-round-as-it-launches-latest-text-to-image-model/), and was unlikely to spend a significant fraction of that on a 10^25 FLOP training run.",Unspecified unreleased,,,,,,,Unknown,"Weâ€™re excited to release Ideogram 1.0, our most advanced text-to-image model to date. Trained from scratch like all Ideogram models, Ideogram 1.0 offers state-of-the-art text rendering, unprecedented photorealism and prompt adherence, and a new feature called Magic Prompt that helps you write detailed prompts for beautiful, creative images.",Canada,,,,,,Unreleased,,,,
Nemotron-4 15B,Language,"Language modeling/generation,Code generation,Question answering,Translation,Quantitative reasoning","Jupinder Parmar, Shrimai Prabhumoye, Joseph Jennings, Mostofa Patwary, Sandeep Subramanian, Dan Su, Chen Zhu, Deepak Narayanan, Aastha Jhunjhunwala, Ayush Dattagupta, Vibhu Jawa, Jiwei Liu, Ameya Mahabaleshwarkar, Osvald Nitski, Annika Brundyn, James Maki, Miguel Martinez, Jiaxuan You, John Kamalu, Patrick LeGresley, Denys Fridman, Jared Casper, Ashwath Aithal, Oleksii Kuchaiev, Mohammad Shoeybi, Jonathan Cohen, Bryan Catanzaro",Unreleased,https://arxiv.org/abs/2402.16819,,Nemotron-4 15B Technical Report,2024-02-27,NVIDIA,15000000000.0,15b,7.5005116e+23,"6ND = 6 FLOP/token/parameter * 15*10^9 parameters * 8*10^12 tokens = 7.2e+23 FLOP

""Nemotron-4 was trained using 384 DGX H100 nodes; each node contains 8 H100 80GB SXM5 GPUs based on the NVIDIA Hopper architecture (NVIDIA, 2022). Each H100 GPU has a peak throughput of 989 teraFLOP/s when doing 16-bit floating point (bfloat16) arithmetic without sparsity.

Table 2 reports more detailed training schedule:

989*10^12 FLOP/sec * 3600 sec/hour * 24 hours * (768 gpus * 0.343 [reported utilization] * 0.8 days +  1536 gpus * 0.333 [reported utilization] * 0.4 days + 2304 gpus * 0.305 [reported utilization] * 11.9 days) = 7.5005116e+23 FLOP",Unspecified unreleased,"""At a high-level, the data blend is split into three different types of data: English natural language data (70%), multilingual natural language data (15%), and source-code data (15%).""",8000000000000.0,"""15-billion-parameter large multilingual language model trained on 8 trillion text tokens""",312.0,"""Training was completed in approximately 13 calendar days.""",NVIDIA H100 SXM5 80GB,Confident,"We introduce Nemotron-4 15B, a 15-billion-parameter large multilingual language model trained on 8 trillion text tokens. Nemotron-4 15B demonstrates strong performance when assessed on English, multilingual, and coding tasks: it outperforms all existing similarly-sized open models on 4 out of 7 downstream evaluation areas and achieves competitive performance to the leading open models in the remaining ones. Specifically, Nemotron-4 15B exhibits the best multilingual capabilities of all similarly-sized models, even outperforming models over four times larger and those explicitly specialized for multilingual tasks.",United States of America,,,,3072.0,0.305,Unreleased,,,,
Mistral Large,Language,Chat,,API access,https://mistral.ai/news/mistral-large/,,"Mistral Large, our new flagship model",2024-02-26,Mistral AI,,,1.12e+25,"https://www.wsj.com/tech/ai/the-9-month-old-ai-startup-challenging-silicon-valleys-giants-ee2e4c48

Mistral spent <20 million euro (meaning approximately 20 million?) to train Mistral Large:

https://x.com/EMostaque/status/1762152740938031484?s=20
""assuming this is on H100s with @Scaleway who are â‚¬1.9/hour => 10m H100 hours (c 30m A100 hrs), 3 months at 4k H100s :timer_clock:"" -Emad Mostaque

Assuming bf16 or fp16, H100 SXM performance is 989 TFLOPS
At 1.9 euro per H100-hour and 30% utilization, spending 20M euro produces 1.12*10^25 FLOP.
https://www.wolframalpha.com/input?i=20+million+%2F+%281.9%2Fhour%29+*+989+TFLOPS+*+0.30 ",Unspecified unreleased,,,,2500.0,Speculation by Emad Mostaque: 20M euro spent at Scaleway (1.9 euro per H100-hour) would be around 3 months on 4000 H100s.,NVIDIA H100 SXM5 80GB,Likely,,France,,,,,,Unreleased,,,,
Gemma 1.1 7B Instruct,Language,"Language modeling/generation,Question answering","Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Laurent Sifre, Morgane RiviÃ¨re, Mihir Sanjay Kale, Juliette Love, Pouya Tafti, LÃ©onard Hussenot and et al.",Open weights (restricted use),https://huggingface.co/google/gemma-1.1-7b-it,,,2024-02-24,Google,8540000000.0,"Safetensors 
Model size 8.54B params",3.0744e+23,6ND = 6*6000000000000*8540000000=3.0744e+23,Unspecified unreleased,"""These models were trained on a dataset of text data that includes a wide variety of sources, totaling 6 trillion tokens. Here are the key components:

Web Documents: A diverse collection of web text ensures the model is exposed to a broad range of linguistic styles, topics, and vocabulary. Primarily English-language content.
Code: Exposing the model to code helps it to learn the syntax and patterns of programming languages, which improves its ability to generate code or understand code-related questions.
Mathematics: Training on mathematical text helps the model learn logical reasoning, symbolic representation, and to address mathematical queries.""",6000000000000.0,"""These models were trained on a dataset of text data that includes a wide variety of sources, totaling 6 trillion tokens. """,,,Google TPU v5e,Confident,"This is Gemma 1.1 7B (IT), an update over the original instruction-tuned Gemma release.

Gemma 1.1 was trained using a novel RLHF method, leading to substantial gains on quality, coding capabilities, factuality, instruction following and multi-turn conversation quality. We also fixed a bug in multi-turn conversations, and made sure that model responses don't always start with ""Sure,"".

We believe this release represents an improvement for most use cases, but we encourage users to test in their particular applications. The previous model will continue to be available in the same repo. We appreciate the enthusiastic adoption of Gemma, and we continue to welcome all feedback from the community.",United States of America,,,,,,Unreleased,"https://huggingface.co/google/gemma-1.1-7b-it

""This repository is publicly accessible, but you have to accept the conditions to access its files and content.""",,,
MegaScale (175B),Language,Language modeling/generation,"Ziheng Jiang, Haibin Lin, Yinmin Zhong, Qi Huang, Yangrui Chen, Zhi Zhang, Yanghua Peng, Xiang Li, Cong Xie, Shibiao Nong, Yulu Jia, Sun He, Hongmin Chen, Zhihao Bai, Qi Hou, Shipeng Yan, Ding Zhou, Yiyao Sheng, Zhuo Jiang, Haohan Xu, Haoran Wei, Zhang Zhang, Pengfei Nie, Leqi Zou, Sida Zhao, Liang Xiang, Zherui Liu, Zhe Li, Xiaoying Jia, Jianxi Ye, Xin Jin, Xin Liu",Unreleased,https://arxiv.org/abs/2402.15627,233.0,"MegaScale: Scaling Large Language Model Training to More Than 10,000 GPUs",2024-02-23,"ByteDance,Peking University",175000000000.0,"Two models are trained for epoch to evaluate the MegaScale training system; one model with 175B and another with 530B parameters. This entry reports the 175B model.

There is a third production model mentioned, with fewer details.",2.7385671436e+23,"Table 2 gives details for the 175B model. Looking at the largest 1 epoch run with 12288 GPUs:
2166.3 aggregate PFlops/sec * 1.75 days * 24 hours/day * 3600 seconds/hour = 3.275e23

This is consistent with the theoretical computation counting estimate, if they factor MFU rate into their 2166.3 figure:
2 Ã— 175B params Ã— 3 Ã— 300B tokens Ã— 1 epoch = 2.29e23

I use the geometric mean of these two:
(3.275e23 + 2.29e23) / 2 = 2.74e23",,175B and 530B models trained for paper use 300B tokens each.,225000000000.0,300B tokens * 0.75 words/token = 225B words,42.0,,NVIDIA A100,Confident,"We present the design, implementation and engineering experience in building and deploying MegaScale, a production system for training large language models (LLMs) at the scale of more than 10,000 GPUs. Training LLMs at this scale brings unprecedented challenges to training efficiency and stability. We take a full-stack approach that co-designs the algorithmic and system components across model block and optimizer design, computation and communication overlapping, operator optimization, data pipeline, and network performance tuning. Maintaining high efficiency throughout the training process (i.e., stability) is an important consideration in production given the long extent of LLM training jobs. Many hard stability issues only emerge at large scale, and in-depth observability is the key to address them. We develop a set of diagnosis tools to monitor system components and events deep in the stack, identify root causes, and derive effective techniques to achieve fault tolerance and mitigate stragglers. MegaScale achieves 55.2% Model FLOPs Utilization (MFU) when training a 175B LLM model on 12,288 GPUs, improving the MFU by 1.34x compared to Megatron-LM. We share our operational experience in identifying and fixing failures and stragglers. We hope by articulating the problems and sharing our experience from a systems perspective, this work can inspire future LLM systems research.","China,China",,,,12288.0,0.552,Unreleased,"repo, but no training code for the big model https://github.com/volcengine/vescale
Model weights unreleased",,,
MegaScale (530B),Language,Language modeling/generation,"Ziheng Jiang, Haibin Lin, Yinmin Zhong, Qi Huang, Yangrui Chen, Zhi Zhang, Yanghua Peng, Xiang Li, Cong Xie, Shibiao Nong, Yulu Jia, Sun He, Hongmin Chen, Zhihao Bai, Qi Hou, Shipeng Yan, Ding Zhou, Yiyao Sheng, Zhuo Jiang, Haohan Xu, Haoran Wei, Zhang Zhang, Pengfei Nie, Leqi Zou, Sida Zhao, Liang Xiang, Zherui Liu, Zhe Li, Xiaoying Jia, Jianxi Ye, Xin Jin, Xin Liu",Unreleased,https://arxiv.org/abs/2402.15627,233.0,"MegaScale: Scaling Large Language Model Training to More Than 10,000 GPUs",2024-02-23,"ByteDance,Peking University",530000000000.0,"Two models are trained for epoch to evaluate the MegaScale training system; one model with 175B and another with 530B parameters. This entry reports the 530B model.

There is a third production model mentioned, with fewer details.",9.6910000000001e+23,"175B models uses 3.2e23 FLOPs (Table 2, bottom row)

With constant dataset size and utilization, FLOPs should scale linearly in # parameters, so: 3.2e23 * (530/175) = 9.7e23",,175B and 530B models trained for paper use 300B tokens each.,300000000000.0,300B tokens,117.9,"175B parameter model is stated to have taken 1.75 days. 500B model used more compute, fewer GPUs, and had slightly worse MFU. Accounting for these: 
1.75 days * 24 hours/day * (9.7e23/3.2e23) * (11200/12288) * (0.552/0.543) = 117.9 hours",NVIDIA A100,Confident,"We present the design, implementation and engineering experience in building and deploying MegaScale, a production system for training large language models (LLMs) at the scale of more than 10,000 GPUs. Training LLMs at this scale brings unprecedented challenges to training efficiency and stability. We take a full-stack approach that co-designs the algorithmic and system components across model block and optimizer design, computation and communication overlapping, operator optimization, data pipeline, and network performance tuning. Maintaining high efficiency throughout the training process (i.e., stability) is an important consideration in production given the long extent of LLM training jobs. Many hard stability issues only emerge at large scale, and in-depth observability is the key to address them. We develop a set of diagnosis tools to monitor system components and events deep in the stack, identify root causes, and derive effective techniques to achieve fault tolerance and mitigate stragglers. MegaScale achieves 55.2% Model FLOPs Utilization (MFU) when training a 175B LLM model on 12,288 GPUs, improving the MFU by 1.34x compared to Megatron-LM. We share our operational experience in identifying and fixing failures and stragglers. We hope by articulating the problems and sharing our experience from a systems perspective, this work can inspire future LLM systems research.","China,China",,,,11200.0,0.543,Unreleased,"they open-sourced their framework but don't see training code for their big model. 
https://github.com/volcengine/vescale
Model weights are unreleased",,,
MegaScale (Production),Language,Language modeling/generation,"Ziheng Jiang, Haibin Lin, Yinmin Zhong, Qi Huang, Yangrui Chen, Zhi Zhang, Yanghua Peng, Xiang Li, Cong Xie, Shibiao Nong, Yulu Jia, Sun He, Hongmin Chen, Zhihao Bai, Qi Hou, Shipeng Yan, Ding Zhou, Yiyao Sheng, Zhuo Jiang, Haohan Xu, Haoran Wei, Zhang Zhang, Pengfei Nie, Leqi Zou, Sida Zhao, Liang Xiang, Zherui Liu, Zhe Li, Xiaoying Jia, Jianxi Ye, Xin Jin, Xin Liu",Unreleased,https://arxiv.org/abs/2402.15627,233.0,"MegaScale: Scaling Large Language Model Training to More Than 10,000 GPUs",2024-02-23,"ByteDance,Peking University",530000000000.0,"Production run is stated to have ""hundreds of billions of parameters"". Since the authors also do a number of experiments with a 530B model, I speculate they've used 530B for the production model.",3.9e+24,"Speculative. The model is stated to have trained for ""several weeks"". Assuming 530B parameters and ""several"" = 3, compute can be estimated from the 175B model's stated PFLOP/sec:
2166.3 aggregate PFlops/sec * 3 weeks * 7 days/week * 24 hours/day * 3600 seconds/hour = 3.9e+24.
As an upper bound, say 8e+24. ",,,,"Speculative. Authors note production system was trained on ""multi-trillions of tokens"". This could refer to training for multiple epochs on the same 300B tokens used to train the 175B and 530B models outlined in more detail in the paper. Alternatively, it could refer to a larger dataset of perhaps 3-9 trillion tokens.",504.0,"Speculative. Authors state ""several weeks"". For analysis, I've assumed this means around 3 weeks.",NVIDIA A100,Speculative,"We present the design, implementation and engineering experience in building and deploying MegaScale, a production system for training large language models (LLMs) at the scale of more than 10,000 GPUs. Training LLMs at this scale brings unprecedented challenges to training efficiency and stability. We take a full-stack approach that co-designs the algorithmic and system components across model block and optimizer design, computation and communication overlapping, operator optimization, data pipeline, and network performance tuning. Maintaining high efficiency throughout the training process (i.e., stability) is an important consideration in production given the long extent of LLM training jobs. Many hard stability issues only emerge at large scale, and in-depth observability is the key to address them. We develop a set of diagnosis tools to monitor system components and events deep in the stack, identify root causes, and derive effective techniques to achieve fault tolerance and mitigate stragglers. MegaScale achieves 55.2% Model FLOPs Utilization (MFU) when training a 175B LLM model on 12,288 GPUs, improving the MFU by 1.34x compared to Megatron-LM. We share our operational experience in identifying and fixing failures and stragglers. We hope by articulating the problems and sharing our experience from a systems perspective, this work can inspire future LLM systems research.","China,China",,,,12288.0,0.48,Unreleased,"Code for MegaScale (also called veScale) training system are released under Apache Licence: https://github.com/volcengine/vescale
The model itself is unreleased.",,,
Gemma 7B,Language,"Language modeling/generation,Chat,Code generation,Question answering,Quantitative reasoning","Gemma Team, Google DeepMind",Open weights (restricted use),https://storage.googleapis.com/deepmind-media/gemma/gemma-report.pdf,,Gemma: Open Models Based on Gemini Research and Technology,2024-02-21,Google DeepMind,8538074112.0,"Table 2, sum of embedding and non-embedding parameters",3.07e+23,"6ND aproximation 6*8.54B*6T = 3.07e23
""Gemma 2B and 7B are trained on 2T and 6T tokens respectively of primarily-English data from web documents, mathematics, and code.""

As confirmation: ""We estimate the carbon emissions from pretraining the Gemma models to be âˆ¼ 131 ð‘¡ð¶ð‘‚2ð‘’ð‘ž. ""

U.S. avg CO2 per kWh is ~0.87lbs 
131 tCO2 * 2000 lb/t * (1 kWh/0.87lb) = 3.01e5 kWh

Per SemiAnalysis TPU v5e uses ~ 5x less power than H100, so ~140 W TDP
3.01e5 kWh * 1000 W/kW * 1 TPUv5e/140 W  = 2.15e6 TPUv5e-hours

In bf16 precision, TPUv5e has peak performance of 197 TF/s, so:
2.15e6 * 3600 * 197e12 * 0.3 = 4.57e23",Unspecified unreleased,"""Gemma 2B and 7B are trained on 2T and 6T tokens respectively of primarily-English data from web documents, mathematics, and code.""",6000000000000.0,"""Gemma 2B and 7B are trained on 2T and 6T tokens respectively of primarily-English data from web documents, mathematics, and code.""
Not explicitly stated that this doesn't involve multiple epochs, but I expect it does not.",,,Google TPU v5e,Confident,,"United States of America,United Kingdom of Great Britain and Northern Ireland",,,,4096.0,,Unreleased,"https://ai.google.dev/gemma/terms

no illegal use or abuse",,,
Sora,"Video,Vision","Video generation,Text-to-video,Image-to-video,Video-to-video",,Hosted access (no API),https://openai.com/index/video-generation-models-as-world-simulators/,,Video generation models as world simulators,2024-02-15,OpenAI,,,,,Unspecified unreleased,"Sora was trained on diverse datasets, including a mix of publicly available data, proprietary data accessed through partnerships, and custom datasets developed in-house. These consist of:

Select publicly available data, mostly collected from industry-standard machine learning datasets and web crawls.

Proprietary data from data partnerships. We form partnerships to access non-publicly available data. For example, we partnered with Shutterstockâ  Pond5 on building and delivering AI-generated images. We also partner to commission and create datasets fit for our needs.

Human data: Feedback from AI trainers, red teamers, and employees. ",,,,,,Unknown,"Sora is OpenAIâ€™s video generation model, designed to take text, image, and video inputs and generate a new video as an output. Users can create videos up to 1080p resolution (20 seconds max) in various formats, generate new content from text, or enhance, remix, and blend their own assets. Users will be able to explore the Featured and Recent feeds which showcase community creations and offer inspiration for new ideas. Sora builds on learnings from DALLÂ·E and GPT models, and is designed to give people expanded tools for storytelling and creative expression. 

Sora is a diffusion model, which generates a video by starting off with a base video that looks like static noise and gradually transforms it by removing the noise over many steps. By giving the model foresight of many frames at a time, weâ€™ve solved a challenging problem of making sure a subject stays the same even when it goes out of view temporarily. Similar to GPT models, Sora uses a transformer architecture, unlocking superior scaling performance. ",United States of America,,,,,,Unreleased,,,,
Gemini 1.5 Pro,"Language,Multimodal","Language modeling,Visual question answering",Gemini Team,API access,https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf,,Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context,2024-02-15,Google DeepMind,,MoE architecture,,Training compute imputed to be 1.58e25 FLOP from benchmark scores. https://colab.research.google.com/drive/1r3pUMhB7Kh0Gls9eG-v_XefWrye9fVQR?usp=sharing,Unspecified unreleased,,,,,,Google TPU v4,Speculative,,"United States of America,United Kingdom of Great Britain and Northern Ireland",,,,,,Unreleased,API access: https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models,,,
Gemini 1.0 Pro Vision,"Vision,Language,Video","Language modeling,Visual question answering,Chat,Translation,Video description,Question answering",Gemini Team,API access,,,The best performing image and video understanding model to handle a broad range of applications. ,2024-02-15,Google DeepMind,,,,"To the extent that we believe Gemini 1 Pro was below 1e25 FLOP, Gemini 1 Pro Vision is unlikely to exceed it.",Unspecified unreleased,,,,,,,Unknown,"Gemini 1.0 Pro Vision supports text, image, and video as inputs.

Max input tokens: 16,384
Max output tokens: 2,048
Max images per prompt: 16
Max video length: 2 minutes
Max videos per prompt: 1
Training data: Up to February 2023","United States of America,United Kingdom of Great Britain and Northern Ireland",,,,,,Unreleased,API Access: https://console.cloud.google.com/vertex-ai/generative/multimodal/create/text?model=gemini-1.0-pro-vision-001,,,
KwaiYii 175B,Language,Language modeling/generation,,,https://blog.csdn.net/kuaishoutech/article/details/140542568,,,2024-02-14,Kuaishou Technology,175000000000.0,175B,,,,,,,,,,Confident,"In June 2024, quick-handed NLP experts reported on the â€œKwaiYiiâ€ model at the Global Artificial Intelligence Technology Conference. The model was developed in early 2023, and the 175B model was released at the end of February 2024. Many capabilities are close to the latest version of GPT-4. Introduced eight key technological innovations and landing practices in scenes such as AI Xiaofu, including solutions to various challenges, which will continue to be iterated in the future.",China,,,,,,Unreleased,,,,
Qwen1.5-32B,Language,"Chat,Language modeling/generation,Quantitative reasoning,Code generation,Translation",Qwen Team,Open weights (restricted use),https://qwenlm.github.io/blog/qwen1.5/,,Introducing Qwen1.5,2024-02-05,Alibaba,32000000000.0,32B,,"upper bound is taken from Qwen1.5 72B training compute estimation

lower bound is taken from Qwen1.5 14B training compute estimation",Unspecified unreleased,,,,,,,Confident,"Qwen1.5 is the beta version of Qwen2, a transformer-based decoder-only language model pretrained on a large amount of data. In comparison with the previous released Qwen, the improvements include:

8 model sizes, including 0.5B, 1.8B, 4B, 7B, 14B, 32B and 72B dense models, and an MoE model of 14B with 2.7B activated;
Significant performance improvement in human preference for chat models;
Multilingual support of both base and chat models;
Stable support of 32K context length for models of all sizes
No need of trust_remote_code.",China,,,,,,Unreleased,"https://huggingface.co/Qwen/Qwen1.5-32B
""If you are commercially using the Materials, and your product or service has more than 100 million monthly active users, You shall request a license from Us""",,,
Qwen1.5-72B,Language,"Chat,Language modeling/generation,Quantitative reasoning,Code generation,Translation",Qwen Team,Open weights (restricted use),https://qwenlm.github.io/blog/qwen1.5/,,Introducing Qwen1.5,2024-02-04,Alibaba,72000000000.0,72B,1.3e+24,"3T training tokens: https://github.com/QwenLM/Qwen2/issues/97 

6 * 72 billion * 3 trillion = ~1.3e24",Unspecified unreleased,"""We pretrained the models with a large amount of data, and we post-trained the models with both supervised finetuning and direct preference optimization.""",3000000000000.0,3 trillion tokens from this response https://github.com/QwenLM/Qwen2/issues/97,,,,Confident,"In recent months, our focus has been on developing a â€œgoodâ€ model while optimizing the developer experience. As we progress towards Qwen1.5, the next iteration in our Qwen series, this update arrives just before the Chinese New Year. With Qwen1.5, we are open-sourcing base and chat models across six sizes: 0.5B, 1.8B, 4B, 7B, 14B, and 72B. In line with tradition, weâ€™re also providing quantized models, including Int4 and Int8 GPTQ models, as well as AWQ and GGUF quantized models. To enhance the developer experience, weâ€™ve merged Qwen1.5â€™s code into Hugging Face transformers, making it accessible with transformers>=4.37.0 without needing trust_remote_code.",China,,,,,,Unreleased,"restriction on >100m monthly users:

https://huggingface.co/Qwen/Qwen1.5-72B/blob/main/LICENSE",,,
Qwen1.5-7B,Language,"Chat,Language modeling/generation,Quantitative reasoning,Code generation,Translation",Qwen Team,Open weights (unrestricted),https://huggingface.co/Qwen/Qwen1.5-7B,,Introducing Qwen1.5,2024-02-04,Alibaba,7000000000.0,7B,1.68e+23,6 FLOP / parameter / token * 7*10^9 parameters * 4*10^12 tokens =  1.68e+23 FLOP,Unspecified unreleased,"""We pretrained the models with a large amount of data, and we post-trained the models with both supervised finetuning and direct preference optimization.""",4000000000000.0,4 trillion tokens from this response https://github.com/QwenLM/Qwen2/issues/97,,,,Confident,"Qwen1.5 is the beta version of Qwen2, a transformer-based decoder-only language model pretrained on a large amount of data. In comparison with the previous released Qwen, the improvements include:

8 model sizes, including 0.5B, 1.8B, 4B, 7B, 14B, 32B and 72B dense models, and an MoE model of 14B with 2.7B activated;
Significant performance improvement in human preference for chat models;
Multilingual support of both base and chat models;
Stable support of 32K context length for models of all sizes
No need of trust_remote_code.",China,,,,,,Unreleased,https://huggingface.co/Qwen/Qwen1.5-7B,,,
Qwen1.5-14B,Language,"Chat,Language modeling/generation,Quantitative reasoning,Code generation,Translation",Qwen Team,Open weights (unrestricted),https://huggingface.co/Qwen/Qwen1.5-14B,,Introducing Qwen1.5,2024-02-04,Alibaba,14000000000.0,14B,3.36e+23,6 FLOP / parameter / token * 14*10^9 parameters * 4*10^12 tokens =  3.36e+23 FLOP,Unspecified unreleased,"""We pretrained the models with a large amount of data, and we post-trained the models with both supervised finetuning and direct preference optimization.""",4000000000000.0,4 trillion tokens from this response https://github.com/QwenLM/Qwen2/issues/97,,,,Confident,"Qwen1.5 is the beta version of Qwen2, a transformer-based decoder-only language model pretrained on a large amount of data. In comparison with the previous released Qwen, the improvements include:

8 model sizes, including 0.5B, 1.8B, 4B, 7B, 14B, 32B and 72B dense models, and an MoE model of 14B with 2.7B activated;
Significant performance improvement in human preference for chat models;
Multilingual support of both base and chat models;
Stable support of 32K context length for models of all sizes
No need of trust_remote_code.",China,,,,,,Unreleased,https://huggingface.co/Qwen/Qwen1.5-14B,,,
OLMo-7B,Language,"Language modeling/generation,Chat","Dirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya Harsh Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, Shane Arora, David Atkinson, Russell Authur, Khyathi Raghavi Chandu, Arman Cohan, Jennifer Dumas, Yanai Elazar, Yuling Gu, Jack Hessel, Tushar Khot, William Merrill, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Peters, Valentina Pyatkin, Abhilasha Ravichander, Dustin Schwenk, Saurabh Shah, Will Smith, Emma Strubell, Nishant Subramani, Mitchell Wortsman, Pradeep Dasigi, Nathan Lambert, Kyle Richardson, Luke Zettlemoyer, Jesse Dodge, Kyle Lo, Luca Soldaini, Noah A. Smith, Hannaneh Hajishirzi",Open weights (unrestricted),https://arxiv.org/abs/2402.00838v1,,OLMo: Accelerating the Science of Language Models,2024-02-01,"Allen Institute for AI,University of Washington",7000000000.0,,1.0332e+23,"direct calculation:
6*7B*2.46trillion=1.0332 Ã— 10^23

(calculation also reporoduced by the developers in https://arxiv.org/pdf/2501.00656)",Dolma,,2000000000000.0,"""We built our training dataset out of a 2T-token sample from our open dataset, Dolma [...] All of our released models have been trained to at least 2T tokens (a single epoch over our training data), and some have been trained beyond that by starting a second epoch over the data with a different shuffling order"" 

Table 1 indicates total tokens seen are 2.46T for the 7B parameter model, though note that a later release in July 2024 has been trained to 2.75T tokens: https://github.com/allenai/OLMo?tab=readme-ov-file",,,"AMD Radeon Instinct MI250X,NVIDIA A100",Confident,"Language models (LMs) have become ubiquitous in both NLP research and in commercial product offerings. As their commercial importance has surged, the most powerful models have become closed off, gated behind proprietary interfaces, with important details of their training data, architectures, and development undisclosed. Given the importance of these details in scientifically studying these models, including their biases and potential risks, we believe it is essential for the research community to have access to powerful, truly open LMs. To this end, this technical report details the first release of OLMo, a state-of-the-art, truly Open Language Model and its framework to build and study the science of language modeling. Unlike most prior efforts that have only released model weights and inference code, we release OLMo and the whole framework, including training data and training and evaluation code. We hope this release will empower and strengthen the open research community and inspire a new wave of innovation.","United States of America,United States of America",,,,,,Open source,"License: The code and model are released under Apache 2.0.

Weights https://huggingface.co/allenai/OLMo-7B
Code https://github.com/allenai/OLMo",,,
Code Llama-70B,Language,Code generation,"Baptiste RoziÃ¨re, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Ellen Tan, Yossef (Yossi) Adi, Jingyu Liu, Tal Remez, JÃ©rÃ©my Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Defossez, Jade Copet, Faisal Azhar, Hugo Touvron, Gabriel Synnaeve, Louis Martin, Nicolas Usunier, Thomas Scialom",Open weights (restricted use),"https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/
https://arxiv.org/abs/2308.12950",2732.0,Code Llama: Open Foundation Models for Code,2024-01-29,Meta AI,70000000000.0,70B,1.26e+24,"Base model saw 2T tokens, Code Llama-70B was trained on an additional 1T. 6NC:
6 * 3T * 70B = 1.26e24",Unspecified unreleased,"We are releasing four sizes of Code Llama with 7B, 13B, 34B, and 70B parameters respectively. Each of these models is trained with 500B tokens of code and code-related data, apart from 70B, which is trained on 1T tokens.",1000000000000.0,Llama 70B training dataset was 2 trillion tokens. Code Llama finetuning dataset was 1 trillion tokens of code.,6480.0,"Assuming Code Llama 70B training continued on same hardware as Llama 2 70B.
Llama 2 70B used 1720320 A100 hours. Training all Code Llama models took 1.4M A100 hours (Table 26). Based on model sizes and number of tokens seen, 70B model used about 985k A100 hours to fine tune (see utilization notes). Total GPU hours is thus around 2.7M",NVIDIA A100 SXM4 80 GB,Confident,"We release Code Llama, a family of large language models for code based on Llama 2 providing state-of-the-art performance among open models, infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks. We provide multiple flavors to cover a wide range of applications: foundation models (Code Llama), Python specializations (Code Llama - Python), and instruction-following models (Code Llama - Instruct) with 7B, 13B and 34B parameters each. All models are trained on sequences of 16k tokens and show improvements on inputs with up to 100k tokens. 7B and 13B Code Llama and Code Llama - Instruct variants support infilling based on surrounding content. Code Llama reaches state-of-the-art performance among open models on several code benchmarks, with scores of up to 53% and 55% on HumanEval and MBPP, respectively. Notably, Code Llama - Python 7B outperforms Llama 2 70B on HumanEval and MBPP, and all our models outperform every other publicly available model on MultiPL-E. We release Code Llama under a permissive license that allows for both research and commercial use.",United States of America,Llama 2-70B,4.2e+23,"Fine tuning from base model uses 1T tokens.
70B * 1T * 6 = 4.2E23",400.0,0.3821,Unreleased,"Llama 2 license. can't use outputs to train models.

https://github.com/meta-llama/llama/blob/main/LICENSE",,,
DeepSeek Coder 33B,Language,Code generation,"Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Y. Wu, Y.K. Li, Fuli Luo, Yingfei Xiong, Wenfeng Liang",Open weights (restricted use),https://arxiv.org/abs/2401.14196,,DeepSeek-Coder: When the Large Language Model Meets Programming -- The Rise of Code Intelligence,2024-01-25,"DeepSeek,Peking University",33000000000.0,33B,3.96e+23,"""Step 1: Initially pre-trained with a dataset consisting of 87% code, 10% code-related language (Github Markdown and StackExchange), and 3% non-code-related Chinese language. Models are pre-trained using 1.8T tokens and a 4K window size in this step.
Step 2: Further Pre-training using an extended 16K window size on an additional 200B tokens, resulting in foundational models (DeepSeek-Coder-Base).
Step 3: Instruction Fine-tuning on 2B tokens of instruction data, resulting in instruction-tuned models (DeepSeek-Coder-Instruct).""

This means it was trained on 2T tokens. 2T * 33B * 6 = 3.96e23",,"""Trained from scratch on 2T tokens, including 87% code and 13% linguistic data in both English and Chinese languages.""",2000000000000.0,"""Trained from scratch on 2T tokens, including 87% code and 13% linguistic data in both English and Chinese languages.""

""The total data volume is 798 GB with 603 million files.""",,,,Likely,"The rapid development of large language models has revolutionized code intelligence in software development. However, the predominance of closed-source models has restricted extensive research and development. To address this, we introduce the DeepSeek-Coder series, a range of open-source code models with sizes from 1.3B to 33B, trained from scratch on 2 trillion tokens. These models are pre-trained on a high-quality project-level code corpus and employ a fill-in-the-blank task with a 16K window to enhance code generation and infilling. Our extensive evaluations demonstrate that DeepSeek-Coder not only achieves state-of-the-art performance among open-source code models across multiple benchmarks but also surpasses existing closed-source models like Codex and GPT-3.5. Furthermore, DeepSeek-Coder models are under a permissive license that allows for both research and unrestricted commercial use.","China,China",,,,,,Unreleased,"code doesn't seem to be training code.

deepseek license:
https://github.com/deepseek-ai/DeepSeek-Coder/blob/main/LICENSE-MODEL",,,
Qwen-VL-Max,"Multimodal,Language,Vision","Chat,Image captioning,Face recognition,Visual question answering",,API access,https://qwenlm.github.io/blog/qwen-vl/,,Introducing Qwen-VL,2024-01-25,Alibaba,7000000000.0,"Not stated. Qwen-VL (less capable, presumably smaller version) is 9.6B

Upd: 7B parameters mentioned here
https://github.com/QwenLM/Qwen-VL#qwen-vl-plus",,,Unspecified unreleased,,,,,,,Confident,"Along with the rapid development of our large language model Qwen, we leveraged Qwenâ€™s capabilities and unified multimodal pretraining to address the limitations of multimodal models in generalization, and we opensourced multimodal model Qwen-VL in Sep. 2023. Recently, the Qwen-VL series has undergone a significant upgrade with the launch of two enhanced versions, Qwen-VL-Plus and Qwen-VL-Max. The key technical advancements in these versions include:

Substantially boost in image-related reasoning capabilities;
Considerable enhancement in recognizing, extracting, and analyzing details within images and texts contained therein;
Support for high-definition images with resolutions above one million pixels and images of various aspect ratios.",China,,,,,,Unreleased,https://help.aliyun.com/zh/dashscope/developer-reference/tongyi-qianwen-vl-plus-api,,,
Fuyu-Heavy,"Multimodal,Language,Vision","Chat,Language modeling/generation,Visual question answering,System control",,Hosted access (no API),https://www.adept.ai/blog/adept-fuyu-heavy,,Adept Fuyu-Heavy: A new multimodal model,2024-01-24,Adept,100000000000.0,"""Fuyu-Heavy is the worldâ€™s third-most-capable multimodal model, behind only GPT4-V and Gemini Ultra, which are 10-20 times bigger""

So possibly around ~100B params, though GPT-4/Gemini params aren't public",,Nvidia hardware,,"curated/generated image data:

""high-quality image pre-training data is scarce, weâ€™ve devoted a lot of effort to collecting, curating, and even creating this data. Thereâ€™s also a delicate balance between text and image tasks â€” we had to develop recipes for striking this balance at scale""",,,,,,Speculative,"Weâ€™re excited to introduce Adept Fuyu-Heavy, a new multimodal model designed specifically for digital agents. Fuyu-Heavy is the worldâ€™s third-most-capable multimodal model, behind only GPT4-V and Gemini Ultra, which are 10-20 times bigger. Weâ€™re excited about this model because:

It excels at multimodal reasoning. To us the killer feature is UI understanding, but it also performs well on more traditional multimodal benchmarks. In particular, Fuyu-Heavy scores higher on the MMMU benchmark than even Gemini Pro.
On standard text-based benchmarks, it matches or exceeds the performance of models in the same compute class despite having to devote some of its capacity to image modeling.
It demonstrates that (with some modifications) we can scale up the Fuyu architecture and reap all of the associated benefits, including handling arbitrary size/shape images and efficiently re-using existing transformer optimizations.",United States of America,,,,,,Unreleased,,,,
GLM-4 (0116),Language,"Language modeling/generation,Question answering,Code generation,Quantitative reasoning,Translation","Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Diego Rojas, Guanyu Feng, Hanlin Zhao, Hanyu Lai, Hao Yu, Hongning Wang, Jiadai Sun, Jiajie Zhang, Jiale Cheng, Jiayi Gui, Jie Tang, Jing Zhang, Juanzi Li, Lei Zhao, Lindong Wu, Lucen Zhong, Mingdao Liu, Minlie Huang, Peng Zhang, Qinkai Zheng, Rui Lu, Shuaiqi Duan, Shudan Zhang, Shulin Cao, Shuxun Yang, Weng Lam Tam, Wenyi Zhao, Xiao Liu, Xiao Xia, Xiaohan Zhang, Xiaotao Gu, Xin Lv, Xinghan Liu, Xinyi Liu, Xinyue Yang, Xixuan Song, Xunkai Zhang, Yifan An, Yifan Xu, Yilin Niu, Yuantao Yang, Yueyan Li, Yushi Bai, Yuxiao Dong, Zehan Qi, Zhaoyu Wang, Zhen Yang, Zhengxiao Du, Zhenyu Hou, Zihan Wang",API access,"https://arxiv.org/abs/2406.12793
https://zhipuai.cn/en/devday",,ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools,2024-01-17,Z.ai (Zhipu AI),,"ChatGLM was 130B parameters, and the paper implies GLM-4 was scaled larger than previous models.",,"- 0116 has slightly worse performance than 0520
- â€œthe GLM-4 models are pre-trained on ten trillions of tokensâ€
- I did not find any information about parameters or compute. 
- â€œGLM-4 gets close to the state-of-the-art models (GPT-4-Turbo, Gemini 1.5 Pro, and Claude 3 Opus)â€  none of these models has parameters disclosed or compute estimation.

6 FLOP / token / parameter * 10000000000000 tokens * 200000000000 parameters = 1.2e+25 FLOP with â€œLikelyâ€ confidence (+/- 1 OOM)",,"""To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage""",10000000000000.0,,,,,Likely,"We introduce ChatGLM, an evolving family of large language models that we have been developing over time. This report primarily focuses on the GLM-4 language series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. They represent our most capable models that are trained with all the insights and lessons gained from the preceding three generations of ChatGLM. To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage. The high-quality alignment is achieved via a multi-stage post-training process, which involves supervised fine-tuning and learning from human feedback. Evaluations show that GLM-4 1) closely rivals or outperforms GPT-4 in terms of general metrics such as MMLU, GSM8K, MATH, BBH, GPQA, and HumanEval, 2) gets close to GPT-4-Turbo in instruction following as measured by IFEval, 3) matches GPT-4 Turbo (128K) and Claude 3 for long context tasks, and 4) outperforms GPT-4 in Chinese alignments as measured by AlignBench. The GLM-4 All Tools model is further aligned to understand user intent and autonomously decide when and which tool(s) touse -- including web browser, Python interpreter, text-to-image model, and user-defined functions -- to effectively complete complex tasks. In practical applications, it matches and even surpasses GPT-4 All Tools in tasks like accessing online information via web browsing and solving math problems using Python interpreter. Over the course, we have open-sourced a series of models, including ChatGLM-6B (three generations), GLM-4-9B (128K, 1M), GLM-4V-9B, WebGLM, and CodeGeeX, attracting over 10 million downloads on Hugging face in the year 2023 alone. The open models can be accessed through this https URL and this https URL.",China,,,,,,Unreleased,"GLM-4 (0116) has been made available through the GLM-4 API at
https://bigmodel.cn",,,
GLM-4,"Language,Multimodal,Image generation","Language modeling/generation,Question answering,Code generation,Text-to-image,Image generation","Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Diego Rojas, Guanyu Feng, Hanlin Zhao, Hanyu Lai, Hao Yu, Hongning Wang, Jiadai Sun, Jiajie Zhang, Jiale Cheng, Jiayi Gui, Jie Tang, Jing Zhang, Juanzi Li, Lei Zhao, Lindong Wu, Lucen Zhong, Mingdao Liu, Minlie Huang, Peng Zhang, Qinkai Zheng, Rui Lu, Shuaiqi Duan, Shudan Zhang, Shulin Cao, Shuxun Yang, Weng Lam Tam, Wenyi Zhao, Xiao Liu, Xiao Xia, Xiaohan Zhang, Xiaotao Gu, Xin Lv, Xinghan Liu, Xinyi Liu, Xinyue Yang, Xixuan Song, Xunkai Zhang, Yifan An, Yifan Xu, Yilin Niu, Yuantao Yang, Yueyan Li, Yushi Bai, Yuxiao Dong, Zehan Qi, Zhaoyu Wang, Zhen Yang, Zhengxiao Du, Zhenyu Hou, Zihan Wang",Hosted access (no API),https://arxiv.org/abs/2406.12793,,ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools,2024-01-17,Z.ai (Zhipu AI),,,,,,"""To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage""",10000000000000.0,,,,,Confident,"We introduce ChatGLM, an evolving family of large language models that we have been developing over time. This report primarily focuses on the GLM-4 language series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. They represent our most capable models that are trained with all the insights and lessons gained from the preceding three generations of ChatGLM. To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage. The high-quality alignment is achieved via a multi-stage post-training process, which involves supervised fine-tuning and learning from human feedback. Evaluations show that GLM-4 1) closely rivals or outperforms GPT-4 in terms of general metrics such as MMLU, GSM8K, MATH, BBH, GPQA, and HumanEval, 2) gets close to GPT-4-Turbo in instruction following as measured by IFEval, 3) matches GPT-4 Turbo (128K) and Claude 3 for long context tasks, and 4) outperforms GPT-4 in Chinese alignments as measured by AlignBench. The GLM-4 All Tools model is further aligned to understand user intent and autonomously decide when and which tool(s) touse -- including web browser, Python interpreter, text-to-image model, and user-defined functions -- to effectively complete complex tasks. In practical applications, it matches and even surpasses GPT-4 All Tools in tasks like accessing online information via web browsing and solving math problems using Python interpreter. Over the course, we have open-sourced a series of models, including ChatGLM-6B (three generations), GLM-4-9B (128K, 1M), GLM-4V-9B, WebGLM, and CodeGeeX, attracting over 10 million downloads on Hugging face in the year 2023 alone. The open models can be accessed through this https URL and this https URL.",China,,,,,,Unreleased,GLM-4 All Tools is accessible via the website https://chatglm.cn,,,
InternLM2-20B,Language,"Chat,Language modeling/generation,Question answering","Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei Chu, Xiaoyi Dong, Haodong Duan, Qi Fan, Zhaoye Fei, Yang Gao, Jiaye Ge, Chenya Gu, Yuzhe Gu, Tao Gui, Aijia Guo, Qipeng Guo, Conghui He, Yingfan Hu, Ting Huang, Tao Jiang, Penglong Jiao, Zhenjiang Jin, Zhikai Lei, Jiaxing Li, Jingwen Li, Linyang Li, Shuaibin Li, Wei Li, Yining Li, Hongwei Liu, Jiangning Liu, Jiawei Hong, Kaiwen Liu, Kuikun Liu, Xiaoran Liu, Chengqi Lv, Haijun Lv, Kai Lv, Li Ma, Runyuan Ma, Zerun Ma, Wenchang Ning, Linke Ouyang, Jiantao Qiu, Yuan Qu, Fukai Shang, Yunfan Shao, Demin Song, Zifan Song, Zhihao Sui, Peng Sun, Yu Sun, Huanze Tang, Bin Wang, Guoteng Wang, Jiaqi Wang, Jiayu Wang, Rui Wang, Yudong Wang, Ziyi Wang, Xingjian Wei, Qizhen Weng, Fan Wu, Yingtong Xiong, Chao Xu, Ruiliang Xu, Hang Yan, Yirong Yan, Xiaogui Yang, Haochen Ye, Huaiyuan Ying, Jia Yu, Jing Yu, Yuhang Zang, Chuyu Zhang, Li Zhang, Pan Zhang, Peng Zhang, Ruijie Zhang, Shuo Zhang, Songyang Zhang, Wenjian Zhang, Wenwei Zhang, Xingcheng Zhang, Xinyue Zhang, Hui Zhao, Qian Zhao, Xiaomeng Zhao, Fengzhe Zhou, Zaida Zhou, Jingming Zhuo, Yicheng Zou, Xipeng Qiu, Yu Qiao, Dahua Lin",Open weights (restricted use),https://arxiv.org/abs/2403.17297,,InternLM2 Technical Report,2024-01-12,"Shanghai AI Lab,SenseTime,Chinese University of Hong Kong (CUHK),Fudan University",20000000000.0,20B,3.12e+23,6ND = 6 * 2600000000000 * 20000000000 = 3.12e+23,Unspecified unreleased,"""The text data in our pre-training dataset can be categorized by source into web pages,
papers, patents, and books. To transform these sources into a pre-training dataset, we first
standardize all data into a specified format, categorize them by type and language, and
store them in JSON Lines (jsonl) format. Then, for all data, we apply several processing
steps including rule-based filtering, data deduplication, safety filtering, and quality filtering.
This results in a rich, safe, and high-quality text dataset.""",2600000000000.0,"""The total number of tokens used for pre-training the 1.8B, 7B, and 20B models ranges from 2.0T to 2.6T, and the pre-training process consists of three distinct phases. """,,,,Confident,"The evolution of Large Language Models (LLMs) like ChatGPT and GPT-4 has sparked discussions on the advent of Artificial General Intelligence (AGI). However, replicating such advancements in open-source models has been challenging. This paper introduces InternLM2, an open-source LLM that outperforms its predecessors in comprehensive evaluations across 6 dimensions and 30 benchmarks, long-context modeling, and open-ended subjective evaluations through innovative pre-training and optimization techniques. The pre-training process of InternLM2 is meticulously detailed, highlighting the preparation of diverse data types including text, code, and long-context data. InternLM2 efficiently captures long-term dependencies, initially trained on 4k tokens before advancing to 32k tokens in pre-training and fine-tuning stages, exhibiting remarkable performance on the 200k ``Needle-in-a-Haystack"" test. InternLM2 is further aligned using Supervised Fine-Tuning (SFT) and a novel Conditional Online Reinforcement Learning from Human Feedback (COOL RLHF) strategy that addresses conflicting human preferences and reward hacking. By releasing InternLM2 models in different training stages and model sizes, we provide the community with insights into the model's evolution.","China,Hong Kong,Hong Kong,China",,,,,,Unreleased,need to apply for commercial license. there's a repo but doesn't look like there's pretraining code.  https://github.com/InternLM/InternLM ,,,
DeepSeek LLM 67B,Language,"Chat,Language modeling/generation,Question answering","Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, Huazuo Gao, Kaige Gao, Wenjun Gao, Ruiqi Ge, Kang Guan, Daya Guo, Jianzhong Guo, Guangbo Hao, Zhewen Hao, Ying He, Wenjie Hu, Panpan Huang, Erhang Li, Guowei Li, Jiashi Li, Yao Li, Y.K. Li, Wenfeng Liang, Fangyun Lin, A.X. Liu, Bo Liu, Wen Liu, Xiaodong Liu, Xin Liu, Yiyuan Liu, Haoyu Lu, Shanghao Lu, Fuli Luo, Shirong Ma, Xiaotao Nie, Tian Pei, Yishi Piao, Junjie Qiu, Hui Qu, Tongzheng Ren, Zehui Ren, Chong Ruan, Zhangli Sha, Zhihong Shao, Junxiao Song, Xuecheng Su, Jingxiang Sun, Yaofeng Sun, Minghui Tang, Bingxuan Wang, Peiyi Wang, Shiyu Wang, Yaohui Wang, Yongji Wang, Tong Wu, Y. Wu, Xin Xie, Zhenda Xie, Ziwei Xie, Yiliang Xiong, Hanwei Xu, R.X. Xu, Yanhong Xu, Dejian Yang, Yuxiang You, Shuiping Yu, Xingkai Yu, B. Zhang, Haowei Zhang, Lecong Zhang, Liyue Zhang, Mingchuan Zhang, Minghua Zhang, Wentao Zhang, Yichao Zhang, Chenggang Zhao, Yao Zhao, Shangyan Zhou, Shunfeng Zhou, Qihao Zhu, Yuheng Zou",Open weights (restricted use),"https://arxiv.org/abs/2401.02954, https://github.com/deepseek-ai/DeepSeek-LLM",,DeepSeek LLM: Scaling Open-Source Language Models with Longtermism,2024-01-05,DeepSeek,67000000000.0,"67B
",8.04e+23,67B parameters * 2T tokens * 6 FLOP / token / parameter = 8.04e23 FLOP,Unspecified unreleased,"""We collect 2 trillion tokens for pre-training, primarily in Chinese and English.""

""We have gained valuable insights from reputable sources such as (Computer, 2023; Gao et al., 2020; Penedo et al., 2023; Touvron et al., 2023a)... We adopted an aggressive deduplication strategy, expanding the deduplication scope. Our analysis revealed that deduplicating the entire Common Crawl corpus results in higher removal of duplicate instances compared to deduplicating within a single dump""",2000000000000.0,"""We collect 2 trillion tokens for pre-training, primarily in Chinese and English""",,,,Confident,"The rapid development of open-source large language models (LLMs) has been truly remarkable. However, the scaling law described in previous literature presents varying conclusions, which casts a dark cloud over scaling LLMs. We delve into the study of scaling laws and present our distinctive findings that facilitate scaling of large scale models in two commonly used open-source configurations, 7B and 67B. Guided by the scaling laws, we introduce DeepSeek LLM, a project dedicated to advancing open-source language models with a long-term perspective. To support the pre-training phase, we have developed a dataset that currently consists of 2 trillion tokens and is continuously expanding. We further conduct supervised fine-tuning (SFT) and Direct Preference Optimization (DPO) on DeepSeek LLM Base models, resulting in the creation of DeepSeek Chat models. Our evaluation results demonstrate that DeepSeek LLM 67B surpasses LLaMA-2 70B on various benchmarks, particularly in the domains of code, mathematics, and reasoning. Furthermore, open-ended evaluations reveal that DeepSeek LLM 67B Chat exhibits superior performance compared to GPT-3.5.",China,,,,,,Unreleased,"repo with inference code and details, but no training code:

https://github.com/deepseek-ai/deepseek-LLM/blob/main/LICENSE-MODEL",,,
YaYi 2.0,Language,Language modeling/generation,"Yin Luo, Qingchao Kong, Nan Xu, Jia Cao, Bao Hao, Baoyu Qu, Bo Chen, Chao Zhu Chenyang Zhao, Donglei Zhang, Fan Feng, Feifei Zhao, Hailong Sun, Hanxuan Yang, Haojun Pan, Hongyu Liu, Jianbin Guo, Jiangtao Du, Jingyi Wang, Junfeng Li, Lei Sun, Liduo Liu, Lifeng Dong, Lili Liu, Lin Wang, Liwen Zhang, Minzheng Wang, Pin Wang, Ping Yu, Qingxiao Li, Rui Yan, Rui Zou, Ruiqun Li, Taiwen Huang, Xiaodong Wang, Xiaofei Wu, Xin Peng, Xina Zhang, Xing Fang, Xinglin Xiao, Yanni Hao, Yao Dong, Yigang Wang, Ying Liu, Yongyu Jiang, Yungan Wang, Yuqi Wang, Zhangsheng Wang, Zhaoxin Yu, Zhen Luo, Wenji Mao, Lei Wang, Dajun Zeng",Open weights (restricted use),https://arxiv.org/abs/2312.14862v1,7.0,"YAYI 2: Multilingual Open-Source Large Language Models
",2023-12-22,Yayi (Wenge),30000000000.0,,4.77e+23,1000 A800 GPUs,,2650000000000,2650000000000.0,,,6*30000000000*2650000000000=4.77e+23,NVIDIA A800 PCIe 40 GB,Likely,"In this technical report, we propose YAYI 2, including both base and chat models, with 30 billion parameters. YAYI 2 is pre-trained from scratch on a multilingual corpus which contains 2.65 trillion tokens filtered by our pre-training data processing pipeline.",China,,,,1000.0,,Open source,"unclear license
https://huggingface.co/wenge-research/yayi2-30b

this license file https://github.com/wenge-research/YAYI2/blob/main/COMMERCIAL_LICENSE
prohibits particular usage (i.e. military, direct competitors) 
To use YAYI2 models commercially, you must apply for a commercial license.

apache 2.0 for code",,,
Gemini Nano-2,"Multimodal,Language,Vision,Audio","Chat,Image captioning,Speech recognition (ASR)",Gemini Team,Unreleased,https://arxiv.org/abs/2312.11805,633.0,Gemini: A Family of Highly Capable Multimodal Models,2023-12-19,Google DeepMind,3250000000.0,3.25B,,"More tokens than Chinchilla-optimal:

""The number of tokens used to train the largest models were determined following the approach in Hoffmann et al. (2022). The smaller models are trained for significantly more tokens to improve performance for a given inference budget, similar to the approach advocated in Touvron et al. (2023a)""

Chinchilla was 1.4T tokens for 70B params, so Chinchilla-optimal for 3.25B params would be ~1.4T/20 = 70B tokens.

So compute was significantly greater than 3.25B * 70B * 6, which is 1.4e21. 

Touvron et al. is the Llama 1 paper, in which a 6.7B model is trained for 1T tokens. Using the same ratio, a 3.25B model would be trained on ~500B tokens. 3.25 * 500B * 6 = 9.75e21. No guarantee that the exact ratio for Nano is close to Llama's, of course.",Unspecified unreleased,"""Gemini models are trained on a dataset that is both multimodal and multilingual. Our pre-training dataset uses data from web documents, books, and code, and includes image, audio, and video data.""",,,,,Google TPU v5e,Confident,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks - notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.","United States of America,United Kingdom of Great Britain and Northern Ireland",,,,,,Unreleased,"May be API access in the future. There is an Android API but it ""is under a closed early access preview program at this time"": https://ai.google.dev/gemini-api/docs/get-started/android_aicore",,,
Lyra-Fr 10B,Language,"Language modeling/generation,Semantic search,Text classification,Question answering,Text summarization,Sentiment classification",,API access,https://aws.amazon.com/fr/blogs/machine-learning/lighton-lyra-fr-model-is-now-available-on-amazon-sagemaker/,,LightOn Lyra-fr model is now available on Amazon SageMaker,2023-12-15,LightOn,10000000000.0,10B,,,,"""Lyra-fr was trained on a large corpus of French curated data""",,,,,,Likely,"We are thrilled to announce the availability of the LightOn Lyra-fr foundation model for customers using Amazon SageMaker. LightOn is a leader in building foundation models specializing in European languages. Lyra-fr is a state-of-the-art French language model that can be used to build conversational AI, copywriting tools, text classifiers, semantic search, and more. You can easily try out this model and use it with Amazon SageMaker JumpStart. JumpStart is the machine learning (ML) hub of SageMaker that provides access to foundation models in addition to built-in algorithms and end-to-end solution templates to help you quickly get started with ML.",France,,,,,,,,,,
Konan LLM 41B,"Language,Vision",Language modeling/generation,"Yang Seung-hyun, Wiretin, Changmin, Kim Jong-tae",Hosted access (no API),"https://en.konantech.com/en/llm/konanllm
https://techfinch.kr/ai/konan-technology-unveils-konan-llm--its-own-ai-language-model
https://www.dbpia.co.kr/journal/articleDetail?nodeId=NODE11610127 ",,Konan LLM: A Korean Large Language Model,2023-12-15,Konan Technology,41000000000.0,,1.722e+23,=41000000000 parameters * 700000000000 tokens [see dataset size notes]  * 6 FLOP / token / parameter =1.722 Ã— 10^23 FLOP,Unspecified unreleased,,700000000000.0,"https://www.konantech.com/pr/press?number=2628&pn=1&stype2=&sfi=subj&sword=

Since 2007, via the real-time AI analysis service pulseK, over 20.5 billion pieces of data have been independently secured.
Among them, only 2 billion high-quality, large-scale data pieces have been used for training.",,,,Likely,"Konan LLM is a Large Language Model developed in-house by Konan Technology. Konan Technology optimized for super-large AI training, it leverages high-quality, large-scale data and over 20 years of expertise in natural language processing.
Konan LLM supports all corporate documentation and creative tasks,
leading the way in workplace innovation.",Korea (Republic of),,,,,,Unreleased,,,,
Poro 34B,Language,"Code generation,Language modeling/generation","Risto Luukkonen, Jonathan Burdge, Elaine Zosa, Aarne Talman, Ville Komulainen, VÃ¤inÃ¶ HatanpÃ¤Ã¤, Peter Sarlin, Sampo Pyysalo",Open weights (unrestricted),https://arxiv.org/abs/2404.01856,,Poro 34B and the Blessing of Multilinguality,2023-12-14,"High-Performance Language Technologies (HPLT),University of Turku",34200000000.0,https://huggingface.co/LumiOpen/Poro-34B,2.052e+23,"6ND = 6*1T*34.2B= 2.04e+23

""This allowed total training cycle throughput of 49618 TFLOPs and 174378 tokens/second.""

the training took around 18 months (https://hplt-project.org/deliverables)
49618*18*30*24*3600*10^12=2.3149774e+24

","mC4,SlimPajama,StarCoder,Dolma","https://huggingface.co/LumiOpen/Poro-34B

""The Finnish dataset is a combination of many Finnish resources:

    Finnish Internet Parsebank
    mC4 multilingual colossal, cleaned Common Crawl
    Common Crawl Finnish
    Finnish Wikipedia
    LÃ¶nnrot Projekti LÃ¶nnrot
    Suomi24 The Suomi 24 Corpus 2001-2020
    Reddit r/Suomi submissions and comments
    STT Finnish News Agency Archive 1992-2018
    Yle Finnish News Archive 2011-2018
    Yle Finnish News Archive 2019-2020
    Yle News Archive Easy-to-read Finnish 2011-2018
    Yle News Archive Easy-to-read Finnish 2019-2020""
""

""Poro is a 34B parameter decoder-only transformer pretrained on Finnish, English and code. It is being trained on 1 trillion tokens (700 billion as of this release). Poro is a fully open source model and is made available under the Apache 2.0 License.""",1000000000000.0,"1T tokens, assuming 0.75 word per token
""Poro is a 34B parameter decoder-only transformer pretrained on Finnish, English and code. It is being trained on 1 trillion tokens. Poro is a fully open source model and is made available under the Apache 2.0 License.""",,,AMD Radeon Instinct MI250X,Confident,"The pretraining of state-of-the-art large language models now requires trillions of words of text, which is orders of magnitude more than available for the vast majority of languages. While including text in more than one language is an obvious way to acquire more pretraining data, multilinguality is often seen as a curse, and most model training efforts continue to focus near-exclusively on individual large languages. We believe that multilinguality can be a blessing and that it should be possible to substantially improve over the capabilities of monolingual models for small languages through multilingual training. In this study, we introduce Poro 34B, a 34 billion parameter model trained for 1 trillion tokens of Finnish, English, and programming languages, and demonstrate that a multilingual training approach can produce a model that not only substantially advances over the capabilities of existing models for Finnish, but also excels in translation and is competitive in its class in generating English and programming languages. We release the model parameters, scripts, and data under open licenses at this https URL.","Multinational,Finland",,,,512.0,,Open source,"Apache 2.0
https://huggingface.co/LumiOpen/Poro-34B

https://github.com/TurkuNLP/Megatron-DeepSpeed",,,
Imagen 2,Image generation,"Text-to-image,Image generation,Video generation","AÃ¤ron van den Oord, Ali Razavi, Benigno Uria, Ã‡aÄŸlar ÃœnlÃ¼, Charlie Nash, Chris Wolff, Conor Durkan, David Ding, Dawid GÃ³rny, Evgeny Gladchenko, Felix Riedel, Hang Qi, Jacob Kelly, Jakob Bauer, Jeff Donahue, Junlin Zhang, Mateusz Malinowski, MikoÅ‚aj BiÅ„kowski, Pauline Luc, Robert Riachi, Robin Strudel, Sander Dieleman, Tobenna Peter Igwe, Yaroslav Ganin, Zach Eaton-Rosen",API access,https://deepmind.google/technologies/imagen-2/,,Imagen 2,2023-12-13,Google DeepMind,,,,,Unspecified unreleased,,,,,,,Unknown,"Imagen 2 is our most advanced text-to-image diffusion technology, delivering high-quality, photorealistic outputs that are closely aligned and consistent with the userâ€™s prompt. It can generate more lifelike images by using the natural distribution of its training data, instead of adopting a pre-programmed style.","United States of America,United Kingdom of Great Britain and Northern Ireland",,,,,,Unreleased,Accessible through Google's Vertex AI platform. Some features currently limited to whitelisted testers.,,,
Mixtral 8x7B,Language,"Language modeling/generation,Question answering,Quantitative reasoning,Translation","Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, LÃ©lio Renard Lavaud, Louis Ternon, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, ThÃ©ophile Gervet, Thibaut Lavril, Thomas Wang, TimothÃ©e Lacroix, William El Sayed.",Open weights (unrestricted),"https://mistral.ai/news/mixtral-of-experts/, https://arxiv.org/abs/2401.04088",,Mixtral of experts: A high quality Sparse Mixture-of-Experts.,2023-12-11,Mistral AI,46700000000.0,"46.7B *sparse* params. 12.9B params used on average:

""Concretely, Mixtral has 46.7B total parameters but only uses 12.9B parameters per token. It, therefore, processes input and generates output at the same speed and for the same cost as a 12.9B model.""",7.74e+23,"Assuming the model was trained on ~1-10 trillions of tokens (same OOM as the models from the comparison in Figure 1. Llama 2 was trained on 2T tokens) + Mistral Small 3 was trained on 8T of tokens, we can estimate training compute with ""speculative"" confidence:

6 FLOP / token / parameter * 12.9 * 10^9 active parameters * 10*10^12 tokens [speculatively] = 7.74e+23 FLOP",Unspecified unreleased,"""Mixtral is pretrained with multilingual data using a context size of 32k tokens""",,,,,,Speculative,"Today, the team is proud to release Mixtral 8x7B, a high-quality sparse mixture of experts model (SMoE) with open weights. Licensed under Apache 2.0. Mixtral outperforms Llama 2 70B on most benchmarks with 6x faster inference. It is the strongest open-weight model with a permissive license and the best model overall regarding cost/performance trade-offs. In particular, it matches or outperforms GPT3.5 on most standard benchmarks.",France,,,,,,Unreleased,Apache 2.0,,,
Mistral Medium,Language,Chat,,API access,https://mistral.ai/news/la-plateforme/,,La Plateforme,2023-12-11,Mistral AI,,"May be 70B, based on this weird leak episode. 

https://venturebeat.com/ai/mistral-ceo-confirms-leak-of-new-open-source-ai-model-nearing-gpt-4-performance/",,,,,,,,,,Unknown,"Mistral-medium. Our highest-quality endpoint currently serves a prototype model, that is currently among the top serviced models available based on standard benchmarks. It masters English/French/Italian/German/Spanish and code and obtains a score of 8.6 on MT-Bench. The following table compare the performance of the base models of Mistral-medium, Mistral-small and the endpoint of a competitor.",France,,,,,,Unreleased,,,,
XVERSE-65B-2,Language,"Chat,Language modeling/generation",,Open weights (restricted use),https://github.com/xverse-ai/XVERSE-65B/blob/main/README_EN.md,,,2023-12-08,"XVERSE Technology,Shenzhen Yuanxiang Technology",65000000000.0,Based on the name. Exact count unknown but may be listed on Hugging Face.,1.24800000000001e+24,C = 6ND = 6 * 3.2T tokens * 65B params = 1.248e24 FLOP,,"[2023/12/08] Released the XVERSE-65B-2 base model. This model builds upon its predecessor through Continual Pre-Training, reaching a total training volume of 3.2 trillion tokens.",3200000000000.0,"Training Data: The model has been thoroughly trained on a diversified and high-quality dataset consisting of 2.6 trillion of tokens, including more than 40 languages such as Chinese, English, Russian, and Spanish. The sampling ratio of different types of data is finely set, which makes the performance of Chinese and English excellent, and also takes into account the effect of other languages.

Assume 0.85 words per token on average for the mix of languages.",4096.0,November 6 to December 8 is 32 days. They did 600B tokens of continual pretraining during this period. The model's total tokens are 3200B. Therefore the total pretraining time was around (32 days * 24 hours/day)*(3200/600) = 4096 hours.,,Confident,,"China,China",,,,,,Open source,"https://github.com/xverse-ai/XVERSE-65B/blob/main/README_EN.md 

license info:
""The use of the source code in this repository must follow the Apache-2.0 open-source license, while the use of the model weights of XVERSE-65B needs to adhere to the Model License Agreement.
The XVERSE-65B model weights are fully open to academic research and support free commercial use. To apply for a commercial license, please fill in the application form. For other questions or collaborations, please contact opensource@xverse.cn.""",,,
Gemini 1.0 Ultra,"Multimodal,Language,Vision","Language modeling,Visual question answering,Chat,Translation",Gemini Team,API access,https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,633.0,Gemini: A Family of Highly Capable Multimodal Models,2023-12-06,Google DeepMind,,,5.0000000001e+25,"This number is an estimate based on limited evidence. In particular, we combine information about the performance of Gemini Ultra on various benchmarks compared to other models, and guesstimates about the hardware setup used for training to arrive at our estimate. Our reasoning and calculations are detailed in this Colab notebook.
https://colab.research.google.com/drive/1sfG91UfiYpEYnj_xB5YRy07T5dv-9O_c",Unspecified unreleased,"""Gemini models are trained on a dataset that is both multimodal and multilingual. Our pretraining dataset uses data from web documents, books, and code, and includes image, audio, and video data... We find that data quality is critical to a highlyperforming model, and believe that many interesting questions remain around finding the optimal dataset distribution for pretraining.""",,,2400.0,"Dylan Patel, author of SemiAnalysis, speculates that the training duration of Gemini may have been 100 days.",Google TPU v4,Speculative,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks â€” notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
","United States of America,United Kingdom of Great Britain and Northern Ireland",,,,57000.0,,Unreleased,API access: https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models,,206082489.8134068,619525432.37549
Gemini 1.0 Pro,"Multimodal,Language,Vision","Language modeling,Visual question answering,Chat,Translation",Gemini Team,API access,https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,633.0,Gemini: A Family of Highly Capable Multimodal Models,2023-12-06,Google DeepMind,,,,"Training compute estimated to be 1.8e25 FLOP from benchmark scores. https://colab.research.google.com/drive/1r3pUMhB7Kh0Gls9eG-v_XefWrye9fVQR?usp=sharing 

Our reasoning and calculations for Gemini 1 Ultra are detailed in this Colab notebook.
https://colab.research.google.com/drive/1sfG91UfiYpEYnj_xB5YRy07T5dv-9O_c

",Unspecified unreleased,"""Gemini models are trained on a dataset that is both multimodal and multilingual. Our pretraining dataset uses data from web documents, books, and code, and includes image, audio, and video data... We find that data quality is critical to a highlyperforming model, and believe that many interesting questions remain around finding the optimal
dataset distribution for pretraining.""",,,,,Google TPU v4,Speculative,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks â€” notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
","United States of America,United Kingdom of Great Britain and Northern Ireland",,,,,,Unreleased,API access: https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models,,,
tsuzumi 7B,Language,"Chat,Language modeling/generation",,,https://group.ntt/en/magazine/blog/tsuzumi/,,"NTT's Large Language Model ""tsuzumi"" is Here!",2023-12-01,NTT Communication Science Laboratories,7000000000.0,7B,,,,,,,,,,Confident,,Japan,,,,,,,,,,
Qwen-72B,Language,"Chat,Code generation,Language modeling/generation,Question answering","Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, Tianhang Zhu",Open weights (restricted use),https://huggingface.co/Qwen/Qwen-72B,,,2023-11-30,Alibaba,72000000000.0,72B,1.3e+24,"72 billion params, 3 trillion tokens
72b * 3T * 6 = 1.3e24",,"""It is pretrained on over 3 trillion tokens, including Chinese, English, multilingual texts, code, and mathematics, covering general and professional fields""",3000000000000.0,Assuming not trained for multiple epochs.,,,,Confident,"Qwen-72B is the 72B-parameter version of the large language model series, Qwen (abbr. Tongyi Qianwen), proposed by Alibaba Cloud. Qwen-72B is a Transformer-based large language model, which is pretrained on a large volume of data, including web texts, books, codes, etc. Additionally, based on the pretrained Qwen-72B, we release Qwen-72B-Chat, a large-model-based AI assistant, which is trained with alignment techniques.",China,,,,,,Unreleased,"up to 100m active users:
https://github.com/QwenLM/Qwen/blob/main/Tongyi%20Qianwen%20LICENSE%20AGREEMENT",,,
Granite 13B,Language,"Chat,Language modeling/generation,Question answering,Text summarization",,API access,https://www.ibm.com/downloads/cas/X9W4O6BM,,Granite Foundation Models,2023-11-30,IBM,13000000000.0,13 billion,2.44e+23,"Estimate using hardware:

""Granite.13b.v1 used 256 A100 GPUs for 1056 hours and 120 TFLOPs.
Granite.13b.v2 was trained on the same infrastructure for an
additional 1152 hours with 120 TFLOPS, bringing the total to
2208 hours""

Seems like 120 TFLOPS is the output per GPU after utilization, though they don't explicitly explain that part. That's 38% utilization.

256 * 2208 * 3600 * 120 TFLOPS = 2.44e23

Using 6ND:

""The second version of the granite.13b models leverages an updated base model trained on 2.5T trillion tokens.""

""The granite.13b.v1 base model is trained for 300K iterations,
with a batch size of 4M tokens, for a total of 1.25 trillion
5 tokens. The granite.13b.v2 base model continued pre-training
on top of the granite.13b.v1 checkpoint for an additional 300K
iterations and a total of 2.5 trillion tokens.""

2.5T * 13B * 6 = 1.95e23","Unspecified unreleased,Common Crawl,arXiv,OPENWEBTEXT","""To support the training of large enterprise-grade foundation
models, including granite.13b, IBM curated a massive dataset
of relevant unstructured language data from sources across
academia, the internet, enterprise (e.g., financial, legal), and
code.""

More breakdowns in paper, 20 sources in total

https://www.ibm.com/docs/en/cloud-paks/cp-data/4.8.x?topic=models-granite-13b-v1-model-card ",2500000000000.0,"2.5T tokens, 1.875T words at 0.75 words/token

https://www.ibm.com/docs/en/cloud-paks/cp-data/5.0.x?topic=models-granite-13b-chat-v2-model-card",2208.0,"""Granite.13b.v1 used 256 A100 GPUs for 1056 hours and 120 TFLOPs. Granite.13b.v2 was trained on the same infrastructure for an
additional 1152 hours with 120 TFLOPS, bringing the total to
2208 hours""",NVIDIA A100,Likely,"We introduce the Granite series of decoder-only foundation models for generative artificial intelligence (AI) tasks that are ready for enterprise use. We report on the architecture, capabilities, underlying data and data governance, training algorithms, compute infrastructure, energy and carbon footprint,
testing and evaluation, socio-technical harms and mitigations, and usage policies.",United States of America,,,,,,Unreleased,,,,
Yuan 2.0,Language,"Language modeling/generation,Translation,Code generation","Shaohua Wu, Xudong Zhao, Shenling Wang, Jiangang Luo, Lingjun Li, Xi Chen, Bing Zhao, Wei Wang, Tong Yu, Rongguo Zhang, Jiahua Zhang, Chao Wang",Open weights (restricted use),https://arxiv.org/abs/2311.15786v1,,YUAN 2.0: A Large Language Model with Localized Filtering-based Attention,2023-11-27,Inspur,102600000000.0,102.6 billion,1.78e+23,"Trained on 288B tokens

6*102.6b*288b = 1.78e23",,"""The pretraining corpus includes a mix of books, codes, and encyclopedia in both Chinese and English (Table 2)""

with synthetic code data:
""Code (CN). Considering the diversity of programming tasks, we also build a synthesized instruction dataset
with 4 million code samples in Chinese. To cover the concepts involved in programming tasks as many as
possible, we collect 15,000 words of programming, computer science, mathematics, and other relevant
topics from the Sogou input dictionary. Two topic words are randomly selected as the seeds for a wellcrafted prompt in each time. Then the prompt will be fed to GPT-3.5 to generate a programming task and
corresponding Python solution. ""

and translated open-source fine-tuning instruction data:
""We construct a fine-tuning dataset focused on code, math and chat tasks.
Code Instruction dataset. We collect some open-source code instruction datasets, including CodeAlpaca20k [28], Evol-Instruct-Code-80k[38], CodeFuse-CodeExercise-Python-27k and CodeFuse-Evolinstruction-66k [39]. We translate the English code instruction into Chinese with GPT-3.5""",288000000000.0,"Most likely the 288B tokens do not represent multiple epochs. As a sense check, Table 2 appears to indicate that 5.73% of pre-training tokens come from synthetically generated text output by GPT-3.5. If the full training corpus is 288B tokens, this would imply ~$24k in API costs at $1.50/1M tokens to generate the data, which seems plausible.",,,,Confident,"In this work, the Localized Filtering-based Attention (LFA) is introduced to incorporate prior knowledge of local dependencies of natural language into Attention. Based on LFA, we develop and release Yuan 2.0, a large language model with parameters ranging from 2.1 billion to 102.6 billion. A data filtering and generation method is presented to build pretraining and fine-tuning dataset in high quality. A distributed training method with non-uniform pipeline parallel, data parallel, and optimizer parallel is proposed, which greatly reduces the bandwidth requirements of intra-node communication, and achieves good performance in large-scale distributed training. Yuan 2.0 models display impressive ability in code generation, math problem-solving, and chat compared with existing models. The latest version of YUAN 2.0, including model weights and source code, is accessible at Github.",China,,,,,,Open source,"commercial ok, but nothing that ""may cause harm to the country and society, or for any services that have not undergone security assessment and filing""
https://huggingface.co/IEITYuan/Yuan2-102B-hf

https://github.com/IEIT-Yuan/Yuan-2.0?tab=License-1-ov-file",,,
Amazon Transcribe,Speech,"Speech-to-text,Speech recognition (ASR)",,API access,https://aws.amazon.com/ru/blogs/machine-learning/amazon-transcribe-announces-a-new-speech-foundation-model-powered-asr-system-that-expands-support-to-over-100-languages/,,Amazon Transcribe announces a new speech foundation model-powered ASR system that expands support to over 100 languages,2023-11-26,Amazon,,"""multi-billion parameter speech foundation model""",,Probably not trained with 10^25 FLOP. Released in 2023 and doesn't seem to be a hugely important product.,Unspecified unreleased,,,,,,,Unknown,"Amazon Transcribe is a fully managed automatic speech recognition (ASR) service that makes it straightforward for you to add speech-to-text capabilities to your applications. Today, we are happy to announce a next-generation multi-billion parameter speech foundation model-powered system that expands automatic speech recognition to over 100 languages. In this post, we discuss some of the benefits of this system, how companies are using it, and how to get started. We also provide an example of the transcription output below.

Transcribeâ€™s speech foundation model is trained using best-in-class, self-supervised algorithms to learn the inherent universal patterns of human speech across languages and accents. It is trained on millions of hours of unlabeled audio data from over 100 languages. The training recipes are optimized through smart data sampling to balance the training data between languages, ensuring that traditionally under-represented languages also reach high accuracy levels.",United States of America,,,,,,Unreleased,,,,
Inflection-2,Language,"Language modeling,Language modeling/generation,Chat,Question answering",,Hosted access (no API),https://inflection.ai/inflection-2,,Inflection-2: The Next Step Up,2023-11-22,Inflection AI,,,1.001e+25,"""Inflection-2 was trained on 5,000 NVIDIA H100 GPUs in fp8 mixed precision for ~10Â²âµ FLOPs""

(the second 1 is there because of airtable being wonky, it's not a real sig fig)",Unspecified unreleased,,,,,,NVIDIA H100 SXM5 80GB,Confident,"Today we are proud to announce that we have completed training of Inflection-2, the best model in the world for its compute class and the second most capable LLM in the world today. Our mission at Inflection is to create a personal AI for everyone. Just a few months ago, we announced Inflection-1 â€” a best-in-class language model that currently powers Pi. Our new model, Inflection-2, is substantially more capable than Inflection-1, demonstrating much improved factual knowledge, better stylistic control, and dramatically improved reasoning.",United States of America,,,,5000.0,,Unreleased,"via Pi, no API",,,285986159.97968376
Lyria,Audio,Audio generation,"Kazuya Kawakami, David Ding, BjÃ¶rn Winckler, CÄƒtÄƒlina Cangea, Tobenna Peter Igwe, Will Grathwohl, Yan Wu, Yury Sulsky, Jacob Kelly, Charlie Nash, Conor Durkan, Yaroslav Ganin, Tom Eccles, Zach Eaton-Rosen, Jakob Bauer, Mikita Sazanovich, Morgane RiviÃ¨re, Evgeny Gladchenko, MikoÅ‚aj BiÅ„kowski, Ali Razavi, Jeff Donahue, Benigno Uria, Sander Dieleman, Sherjil Ozair, John Schultz, Ankush Gupta, Junlin Zhang, Drew Jaegle, AÃ¤ron van den Oord.",Unreleased,https://deepmind.google/discover/blog/transforming-the-future-of-music-creation/,,Transforming the future of music creation,2023-11-16,Google DeepMind,,,,,Unspecified unreleased,,,,,,,Unknown,"Music contains huge amounts of information â€” consider every beat, note, and vocal harmony in every second. When generating long sequences of sound, itâ€™s difficult for AI models to maintain musical continuity across phrases, verses, or extended passages. Since music often includes multiple voices and instruments at the same time, it's much harder to create than speech.

Built by Google DeepMind, the Lyria model excels at generating high-quality music with instrumentals and vocals, performing transformation and continuation tasks, and giving users more nuanced control of the outputâ€™s style and performance.","United States of America,United Kingdom of Great Britain and Northern Ireland",,,,,,Unreleased,,,,
Nemotron-3-8B,Language,"Chat,Language generation,Language modeling/generation,Translation,Code generation,Question answering",,Open weights (restricted use),"https://developer.nvidia.com/blog/nvidia-ai-foundation-models-build-custom-enterprise-chatbots-and-co-pilots-with-production-ready-llms/

https://catalog.ngc.nvidia.com/orgs/nvidia/teams/nemo/models/nemotron-3-8b-base-4k",,NVIDIA AI Foundation Models: Build Custom Enterprise Chatbots and Co-Pilots with Production-Ready LLMs,2023-11-15,NVIDIA,8000000000.0,,1.8e+23,"https://huggingface.co/nvidia/nemotron-3-8b-base-4k

""This model was trained on a dataset containing 3.8 Trillion tokens of text""

8 billion * 3.8 trillion * 6 = 1.8e23

Also, using the hardware method: ""1,024 A100s were used for 19 days to train the model.""

19*1024 * 312 trillion * 24 * 3600 * 0.3 = 1.57e23","Unspecified unreleased,Flan,P3 (Public Pool of Prompts)","""NVIDIA models are trained on a diverse set of public and proprietary datasets. This model was trained on a dataset containing 3.8 Trillion tokens of text. The dataset contains 53 different human languages (including English, German, Russian, Spanish, French, Japanese, Chinese, Italian, and Dutch) and 37 programming languages. The model also uses the training subsets of downstream academic benchmarks from sources like FLANv2, P3, and NaturalInstructions v2""",3800000000000.0,,456.0,19 days,NVIDIA A100,Confident,"Large language models (LLMs) are revolutionizing data science, enabling advanced capabilities in natural language understanding, AI, and machine learning. Custom LLMs, tailored for domain-specific insights, are finding increased traction in enterprise applications.

The NVIDIA Nemotron-3 8B family of foundation models is a powerful new tool for building production-ready generative AI applications for the enterpriseâ€“fostering innovations ranging from customer service AI chatbots to cutting-edge AI products.",United States of America,,,,1024.0,0.3473,Unreleased,"can't use to train other models:

https://developer.download.nvidia.com/ai-foundation-models/nvidia-ai-foundation-models-license-10Nov2023.pdf",,,
Samsung Gauss Language,Language,"Language modeling/generation,Question answering,Text summarization,Translation",,,https://techcrunch.com/2023/11/08/samsung-unveils-chatgpt-alternative-samsung-gauss-that-can-generate-text-code-and-images/,,,2023-11-08,Samsung,,,,,,,,,,,,Unknown,"Just a few days after OpenAIâ€™s developer event, Samsung unveiled its own generative AI model, Samsung Gauss, at the Samsung AI Forum 2023.

Samsung Gauss, developed by the tech giantâ€™s research unit Samsung Research, consists of three tools: Samsung Gauss Language, Samsung Gauss Code and Samsung Gauss Image.

Samsung Gauss Language is a large language model that can understand human language and answer questions like ChatGPT. It can be used to increase productivity in several ways. For instance, it can help you write and edit emails, summarize documents and translate languages. Samsung plans to incorporate the large language model into its devices like phones, laptops and tablets to make the companyâ€™s smart devices a bit smarter. When asked if it supports both English and Korean as interaction languages, a spokesperson of Samsung declined to comment on it.",Korea (Republic of),,,,,,,,,,
Samsung Gauss Code,Language,Code generation,,,https://techcrunch.com/2023/11/08/samsung-unveils-chatgpt-alternative-samsung-gauss-that-can-generate-text-code-and-images/,,,2023-11-08,Samsung,,,,,,,,,,,,Unknown,"Just a few days after OpenAIâ€™s developer event, Samsung unveiled its own generative AI model, Samsung Gauss, at the Samsung AI Forum 2023.

Samsung Gauss, developed by the tech giantâ€™s research unit Samsung Research, consists of three tools: Samsung Gauss Language, Samsung Gauss Code and Samsung Gauss Image.

Samsung Gauss Language is a large language model that can understand human language and answer questions like ChatGPT. It can be used to increase productivity in several ways. For instance, it can help you write and edit emails, summarize documents and translate languages. Samsung plans to incorporate the large language model into its devices like phones, laptops and tablets to make the companyâ€™s smart devices a bit smarter. When asked if it supports both English and Korean as interaction languages, a spokesperson of Samsung declined to comment on it.",Korea (Republic of),,,,,,,,,,
Samsung Gauss Image,Image generation,Image generation,,,https://techcrunch.com/2023/11/08/samsung-unveils-chatgpt-alternative-samsung-gauss-that-can-generate-text-code-and-images/,,,2023-11-08,Samsung,,,,,Unspecified unreleased,,,,,,,Unknown,"Just a few days after OpenAIâ€™s developer event, Samsung unveiled its own generative AI model, Samsung Gauss, at the Samsung AI Forum 2023.

Samsung Gauss, developed by the tech giantâ€™s research unit Samsung Research, consists of three tools: Samsung Gauss Language, Samsung Gauss Code and Samsung Gauss Image.

Samsung Gauss Language is a large language model that can understand human language and answer questions like ChatGPT. It can be used to increase productivity in several ways. For instance, it can help you write and edit emails, summarize documents and translate languages. Samsung plans to incorporate the large language model into its devices like phones, laptops and tablets to make the companyâ€™s smart devices a bit smarter. When asked if it supports both English and Korean as interaction languages, a spokesperson of Samsung declined to comment on it.",Korea (Republic of),,,,,,,,,,
Jais-30b (phase 1)																												 ,Language,"Language modeling/generation,Question answering",,Open weights (unrestricted),https://www.g42.ai/resources/publications/Jais-30B,,Jais-30B: Expanding the Horizon in Open-Source Arabic NLP,2023-11-08,"Cerebras Systems,Mohamed bin Zayed University of Artificial Intelligence (MBZUAI),Inception G42,G42",30000000000.0,"30B

""The backbone of Jais-30B is a causal decoder-only large language model. It is engineered with 48 transformer blocks, 56 attention heads, and an embedding dimension of 7168. """,1.0372049e+23,"6 FLOP / token / parameter * 30 * 10^9 parameters * 427 * 10^9 tokens [ see dataset size notes] = 7.686e+22 FLOP

7500000000000000 FLOP / chip / sec * 16 chips *  1080 hours * 3600 sec / hour * 0.3 [assumed utilization] = 1.39968e+23 FLOP

sqrt(7.686e+22*1.39968e+23) = 1.0372049e+23 FLOP","Common Crawl,CC-News",""" At the core of this is a ratio of 1:2 of Arabic to English data.""

""This dataset was built upon the pre-training data that was used to train the Jais 13B model.""

""The process of collecting this data involved web crawling across a variety of sources, most notably the Common Crawl, which is known for archiving web data. We gathered snapshots of data dumps spanning the time frame from 2021 to 2023. Additionally, we placed special emphasis on collecting news articles from the CCNews source, covering a broad range of time from 2018 to 2023, in both English and Arabic languages. News, books, and journals in Arabic/ English from the UAE and the region were also collected and included in the pretraining. """,427000000000.0,"""126 billion Arabic tokens, 251 billion English tokens, and 50 billion code tokens"" 

total: 427 billion tokens

Batch size	2640
Steps	79k",1080.0,"""Phase 1 of the training lasted approximately 45 days on 16 CS2 nodes within the Condor Galaxy supercomputer.""

45 days = 1080 hours",Cerebras CS-2,Confident,"Following the successful release of Jais-13B and Jais-13B-chat in August 2023, we are excited to launch the new state-of-the-art Arabic centric large language model Jais-30B and Jais-30B-chat. With more than twice the number of parameters as that in our previous release, Jais-30B and Jais-30B-chat models exhibit vastly better performance in both Arabic and English languages. Like its predecessor, the Jais-30B is the most powerful open bilingual model at its scale. Not only are the Jais-30B models the best in the world at Arabic NLP and generative AI tasks, they also are highly competitive with English language models of a similar size.

We are also proud to announce the release of Jais-30B and Jais-30B-chat model weights along with the accompanying inference code to the community. This release marks a significant milestone in our ongoing commitment to elevate Arabic language processing by positioning it at the forefront of generative AI research and development.","United States of America,United Arab Emirates,United Arab Emirates,United Arab Emirates",,,,16.0,,Unreleased,"apache 2.0
https://huggingface.co/inceptionai/jais-30b-v1",,,
XVERSE-13B-2,Language,"Language generation,Language modeling/generation,Question answering,Text summarization,Translation",,Open weights (restricted use),https://huggingface.co/xverse/XVERSE-13B,,,2023-11-06,"XVERSE Technology,Shenzhen Yuanxiang Technology",13000000000.0,13B,,"Not enough info, eg number of epochs",,"""The model has been thoroughly trained on a diversified and high-quality dataset consisting of 3.2 trillion of tokens, including more than 40 languages such as Chinese, English, Russian, and Spanish. The sampling ratio of different types of data is finely set, which makes the performance of Chinese and English excellent, and also takes into account the effect of other languages.""",3200000000000.0,"Multilingual, 3.2 trillion tokens. Note that model was originally stated to have been trained on 1.4T tokens, so while the wording suggests a dataset of 3.2T unique tokens, it may actually be referencing the number of tokens seen by the model (i.e. possibly over multiple epochs).",,,,Likely,"XVERSE-13B is a multilingual large language model, independently developed by Shenzhen Yuanxiang Technology. Its key features are as follows:

Model Structure: XVERSE-13B uses the mainstream Decoder-only Transformer network structure, supports 8k context length, the longest one among models of the same size, which can meet the need of longer multi-round dialogues, knowledge question-answering, and summarization. This makes the model more versatile in application scenarios.
Training Data: The model has been thoroughly trained on a diversified and high-quality dataset consisting of 3.2 trillion of tokens, including more than 40 languages such as Chinese, English, Russian, and Spanish. The sampling ratio of different types of data is finely set, which makes the performance of Chinese and English excellent, and also takes into account the effect of other languages.
Tokenization: Based on the BPE (Byte-Pair Encoding) algorithm, a tokenizer with a vocabulary size of 100,534 has been trained using hundreds of gigabytes of language data. This tokenizer is capable of supporting multilingual without the need for additional vocabulary expansion.
Training Framework: Several key technologies have also been independently developed, including efficient operators, memory optimization, parallel scheduling strategies, overlap of data-computation-communication, and synergy between platforms and frameworks. These advancements enhance training efficiency and model stability. With these technologies, the peak computational power utilization rate on a thousand-card cluster can reach 58.5%, ranking at the forefront of the industry.","China,China",,,,,,Open source,must apply for commercial license,,,
Whisper v3,Speech,Speech recognition (ASR),,Open weights (unrestricted),https://huggingface.co/openai/whisper-large-v3,,,2023-11-06,OpenAI,1550000000.0,,2.7e+23,"Could derive this in terms of Whisper v1, which according to the paper was trained for 680k hours for between 2-3 epochs. Whisper v3 was trained on 5 million hours for 2 epochs, or ~5-7x as much data, and has the same architecture. We have an estimate of 4.65e22 for Whisper 1.

Assume Whisper v1 was trained on 2.5 epochs, or 2.5*680k = 1.7M hours. Whisper v3 was trained on 10M hours. 10/1.7 * 4.65e22 ~= 2.7e23",Unspecified unreleased,"""The Whisper large-v3 model is trained on 1 million hours of weakly labeled audio and 4 million hours of pseudolabeled audio collected using Whisper large-v2""",60000000000.0,"English audio is roughly 228 wpm: https://docs.google.com/document/d/1G3vvQkn4x_W71MKg0GmHVtzfd9m0y3_Ofcoew0v902Q/edit#heading=h.sxcem9l5k3ce

The dataset is multilingual and other languages seem to have lower wpms. So using 200 wpm, we have

200*60*5 million hours = 60,000,000,000 (60B) words",,,,Likely,"Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours of labelled data, Whisper models demonstrate a strong ability to generalise to many datasets and domains without the need for fine-tuning.

Whisper was proposed in the paper Robust Speech Recognition via Large-Scale Weak Supervision by Alec Radford et al. from OpenAI. The original code repository can be found here.",United States of America,,,,,,Unreleased,"Apache 2.0: https://huggingface.co/openai/whisper-large-v3 

this seems to be inference code not training: https://github.com/openai/whisper ",,,
GPT-4 Turbo (Nov 2023),"Multimodal,Vision,Language,Image generation","Chat,Language modeling/generation,Image generation,Speech synthesis,Table tasks,Visual question answering,Image captioning",,API access,https://openai.com/blog/new-models-and-developer-products-announced-at-devday,,New models and developer products announced at DevDay,2023-11-06,OpenAI,,Not known. Maybe smaller/sparser than GPT-4.,,Training compute estimated to be 2.2e25 FLOP using benchmark imputation. https://colab.research.google.com/drive/1r3pUMhB7Kh0Gls9eG-v_XefWrye9fVQR?usp=sharing,Unspecified unreleased,"This model was announced in November 2023, so I assume that this is the preview model, which has a knowledge cutoff date of April 2023 - which I assume means April 1, 2023 - according to https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models?tabs=global-standard%2Cstandard-chat-completions#gpt-35. ",,,,,,Unknown,"Today, we shared dozens of new additions and improvements, and reduced pricing across many parts of our platform. These include:

New GPT-4 Turbo model that is more capable, cheaper and supports a 128K context window",United States of America,,,,,,Unreleased,,,,
Grok-1,Language,"Language modeling,Chat",,Open weights (unrestricted),"https://x.ai/model-card/, https://x.ai/blog/grok-os",,Announcing Grok,2023-11-04,xAI,314000000000.0,"""314B parameter Mixture-of-Experts model with 25% of the weights active on a given token"". So effectively 78B parameters

Mixture of 8 experts: https://github.com/xai-org/grok-1",2.90000000001e+24,"""On these benchmarks, Grok-1 displayed strong results, surpassing all other models in its compute class, including ChatGPT-3.5 and Inflection-1. It is only surpassed by models that were trained with a significantly larger amount of training data and compute resources like GPT-4""

Per table, Grok-1 is surpassed by Palm 2, Claude 2, GPT-4, so it required less compute than these three models. Palm 2 was trained on 7e24 FLOP.

GPT-3.5 is ~2.6e24. Inflection-1's compute is not public/known by us but Inflection says Inflection-1 compute was <= Palm-540B's (which was ~2.5e24). 

For optimal training, our current working hypothesis is that you still need something like Chinchilla scaling on the total number of parameters in the model, even for MoE models, so optimal dataset size would be 20*310B tokens. With 25%*314B params active per forward pass, this would be around 3e24 FLOP.
https://www.wolframalpha.com/input?i=20*310+billion+*+6+*+25%25+*+314+billion",Unspecified unreleased,"""Base model trained on a large amount of text data, not fine-tuned for any particular task.""

""The training data used for the release version of Grok-1 comes from both the Internet up to Q3 2023 and the data provided by our AI Tutors.""

Knowledge cutoff date is October 2023, according to https://www.promptingguide.ai/models/grok-1.",6200000000000.0,"(Speculative confidence, see compute notes)",,,,Likely,"Grok is an AI modeled after the Hitchhikerâ€™s Guide to the Galaxy, so intended to answer almost anything and, far harder, even suggest what questions to ask!

Grok is designed to answer questions with a bit of wit and has a rebellious streak, so please donâ€™t use it if you hate humor!

A unique and fundamental advantage of Grok is that it has real-time knowledge of the world via the ð• platform. It will also answer spicy questions that are rejected by most other AI systems.

Grok is still a very early beta product â€“ the best we could do with 2 months of training â€“ so expect it to improve rapidly with each passing week with your help.",United States of America,,,,,,Unreleased,"apache 2.0
https://github.com/xai-org/grok-1",,,
Grok-0,Language,"Chat,Language modeling/generation",,Hosted access (no API),https://x.ai/,,Announcing Grok,2023-11-04,xAI,33000000000.0,33 billion,,"Half of Llama 2-70B? (which we estimated at 8e23) ""This early model approaches LLaMA 2 (70B) capabilities on standard LM benchmarks but uses only half of its training resources""",Unspecified unreleased,,,no information about the dataset found except for this speculation https://docs.google.com/spreadsheets/d/1O5KVQW1Hx5ZAkcg8AIRjbQLQzx2wVaLl0SqUu-ir9Fs/edit?gid=1158069878#gid=1158069878,,,,Likely,"""The engine powering Grok is Grok-1, our frontier LLM, which we developed over the last four months. Grok-1 has gone through many iterations over this span of time.

After announcing xAI, we trained a prototype LLM (Grok-0) with 33 billion parameters. This early model approaches LLaMA 2 (70B) capabilities on standard LM benchmarks but uses only half of its training resources.""",United States of America,,,,,,Unreleased,,,,
Yi-34B,Language,"Chat,Language modeling/generation,Translation,Code generation","Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, Kaidong Yu, Peng Liu, Qiang Liu, Shawn Yue, Senbin Yang, Shiming Yang, Tao Yu, Wen Xie, Wenhao Huang, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu, Pengcheng Nie, Yuchi Xu, Yudong Liu, Yue Wang, Yuxuan Cai, Zhenyu Gu, Zhiyuan Liu, Zonghong Dai",Open weights (restricted use),https://arxiv.org/abs/2403.04652,,Yi: Open Foundation Models by 01.AI,2023-11-02,01.AI,34000000000.0,34b,6.1e+23,"""The dataset we use contains Chinese & English only. We used approximately 3T tokens"" sounds like this means it was trained on 3T tokens, not necessarily that the dataset contains 3T tokens?

If so, 34b * 3T * 6 = 6.1e23",Unspecified unreleased,"Chinese and English dataset

""For pretraining, we construct 3.1 trillion tokens of English and Chinese corpora using a cascaded data deduplication and quality filtering pipeline. For finetuning, we polish a small scale (less than 10K) instruction dataset over multiple iterations such that every single instance has been verified directly by our machine learning engineers. For vision-language, we combine the chat language model with a vision transformer encoder and train the model to align visual representations to the semantic space of the language model.""",3100000000000.0,"""language models pretrained from scratch on 3.1T highly-engineered large amount of data, and finetuned on a small but meticulously polished alignment data.""",,,NVIDIA A100,Confident,The Yi series models are large language models trained from scratch by developers at 01.AI.,China,,,,128.0,,Unreleased,"apply for commercial license:
no training code
https://github.com/01-ai/Yi/blob/main/MODEL_LICENSE_AGREEMENT.txt

the model https://huggingface.co/01-ai/Yi-34B-Chat Apache 2.0
""If you create derivative works based on this model, please include the following attribution in your derivative works: ....""",,,
Cohere Embed,Language,"Semantic embedding,Retrieval-augmented generation","Nils Reimers, Elliott Choi, Amr Kayid, Alekhya Nandula, Manoj Govindassamy, Abdullah Elkady",API access,https://txt.cohere.com/introducing-embed-v3/,,Cohere Command & Embed on Amazon Bedrock,2023-11-02,Cohere,,,,"https://docs.cohere.com/docs/environmental-impact

Embed v2 (older version) produced 6689.76 kg CO2 to train. Using the calculator Cohere links (https://mlco2.github.io/impact/) that's the equivalent of 80,000 TPUv3-hours in the ""us-west1"" region. That's 3.5e22 FLOP without considering utilization. However, I have no idea which region Cohere's GPUs are in (looks like CO2/energy can vary a lot by region), and they probably used a more recent GPU.",Unspecified unreleased,,,"""First, they have been trained on questions and answers from a large web crawl. When we presented our multilingual-v2.0 model last year, we had a collection of over 1.4 billion question-and-answer pairs from 100+ languages on basically every topic on the internet.""

""Hence, the second stage involved measuring content quality. We used over 3 million search queries from search engines and retrieved the top-10 most similar documents for each query.""",,,,Unknown,"We're excited to introduce Embed v3, our latest and most advanced embeddings model. Embed v3 offers state-of-the-art performance per trusted MTEB and BEIR benchmarks.",Canada,,,,,,Unreleased,,,,
BlueLM 70B,Language,"Chat,Language modeling/generation,Question answering",,Unreleased,https://baijiahao.baidu.com/s?id=1781445143383237948&wfr=spider&for=pc,,,2023-11-02,vivo AI lab,70000000000.0,,4.2e+23,6ND = 6*70B*1000B=4.2e+23,Unspecified unreleased,,,"1000B Text data
10B Image data
100M video data
100M Knowledge graph

(from the conference handout)",,,,Confident,,China,,,,,,Unreleased,information about the model is from their paper catalogue and not found on the internet,,,
BlueLM 130B,Language,"Chat,Language modeling/generation,Question answering",,Unreleased,https://baijiahao.baidu.com/s?id=1781445143383237948&wfr=spider&for=pc,,,2023-11-02,vivo AI lab,130000000000.0,,7.8e+23,6ND = 6*130B*1000B=7.8e+23,Unspecified unreleased,,,"1000B Text data
10B Image data
100M video data
100M Knowledge graph

(from the conference handout)",,,,Confident,,China,,,,,,Unreleased,information about the model is from their paper catalogue and not found on the internet,,,
BlueLM 175B,Language,"Chat,Language modeling/generation,Question answering",,Unreleased,https://baijiahao.baidu.com/s?id=1781445143383237948&wfr=spider&for=pc,,,2023-11-02,vivo AI lab,175000000000.0,,1.05e+24,6ND = 6*175B*1000B=1.05e+24,Unspecified unreleased,,,"1000B Text data
10B Image data
100M video data
100M Knowledge graph

(from the conference handout)",,,,Confident,,China,,,,,,Unreleased,information about the model is from their paper catalogue and not found on the internet,,,
Yi 6B,Language,"Chat,Language modeling/generation,Translation,Code generation","Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, Kaidong Yu, Peng Liu, Qiang Liu, Shawn Yue, Senbin Yang, Shiming Yang, Tao Yu, Wen Xie, Wenhao Huang, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu, Pengcheng Nie, Yuchi Xu, Yudong Liu, Yue Wang, Yuxuan Cai, Zhenyu Gu, Zhiyuan Liu, Zonghong Dai",Open weights (restricted use),https://arxiv.org/abs/2403.04652,,Yi: Open Foundation Models by 01.AI,2023-11-02,01.AI,6000000000.0,6B,1.26e+23,6*7*10^9*3*10^12 = 1.26e+23,Unspecified unreleased,,3100000000000.0,"""language models pretrained from scratch on 3.1T highly-engineered large amount of
data, and finetuned on a small but meticulously polished alignment data.""",,,,Confident,The Yi series models are large language models trained from scratch by developers at 01.AI.,China,,,,,,Unreleased,"llama license
https://huggingface.co/01-ai/Yi-6B

no training code",,,
Nanbeige-16B,Language,"Chat,Language modeling/generation,Code generation,Question answering",,Open weights (unrestricted),https://github.com/Nanbeige/Nanbeige/blob/main/README_EN.md,,,2023-11-01,Nanbeige LLM Lab,16000000000.0,16 billion,2.4e+23,"""It uses 2.5T Tokens for pre-training"". I think that's the number of tokens the model was trained on, not the dataset size, but I'm not sure.

16 billion * 2.5 trillion * 6 = 2.4e23",Unspecified unreleased,"""The training data includes a large amount of high-quality internet corpus, various books, code, etc""",2500000000000.0,"""It uses 2.5T Tokens for pre-training""",,,,Likely,"Nanbeige-16B is a 16 billion parameter language model developed by Nanbeige LLM Lab. It uses 2.5T Tokens for pre-training. The training data includes a large amount of high-quality internet corpus, various books, code, etc. It has achieved good results on various authoritative evaluation data sets. This release includes the Base, Chat, Base-32k and Chat-32k.",China,,,,,,Open source,"Apache 2.0

training code: https://github.com/Nanbeige/Nanbeige/blob/main/scripts/train.sh ",,,
LingoWhale-8B,Language,"Language modeling/generation,Code generation,Translation",,Open weights (non-commercial),https://github.com/DeepLangAI/LingoWhale-8B/blob/main/README_EN.md,,,2023-11-01,DeepLang AI,8000000000.0,,,,,"""pre-trained on a large volume of high-quality bilingual data"" Chinese + English",,,,,,Confident,"LingoWhale-8B is the first open-source model in the LingoWhale series introduced by DeepLangAI. It's a bilingual (Chinese-English) large language model.

LingoWhale-8B has been pre-trained on a large volume of high-quality bilingual data and exhibits powerful capabilities as a foundation model. It has achieved leading results on multiple public benchmarks. During its pre-training phase, the model was trained with a context window of 8K, allowing it to comprehend and generate longer sequences.

LingoWhale-8B is fully open for academic research. Users can apply for commercial use by email, and once granted official commercial permission, they can use it for free.",China,,,,,,Open (non-commercial),"requires form for commercial:
https://github.com/DeepLangAI/LingoWhale-8B/blob/main/MODEL_LICENSE.md",,,
BlueLM 7B,Language,"Chat,Translation,Language modeling/generation,Question answering,Code generation",,Open weights (restricted use),https://github.com/vivo-ai-lab/BlueLM/blob/main/BlueLM_technical_report.pdf,,BlueLM: An Open Multilingual 7B Language Model,2023-10-31,vivo AI lab,7000000000.0,"""BlueLM is a large-scale pre-trained language model independently developed by vivo AI Global Research Institute. This release includes 7B base (base) model and 7B conversation (chat) model. At the same time, we have open sourced the long text base (base) model that supports 32K and conversation (chat) model."" from GitHub https://github.com/vivo-ai-lab/BlueLM

",1.0920000000001e+23,"C = 6DN = 6 * 2.6T * 7B = 1.092*10^23 FLOP
https://www.wolframalpha.com/input?i=6*7+billion+*+2.6+trillion
(assuming 1 epoch)

Figure 1 gives compute of 10^12 FLOPs  but this seems improbable

Training over 2.59T tokens took approximately 26 days using the vivolm system, with a throughput of 3150 tokens/sec/GPU.",Unspecified unreleased,,2592000000000.0,"""Larger amounts of high-quality data : high-quality corpus for training, reaching a scale of 2.6 trillion tokens. The corpus includes Chinese, English and a small amount of Japanese and Korean data"" from GitHub

see 2.1 https://github.com/vivo-ai-lab/BlueLM/blob/main/BlueLM_technical_report.pdf",,,,Confident,"BlueLM is a large-scale open-source language model independently developed by the vivo AI Lab. This release includes 2K and 32K context length versions for both Base and Chat models.

High-quality Data: BlueLM is trained on a high-quality data with 2.6 trillion tokens. Our train corpus mainly consists of Chinese and English data, with a small amount of Japanese and Korean data.
Stronger Performance: BlueLM-7B-Chat achieves a strong competitive performance in C-Eval and CMMLU benchmarks of the same size.
Longer Context: We have extended the context length of both BlueLM-7B-Base-32K and BlueLM-7B-Chat-32K models from 2K to 32K. The models can support longer context understanding while maintaining the same basic capabilities.
Model License: BlueLM weights are open for academic research and commercial use.",China,,,,,,Unreleased,"https://github.com/vivo-ai-lab/BlueLM/blob/main/MODEL_LICENSE_EN.pdf

Our code is licensed under the Apache-2.0 and Community License for BlueLM Model. The BlueLM weights are completely open for academic research, and free commercial use is allowed after completing the questionnaire.

""BlueLM weights are open for academic research and commercial use.""",,,
Tongyi Qianwen 2.0,Language,"Chat,Language modeling/generation",,API access,https://www.alibabacloud.com/blog/alibaba-cloud-launches-tongyi-qianwen-2-0-and-industry-specific-models-to-support-customers-reap-benefits-of-generative-ai_600526,,Alibaba Cloud Launches Tongyi Qianwen 2.0 and Industry-specific Models to Support Customers Reap Benefits of Generative AI,2023-10-31,Alibaba,,"""Tongyi Qianwen 2.0, a generic LLM with a few hundreds of billions of parameters""",,,Unspecified unreleased,,,,,,,Unknown,"Alibaba Cloud, the digital technology and intelligence backbone of Alibaba Group, today announced the launch of Tongyi Qianwen 2.0, its latest large language model (LLM), along with new industry-specific models at its annual flagship tech event Apsara Conference. This release signifies another significant progress in Alibaba Cloud's pursuit of cutting-edge AI innovation and its ongoing commitment to fuel digital transformation in businesses.",China,,,,,,Unreleased,,,,
Mi:dm 200B,Language,Language modeling/generation,,API access,https://genielabs.ai/midm/about,,,2023-10-31,KT,200000000000.0,200B,1.2e+24,6ND=1000000000000*200000000000.00*6=1.2 Ã— 10^24,,,1000000000000.0,Mi:dm is the first Korean LLM trained on over 1 trillion tokens.,,,,Confident,"TL;DR:
KT Corp introduces Mi:dm, a massive AI model aimed at diverse sectors.
Mi:dm is the first Korean LLM trained on over 1 trillion tokens.
It offers four models, from basic to large, with up to 200 billion parameters.
KT plans to share Mi:dmâ€™s foundational model with other companies.
Three advanced technologies reduce AI hallucinations by up to 70%.
Collaborations with AI startups, including Upstage, aim to conquer the global generative AI market.",Korea (Republic of),,,,,,Unreleased,"KT said it will open up the foundation model of Mi:dm to other companies, providing a full AI development package, including KT Cloud's hyperscale AI computing service and AI chip startup Rebellions Inc.'s neural processing unit infrastructure, fostering the development of various AI services.",,,
Skywork-13B,Language,"Language modeling,Language modeling/generation,Translation","Tianwen Wei, Liang Zhao, Lichang Zhang, Bo Zhu, Lijie Wang, Haihua Yang, Biye Li, Cheng Cheng, Weiwei LÃ¼, Rui Hu, Chenxia Li, Liu Yang, Xilin Luo, Xuejie Wu, Lunan Liu, Wenjun Cheng, Peng Cheng, Jianhao Zhang, Xiaoyu Zhang, Lei Lin, Xiaokun Wang, Yutuan Ma, Chuanhai Dong, Yanqi Sun, Yifu Chen, Yongyi Peng, Xiaojuan Liang, Shuicheng Yan, Han Fang, Yahui Zhou",Open weights (restricted use),https://arxiv.org/abs/2310.19341,119.0,Skywork: A More Open Bilingual Foundation Model,2023-10-30,Kunlun Inc.,13000000000.0,13B,2.5e+23,"""Our Skywork-13B is trained on a cluster of 64 NVIDIA-HGX-A800 nodes, a total of 512 A800-80G SXM GPUs... The training process of Skywork-13B spanned a total of 39 days.""

They note that ""we achieved a token throughput of 1873 per GPU per second and a model flops utilization (MFU) of 56.5%... "". 

""MFU"" was coined in the Palm paper (https://arxiv.org/pdf/2204.02311.pdf) and only counts operations used to train the model, not all operations observed on the hardware. MFU is lower than traditionally measured utilization.

Using the 56.5% number, and a peak tensor performance of 623.8 TFLOPS for the A800, this suggests 512 * 623.8 TFLOPS * 39 days * 86400 seconds/day * 0.565 = 6.08e23 FLOP.

Based on C=6ND, with 13B parameters and 3.2T tokens, we have C=6*(13B)*(3.2T)=2.5e23 FLOP.

Since the reported MFU is quite high, and would imply a higher compute usage than 6ND, it seems they may have trained on mixed precision and with the GPUs not always operating in the 623.8 TFLOPS mode.",SkyPile,"""In order to train Skywork-13B, we build SkyPile, a vast, high quality corpus comprising more than 6 trillion tokens. A segment of the corpus, comprising over 150 billion tokens of web text, has been open sourced to facilitate research and training on Chinese LLMs""",3180000000000.0,"The full SkyPile dataset is 6 trillion tokens, roughly half English and half Chinese: (https://huggingface.co/Skywork/Skywork-13B-base).

The model is trained for the equivalent of 0.53 epochs on the full dataset, or 3.18 trillion unique tokens. This is around 2.78 trillion words, based on an average of 1 word/token for the Chinese portion and 0.75 word/token on the English portion.",940.0,39 days,NVIDIA A800 PCIe 40 GB,Confident,"In this technical report, we present Skywork-13B, a family of large language models (LLMs) trained on a corpus of over 3.2 trillion tokens drawn from both English and Chinese texts. This bilingual foundation model is the most extensively trained and openly published LLMs of comparable size to date. We introduce a two-stage training methodology using a segmented corpus, targeting general purpose training and then domain-specific enhancement training, respectively. We show that our model not only excels on popular benchmarks, but also achieves state of the art performance in Chinese language modeling on diverse domains. Furthermore, we propose a novel leakage detection method, demonstrating that test data contamination is a pressing issue warranting further investigation by the LLM community. To spur future research, we release Skywork-13B along with checkpoints obtained during intermediate stages of the training process. We are also releasing part of our SkyPile corpus, a collection of over 150 billion tokens of web text, which is the largest high quality open Chinese pre-training corpus to date. We hope Skywork-13B and our open corpus will serve as a valuable open-source resource to democratize access to high-quality LLMs.",China,,,,512.0,0.4637,Open (restricted use),"commercial but restrictive license: https://github.com/SkyworkAI/Skywork/blob/main/LICENSE

part of the training data is open, but only 2.5%: ""In order to train Skywork-13B, we build SkyPile, a vast, high quality corpus comprising more than 6 trillion tokens. A segment of the corpus, comprising over 150 billion tokens of web text, has been open sourced to facilitate research and training on Chinese LLMs""

training code: https://github.com/SkyworkAI/Skywork/blob/main/train/train.py ",0.565,,
Xinghan Foundation Model,"Multimodal,Video,Language,Vision","Video description,Visual question answering,Language modeling/generation",,,https://www.prnewswire.com/news-releases/dahua-announces-think-2-0-strategy-to-accelerate-innovation-for-a-digital-intelligent-future-301967122.html,,Dahua Announces Think# 2.0 Strategy to Accelerate Innovation for a Digital Intelligent Future,2023-10-25,Dahua Technology,,,,,,,,,,,,Unknown,"At the summit, Dahua's ""Xinghan"" Foundation Model was launched. With video as the core, this multimodal fusion industry foundation model catapults the accuracy and generalization of AI algorithms, with breakthroughs in visual cognition capabilities, independent analysis of various scenarios, and efficient fulfillment of massive fragmented needs, especially urban governance and power industry applications.",China,,,,,,,,,,
Spark 3.0,Language,"Code generation,Language generation,Language modeling/generation,Question answering",,,https://www.gizmochina.com/2023/10/26/iflytek-spark-3-0-vs-openai-gpt/,,,2023-10-24,iFlytek,,,,"""The company said that Spark 3.0 has been trained on a dataset of 1.2 trillion words and code, and that it can perform a variety of tasks, including text generation, language understanding, knowledge answering, logical reasoning, mathematical computation, code generation, and multimodal interaction."" https://www.gizmochina.com/2023/10/26/iflytek-spark-3-0-vs-openai-gpt/",,,1200000000000.0,"""Spark 3.0 has been trained on a dataset of 1.2 trillion words and code""",,,,Likely,,China,,,,,,,,,,
DALLÂ·E 3,Image generation,"Image generation,Text-to-image","James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, Wesam Manassra, Prafulla Dhariwal, Casey Chu, Yunxin Jiao, Aditya Ramesh",API access,https://cdn.openai.com/papers/dall-e-3.pdf,1448.0,Improving Image Generation with Better Captions,2023-10-19,OpenAI,,,,,Unspecified unreleased,,,,,,,Unknown,"We show that prompt following abilities of text-to-image models can be substantially improved by training on highly descriptive generated image captions.
Existing text-to-image models struggle to follow detailed image descriptions and often ignore words or confuse the meaning of prompts. We hypothesize that this issue stems from noisy and inaccurate image captions in the training dataset. We address this by training a bespoke image captioner and use it to recaption the training dataset. We then train several text-to-image models and find that training on these synthetic captions reliably improves prompt following ability. Finally, we use these findings to build DALL-E 3: a new text-to-image generation system, and benchmark its performance on an evaluation designed to measure prompt following, coherence, and aesthetics, finding that it compares favorably to competitors. We publish samples and code for these evaluations so that future research can continue optimizing this important aspect of text-to-image systems.",United States of America,,,,,,Unreleased,https://platform.openai.com/docs/models/dall-e,,,
ERNIE 4.0,"Multimodal,Language,Video,Image generation","Chat,Language modeling/generation,Video generation,Image generation",,API access,https://www.prnewswire.com/news-releases/baidu-launches-ernie-4-0-foundation-model-leading-a-new-wave-of-ai-native-applications-301958681.html,,"Baidu Launches ERNIE 4.0 Foundation Model, Leading a New Wave of AI-Native Applications",2023-10-17,Baidu,,"""similar architecture with 3.5 version""  -interpreter dub at 01:25:08 https://www.youtube.com/watch?v=wYozcsavRuM",,"Unlikely to be >1e25 FLOP, ERNIE 4.5 was <1e25 FLOP.

may be mentioned here https://www.youtube.com/watch?v=wYozcsavRuM",,may be mentioned here https://www.youtube.com/watch?v=wYozcsavRuM,,,,,,Unknown,"Baidu, Inc. (NASDAQ: BIDU and HKEX: 9888), a leading AI company with strong Internet foundation, today hosted its annual flagship technology conference Baidu World 2023 in Beijing, marking the conference's return to an offline format after four years. With the theme ""Prompt the World,"" this year's Baidu World conference saw Baidu launch ERNIE 4.0, Baidu's next-generation and most powerful foundation model offering drastically enhanced core AI capabilities. Baidu also showcased some of its most popular applications, solutions, and products re-built around the company's state-of-the-art generative AI.                                                                               

Robin Li, Co-founder, Chairman and CEO of Baidu, announced ERNIE 4.0 at Baidu World 2023
""ERNIE 4.0 has achieved a full upgrade with drastically improved performance in understanding, generation, reasoning, and memory,"" Robin Li, Co-founder, Chairman and CEO of Baidu, said at the event. ""These four core capabilities form the foundation of AI-native applications and have now unleashed unlimited opportunities for new innovations.""

",China,,,,,,Unreleased,"""ERNIE 4.0 is now accessible to invited users on ERNIE Bot, and the API will be available upon application to enterprise clients via Qianfan foundation model platform.""",,,
Fuyu-8B,"Multimodal,Language,Vision","Chat,Image classification,Visual question answering,Image captioning","Rohan Bavishi, Erich Elsen, Curtis Hawthorne, Maxwell Nye, Augustus Odena, Arushi Somani, SaÄŸnak TaÅŸÄ±rlar",Open weights (non-commercial),"https://www.adept.ai/blog/fuyu-8b, www.huggingface.co/adept/fuyu-8b",,Fuyu-8B: A Multimodal Architecture for AI Agents,2023-10-17,Adept,8000000000.0,"8B

Also a ""Fuyu-medium"" with unstated param count (<56B: ""Fuyu-Medium performs comparably to PALM-E-562B despite having fewer than a tenth as many parameters"")",,,,,,,,,,Confident,"Weâ€™re releasing Fuyu-8B, a small version of the multimodal1 model that powers our product. The model is available on HuggingFace. We think Fuyu-8B is exciting because:

It has a much simpler architecture and training procedure than other multi-modal models, which makes it easier to understand, scale, and deploy.
Itâ€™s designed from the ground up for digital agents, so it can support arbitrary image resolutions, answer questions about graphs and diagrams, answer UI-based questions, and do fine-grained localization on screen images.
Itâ€™s fast - we can get responses for large images in less than 100 milliseconds.
Despite being optimized for our use-case, it performs well at standard image understanding benchmarks such as visual question-answering and natural-image-captioning.",United States of America,,,,,,Unreleased,non-commercial: https://huggingface.co/adept/fuyu-8b,,,
Aquila2 34B,Language,"Chat,Language modeling/generation",,Open weights (unrestricted),"https://github.com/FlagAI-Open/Aquila2
https://huggingface.co/BAAI/Aquila2-34B",,Aquila2 Technical Report,2023-10-13,Beijing Academy of Artificial Intelligence / BAAI,34000000000.0,"34B
safetensors say it is 18.2B params

There's also a 70B ""experimental"" version: https://github.com/FlagAI-Open/Aquila2",,"6ND = 6*34000000000*2000000000000=4.08e+23

'Likely' confidence because unknown number of epochs, dataset size and number of parameters are not certain.","The Pile,WuDao Corpora"," The training data corpus consists of an equal proportion of English and Chinese, sourced from WudaoCorpus [11] and Pile [12] for Chinese and English data, respectively.",2000000000000.0,"""we have investigated all 2 trillion tokens of data"" from https://github.com/FlagAI-Open/Aquila2",,,NVIDIA A100 SXM4 40 GB,Likely,"We announce that our Aquila2 series is now open source, comprising Aquila2 (the base language models: Aquila2-7B, Aquila2-34B and Aquila2-70B-Expr) and AquilaChat2 (the chat models, namely AquilaChat2-7B, AquilaChat2-34B and AquilaChat2-70B-Expr, as well as the long-text chat models, namely AquilaChat2-7B-16k and AquilaChat2-34B-16k). You can find the links in the following table. Kindly click on them to access the model cards.",China,,,,512.0,,Open source,apache 2.0,,,
Jiutian,Language,Language modeling/generation,,Open weights (unrestricted),https://www.globaltimes.cn/page/202310/1299716.shtml,,,2023-10-12,China Mobile,13900000000.0,"A 13.9B parameter model is mentioned prominently at
https://jiutian.10086.cn/portal/#/home
2025-01-13.",1.668e+23,6*13.9e9*2e12=1.668e23,,,2000000000000.0,"""Designed to enhance efficiency, the model has trained over 2 trillion tokens""",,,,Likely,"China Mobile, the largest telecom operator in the world by subscribers, unveiled its ""Jiutian"" artificial intelligence (AI) large-scale model on Thursday, which has reportedly won support from large enterprises including China Ocean Shipping (Group) Co and China Railway Construction Co.",China,,,,,,Unreleased,"seems like it is the same model as this one under Apache 2.0 license:
https://modelscope.cn/models/JiuTian-AI/JIUTIAN-139MoE-chat",,,
Mistral 7B,Language,"Code generation,Language generation,Language modeling/generation,Question answering","Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, LÃ©lio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, TimothÃ©e Lacroix, William El Sayed",Open weights (unrestricted),https://arxiv.org/abs/2310.06825,2900.0,Mistral 7B,2023-10-10,Mistral AI,7000000000.0,,,,Unspecified unreleased,"""Unfortunately we're unable to share details about the training and the datasets (extracted from the open Web) due to the highly competitive nature of the field.""",,,,,,Confident,"We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered for superior performance and efficiency. Mistral 7B outperforms Llama 2 13B across all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and code generation. Our model leverages grouped-query attention (GQA) for faster inference, coupled with sliding window attention (SWA) to effectively handle sequences of arbitrary length with a reduced inference cost. We also provide a model fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses the Llama 2 13B -- Chat model both on human and automated benchmarks. Our models are released under the Apache 2.0 license.",France,,,,,,Unreleased,apache 2.0,,,
CodeFuse-13B,Language,Code generation,"Peng Di, Jianguo Li, Hang Yu, Wei Jiang, Wenting Cai, Yang Cao, Chaoyu Chen, Dajun Chen, Hongwei Chen, Liang Chen, Gang Fan, Jie Gong, Zi Gong, Wen Hu, Tingting Guo, Zhichao Lei, Ting Li, Zheng Li, Ming Liang, Cong Liao, Bingchang Liu, Jiachen Liu, Zhiwei Liu, Shaojun Lu, Min Shen, Guangpei Wang, Huan Wang, Zhi Wang, Zhaogui Xu, Jiawei Yang, Qing Ye, Gehao Zhang, Yu Zhang, Zelin Zhao, Xunjin Zheng, Hailian Zhou, Lifu Zhu, Xianying Zhu",Open weights (unrestricted),https://arxiv.org/abs/2310.06266,26.0,CodeFuse-13B: A Pretrained Multi-lingual Code Large Language Model,2023-10-10,Ant Group,13000000000.0,,3.09e+23,"""CodeFuse-13B was trained using 512 Nvidia A100 GPU cards, with
a Hardware FLOPs Utilization (HFU) of approximately 60%. The
training process took approximately 40 days to complete."" Later they state utilization of 56%

512 * 312 trillion * 40 * 24 * 3600 * 0.56 = 3.09e23

Using params*tokens, we have 13 billion * 1 trillion * 6 = 7.8e22. might be a sign of multiple epochs? 1T is the size of the dataset; they don't clearly state the number of training tokens","The Stack,GitHub","80% code, 10% English, 10% Chinese: ""The pre-training data for CodeFuse consists of 196TB of code, 1.75TB of Chinese raw data, and 1.7TB of English raw data, totaling 200TB, that are tokenized into 800 billion
tokens of code, 100 billion tokens of Chinese corpus, and 100 billion
tokens of English corpus (see Section 3.1).""

""We collected about 200+ TB of code-related data, and finally refined it to around 1.6TB (1T Token) of clean data suitable for pre-training.""",1000000000000.0,"1T tokens, mostly code but some Chinese/English",960.0,~40 days,NVIDIA A100 SXM4 80 GB,Confident,"Code Large Language Models (Code LLMs) have gained significant attention in the industry due to their wide applications in the full lifecycle of software engineering. However, the effectiveness of existing models in understanding non-English inputs for multi-lingual code-related tasks is still far from well studied. This paper introduces CodeFuse-13B, an open-sourced pre-trained code LLM. It is specifically designed for code-related tasks with both English and Chinese prompts and supports over 40 programming languages. CodeFuse achieves its effectiveness by utilizing a high quality pre-training dataset that is carefully filtered by program analyzers and optimized during the training process. Extensive experiments are conducted using real-world usage scenarios, the industry-standard benchmark HumanEval-x, and the specially designed CodeFuseEval for Chinese prompts. To assess the effectiveness of CodeFuse, we actively collected valuable human feedback from the AntGroup's software development process where CodeFuse has been successfully deployed. The results demonstrate that CodeFuse-13B achieves a HumanEval pass@1 score of 37.10%, positioning it as one of the top multi-lingual code LLMs with similar parameter sizes. In practical scenarios, such as code generation, code translation, code comments, and testcase generation, CodeFuse performs better than other models when confronted with Chinese prompts.",China,,,,512.0,,Unreleased,apache: https://github.com/codefuse-ai/codefuse-chatbot?tab=License-1-ov-file#readme,0.5772,,
Qwen-14B,Language,Language modeling/generation,"Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, Tianhang Zhu",Open weights (restricted use),https://arxiv.org/abs/2309.16609,3006.0,Qwen Technical Report,2023-09-28,Alibaba,14000000000.0,14B,2.5e+23,"3T tokens per Table 1

14B*3T*6 = 2.5e23",,"""Our dataset is designed to meet these requirements and includes public
web documents, encyclopedia, books, codes, etc. Additionally, our dataset is multilingual, with a significant portion of the data being in English and Chinese.""",3000000000000.0,"""We have pretrained the language models, namely QWEN, on massive datasets containing trillions of tokens""
Table 1 indicates 3T tokens for Qwen-14B, and the above quote suggests the 3T aren't from multiple epochs on a smaller dataset.",,,,Confident,"Large language models (LLMs) have revolutionized the field of artificial intelligence, enabling natural language processing tasks that were previously thought to be exclusive to humans. In this work, we introduce Qwen, the first installment of our large language model series. Qwen is a comprehensive language model series that encompasses distinct models with varying parameter counts. It includes Qwen, the base pretrained language models, and Qwen-Chat, the chat models finetuned with human alignment techniques. The base language models consistently demonstrate superior performance across a multitude of downstream tasks, and the chat models, particularly those trained using Reinforcement Learning from Human Feedback (RLHF), are highly competitive. The chat models possess advanced tool-use and planning capabilities for creating agent applications, showcasing impressive performance even when compared to bigger models on complex tasks like utilizing a code interpreter. Furthermore, we have developed coding-specialized models, Code-Qwen and Code-Qwen-Chat, as well as mathematics-focused models, Math-Qwen-Chat, which are built upon base language models. These models demonstrate significantly improved performance in comparison with open-source models, and slightly fall behind the proprietary models.",China,,,,,,Unreleased,"commercial allowed, can't use to train models
https://github.com/QwenLM/Qwen/blob/main/Tongyi%20Qianwen%20LICENSE%20AGREEMENT",,,
Amazon Titan,"Language,Image generation","Semantic search,Image generation,Language modeling/generation,Code generation,Chat,Text-to-image,Translation",,API access,https://aws.amazon.com/bedrock/titan/,,,2023-09-28,Amazon,200000000000.0,"200B dense model
https://importai.substack.com/p/import-ai-365-wmd-benchmark-amazon",4.8e+24,"trained using NVIDIA NeMo: https://blogs.nvidia.com/blog/nemo-amazon-titan/

13,760 NVIDIA A100 chips (using 1,720 P4d nodes). It took 48 days to train.
from https://importai.substack.com/p/import-ai-365-wmd-benchmark-amazon

counting operations: 6*200000000000*4000000000000=4.8e+24

gpu usage: 312000000000000(FLOP/s)*0.3*13760*1152*3600=5.3413281792e+24",,,4000000000000.0,"4T tokens of data, based on comments from amazon engineer James Hamilton at a 2024 talk: https://perspectives.mvdirona.com/2024/01/cidr-2024/
Also cited here:
https://lifearchitect.ai/titan/",1152.0,,NVIDIA A100,Likely,,United States of America,,,,13760.0,0.2696,Unreleased,,,23965159.94750839,441735752.9808057
Qwen-7B,Language,"Language modeling/generation,Translation","Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, Tianhang Zhu",Open weights (restricted use),"https://arxiv.org/abs/2309.16609, https://huggingface.co/Qwen/Qwen-7B",3006.0,Qwen Technical Report,2023-09-28,Alibaba,7000000000.0,7B,1.01e+23,"2.4T tokens per Table 1

7b*2.4T*6 = 1.01e23",Unspecified unreleased,"""Our dataset is designed to meet these requirements and includes public
web documents, encyclopedia, books, codes, etc. Additionally, our dataset is multilingual, with a significant portion of the data being in English and Chinese.""",2400000000000.0,"""We have pretrained the language models, namely QWEN, on massive datasets containing trillions of tokens""
Table 1 indicates 2.4T tokens for Qwen-7B, and the above quote suggests the 2.4T aren't from multiple epochs on a smaller dataset.",,,,Confident,"Large language models (LLMs) have revolutionized the field of artificial intelligence, enabling natural language processing tasks that were previously thought to be exclusive to humans. In this work, we introduce Qwen, the first installment of our large language model series. Qwen is a comprehensive language model series that encompasses distinct models with varying parameter counts. It includes Qwen, the base pretrained language models, and Qwen-Chat, the chat models finetuned with human alignment techniques. The base language models consistently demonstrate superior performance across a multitude of downstream tasks, and the chat models, particularly those trained using Reinforcement Learning from Human Feedback (RLHF), are highly competitive. The chat models possess advanced tool-use and planning capabilities for creating agent applications, showcasing impressive performance even when compared to bigger models on complex tasks like utilizing a code interpreter. Furthermore, we have developed coding-specialized models, Code-Qwen and Code-Qwen-Chat, as well as mathematics-focused models, Math-Qwen-Chat, which are built upon base language models. These models demonstrate significantly improved performance in comparison with open-source models, and slightly fall behind the proprietary models.",China,,,,,,Unreleased,"commercial allowed, can't use to train models
https://github.com/QwenLM/Qwen/blob/main/Tongyi%20Qianwen%20LICENSE%20AGREEMENT",,,
PLaMo-13B,Language,"Language modeling/generation,Chat,Question answering","Preferred Networks, Inc",Open weights (unrestricted),https://huggingface.co/pfnet/plamo-13b,, PLaMo-13B,2023-09-28,Preferred Networks Inc,13000000000.0,,1.17e+23,"6ND = 6*13e9*1.5e12=1.17e+23
from https://huggingface.co/pfnet/plamo-13b#model-details

480 GPUs * 30 days [assumed, likely less] * 24 hours * 3600 s * 77970000000000 FLOP/s * 41.0 [reported utilization] = 3.9772934e+24


","C4,Project Gutenberg,RedPajama,mC4,Wikipedia (ja)",from https://huggingface.co/pfnet/plamo-13b#training-dataset,1500000000000.0,"Trained tokens: 1.5T tokens (English: 1.32T tokens, Japanese: 0.18T tokens)
from https://huggingface.co/pfnet/plamo-13b#model-details

0.75*1.32T + 0.18T = 1170000000000
0.75 words per token for English
1 for Japanese ",720.0,"""We used 60 ABCI A nodes (480 GPUs) for just under a month, and trained the training data with a total of 1.4T tokens with a context length of 4096.""

https://tech.preferred.jp/ja/blog/llm-plamo/",NVIDIA A100 SXM4 40 GB,Confident,,Japan,,,,480.0,0.41,Unreleased,Apache 2.0 for weights. Open data,,,
Show-1,Video,"Video generation,Text-to-video","David Junhao Zhang, Jay Zhangjie Wu, Jia-Wei Liu, Rui Zhao, Lingmin Ran, Yuchao Gu, Difei Gao, Mike Zheng Shou",Open weights (non-commercial),https://arxiv.org/abs/2309.15818,,Show-1: Marrying Pixel and Latent Diffusion Models for Text-to-Video Generation,2023-09-27,National University of Singapore,,,,,WebVid-10M,"""WebVid-10M is a large-scale dataset of short videos with textual descriptions sourced from stock footage sites. The videos are diverse and rich in their content. 10.7M video-caption pairs. 52K total video hours.""",,"WebVid-10M
10.7M video-caption pairs. 52K total video hours.",,,NVIDIA A100,Unknown,"Significant advancements have been achieved in the realm of large-scale pre-trained text-to-video Diffusion Models (VDMs). However, previous methods either rely solely on pixel-based VDMs, which come with high computational costs, or on latent-based VDMs, which often struggle with precise text-video alignment. In this paper, we are the first to propose a hybrid model, dubbed as Show-1, which marries pixel-based and latent-based VDMs for text-to-video generation. Our model first uses pixel-based VDMs to produce a low-resolution video of strong text-video correlation. After that, we propose a novel expert translation method that employs the latent-based VDMs to further upsample the low-resolution video to high resolution. Compared to latent VDMs, Show-1 can produce high-quality videos of precise text-video alignment; Compared to pixel VDMs, Show-1 is much more efficient (GPU memory usage during inference is 15G vs 72G). We also validate our model on standard video generation benchmarks. Our code and model weights are publicly available at this https URL.",Singapore,,,,,,Unreleased,"https://github.com/showlab/Show-1 don't see training code
Attribution-NonCommercial 4.0 International
",,,
Baichuan 2-7B,Language,"Language modeling/generation,Question answering,Translation","Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Chao Yin, Chenxu Lv, Da Pan, Dian Wang, Dong Yan, Fan Yang, Fei Deng, Feng Wang, Feng Liu, Guangwei Ai, Guosheng Dong, Haizhou Zhao, Hang Xu, Haoze Sun, Hongda Zhang, Hui Liu, Jiaming Ji, Jian Xie, Juntao Dai, Kun Fang, Lei Su, Liang Song, Lifeng Liu, Liyun Ru, Luyao Ma, Mang Wang, Mickel Liu, MingAn Lin, Nuolan Nie, Peidong Guo, Ruiyang Sun, Tao Zhang, Tianpeng Li, Tianyu Li, Wei Cheng, Weipeng Chen, Xiangrong Zeng, Xiaochuan Wang, Xiaoxi Chen, Xin Men, Xin Yu, Xuehai Pan, Yanjun Shen, Yiding Wang, Yiyu Li, Youxin Jiang, Yuchen Gao, Yupeng Zhang, Zenan Zhou, Zhiying Wu",Open weights (restricted use),https://arxiv.org/pdf/2309.10305,912.0,"Baichuan 2: Open Large-scale Language Models
",2023-09-20,Baichuan,7000000000.0,,1.092e+23,"7b * 2.6t * 6 = 1.092e23
Also mentions 1,024 NVIDIA A800 GPUs at 180 TFLOPS per GPU",,,2600000000000.0,,,,NVIDIA A800 PCIe 40 GB,Confident,"In this technical report, we present Baichuan 2, a series of large-scale multilingual language models containing 7 billion and 13 billion parameters, trained from scratch, on 2.6 trillion tokens. ",China,,,,1024.0,,Open source,"https://huggingface.co/baichuan-inc/Baichuan2-7B-Base

license here: https://github.com/baichuan-inc/Baichuan2?tab=readme-ov-file
Baichuan 2 æ¨¡åž‹ç¤¾åŒºè®¸å¯åè®® (Community License Agreement)
restrictions on commercial applications with many DAUs and particular types of businesses

Apache 2.0 for code",0.5772,,
Hunyuan,"Language,Image generation,Multimodal","Language modeling/generation,Image generation,Question answering",,API access,https://www.tencent.com/en-us/articles/2201685.html,,"Tencent Unveils Hunyuan, its Proprietary Large Foundation Model on Tencent Cloud",2023-09-07,Tencent,100000000000.0,"""Presently, the Hunyuan model has over 100 billion parameters, with more than two trillion tokens in pre-training data.""",1.2e+24,6ND = 6*100*10^9*2*10^12 = 1.2*10^24,Unspecified unreleased,,2000000000000.0,"""Presently, the Hunyuan model has over 100 billion parameters, with more than two trillion tokens in pre-training data.""",,,,Confident,"Enterprises in China may now access Hunyuan via Tencentâ€™s public cloud platform and finetune it to their specific needs. The platform features strong Chinese language processing abilities, advanced logical reasoning, and comes with reliable task execution abilities.

Tencentâ€™s foundation model supports a wide array of functions spanning the creation of images, copywriting, text recognition, and customer service, to name a few. These will be instrumental in key industries like finance, public services, social media, e-commerce, transportation, games, and many more.

This empowers enterprises to build powerful tools, in addition to training their own unique large models derived from Tencentâ€™s Model-as-a-Service (MaaS) offering, which was first introduced in June this year. The MaaS provides enterprises with economically viable, industry-specific large models, featuring more than 50 solutions spanning 20 major industries. This creates a virtuous cycle in which enterprises refine their large models with Hunyuan to create uniquely intelligent services across their operations. ",China,,,,,,Unreleased,,,,
Falcon-180B,Language,"Language modeling,Language modeling/generation,Question answering","Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, MÃ©rouane Debbah, Ã‰tienne Goffinet, Daniel Hesslow, Julien Launay, Quentin Malartic, Daniele Mazzotta, Badreddine Noune, Baptiste Pannier, Guilherme Penedo",Open weights (restricted use),https://falconllm.tii.ae/falcon-180b.html; https://arxiv.org/abs/2311.16867,598.0,The Falcon Series of Open Language Models,2023-09-06,Technology Innovation Institute,180000000000.0,"""Falcon 180B is a super-powerful language model with 180 billion parameters""",3.76e+24,"43,500 petaflop-days per Table 1 of the paper

43500 * 1e15 * 24 * 3600 = 3.76e24


C = 6ND = 6 FLOP/token/parameter * 3.5 trillion tokens * 180 billion parameters = 3.78*10^24 FLOP",RefinedWeb,"""The Falcon series is made of three causal decoder-only models trained on up to 4,096 A100. We assembled a pretraining dataset of 3,500 billion tokens, predominantly sourced from our work on RefinedWeb (Penedo et al., 2023)â€“a massive filtered and deduplicated web dataset""

Training dataset composition is described in Table 3. Falcon was trained for 1 epoch.",2625000000000.0,3.5 trillion tokens * (~3 words per 4 tokens) ~= 2.625 trillion words,4320.0,"Stanford CRFM foundation model ecosystem graph data page https://crfm.stanford.edu/ecosystem-graphs/index.html?asset=Falcon-180B says 9 months, which is the maximum possible amount of time: training began sometime in 2023, and it was released in September. 

However, 6 months is more realistic. That is the length of the gap between Falcon 40B and Falcon 180B. Additionally, the amount of compute is specified in the paper, so there is only one degree of freedom in the uncertain values of training duration and hardware utilization rate. At six months, the utilization is unusually low, so the training was probably not longer than that.",NVIDIA A100 SXM4 40 GB,Confident,"Falcon 180B is a super-powerful language model with 180 billion parameters, trained on 3.5 trillion tokens. It's currently at the top of the Hugging Face Leaderboard for pre-trained Open Large Language Models and is available for both research and commercial use.

This model performs exceptionally well in various tasks like reasoning, coding, proficiency, and knowledge tests, even beating competitors like Meta's LLaMA 2.

Among closed source models, it ranks just behind OpenAI's GPT 4, and performs on par with Google's PaLM 2 Large, which powers Bard, despite being half the size of the model.",United Arab Emirates,,,,4096.0,0.1892,Unreleased,"""Falcon 180b can be commercially used but under very restrictive conditions, excluding any ""hosting use""."" https://huggingface.co/blog/falcon-180b",,26751806.453032624,131493433.44544913
Baichuan2-13B,Language,Chat,"Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Ce Bian, Chao Yin, Chenxu Lv, Da Pan, Dian Wang, Dong Yan, Fan Yang, Fei Deng, Feng Wang, Feng Liu, Guangwei Ai, Guosheng Dong, Haizhou Zhao, Hang Xu, Haoze Sun, Hongda Zhang, Hui Liu, Jiaming Ji, Jian Xie, JunTao Dai, Kun Fang, Lei Su, Liang Song, Lifeng Liu, Liyun Ru, Luyao Ma, Mang Wang, Mickel Liu, MingAn Lin, Nuolan Nie, Peidong Guo, Ruiyang Sun, Tao Zhang, Tianpeng Li, Tianyu Li, Wei Cheng, Weipeng Chen, Xiangrong Zeng, Xiaochuan Wang, Xiaoxi Chen, Xin Men, Xin Yu, Xuehai Pan, Yanjun Shen, Yiding Wang, Yiyu Li, Youxin Jiang, Yuchen Gao, Yupeng Zhang, Zenan Zhou, Zhiying Wu",Open weights (restricted use),"https://huggingface.co/baichuan-inc/Baichuan2-13B-Base, https://arxiv.org/abs/2309.10305",,Baichuan 2: Open Large-scale Language Models,2023-09-06,Baichuan,13000000000.0,,2.03e+23,"They describe the dataset as having 2.6T tokens, but the checkpoint graph makes it clear that's also the number of tokens the model was trained on.

13b * 2.6t * 6 = 2.03e23",,"2.6 trillion tokens, bilingual.

paper/model card don't give breakdown between English and Chinese",2275000000000.0,"2.6T tokens, or ~2.3T words assuming that the dataset is roughly even English (0.75 words/token) and Chinese (1 word/token)

1.3T Chinese tokens * (1 word/token) = 1.3T Chinese words
1.3T English tokens * (0.75 words/token) = 0.975T English words
total: 2.275T, or ~2.3T",,,,Confident,,China,,,,1024.0,,Unreleased,"Baichuan community license, restrictive commercial: https://huggingface.co/baichuan-inc/Baichuan2-13B-Base",0.5772,,
TigerBot-70B,Language,"Chat,Language generation,Language modeling/generation,Question answering","Ye Chen, Wei Cai, Liangmin Wu, Xiaowei Li, Zhanxuan Xin, Cong Fu",Open weights (restricted use),"https://github.com/TigerResearch/TigerBot/blob/main/README_en.md, https://arxiv.org/abs/2312.08688",,TigerBot: An Open Multilingual Multitask LLM,2023-09-06,Tigerobo,70000000000.0,70B,1.02e+24,"~1.02e24

Tigerobo did ~2.1e23 additional pre-training. We estimated Llama 2 was trained on 8.1e23 FLOP.",,"""Tigerbot-70b is further pre-trained on the foundation of Llama-2-70b using high-quality multi-language data of 300 billion tokens. ""

""We collected data from Chinese books, the internet, and encyclopedia-type data based on the distribution of GPT3 pretraining data, and filtered the data through source quality control and tf-idf soft deduplication. From 20TB of data, we filtered down to 2TB, maintaining the proportion of language and categories. On this basis, we randomly sampled 100G of data and released it open source.""",300000000000.0,,,,NVIDIA A100,Confident,"(translated from https://github.com/TigerResearch/TigerBot/wiki/TigerBot%E2%80%9070B%E5%8F%91%E5%B8%83%EF%BC%81)

We are pleased to release Tigerbot-70b, which continues to be open source and free for commercial use, including:

Tigerbot-70b-base: Continuing pre-training on the basis of Llama-2-70b, the model's comprehensive capabilities are better than Llama-2-70b in 10 mainstream benchmark tests such as mmlu, reaching SOTA in the industry.

a. Using high-quality multi-lingual data of 300 billion tokens,

b. The algorithm uses GQA, flash-attn, RoPE, holistic-training and other technologies,

c. The training uses tensor/pipeline-partition technology, and the computing efficiency reaches the SOTA reported in the Llama-2 paper.",China,Llama 2-70B,1.26e+23,70b * 300b * 6 = 126000*10^18 = 1.26*10^23,512.0,,Open source,"Apache 2.0 https://github.com/TigerResearch/TigerBot/blob/main/README_en.md

but it's also a Llama 2 finetune.

training code here: https://github.com/TigerResearch/TigerBot/tree/main/train 

They released a 5% sample of training data: "" On this basis, we randomly sampled 100G of data and released it open source.""",,,
360 Smart Brain,"Multimodal,Language,Image generation","Language generation,Chat,Image generation",,,https://www.hayo.com/article/64f68f1d8578eea6c7ec663f,,The 360 â€‹â€‹Brain Model is now open to the public,2023-09-04,360 Security Technology,,"""hundreds of billions"" https://www.hayo.com/article/650babb37c769bcba319ed83",,"hundreds of billions of parameters, trained on over 1T tokens, per this: https://www.hayo.com/article/650babb37c769bcba319ed83

vague report from a Google-translated article, though",,"hundreds of billions of parameters, trained on over 1T tokens, per this: https://www.hayo.com/article/650babb37c769bcba319ed83",1000000000000.0,,,,,Likely,"According to news on September 5, the 360 â€‹â€‹Smart Brain large model will be open to the public from now on and will be fully accessible to the 360 â€‹â€‹â€œFamily Bucketâ€.

360 Zhi Nao will be open to the public on five major platforms. Users can download the â€œ360 Zhi Naoâ€ App through the 360 â€‹â€‹Zhi Nao official website and major application stores.",China,,,,,,,,,,
ABAB,Language,Language generation,,API access,"https://kr-asia.com/baidus-ernie-bot-among-eight-chinese-llm-products-approved-for-public-launch, https://api.minimax.chat/",,,2023-08-30,MiniMax,,"""hundreds of billions"" per https://api.minimax.chat/",,,Unspecified unreleased,,,,,,,Unknown,"MiniMax has released three foundational model architectures: text-to-visual, text-to-audio, and text-to-text. The startup has also introduced a self-developed general LLM â€œABABâ€, named after the sound of baby babble.",China,,,,,,Unreleased,,,,
Luca 2.0,,,,Hosted access (no API),https://www.163.com/dy/article/IDBGA8840511FQO9.html,,,2023-08-29,Mianbi Intelligence,100000000000.0,"https://www.leiphone.com/category/ai/23kbzQXj60xZgUgO.html 
English translation:
""Li Dahai: From a technical point of view, the CPM2 (Chinese Pretrained Model) 100 billion model we launched at that time was a sparse model of MoE, which is different from the 100 billion model we are promoting now.""

This suggests it is a dense model.",1.2e+24,"Assume Chinchilla-optimal dataset size:
20 * 100B * 100B * 6 = 1.2e24 FLOP",,,100000000.0,"""Assume Chinchilla-optimal dataset size"" from training compute notes",,,,Likely,,China,,,,,,Unreleased,"could be accessible via API, though I have not found documentation

here is accessible for chatting: https://luca.cn/home",,,
MathGPT,Language,Quantitative reasoning,,Hosted access (no API),https://www.gizmochina.com/2023/08/24/mathgpt-launch-public-beta-next-gen-math-assistant/,,TALâ€™s MathGPT launches public beta: your next-gen math assistant,2023-08-24,TAL Education Group (Xueersi),,,,,,,,,,,,Unknown,"During its 20th-anniversary event, TAL Education Group launched the public beta testing of its innovative mathematical large model, MathGPT. This LLM model is designed primarily for global mathematics enthusiasts and research institutions, marking a significant milestone as Chinaâ€™s first large model tailored for mathematics.",China,,,,,,Unreleased,,,,
HyperCLOVA X,Language,"Language modeling/generation,Chat,Search,Translation,Code generation,Question answering,Quantitative reasoning","Kang Min Yoo, Jaegeun Han, Sookyo In, Heewon Jeon, Jisu Jeong, Jaewook Kang, Hyunwook Kim, Kyung-Min Kim, Munhyong Kim, Sungju Kim, Donghyun Kwak, Hanock Kwak, Se Jung Kwon, Bado Lee, Dongsoo Lee, Gichang Lee, Jooho Lee, Baeseong Park, Seongjin Shin, Joonsang Yu, Seolki Baek, Sumin Byeon, Eungsup Cho, Dooseok Choe, Jeesung Han, Youngkyun Jin, Hyein Jun, Jaeseung Jung, Chanwoong Kim, Jinhong Kim, Jinuk Kim, Dokyeong Lee, Dongwook Park, Jeong Min Sohn, Sujung Han, Jiae Heo, Sungju Hong, Mina Jeon, Hyunhoon Jung, Jungeun Jung, Wangkyo Jung, Chungjoon Kim, Hyeri Kim, Jonghyun Kim, Min Young Kim, Soeun Lee, Joonhee Park, Jieun Shin, Sojin Yang, Jungsoon Yoon, Hwaran Lee, Sanghwan Bae, Jeehwan Cha, Karl Gylleus, Donghoon Ham, Mihak Hong, Youngki Hong, Yunki Hong, Dahyun Jang, Hyojun Jeon, Yujin Jeon, Yeji Jeong, Myunggeun Ji, Yeguk Jin, Chansong Jo, Shinyoung Joo, Seunghwan Jung, Adrian Jungmyung Kim, Byoung Hoon Kim, Hyomin Kim, Jungwhan Kim, Minkyoung Kim, Minseung Kim, Sungdong Kim, Yonghee Kim, Youngjun Kim, Youngkwan Kim, Donghyeon Ko, Dughyun Lee, Ha Young Lee, Jaehong Lee, Jieun Lee, Jonghyun Lee, Jongjin Lee, Min Young Lee, Yehbin Lee, Taehong Min, Yuri Min, Kiyoon Moon, Hyangnam Oh, Jaesun Park, Kyuyon Park, Younghun Park, Hanbae Seo, Seunghyun Seo, Mihyun Sim, Gyubin Son, Matt Yeo, Kyung Hoon Yeom, Wonjoon Yoo et al. (296 additional authors not shown)",API access,"https://arxiv.org/abs/2404.01954
https://techcrunch.com/2023/08/24/koreas-internet-giant-naver-unveils-generative-ai-services/, https://www.ncloud.com/solution/featured/hyperclovax",,Koreaâ€™s internet giant Naver unveils generative AI services,2023-08-24,NAVER,,"Unknown

Previous version had 204B parameters: ""Naver says HyperCLOVA has more than 204 billion parameters, but it did not disclose how many parameters have been trained on the HyperCLOVA X""

Maybe ambiguous whether HyperCLOVA X is a new and separate model? But HyperClova is pretty old.

""HyperCLOVA X is built on HyperCLOVA and improves on the previous LLMs""

https://www.ncloud.com/solution/featured/hyperclovax",,Estimations for 82B model are marked as lower bound estimations,,,,,,,,Speculative,"South Korean internet search company Naver on Thursday rolled out its own generative artificial intelligence tool, HyperCLOVA X. The companyâ€™s large language model (LLM) offers services such as a ChatGPT-like AI chatbot, CLOVA X, and a generative AI-based search engine, Cue, equivalent to Microsoft Bing. ",Korea (Republic of),,,,,,Unreleased,"API access for HyperCLOVA X is available at CLOVA Studio, a Hyperscale AI development tool optimized for businesses and provided via NAVER Cloud Platform. The chat service is available at https://clovax.naver.com/.",,,
Eleven Multilingual v2,Speech,"Text-to-speech (TTS),Speech synthesis",,API access,https://elevenlabs.io/blog/eleven-multilingual-v2,,"Lifelike, consistent quality speech synthesis model",2023-08-22,ElevenLabs,,,,,Unspecified unreleased,,,,,,,Unknown,"Eleven Multilingual v2 is our most advanced, emotionally-aware speech synthesis model. It produces natural, lifelike speech with high emotional range and contextual understanding across multiple languages.

The model delivers consistent voice quality and personality across all supported languages while maintaining the speakerâ€™s unique characteristics and accent.

This model excels in scenarios requiring high-quality, emotionally nuanced speech:

Character Voiceovers: Ideal for gaming and animation due to its emotional range.
Professional Content: Well-suited for corporate videos and e-learning materials.
Multilingual Projects: Maintains consistent voice quality across language switches.
Stable Quality: Produces consistent, high-quality audio output.
While it has a higher latency & cost per character than Flash models, it delivers superior quality for projects where lifelike speech is important.

Our v2 models support 29 languages",United States of America,,,,,,Unreleased,,,,
Dou Bao,Language,"Chat,Language modeling/generation",,Hosted access (no API),https://pandaily.com/bytedance-launches-its-first-large-scale-ai-conversation-product-dou-bao/,,,2023-08-18,ByteDance,,,,,,,,,,,,Unknown,"The first AI conversational app â€œDou Baoâ€ and its web version have recently been launched, with the download channel for Android already open. The â€œDou Baoâ€ is the internal codename â€œGraceâ€ AI project by ByteDance, and currently has functions such as text-based conversation and image-based conversation.",China,,,,,,Unreleased,,,,
KwaiYii 13B,Language,Chat,,Unreleased,https://github.com/kwai/KwaiYii,,"""KwaiYii"" large-scale language model (KwaiYii)",2023-08-16,Kuaishou Technology,13000000000.0,13B,,,,,,,,,,Likely,"""KwaiYii"" is a series of large-scale language models (LLM) independently developed by the Kuaishou AI team from scratch. It currently includes models with multiple parameter sizes and covers pre-training models. (KwaiYii-Base), dialogue model (KwaiYii-Chat). Here we introduce the 13B scale series model KwaiYii-13B.""",China,,,,,,,,,,
VARCO LLM 2.0 base,Language,"Language modeling/generation,Chat,Translation,Question answering",,API access,"https://ncsoft.github.io/ncresearch/varco-llm-details/
https://aws.amazon.com/marketplace/pp/prodview-d7amr4yxpibew?sr=0-3&ref_=beagle&applicationId=AWSMPContessa",,VARCO LLM 2.0 is NCSOFT's large language model that can be applied to the development of natural language processing-based AI services.,2023-08-16,NCSOFT,13000000000.0,,1.248e+23,=1600000000000 tokens * 6 FLOP / token / parameter * 13000000000 parameters = 1.248Ã—10^23 FLOP,,"""Our LLM is trained with datasets that are either publicly available for pretraining, collected from the Internet or internally constructed,â€ Jehee Lee, CRO of NCSOFT, told Engadget via email.",1600000000000.0,https://ncsoft.github.io/ncresearch/varco-llm-details/,,,,Likely,"VARCO LLM 2.0 is NCSOFT's large language model that can be applied to the development of various natural language processing-based AI services such as text generation, question answering, chatbots, summarization, and information extraction. NCSOFT's VARCO LLM 2.0 was developed with our own technology, including data construction, pre-training, instruction tuning and alignment tuning. We evaluated VARCO LLM 2.0 on various NLP tasks and its performance has significantly improved compared to VARCO LLM 1.0, and it boasts the highest performance among other Korean LLMs of similar sizes. In particular, it has been trained to be used in high-level natural language processing applications such as creative writing, summarization, question and answering, chatbots and translation, and shows high performance in related quantitative indicators. For inquiries regarding further performance improvement or collaboration for service applications, please contact us by email (varco_llm@ncsoft.com).

Korean Text Generation : VARCO LLM 2.0 is optimized for Korean natural language generation applications. In particular, it provides more natural and creative responses in understanding user instructions and generating text.",Korea (Republic of),,,,,,Unreleased,,,,
Code Llama-34B,Language,Code generation,"Baptiste RoziÃ¨re, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Ellen Tan, Yossef (Yossi) Adi, Jingyu Liu, Tal Remez, JÃ©rÃ©my Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Defossez, Jade Copet, Faisal Azhar, Hugo Touvron, Gabriel Synnaeve, Louis Martin, Nicolas Usunier, Thomas Scialom",Open weights (restricted use),"https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/
https://arxiv.org/abs/2308.12950",2732.0,Code Llama: Open Foundation Models for Code,2023-08-14,Meta AI,34000000000.0,34B,5.3e+23,"1.22e23 finetune compute, or ~5.3e23 including Llama-2 34B base compute. See finetune compute notes for calculation.
",Unspecified unreleased,"""As shown in Table 1, Code Llama is trained predominantly on a near-deduplicated dataset of publicly available code. We also source 8% of our samples data from natural language datasets related to
code. This dataset contains many discussions about code and code snippets included in natural language questions or answers. To help the model retain natural language understanding skills, we also sample a small proportion of our batches from a natural language dataset""",600000000000.0,"Llama 2 used 2T tokens, and ""We train Code Llama on 500B additional tokens and Code Llama - Python further on 100B tokens""

2T + 500B + 100B = 2600000000000",,,NVIDIA A100 SXM4 80 GB,Confident,"We release Code Llama, a family of large language models for code based on Llama 2 providing state-of-the-art performance among open models, infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks. We provide multiple flavors to cover a wide range of applications: foundation models (Code Llama), Python specializations (Code Llama - Python), and instruction-following models (Code Llama - Instruct) with 7B, 13B and 34B parameters each. All models are trained on sequences of 16k tokens and show improvements on inputs with up to 100k tokens. 7B and 13B Code Llama and Code Llama - Instruct variants support infilling based on surrounding content. Code Llama reaches state-of-the-art performance among open models on several code benchmarks, with scores of up to 53% and 55% on HumanEval and MBPP, respectively. Notably, Code Llama - Python 7B outperforms Llama 2 70B on HumanEval and MBPP, and all our models outperform every other publicly available model on MultiPL-E. We release Code Llama under a permissive license that allows for both research and commercial use.",United States of America,Llama 2-34B,1.22e+23,"Training the nine Code Llama models took 400k A100-hours across all the models, per model card. It's nine models because there are three base models at 7B, 13B, 34B, and then Instruct and Python models across all three sizes. I'll calculate for Code Llama Python-34B since it's the most trained.

Code Llama-base is trained from Llama 2 with 500B tokens: ""We train Code Llama on 500B tokens during the initial phase, starting from the 7B, 13B, and 34B versions of Llama 2""

Code Llama-Python required an additional 100B tokens in fine-tuning: ""We train Code Llama on 500B additional tokens and Code Llama - Python further on 100B tokens.""
Code Llama-Instruct is fine-tuned on 5B tokens: ""For Code Llama - Instruct, we train with a batch size of 524,288 tokens and on approx. 5B tokens in total.""

34B * (500B+100B) * 6 = 1.22e23

Using the hardware method, we get 400,000 * 3600 * 312 trillion * 0.3 = 1.32e23 for the whole family. Using 34/(34+13+7) as the proportion of compute used for the 34B model, we get 0.63 * 1.32e23 = 8.5e22. (this is rounding down instruct tuning to 0).

Token*params estimate is probably better; seems like evidence of utilization > 0.3.",,,Unreleased,"Llama 2 license. can't use outputs to train models.

https://github.com/meta-llama/llama/blob/main/LICENSE",,,
Baichuan2-53B,Language,"Language modeling/generation,Chat",,Unreleased,https://technode.com/2023/08/09/chinese-ai-startup-baichuan-rolls-out-third-llm-in-four-months/,,Chinese AI startup Baichuan rolls out third LLM in four months,2023-08-09,Baichuan,53000000000.0,,8.268e+23,"Given that it was announced at a similar time to the other Baichuan2 models, this assumes that the dataset size is the same at 2.6T tokens while the parameter count was scaled up. This would be consistent with many other model releases, such as Meta's Llama models.
53b * 2.6t * 6 = 8.268e23
",,,,,,,,Likely,"On Tuesday, four-month-old AI startup Baichuan Intelligent Technology unveiled its first closed-source model equipped with 53 billion parameters. Following the Chinese companyâ€™s rapid release of two open-source large language models since its founding in April, the new model demonstrates the firmâ€™s fast pace in delivering pre-trained models for larger parameters. The freshly introduced model, Baichuan-53B, is mainly for corporate clients and focused on text generation. A ChatGPT-like chat service built on the model entered internal testing on the same day the model was launched, its official website shows, with plans for the firm to publicly launch APIs and associated components next month.",China,,,,,,Unreleased,,,,
Claude Instant,Language,"Language modeling,Chat",,API access,https://www.anthropic.com/news/releasing-claude-instant-1-2,,Releasing Claude Instant 1.2,2023-08-09,Anthropic,,"speculatively, Anthropic charges 1/10 as much for Claude Instant as Claude 2, so it may have around 1/10 the parameters (Claude 2 parameters are not public info)

https://cdn.sanity.io/files/4zrzovbb/website/90df03aed08b794ab03c5a7bf28b2ad9cf26cf3c.pdf",,,Unspecified unreleased,"Assuming this refers to Instant 1.2, this has a knowledge cutoff date of January 2023, according to https://github.com/HaoooWang/llm-knowledge-cutoff-dates?tab=readme-ov-file and https://docsbot.ai/models/claude-instant-1-2.",,,,,,Unknown,"Businesses working with Claude can now access our latest version of Claude Instant, version 1.2, available through our API. Claude Instant is our faster, lower-priced yet still very capable model, which can handle a range of tasks including casual dialogue, text analysis, summarization, and document comprehension.

Claude Instant 1.2 incorporates the strengths of our latest model Claude 2 in real-world use cases and shows significant gains in key areas like math, coding, reasoning, and safety. It generates longer, more structured responses and follows formatting instructions better. Instant 1.2 also shows improvements in quote extraction, multilingual capabilities, and question answering.",United States of America,,,,,,Unreleased,,,,
CALM,Robotics,Animal (human/non-human) imitation,"Chen Tessler, Yoni Kasten, Yunrong Guo, Shie Mannor, Gal Chechik, Xue Bin Peng",,"https://research.nvidia.com/labs/par/calm/
https://arxiv.org/abs/2305.02195",104.0,CALM: Conditional Adversarial Latent Models for Directable Virtual Characters,2023-08-06,"NVIDIA,Technion - Israel Institute of Technology",,,,The total pre-training of the networks involved 5 billion environment steps. The low-level policy operates at 30Hz while the high-level policy operates at 6Hz.,,"160 diverse motion clips totaling 30 minutes, from a motion capture dataset. These include motions like walking, running, sword strikes, etc.",,,,,NVIDIA A100,Unknown,"In this work, we present Conditional Adversarial Latent Models (CALM),
an approach for generating diverse and directable behaviors for user-controlled
interactive virtual characters. Using imitation learning, CALM learns a representation of movement that captures the complexity and diversity of human
motion, and enables direct control over character movements. The approach
jointly learns a control policy and a motion encoder that reconstructs key
characteristics of a given motion without merely replicating it. The results
show that CALM learns a semantic motion representation, enabling control
over the generated motions and style-conditioning for higher-level task training. Once trained, the character can be controlled using intuitive interfaces,
akin to those found in video games.","United States of America,Israel",,,,,,,,,,
Zi Yue,Language,Chat,,,https://www.chinadaily.com.cn/a/202307/28/WS64c3226ea31035260b8190a4.html,,NetEase Youdao launches first large model in education,2023-07-28,NetEase,,,,,,,,,,,,Unknown,"As Open AI's ChatGPT takes the tech world by storm, Chinese educational technology firm NetEase Youdao launched its large model, along with up to six applications, on Thursday, which marked the birth of one of China's first large models in the education sector.",China,,,,,,,,,,
Zi Yue 2.0,Language,Chat,,,https://m.aastocks.com/en/usq/news/comment.aspx?source=AAFN&id=NOW.1317044&catg=1,,"NetEase Youdao Upgrades 'Ziyue' Foundation Model to 2.0 Ver, Encompassing More Subjects & Teaching Areas",2023-07-28,NetEase,,,,,,,,,,,,Unknown,"Chinese media reported that Youdao (DAO.US)     has released the 2.0 upgrade for its Ziyue educational foundation model. Youdao also launched Youdao Speed Reading (literal translation of ""æœ‰é“é€Ÿè®€""), new-generation virtual personality verbal language trainer, AI home tutors, and Youdao-branded new-generation intelligent hardware applications.

It is reported that Ziyue 2.0 has been upgraded in the knowledge question and answering ability within the education scene, with it expanding to more subjects and teaching areas. The amount of educational data has been largely expanded, the model's context window has been upgraded to 16,000 tokens, and new Agent and retrieval enhancement capabilities have been added.",China,,,,,,,,,,
EXAONE 2.0,"Multimodal,Image generation,Language,Biology,Vision","Language modeling,Image generation,Visual question answering",,Unreleased,https://aws.amazon.com/solutions/case-studies/lg-ai-research-case-study/,,LG AI Research Develops Foundation Model Using Amazon SageMaker,2023-07-19,LG AI Research,300000000000.0,300 billion,,,Unspecified unreleased,"From KoreaTimes (https://www.koreatimes.co.kr/www/tech/2023/12/129_355258.html)

""EXAONE 2.0 studied about 45 million specialized documents and 350 million images, including patents and papers secured through partnerships.

Considering that much of the existing expertise data is in English, EXAONE 2.0 is developed as a bilingual model that can understand and answer both in Korean and English at the same time. It also learns over four times more data than the previous model.""",,,,,,Speculative,,Korea (Republic of),,,,,,Unreleased,,,,
Llama 2-70B,Language,"Language modeling,Language modeling/generation,Question answering","Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, Thomas Scialom
",Open weights (restricted use),"https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/
https://arxiv.org/abs/2307.09288",15053.0,Llama 2: Open Foundation and Fine-Tuned Chat Models,2023-07-18,Meta AI,70000000000.0,"Llama has been released in 7B, 13B, 34B, and 70B variants.",8.1e+23,"""Pretraining utilized a cumulative 3.3M GPU hours of computation on hardware of type A100-80GB"" of which 1720320 GPU hours were used to train the 70B model.

311.84 BF16 TFLOP/s * 1720320 hours * 0.40 utilization = 7.725e+23 FLOP.

Alternatively: the model was trained for 1 epoch on 2 trillion tokens and has 70B parameters. C = 6ND = 6*70B*2T = 8.4e+23 FLOP.",Llama 2 dataset,"2 trillion tokens of publicly available text, with no text from Meta's products.
""Our training corpus includes a new mix of data from publicly available sources, which does not include data from Metaâ€™s products or services. We made an effort to remove data from certain sites known to contain a high volume of personal information about private individuals. We trained on 2 trillion tokens of data as this provides a good performanceâ€“cost trade-off, up-sampling the most factual sources in an effort to increase knowledge and dampen hallucinations.""
Knowledge cutoff date is September 2022, according to https://huggingface.co/meta-llama/Llama-2-70b. ",2000000000000.0,"[tokens]

2 trillion tokens ~= 1.5 trillion words",1728.0,"Model was trained from January 2023 to July 2023, which is six months. However, the training run duration did not take up this whole period. According to a Meta employee interviewed by Epoch, Llama 2 34B and 70B were trained on different clusters, with overlapping training periods. Based on an estimate of 1000 GPUs, it would have taken 72 days.",NVIDIA A100 SXM4 80 GB,Confident,"In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closedsource models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.",United States of America,,,,1000.0,0.4191975017,Unreleased,"Llama 2 license. can't use outputs to train models.

https://github.com/meta-llama/llama/blob/main/LICENSE

https://huggingface.co/meta-llama/Llama-2-70b",,,
Llama 2-34B,Language,Language modeling,"Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, Thomas Scialom
",Unreleased,"https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/
https://arxiv.org/abs/2307.09288",15053.0,Llama 2: Open Foundation and Fine-Tuned Chat Models,2023-07-18,Meta AI,34000000000.0,"Llama has been released in 7B, 13B, 34B, and 70B variants.",4.08e+23,"All models sizes trained on 2.0T tokens, per table 1
2T * 34b * 6 = 4.08e23

Also trained on 1038336 A100-hours, which is 3.5e23 at 30% utilization. So the utilization was probably around 35%.",Llama 2 dataset,"2 trillion tokens of publicly available text, with no text from Meta's products.
""Our training corpus includes a new mix of data from publicly available sources, which does not include data from Metaâ€™s products or services. We made an effort to remove data from certain sites known to contain a high volume of personal information about private individuals. We trained on 2 trillion tokens of data as this
provides a good performanceâ€“cost trade-off, up-sampling the most factual sources in an effort to increase knowledge and dampen hallucinations.""",,,,,NVIDIA A100 SXM4 80 GB,Confident,"In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned
large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.
Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our
models outperform open-source chat models on most benchmarks we tested, and based on
our human evaluations for helpfulness and safety, may be a suitable substitute for closedsource models. We provide a detailed description of our approach to fine-tuning and safety
improvements of Llama 2-Chat in order to enable the community to build on our work and
contribute to the responsible development of LLMs.",United States of America,,,,,,Unreleased,,,,
Llama 2-13B,Language,Language modeling,"Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, Thomas Scialom
",Open weights (restricted use),"https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/
https://arxiv.org/abs/2307.09288",15053.0,Llama 2: Open Foundation and Fine-Tuned Chat Models,2023-07-18,Meta AI,13000000000.0,"Llama has been released in 7B, 13B, and 70B variants.",1.6e+23,13 billion parameters * 2 trillion tokens * 6 FLOP / token / parameter = 1.6e23 FLOP,Llama 2 dataset,"2 trillion tokens of publicly available text, with no text from Meta's products.
""Our training corpus includes a new mix of data from publicly available sources, which does not include data from Metaâ€™s products or services. We made an effort to remove data from certain sites known to contain a high volume of personal information about private individuals. We trained on 2 trillion tokens of data as this provides a good performanceâ€“cost trade-off, up-sampling the most factual sources in an effort to increase knowledge and dampen hallucinations.""
Knowledge cutoff date is September 2022, according to https://huggingface.co/meta-llama/Llama-2-70b. ",2000000000000.0,2 trillion tokens ~= 1.5 trillion words,,,NVIDIA A100 SXM4 80 GB,Confident,"In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closedsource models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.",United States of America,,,,,,Unreleased,"Llama 2 license. can't use outputs to train models.

https://github.com/meta-llama/llama/blob/main/LICENSE",,,
BaiLing,Language,"Language modeling/generation,Question answering",,Unreleased,"https://www.antgroup.com/en/technology/new-tech-Technology-Antfocuses-Tabcomnent-detail/20241028001

https://finance.yahoo.com/news/ant-group-gets-green-light-093000023.html",,,2023-07-15,Ant Group,,,,,Unspecified unreleased,,,,,,,Unknown,"Ant Groupâ€™s BaiLing foundation model has made significant advancements in computing power, security, and knowledge processing. It has established a computing cluster with tens of thousands of heterogeneous accelerator cards, integrated security capabilities ranging from detection to defense, and the ability to process trillions of tokens.",China,,,,,,Unreleased,,,,
ChatRhino,Language,Chat,,,https://jdcorporateblog.com/jd-com-introduces-chatrhino-empowering-industry-innovations-with-an-advanced-large-language-model/,,JD.com Introduces ChatRhino: Empowering Industry Innovations with an Advanced Large Language Model,2023-07-13,JD.com,100000000000.0,"""ChatRhino sets a new benchmark as a 100-billion-parameter model"", could be substantially rounded",,,,"Mix of general and supply chain data: ""By combining 70% generalized data with 30% native intelligent supply chain data, JDâ€™s latest AI model offers targeted solutions for real industry challenges across sectors such as retail, logistics, finance, health, and city""",,,,,,Confident,"JD.com today unveiled its ChatRhino (Yanxi in Chinese) large language model (LLM) on its 2023 JDDiscovery tech summit, tailored to serve various industries. By combining 70% generalized data with 30% native intelligent supply chain data, JDâ€™s latest AI model offers targeted solutions for real industry challenges across sectors such as retail, logistics, finance, health, and city. Building upon the success of the billion-parameter model K-PLUG launched in 2021 and the 10-billion-parameter model Vega introduced in 2022, JDâ€™s ChatRhino sets a new benchmark as a 100-billion-parameter model.",China,,,,,,,,,,
Claude 2,Language,"Language modeling,Chat,Language modeling/generation,Question answering",,API access,"https://www.anthropic.com/index/claude-2, https://www-files.anthropic.com/production/images/Model-Card-Claude-2.pdf",,,2023-07-11,Anthropic,,,3.866e+24,https://colab.research.google.com/drive/1MdPuhS4Emaf23VXYZ-ooExDW-5GXZkw0#scrollTo=Ds0Q5X8aMnOY,Unspecified unreleased,"From model card: ""Claude models are trained on a proprietary mix of publicly available information from the Internet, datasets
that we license from third party businesses, and data that our users affirmatively share or that crowd workers provide. Some of the human feedback data used to finetune Claude was made public [12] alongside our RLHF [2] and red-teaming [4] research.
Claude 2â€™s training data cuts off in early 2023, and roughly 10 percent of the data included was non-English.""
This has a knowledge cutoff date of January 2024 - which I assume means January 1, 2024 - https://www.youreverydayai.com/knowledge-cutoff-what-it-is-and-why-it-matters-for-large-language-models/.",,,,,,Speculative,,United States of America,,,,,,Unreleased,,,,
TeleChat,Language,"Chat,Language modeling/generation,Text summarization,Translation",,Unreleased,https://m.thepaper.cn/baijiahao_23766944,,,2023-07-07,China Telecom,,,,,,"(Google translated from https://m.thepaper.cn/baijiahao_23766944) ""TeleChat uses a large amount of high-quality Chinese and English corpus for pre-training, and uses tens of millions of question and answer data for fine-tuning""",,,,,,Unknown,"(Google translated) At China Telecom's ""Computing and Network IntegrationÂ·Sunac Future"" sub-forum, China Telecom Digital Intelligence Technology Branch (hereinafter referred to as: Telecom Zhike) officially released China Telecom's large language model TeleChat and demonstrated the large model empowering data Products in three directions: middle platform, intelligent customer service and intelligent government affairs.",China,,,,,,Unreleased,,,,
Pangu 3.0,"Multimodal,Language,Image generation,Vision","Language modeling/generation,Image generation",,API access,https://www.huaweicloud.com/intl/en-us/news/20230707180809498.html,,"Reshaping Industries with AI: Huawei Cloud Launches Pangu Models 3.0 and Ascend AI Cloud Services
",2023-07-07,Huawei,100000000000.0,"100B? I think the five foundation models are all included in the same system, instead of being five different variants of Pangu, but that's not very clear. I think that's implied by ""All of these capability sets will be made available to customers and partners, and will be consistent regardless of the model size"". 

https://www.huaweicloud.com/intl/en-us/news/20230707180809498.html 
""Huawei Cloud Pangu models were designed to focus on the practical needs of specific industry scenarios. The newly-launched Pangu Models 3.0 use a ""5+N+X"" three-layer architecture.

The L0 layer consists of five foundation models: NLP, CV, multimodal, prediction, and scientific computing, which provide general skills to power an endless possibility of industry-specific applications. Pangu Models 3.0 are available in different sizes: 10 billion parameters, 38 billion parameters, 71 billion parameters, and 100 billion parameters, meeting diverse customer needs and different standards on latency and response times. Brand-new capability sets are also provided, such as knowledge-based Q&A, copywriting, and code generation for the Pangu NLP model; and image generation and understanding for the Pangu multimodal model. All of these capability sets will be made available to customers and partners, and will be consistent regardless of the model size (number of parameters).

The L1 layer consists of N industry-tailored models. Huawei Cloud can provide customers with industry models it has trained on open industry datasets, including Pangu models for government, finance, manufacturing, mining, and meteorology. Alternatively, customers can train their own models using their own datasets based on Huawei's L0 or L1 Pangu models.

The L2 layer provides pre-trained models for specific industry scenarios and tasks, such as intelligent government hotline, intelligent branch assistant, lead compound screening, conveyor belt foreign object detection, and typhoon trajectory prediction. These models can be quickly deployed off-the-shelf.""

",,,Unspecified unreleased,,,,,,,Likely,"Huawei Cloud Pangu models were designed to focus on the practical needs of specific industry scenarios. The newly-launched Pangu Models 3.0 use a ""5+N+X"" three-layer architecture.

The L0 layer consists of five foundation models: NLP, CV, multimodal, prediction, and scientific computing, which provide general skills to power an endless possibility of industry-specific applications. Pangu Models 3.0 are available in different sizes: 10 billion parameters, 38 billion parameters, 71 billion parameters, and 100 billion parameters, meeting diverse customer needs and different standards on latency and response times. Brand-new capability sets are also provided, such as knowledge-based Q&A, copywriting, and code generation for the Pangu NLP model; and image generation and understanding for the Pangu multimodal model. All of these capability sets will be made available to customers and partners, and will be consistent regardless of the model size (number of parameters).",China,,,,,,,,,,
InternLM,Language,Language modeling,,Unreleased,https://github.com/InternLM/InternLM-techreport/blob/main/InternLM.pdf,,,2023-07-06,"Shanghai AI Lab,SenseTime",104000000000.0,"""We present InternLM, a multilingual foundational language model with 104B parameters""",9.984e+23,6 * 104b * 1.6T = 9.984e23,,,1600000000000.0,"""InternLM is pre-trained on a large corpora with 1.6T tokens""",,Training performance for the open-source InternLM-7B: https://github.com/InternLM/InternLM/blob/main/doc/en/train_performance.md,NVIDIA A100 SXM4 80 GB,Confident,"Pre-training a bilingual 100B Foundation model on data with over a trillion tokens, the model exhibits excellent performance in scenarios such as Chinese, English, and coding due to the appropriate data ratio. Based on the foundation model, the application of high-quality human annotated dialogue data combined with RLHF technology enables the InternLM large language model to respond to complex commands during human interaction, while also demonstrating responses in line with human morality and values.","China,Hong Kong",,,,,,Unreleased,Though they released 7b and 20b models (https://github.com/InternLM/InternLM/tree/main/model_cards) 100b model is not found,,,
xTrimoPGLM -100B,Biology,"Proteins,Protein or nucleotide language model (pLM/nLM),Protein generation","Bo Chen, Xingyi Cheng, Yangli-ao Geng, Shen Li, Xin Zeng, Boyan Wang, Jing Gong, Chiming Liu, Aohan Zeng, Yuxiao Dong, Jie Tang, Le Song",Unreleased,https://www.biorxiv.org/content/10.1101/2023.07.05.547496v4,135.0,xTrimoPGLM: Unified 100B-Scale Pre-trained Transformer for Deciphering the Language of Protein,2023-07-06,"Tsinghua University,BioMap Research",100000000000.0,"Abstract: ""training xTrimoPGLM at an unprecedented scale of 100 billion
parameters and 1 trillion training tokens""",6.2e+23,"""xTrimoPGLM-100B is trained on a cluster of 96 DGX-A100 GPU (8Ã—40G) servers in FP16 precision from January 18 to June 30, 2023. During this time, xTrimoPGLM-100B has consumed 1 trillion tokens from the dataset consisting of Uniref90 and ColAbFoldDB. As of the current date, xTrimoPGLM-100B continues its pre-training process to pass through as many tokens as possible""

6 * 100 billion params * 1T tokens = 6e23

8*96 * 312 trillion * 163 days * 24 * 3600 * 0.3 ~= 1e24

directly given in the paper (Table 9, or Table 4 in new version): 6.2E+23 ",UniRef50,,,~24M protein sequences,3912.0,163 days,NVIDIA A100 SXM4 40 GB,Confident,"Protein language models have shown remarkable success in learning biological information from protein sequences. However, most existing models are limited by either autoencoding or autoregressive pre-training objectives, which makes them struggle to handle protein understanding and generation tasks concurrently. This paper proposes a unified protein language model, xTrimoPGLM, to address these two types of tasks simultaneously through an innovative pre-training framework. Our key technical contribution is an exploration of the compatibility and the potential for joint optimization of the two types of objectives, which has led to a strategy for training xTrimoPGLM at an unprecedented scale of 100 billion parameters and 1 trillion training tokens. Our extensive experiments reveal that xTrimoPGLM significantly outperforms other advanced baselines in diverse protein understanding tasks (13 out of 15 tasks across four categories) and generates novel protein sequences which are structurally similar to natural ones. Furthermore, using the same xTrimoPGLM framework, we train an antibody-specific model (xTrimoPGLM-Ab) using 1 billion parameters. This model set a new record in predicting antibody naturalness and structures, both essential to the field of antibody-based drug design, and demonstrated a significantly faster inference speed than AlphaFold2. These results highlight the substantial capability and versatility of xTrimoPGLM in understanding and generating protein sequences.","China,China",,,,768.0,,Unreleased,,,,
NEC LLM 13B,Language,"Language modeling/generation,Chat",,,https://jpn.nec.com/press/202307/20230706_02.html,,NECã€130å„„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ä¸–ç•Œãƒˆãƒƒãƒ—ã‚¯ãƒ©ã‚¹ã®æ—¥æœ¬èªžæ€§èƒ½ã‚’æœ‰ã™ã‚‹è»½é‡ãªLLMã‚’é–‹ç™º,2023-07-06,NEC Laboratories,13000000000.0,13B,,"""NEC's LLM relieved this time was also trained using 512 GPUs installed on NEC's AI supercomputer""
from https://jpn.nec.com/rd/technologies/202308/index.html",,,,,,,,Confident,,United States of America,,,,512.0,,,,,,
Stable Diffusion XL (SDXL),Image generation,"Image generation,Text-to-image","Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas MÃ¼ller, Joe Penna, Robin Rombach",Open weights (non-commercial),https://arxiv.org/abs/2307.01952,3712.0,SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis,2023-07-04,Stability AI,3400000000.0,"""...result in a model size of 2.6B parameters in the UNet, see Tab. 1. The text encoders have a total size of 817M parameters.""",,,Unspecified unreleased,,,,,,,Speculative,"We present SDXL, a latent diffusion model for text-to-image synthesis. Compared to previous versions of Stable Diffusion, SDXL leverages a three times larger UNet backbone: The increase of model parameters is mainly due to more attention blocks and a larger cross-attention context as SDXL uses a second text encoder. We design multiple novel conditioning schemes and train SDXL on multiple aspect ratios. We also introduce a refinement model which is used to improve the visual fidelity of samples generated by SDXL using a post-hoc image-to-image technique. We demonstrate that SDXL shows drastically improved performance compared the previous versions of Stable Diffusion and achieves results competitive with those of black-box state-of-the-art image generators. In the spirit of promoting open research and fostering transparency in large model training and evaluation, we provide access to code and model weights at this https URL",United Kingdom of Great Britain and Northern Ireland,,,,,,Unreleased,"SDXL 0.9 Research License:
https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9

MIT license for inference code, not sure if training code is here:
https://github.com/Stability-AI/generative-models/tree/main",,,
ERNIE 3.5,Language,"Language modeling,Language modeling/generation",,Hosted access (no API),http://research.baidu.com/Blog/index-view?id=185,,Introducing ERNIE 3.5: Baiduâ€™s Knowledge-Enhanced Foundation Model Takes a Giant Leap Forward,2023-06-27,Baidu,,,,,,,,,,,,Unknown,,China,,,,,,Unreleased,,,,
Inflection-1,Language,Language modeling,,Hosted access (no API),https://inflection.ai/assets/Inflection-1.pdf,,Inflection-1 technical memo,2023-06-23,Inflection AI,,,1.0001e+24,"<= 2.5e24

They define two ""compute classes"", one for models with more compute than PaLM 540B, i.e. GPT-4 and PaLM 2, and one for models with as much compute or less, i.e. GPT-3.5, Chinchilla, LLaMA, and Inflection-1.

PaLM 540B required 2.5e24 FLOP to train (confirmed by Google)",,"""Inflection-1 was trained using thousands of NVIDIA H100 GPUs on a very large dataset.""",,,,,NVIDIA H100 SXM5 80GB,Speculative,"Large language models (LLMs) based on the Transformer architecture have been shown to possess a range of advanced capabilities in language generation and understanding. These capabilities have paved the way for deployment of LLMs in products like OpenAIâ€™s Chat-GPT and Googleâ€™s Bard. At Inflection AI, our mission is to create personal AIs for everyone, and in May 2023 we released Pi (pi.ai) â€“ an LLM-based personal AI which is designed to be empathetic, useful, and safe. In this work we introduce the foundation model powering Pi, dubbed Inflection-1, and evaluate its performance characteristics across a variety of benchmarks.",United States of America,,,,,,Unreleased,,,,
MPT-30B,Language,"Language generation,Code generation",,Open weights (unrestricted),https://huggingface.co/mosaicml/mpt-30b,,,2023-06-22,MosaicML,30000000000.0,30B,1.8900000000001e+23,"According to their blog post,
""MPT-30B FLOPs ~= 6 * 30e9 [params] * 1.05e12 [tokens] = 1.89e23 FLOPs""","mC4,C4,RedPajama,The Stack",https://www.databricks.com/sites/default/files/inline-images/open-source-foundations-models-1.png,1050000000000.0,"~4T tokens across sources, but only trained on 1.05T of these",278.4,30B: 512x H100-80gb for 11.6 days,NVIDIA H100 SXM5 80GB,Confident,"MPT-30B is a decoder-style transformer pretrained from scratch on 1T tokens of English text and code. This model was trained by MosaicML.

MPT-30B is part of the family of Mosaic Pretrained Transformer (MPT) models, which use a modified transformer architecture optimized for efficient training and inference.

MPT-30B comes with special features that differentiate it from other LLMs, including an 8k token context window (which can be further extended via finetuning; see MPT-7B-StoryWriter), support for context-length extrapolation via ALiBi, and efficient inference + training via FlashAttention. It also has strong coding abilities thanks to its pretraining mix. MPT models can also be served efficiently with both standard HuggingFace pipelines and NVIDIA's FasterTransformer. The size of MPT-30B was also specifically chosen to make it easy to deploy on a single GPUâ€”either 1xA100-80GB in 16-bit precision or 1xA100-40GB in 8-bit precision.",United States of America,,,,512.0,,Open source,"apache 2.0 for weights.

pretrain code here: https://github.com/mosaicml/llm-foundry/tree/main/scripts/train/yamls/pretrain ",,,
Wu Dao Aquila-7B,Language,"Chat,Code generation",,Open weights (restricted use),"https://spectrum.ieee.org/china-chatgpt-wu-dao

https://huggingface.co/BAAI/Aquila-7B

https://github.com/FlagAI-Open/FlagAI/blob/master/examples/Aquila/README_en.md",,,2023-06-10,Beijing Academy of Artificial Intelligence / BAAI,7000000000.0,,,,,,,,,,NVIDIA A100,Confident,"Who said all large-language models (LLMs) necessarily need to be large? In Chinaâ€™s case, LLMs are currently downsizing in their size and number of parameters. According to sources, this is because the country is now focusing on enabling Chinese startups and smaller entities to build their own generative AI applications. As part of this downscaling trend, in June the Beijing Academy of Artificial Intelligence (BAAI) introduced Wu Dao 3.0, a series of open-source LLMs.

Based on interviews with high-ranking, anonymous sources involved in the project, IEEE Spectrum can report that Wu Dao 3.0 builds on the academyâ€™s work with Wu Dao 2.0, a sparse, multimodal generative AI modelâ€”as has been widely reported about version 2.0â€”with 1.75 trillion parameters. Although there is no single set of parameters for Wu Dao 3.0 (itâ€™s a range of models with a variety of parameter counts) all are well below the 1.75 trillion high-water mark that version 2.0 set.",China,,,,,,Open source,"Apache 2.0 for code: https://huggingface.co/BAAI/Aquila-7B
BAAI license for weights, commercial but restrictions around rights/PRC laws: https://huggingface.co/BAAI/Aquila-7B/resolve/main/BAAI%20Aquila%20Model%20License%20Agreement.pdf",,,
Wu Dao Aquila-33B,Language,"Chat,Code generation,Language modeling/generation,Question answering,Text summarization",,Unreleased,"https://spectrum.ieee.org/china-chatgpt-wu-dao

https://huggingface.co/BAAI/Aquila-7B",,,2023-06-10,Beijing Academy of Artificial Intelligence / BAAI,33000000000.0,33B for largest model: https://huggingface.co/BAAI/Aquila-7B,,,,,,,,,NVIDIA A100,Confident,"Who said all large-language models (LLMs) necessarily need to be large? In Chinaâ€™s case, LLMs are currently downsizing in their size and number of parameters. According to sources, this is because the country is now focusing on enabling Chinese startups and smaller entities to build their own generative AI applications. As part of this downscaling trend, in June the Beijing Academy of Artificial Intelligence (BAAI) introduced Wu Dao 3.0, a series of open-source LLMs.

Based on interviews with high-ranking, anonymous sources involved in the project, IEEE Spectrum can report that Wu Dao 3.0 builds on the academyâ€™s work with Wu Dao 2.0, a sparse, multimodal generative AI modelâ€”as has been widely reported about version 2.0â€”with 1.75 trillion parameters. Although there is no single set of parameters for Wu Dao 3.0 (itâ€™s a range of models with a variety of parameter counts) all are well below the 1.75 trillion high-water mark that version 2.0 set.",China,,,,,,,"""coming soon"" per https://huggingface.co/BAAI/Aquila-7B. This page was published a year ago so seems unlikely it will ever be released.",,,
MusicGen,Audio,Audio generation,"Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi, Alexandre DÃ©fossez",Open weights (non-commercial),https://arxiv.org/abs/2306.05284,574.0,Simple and Controllable Music Generation,2023-06-08,Meta AI,3359000000.0,"""We train autoregressive transformer models at different sizes: 300M, 1.5B, 3.3B parameters""

Uses EnCodec 32kHz (HF version has 59M params) for audio tokenization.",,"We train the 300M, 1.5B and 3.3B parameter models, using respectively 32, 64 and 96 GPUs, with mixed precision.

Unclear how many epochs used so FLOP calculation is not feasible.",ShutterStock and Pond5 music data collections,"""We use 20K hours of licensed music to train MUSICGEN. Specifically, we rely on an internal dataset of 10K high-quality music tracks, and on the ShutterStock and Pond5 music data collections with respectively 25K and 365K instrument-only music tracks. All datasets consist of full-length music sampled at 32 kHz with metadata composed of a textual description and additional information such as the genre, BPM, and tags.""",,"""We train on 30-second audio crops sampled at random from the full track... We use 20K hours of licensed music""

20000 hours * 60 min/hour * 2 inputs/min = 2400000 input sequences

EnCodec is run at 32kHz but after convolutions has a frame rate of 50 Hz, suggesting 2400000 * 30s * 50/s = 3,600,000,000 audio tokens.

Not confident enough in this calculation to add to database.",,,,Likely,"We tackle the task of conditional music generation. We introduce MusicGen, a single Language Model (LM) that operates over several streams of compressed discrete music representation, i.e., tokens. Unlike prior work, MusicGen is comprised of a single-stage transformer LM together with efficient token interleaving patterns, which eliminates the need for cascading several models, e.g., hierarchically or upsampling. Following this approach, we demonstrate how MusicGen can generate high-quality samples, while being conditioned on textual description or melodic features, allowing better controls over the generated output. We conduct extensive empirical evaluation, considering both automatic and human studies, showing the proposed approach is superior to the evaluated baselines on a standard text-to-music benchmark. Through ablation studies, we shed light over the importance of each of the components comprising MusicGen. Music samples, code, and models are available at this https URL.",United States of America,,,,,,Open source,"Code is released under MIT, model weights are released under CC-BY-NC 4.0

https://github.com/facebookresearch/audiocraft/blob/main/docs/MUSICGEN.md",,,
PolySphere-1,Language,"Chat,Language modeling/generation,Japanese language modeling",AI Inside,Unreleased,"https://inside.ai/en/news/2023/06/08/aiinside-xresearch/
https://inside.ai/en/news/2023/06/29/polysphere-token/",,"AI inside Establishes â€œXResearchâ€ for R&D and Social Implementation of Generative AI and LLM, Providing the Alpha Version of a 14 Billion-Parameter Japanese LLM Service",2023-06-08,AI inside,14000000000.0,"14B from https://inside.ai/en/news/2023/06/08/aiinside-xresearch/
",,,,,,,,,,Likely,,Japan,,,,,,Unreleased,,,,
PaLM 2,Language,"Language modeling,Language modeling/generation","Andrew M. Dai, David R. So, Dmitry Lepikhin, Jonathan H. Clark, Maxim Krikun, Melvin Johnson, Nan Du, Rohan Anil, Siamak Shakeri, Xavier Garcia, Yanping Huang, Yi Tay, Yong Cheng, Yonghui Wu, Yuanzhong Xu, Yujing Zhang, Zachary Nado, Bryan Richter, Alex Polozov, Andrew Nystrom, Fangxiaoyu Feng, Hanzhao Lin, Jacob Austin, Jacob Devlin, Kefan Xiao, Orhan Firat, Parker Riley, Steven Zheng, Yuhuai Wu, Zhongtao Liu, Jiahui Yu, Guy Gur-Ari, Weikang Zhou, Sneha Kudugunta, Sunipa Dev, Frederick Liu, Gustavo Hernandez Abrego, Kelvin Xu, Abe Ittycheriah, Daniel Sohn, John Nham, Le Hou, Siyuan Qiao, Pidong Wang, Zirui Wang, Laurent El Shafey, Hyeontaek Lim, Marcello Maggioni, Michael Isard, Paul Barham, Qiao Zhang, Tao Wang, Yash Katariya, Aurko Roy, Benjamin Lee, Brennan Saeta, Ce Zheng, Hadi Hashemi, Junwhan Ahn, Rajkumar Samuel, Steven Hand, Zhifeng Chen, Kiran Vodrahalli, Aakanksha Chowdhery, Ethan Dyer, Emanuel Taropa, Vlad Feinberg, James Bradbury, Reiner Pope, Wei Li, YaGuang Li, Eric Chu, Jeffrey Hui, Joshua Howland, Vlad Fienber, Aroma Mahendru, Michele Catasta, Vedant Misra, Kevin Robinson, Maysam Moussalem, Sebastian Ruder, Erica Moreira, Eric Ni, Paige Bailey, Lucas Gonzalez, Alexandre Passos, Slav Petrov, Gaurav Mishra, Mark Omernick, Ambrose Slone, Andrea Hu, Colin Cherry, Denny Zhou, Jan Botha, John Wieting, Joshua Maynez, Kathleen Kenealy, Kevin Brooks, Linting Xue, Markus Freitag, Martin Polacek, Pengcheng Yin, Sebastian Gehrmann, Xuezhi Wang, Kathy Meier-Hellstern, Christopher A. Choquette-Choo, Daniel Smilkov, Emily Reif, Alicia Parrish, Alex Castro Ros, ClÃ©ment Crepy, Dasha Valter, Jeremy Hurwitz, Katherine Lee, Mark DÃ­az, Marie Pellat, Matthew Jagielski, Renee Shelby, Shachi Dave",API access,https://arxiv.org/abs/2305.10403,1734.0,PaLM 2 Technical Report,2023-05-10,Google,340000000000.0,"Model Architecture: ""PaLM-2 is a new state-of-the-art language model. We have small, medium, and large variants that use stacked layers based on the Transformer architecture, with varying parameters depending on model size. Further details of model size and architecture are withheld from external publication.""
However, the parameter count was leaked to CNBC: https://www.cnbc.com/2023/05/16/googles-palm-2-uses-nearly-five-times-more-text-data-than-predecessor.html",7.34e+24,"Compute Requirements ""Not reported.""
Paper suggests heuristic of  C=6ND. Based on 340B parameters and 3.6T tokens, training compute would be around 7.3*10^24 FLOP.",,"""The PaLM 2 pre-training corpus is composed of a diverse set of sources: web documents, books, code, mathematics, and conversational data. The pre-training corpus is significantly larger than the corpus used to train PaLM (Chowdhery et al., 2022). PaLM 2 is trained on a dataset that includes a higher percentage of non-English data than previous large language models, which is beneficial for multilingual tasks"" (page 9)

â€œPaLM 2's knowledge cutoff time is mid-2021. Knowledge about events after that time is limited,â€ according to https://ai.google.dev/palm_docs/palm. September 2021 according to https://computercity.com/artificial-intelligence/knowledge-cutoff-dates-llms",2700000000000.0,"""The pre-training corpus is significantly larger than the corpus used to train PaLM"" so greater than 6e+11. According to the leaked documents viewed by CNBC, the corpus was 3.6 trillion tokens or around 2.7*10^12 words.

https://www.cnbc.com/2023/05/16/googles-palm-2-uses-nearly-five-times-more-text-data-than-predecessor.html",,,Google TPU v4,Likely,"We introduce PaLM 2, a new state-of-the-art language model that has better multilingual and reasoning capabilities and is more compute-efficient than its predecessor PaLM (Chowdhery et al., 2022). PaLM 2 is a Transformer-based model trained using a mixture of objectives similar to UL2 (Tay et al., 2023). Through extensive evaluations on English and multilingual language, and reasoning tasks, we demonstrate that PaLM 2 has significantly improved quality on downstream tasks across different model sizes, while simultaneously exhibiting faster and more efficient inference compared to PaLM. This improved efficiency enables broader deployment while also allowing the model to respond faster, for a more natural pace of interaction. PaLM 2 demonstrates robust reasoning capabilities exemplified by large improvements over PaLM on BIG-Bench and other reasoning tasks. PaLM 2 exhibits stable performance on a suite of responsible AI evaluations, and enables inference-time control over toxicity without additional overhead or impact on other capabilities. Overall, PaLM 2 achieves state-of-the-art performance across a diverse set of tasks and capabilities.",United States of America,,,,,,Unreleased,,,,
CodeGen2,Language,Code generation,"Erik Nijkamp, Hiroaki Hayashi, Caiming Xiong, Silvio Savarese, Yingbo Zhou",Open weights (unrestricted),https://arxiv.org/abs/2305.02309,218.0,CodeGen2: Lessons for Training LLMs on Programming and Natural Languages,2023-05-03,Salesforce,16000000000.0,16B for largest CodeGen2 model,,,Stack v1.1,"""We examine our recipe on four model sizes: 1B, 3.7B, 7B, and 16B, and
refer to them as CodeGen2.
1 For training a subset of the Stack v1.1 (Kocetkov et al., 2022), filtered
with a stronger permissive license guideline, is used""",,,,,,Confident,"Large language models (LLMs) have demonstrated remarkable abilities in representation learning for program synthesis and understanding tasks. The quality of the learned representations appears to be dictated by the neural scaling laws as a function of the number of model parameters and observations, while imposing upper bounds on the model performance by the amount of available data and compute, which is costly.
In this study, we attempt to render the training of LLMs for program synthesis more efficient by unifying four key components: (1) model architectures, (2) learning methods, (3) infill sampling, and, (4) data distributions. Specifically, for the model architecture, we attempt to unify encoder and decoder-based models into a single prefix-LM. For learning methods, (i) causal language modeling, (ii) span corruption, (iii) infilling are unified into a simple learning algorithm. For infill sampling, we explore the claim of a ""free lunch"" hypothesis. For data distributions, the effect of a mixture distribution and multi-epoch training of programming and natural languages on model performance is explored.
We conduct a comprehensive series of empirical experiments on 1B LLMs, for which failures and successes of this exploration are distilled into five lessons. We will provide a final recipe for training and release CodeGen2 models in size 1B, 3.7B, 7B, and, 16B parameters, along with the training framework as open-source: this https URL.",United States of America,,,,,,,apache 2.0,,,
Eleven Multilingual v1,Speech,"Text-to-speech (TTS),Speech synthesis",,API access,https://elevenlabs.io/blog/eleven-multilingual-v1,,Introducing Eleven Multilingual v1: Our New Speech Synthesis Model,2023-04-27,ElevenLabs,,,,,Unspecified unreleased,,,,,,,Unknown,"Today, weâ€™re thrilled to launch Eleven Multilingual v1 - our advanced speech synthesis model supporting seven new languages: French, German, Hindi, Italian, Polish, Portuguese, and Spanish. Building on top of the research that powered Eleven Monolingual v1, our current deep learning approach leverages more data, more computational power, and novel techniques inside an increasingly sophisticated model, capable of understanding textual nuances and delivering an emotionally rich performance. This advancement expands the creative horizons for creators, game developers, and publishers, and it paves the way for using generative media to create more localized, accessible, and imaginative content.",United States of America,,,,,,Unreleased,,,,
ruGPT-3.5 13B,Language,"Chat,Language modeling/generation",,Open weights (unrestricted),https://huggingface.co/ai-forever/ruGPT-3.5-13B,,ruGPT-3.5 13B,2023-04-24,Sber,13000000000.0,13B,1.0699776e+23,"""Model was trained using Deepspeed and Megatron libraries, on 300B tokens dataset for 3 epochs, around 45 days on 512 V100. After that model was finetuned 1 epoch with sequence length 2048 around 20 days on 200 GPU A100 on additional data""

512 GPUs * 125000000000000 FLOPs/s [peak] * 45 days * 24 hours * 3600 s * 0.3 + 200 GPUs * 312000000000000 FLOPs/s [peak for fp16] * 20 days * 24 hours * 3600 s * 0.3 = 1.0699776e+23

they probably used fp16 as in their similar project: https://habr.com/ru/companies/sberdevices/articles/780334/

6ND = 6*13B*300B*3 = 70200*10^18 = 7*10^24",,,300000000000.0,,1080.0,,"NVIDIA A100,NVIDIA Tesla V100 SXM2",Confident,,Russia,,,,512.0,,Unreleased,"MIT license
https://huggingface.co/ai-forever/ruGPT-3.5-13B/discussions",,,
Claude 1.3,Language,"Language modeling,Chat",,API access,https://twitter.com/AnthropicAI/status/1648353600350060545?lang=en,,,2023-04-18,Anthropic,,,,,Unspecified unreleased,,,,,,,Unknown,"We are offering a new version of our model, Claude-v1.3, that is safer and less susceptible to adversarial attacks.
For businesses using Claude, capabilities in all domains should stay the same or improve as you upgrade from previous versions. We always work to improve safety and performance in tandem.
This new model is already powering the Claude App for Slack and Claude+ in the Poe app, if youâ€™d like to try it out today!",United States of America,,,,,,Unreleased,,,,
AiLMe-100B v3,"Multimodal,Language,Video,Image generation,Audio","Image generation,Audio generation,Video generation,Language generation,Language modeling/generation,Question answering",,Unreleased,"https://cevalbenchmark.com/static/model.html?method=AiLMe-100B%20v3
https://tech.chinadaily.com.cn/a/202304/20/WS6440a111a310537989370a76.html",,,2023-04-18,"Qilin Hesheng Network Technology Co., Ltd. (APUS)",100000000000.0,,,,Unspecified unreleased,,,,,,,Confident,,China,,,,,,Unreleased,,,,
Anthropic LM 52B,Language,"Language modeling/generation,Question answering","Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, Jared Kaplan",Unreleased,https://arxiv.org/abs/2204.05862,3431.0,Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback,2023-04-12,Anthropic,52000000000.0,,,,,"The 52B preference model was trained on a mixture of helpfulness and harmlessness (red teaming) datasets collected by Anthropic using crowdworkers conversing with language models in a feedback interface.
The helpfulness dataset contains around 44k comparisons and the harmlessness dataset contains around 42k comparisons. The data consists of multi-turn dialogues where crowdworkers choose the more helpful or more harmful response at each turn.
The model was trained using a technique called preference model pretraining on additional datasets before finetuning on Anthropic's human feedback data.",,,,,,Confident,"We apply preference modeling and reinforcement learning from human feedback (RLHF) to finetune language models to act as helpful and harmless assistants. We find this alignment training improves performance on almost all NLP evaluations, and is fully compatible with training for specialized skills such as python coding and summarization. We explore an iterated online mode of training, where preference models and RL policies are updated on a weekly cadence with fresh human feedback data, efficiently improving our datasets and models. Finally, we investigate the robustness of RLHF training, and identify a roughly linear relation between the RL reward and the square root of the KL divergence between the policy and its initialization. Alongside our main results, we perform peripheral analyses on calibration, competing objectives, and the use of OOD detection, compare our models with human writers, and provide samples from our models using prompts appearing in recent related work.",United States of America,,,,,,Unreleased,,,,
SenseChat,Language,"Chat,Language modeling/generation",,API access,https://www.sensetime.com/en/news-detail/51166397?categoryId=1072,,"SenseTime Launches â€œSenseNovaâ€ Foundation Model Sets and AI Computing Systems, Advancing AGI Development",2023-04-10,SenseTime,180000000000.0,"https://www.thepaper.cn/newsDetail_forward_22639611
Translation:
""SenseTime launched the ""SenseNova"" large model system, which includes natural language generation, image generation services, pre-labeling for perception models, and model development. The ""SenseChat"" application platform, powered by a 180-billion parameter Chinese language model, supports ultra-long text comprehension and offers capabilities such as question answering, understanding, and generation in Chinese.""

Link says ""hundreds of billions"" but the more precise number above seems more credible.",3.89e+24,"â€œOver the course of five years, SenseTime has built SenseCore, a leading AI infrastructure with 27,000 GPUs, capable of delivering a total computational power of 5,000 petaflopsâ€

Assuming they used this entire cluster with 30 days of training (rough average of frontier model training times since 2016), 30% utilization rate: 5000e15 * 0.3 * 30 * 24 * 60 * 60 = 3.89e24 FLOP.

Assuming the model is dense and trained Chinchilla-optimal: 20 tokens/parameter * (180e9 parameters)**2 * 6 = 3.89e24 FLOP. (The two estimates match by coincidence.)

The model seems more likely than not to be dense, given that news of SenseChat 5.0 makes a point of stating its MoE architecture, whereas SenseChat 1.0 does not mention architecture.

Given uncertainties (e.g. the model is possibly MoE, could have been overtrained or undertrained, could have trained longer or shorter), likely between 1e23 and 3e25 FLOP.",,,,,,,,Speculative,"SenseTime hosted a Tech Day event, sharing their strategic plan for advancing AGI (Artificial General Intelligence) development through the combination of â€œfoundation models + large-scale computingâ€ systems. Under this strategy, SenseTime unveiled the â€œSenseNovaâ€ foundation model set, introducing a variety of foundation models and capabilities in natural language processing, content generation, automated data annotation, and custom model training. At the event, SenseTime not only showcased their large language modelâ€™s capabilities, but also demonstrated a series of generative AI models and applications, such as text-to-image creation, 2D/3D digital human generation, and complex scenario/detailed object generation. Additionally, they introduced their AGI research and development platform facilitated by the integration of â€œfoundation models + large-scale computingâ€ systems.",Hong Kong,,,,,,Unreleased,,,,
BloombergGPT,Language,"Language modeling,Language modeling/generation,Question answering,Financial management,Text classification","Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Sebastian Gehrmann, Prabhanjan Kambadur, David Rosenberg, Gideon Mann",Unreleased,https://arxiv.org/abs/2303.17564,1112.0,BloombergGPT: A Large Language Model for Finance,2023-03-30,"Bloomberg,Johns Hopkins University",50558868480.0,,2.36e+23,"2.36e23 per Table 4

(using our usual hardware method, 512 A100s over 53 days would be 512 * 312 teraFLOP/s * 53 * 24 * 3600 * 0.3 = 2.19e23)",,"""To train BloombergGPT, we construct â€œFinPileâ€, a comprehensive dataset consisting of a range of English financial documents including news, filings, press releases, web-scraped financial documents, and social media drawn from the Bloomberg archives. These documents have been acquired through our business process over the past two decades. We augment FinPile with public data widely used to train LLMs. The result is a training corpus that is roughly half domain-specific text and half general-purpose text.""",532000000000.0,"708.9 billion tokens. At 0.75 English words per token, that's 532B words",1270.0,"""~53 days""",NVIDIA A100,Confident,"The use of NLP in the realm of financial technology is broad and complex, with applications ranging from sentiment analysis and named entity recognition to question answering. Large Language Models (LLMs) have been shown to be effective on a variety of tasks; however, no LLM specialized for the financial domain has been reported in literature. In this work, we present BloombergGPT, a 50 billion parameter language model that is trained on a wide range of financial data. We construct a 363 billion token dataset based on Bloomberg's extensive data sources, perhaps the largest domain-specific dataset yet, augmented with 345 billion tokens from general purpose datasets. We validate BloombergGPT on standard LLM benchmarks, open financial benchmarks, and a suite of internal benchmarks that most accurately reflect our intended usage. Our mixed dataset training leads to a model that outperforms existing models on financial tasks by significant margins without sacrificing performance on general LLM benchmarks. Additionally, we explain our modeling choices, training process, and evaluation methodology. We release Training Chronicles (Appendix C) detailing our experience in training BloombergGPT.","United States of America,United States of America",,,,512.0,0.246,Unreleased,,0.327,,
LightOn Mini,Language,"Language modeling/generation,Chat",,Hosted access (no API),https://www.lighton.ai/blog/lighton-s-blog-4/lighton-s-large-language-model-of-40-billion-parameters-mini-19,,LightOn's Large Language Model of 40 billion parameters: MINI,2023-03-21,LightOn,40000000000.0,"""Boasting an impressive 40 billion parameters, Mini is a formidable addition to the growing array of language models available in the market today.""",2.4e+23,6ND aproximation: 6*40B*1T = 2.4e23,,"""The amount of data in Mini corpus is 1 trillion tokens. We mainly used data from the public web to pre-train our model, with strong filtering, toxicity reduction, and deduplication to ensure that only high-quality data is retained.""",1000000000000.0,"""The amount of data in Mini corpus is 1 trillion tokens. We mainly used data from the public web to pre-train our model, with strong filtering, toxicity reduction, and deduplication to ensure that only high-quality data is retained.""  assuming 0.75 words per token - 750000000000.0 words",,,,Confident,,France,,,,,,Unreleased,,,,
Firefly,Image generation,"Image generation,Text-to-image",,Hosted access (no API),https://news.adobe.com/news/news-details/2023/Adobe-Unveils-Firefly-a-Family-of-new-Creative-Generative-AI/default.aspx,,"Adobe Unveils Firefly, a Family of new Creative Generative AI",2023-03-21,Adobe,,,,,Adobe Stock,"""The current Firefly generative AI model is trained on a dataset of licensed content, such as Adobe Stock, and public domain content where copyright has expired.""

https://www.adobe.com/products/firefly.html",,,,,,Unknown,"Today, Adobe (Nasdaq:ADBE) introduced Adobe Firefly, a new family of creative generative AI models, first focused on the generation of images and text effects. Adobe Firefly will bring even more precision, power, speed and ease directly into Creative Cloud, Document Cloud, Experience Cloud and Adobe Express workflows where content is created and modified. Adobe Firefly will be part of a series of new Adobe Sensei generative AI services across Adobeâ€™s clouds.",United States of America,,,,,,Unreleased,,,,
Gen-2,Video,"Video generation,Text-to-video,Image-to-video,Video-to-video",Gen-2 authors,API access,https://research.runwayml.com/gen2,,"Gen-2: Generate novel videos with text, images or video clips",2023-03-20,Runway,,,,,,,,,,,,Unknown,,United States of America,,,,,,Unreleased,,,,
PanGu-Î£,Language,"Code generation,Language modeling,Translation,Question answering","Xiaozhe Ren, Pingyi Zhou, Xinfan Meng, Xinjing Huang, Yadao Wang, Weichao Wang, Pengfei Li, Xiaoda Zhang, Alexander Podolskiy, Grigory Arshinov, Andrey Bout, Irina Piontkovskaya, Jiansheng Wei, Xin Jiang, Teng Su, Qun Liu, Jun Yao",Unreleased,https://arxiv.org/abs/2303.10845,78.0,PanGu-Î£: Towards Trillion Parameter Language Model with Sparse Heterogeneous Computing,2023-03-20,Huawei Noah's Ark Lab,1085000000000.0,"""In this work, we present PanGu-Î£ , a large language model with sparse architecture containing 1.085 trillion parameters.""",4.67e+23,"It has sparse architecture, so we can't use C=6ND.
""We develop PanGu-Î£ model under the framework of MindSpore and train it on a cluster with only 512 Ascend 910 AI Accelerators with 329 billion tokens over 100 days.""
100 days * 512 processors * 320 teraFLOPS/processor * 33% utilization = 4.67e+23 FLOP
https://www.wolframalpha.com/input?i=100+days+*+512+*+320+terahertz+*+0.33",Unspecified unreleased,"""329B tokens in more than 40 natural and programming languages""",246750000000.0,329B tokens ~= 247B words,2400.0,"We develop PanGu-Î£ model under the framework of MindSpore 5
and train it on a cluster with only 512 Ascend 910 AI Accelerators [28] with 329 billion tokens over 100 days.",Huawei Ascend 910,Confident,"The scaling of large language models has greatly improved natural language understanding, generation, and reasoning. In this work, we develop a system that trained a trillion-parameter language model on a cluster of Ascend 910 AI processors and MindSpore framework, and present the language model with 1.085T parameters named PanGu-{\Sigma}. With parameter inherent from PanGu-{\alpha}, we extend the dense Transformer model to sparse one with Random Routed Experts (RRE), and efficiently train the model over 329B tokens by using Expert Computation and Storage Separation(ECSS). This resulted in a 6.3x increase in training throughput through heterogeneous computing. Our experimental findings show that PanGu-{\Sigma} provides state-of-the-art performance in zero-shot learning of various Chinese NLP downstream tasks. Moreover, it demonstrates strong abilities when fine-tuned in application data of open-domain dialogue, question answering, machine translation and code generation.",China,,,,512.0,,Unreleased,,,,
GPT-4 (Mar 2023),"Multimodal,Language,Vision","Language modeling,Language modeling/generation,Question answering,Visual question answering","OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, SimÃ³n Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain et al. (181 additional authors not shown)",API access,https://arxiv.org/abs/2303.08774,20421.0,GPT-4 Technical Report,2023-03-15,OpenAI,1800000000000.0,"Rumored to be 1.8T parameter MoE with 280B activated on the forward pass, per https://www.semianalysis.com/p/gpt-4-architecture-infrastructure. Other sources estimate 1.76T with 220B per forward pass https://web.archive.org/web/20230712123915/https://the-decoder.com/gpt-4-architecture-datasets-costs-and-more-leaked/",2.1e+25,"90% CI: 8.2E+24 to 4.4E+25

NOTE: this is a rough estimate based on public information, much less information than most other systems in the database.

Calculation and confidence intervals here: https://colab.research.google.com/drive/1O99z9b1I5O66bT78r9ScslE_nOj5irN9?usp=sharing",Unspecified unreleased,"Assuming this is the earliest model in the family, the knowledge cutoff date was originally September 2021 - which I assume means September 1, 2021 - according to https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models?tabs=global-standard%2Cstandard-chat-completions#gpt-35.",4900000000000.0,"Speculative. Reported secondhand by online sources such as Semianalysis, but not verified by OpenAI. If total number of tokens seen was 13T, text was repeated for 2 epochs, and text was the majority of tokens, then dataset size roughly is 13T*0.75/2 = 4.9T words.

Note this examines only the text dataset, since GPT-4 was first and foremost a language model. However, the vision component had its own vision dataset, which we believe accounted for a much smaller part of the compute budget.",2280.0,(Speculative) SemiAnalysis conjectures that GPT-4 training took 90-100 days with utilization of 32-36%.,NVIDIA A100 SXM4 40 GB,Likely,"We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.",United States of America,,,,25000.0,0.34,Unreleased,,,81392748.68255694,806190976.8020828
Falcon-40B,Language,"Language modeling,Language modeling/generation,Question answering",,Open weights (unrestricted),https://arxiv.org/abs/2311.16867; https://www.tii.ae/news/abu-dhabi-based-technology-innovation-institute-introduces-falcon-llm-foundational-large,0.0,Abu Dhabi-based Technology Innovation Institute Introduces Falcon LLM: Foundational Large Language Model (LLM) outperforms GPT-3 with 40 Billion Parameters,2023-03-15,Technology Innovation Institute,40000000000.0,Model comes in 7B and 40B variants.,2.4e+23,"C = 6ND = 6 * 40B * 1000B = 2.4e+23 FLOP (assuming one epoch)

Table 1 from https://arxiv.org/pdf/2311.16867 Falcon paper

2,800 petaflop-days * 1e15 * 24 * 3600 = 2.4192e+23 FLOPs",RefinedWeb,"Falcon-40B was trained on 1,000B tokens of RefinedWeb, a high-quality filtered and deduplicated web dataset which we enhanced with curated corpora. Significant components from our curated copora were inspired by The Pile (Gao et al., 2020).",1000000000000.0,1000B tokens ~= 750B words,1440.0,"""Falcon-40B was trained on AWS SageMaker, on 384 A100 40GB GPUs in P4d instances.""
""Training started in December 2022 and took two months.""",NVIDIA A100,Confident,,United Arab Emirates,,,,384.0,0.3864,Unreleased,apache 2.0,,,
Claude,Language,"Language modeling,Chat,Language modeling/generation,Question answering",,API access,https://www.anthropic.com/index/introducing-claude,,Introducing Claude,2023-03-14,Anthropic,,,,,Unspecified unreleased,"Assuming that â€œClaudeâ€ refers to â€œClaude 1â€, this has a knowledge cutoff date of March 2023 - which I assume means March 1, 2023 - https://www.youreverydayai.com/knowledge-cutoff-what-it-is-and-why-it-matters-for-large-language-models/.",,,,,,Unknown,"Claude is a next-generation AI assistant based on Anthropicâ€™s research into training helpful, honest, and harmless AI systems. Accessible through chat interface and API in our developer console, Claude is capable of a wide variety of conversational and text processing tasks while maintaining a high degree of reliability and predictability.",United States of America,,,,,,Unreleased,,,,
Cohere Command,Language,Language modeling/generation,,API access,https://cohere.com/models/command,,"World-class AI, at your command",2023-03-01,Cohere,52000000000.0,"52B for larger version

https://aws.amazon.com/bedrock/cohere-command-embed/

Cohere Command has had a few different sizes over time and is continuously updated, but there's been a 52B version since at least March 2023: https://twitter.com/percyliang/status/1638236921754443776",,,,,,,,"https://docs.cohere.com/docs/environmental-impact

2696.16 kg carbon for base-light and 6689.76 kg carbon for embed-english. Nothing listed for the large model. 

It's possible to back out GPU-hours using this calculator, though it varies by region and Cohere doesn't specify the region.

https://mlco2.github.io/impact/",Google TPU v4,Speculative,,Canada,,,,,,Unreleased,,,,
Jurassic-2 Jumbo,Language,"Language modeling,Translation,Language modeling/generation,Question answering",,API access,https://www.ai21.com/blog/introducing-j2,,Announcing Jurassic-2 and Task-Specific APIs,2023-03-01,AI21 Labs,178000000000.0,Source is https://crfm.stanford.edu/helm/latest/#/leaderboard as viewed on 2023-12-06,,,,,,,,,,,"Announcing the launch of Jurassic-2, the latest generation of AI21 Studioâ€™s foundation models, a game-changer in the field of AI, with top-tier quality and new capabilities. And that's not all - we're also releasing our task-specific APIs, with plug-and-play reading and writing capabilities that outperform competitors.",Israel,,,,,,Unreleased,,,,
CodeGen-Mono 16.1B,Language,"Code generation,Code autocompletion","Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, Caiming Xiong",Open weights (unrestricted),https://arxiv.org/abs/2203.13474,1336.0,CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis,2023-02-27,Salesforce,16100000000.0,16.1B parameters,,,"The Pile,Big Query,BigPython","""The family of CODEGEN models is trained sequentially on three datasets: The Pile, BigQuery, and BigPython.""",568200000000.0,Table 5,,,Google TPU v4,Likely,"""Program synthesis strives to generate a computer program as a solution to a given problem specification, expressed with input-output examples or natural language descriptions. The prevalence of large language models advances the state-of-the-art for program synthesis, though limited training resources and data impede open access to such models. To democratize this, we train and release a family of large language models up to 16.1B parameters, called CODEGEN, on natural language and programming language data, and open source the training library JAXFORMER. We show the utility of the trained model by demonstrating that it is competitive with the previous state-of-the-art on zero-shot Python code generation on HumanEval. We further investigate the multi-step paradigm for program synthesis, where a single program is factorized into multiple prompts specifying subproblems. To this end, we construct an open benchmark, Multi-Turn Programming Benchmark (MTPB), consisting of 115 diverse problem sets that are factorized into multi-turn prompts. Our analysis on MTPB shows that the same intent provided to CODEGEN in multi-turn fashion significantly improves program synthesis over that provided as a single turn. We make the training library JAXFORMER and model checkpoints available as open source contribution: this https URL.""",United States of America,,,,,,,apache 2.0,,,
LLaMA-33B,Language,"Language modeling,Code generation,Language modeling/generation,Question answering","Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, TimothÃ©e Lacroix, Baptiste RoziÃ¨re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample",Open weights (non-commercial),https://arxiv.org/abs/2302.13971,17466.0,LLaMA: Open and Efficient Foundation Language Models,2023-02-27,Meta AI,32500000000.0,Table 2 in the paper,2.7300000000001e+23,1.4T tokens * 32.5B params * 6 FLOP/token/param = 2.73e+23 FLOP,"CCNet,GitHub,Wikipedia,books,arXiv,Stack Exchange",See Table 1,1340000000000.0,"Table 1 indicates that 1.4T tokens involved sampling sub-datasets at more or less than one epoch. Correcting for this:

(1.1 epoch * 3.3TB) + (1.06 epoch * 0.783TB) + ... = 1.4T tokens
5.24 epoch-TBs = 1.4T tokens
5.24 epoch-TB * 1000 GB/TB * 200M token/GB = 1.4T tokens
1.05T epoch*token = 1.4T tokens
1 epoch = 1.34T tokens",,,,Confident,"We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.",United States of America,,,,,,Unreleased,"""we are releasing our model under a noncommercial license focused on research use cases"" https://ai.meta.com/blog/large-language-model-llama-meta-ai/",,,
LLaMA-65B,Language,"Language modeling,Code generation","Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, TimothÃ©e Lacroix, Baptiste RoziÃ¨re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample",Open weights (non-commercial),https://arxiv.org/abs/2302.13971,17466.0,LLaMA: Open and Efficient Foundation Language Models,2023-02-24,Meta AI,65200000000.0,"Model card, table 1: https://github.com/facebookresearch/llama/blob/53011c3d7946dadb8274a4c5c7586ab54edf792d/MODEL_CARD.md",5.5e+23,1.4e12 tokens * 6.52e10 parameters * 6 FLOP/token/parameter = 5.5e23 FLOP,"CCNet,GitHub,Wikipedia,books,arXiv,Stack Exchange","The model was trained using the following source of data: CCNet [67%], C4 [15%], GitHub [4.5%], Wikipedia [4.5%], Books [4.5%], ArXiv [2.5%], Stack Exchange[2%]. The Wikipedia and Books domains include data in the following languages: bg, ca, cs, da, de, en, es, fr, hr, hu, it, nl, pl, pt, ro, ru, sl, sr, sv, uk. See the paper for more details about the training set and corresponding preprocessing.",1340000000000.0,"Table 1 indicates that 1.4T tokens involved sampling sub-datasets at more or less than one epoch. Correcting for this:

(1.1 epoch * 3.3TB) + (1.06 epoch * 0.783TB) + ... = 1.4T tokens
5.24 epoch-TBs = 1.4T tokens
5.24 epoch-TB * 1000 GB/TB * 200M token/GB = 1.4T tokens
1.05T epoch*token = 1.4T tokens
1 epoch = 1.34T tokens
",500.0,"""When training a 65B-parameter model, our code processes around 380 tokens/sec/GPU on 2048 A100 GPU with 80GB of RAM. This means that training over our dataset containing 1.4T tokens takes approximately 21 days.""",NVIDIA A100,Confident,"We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.",United States of America,,,,2048.0,0.4746,Unreleased,"""we are releasing our model under a noncommercial license focused on research use cases"" https://ai.meta.com/blog/large-language-model-llama-meta-ai/",,,
Anthropic LM 175B,Language,"Language modeling/generation,Question answering","Deep Ganguli, Amanda Askell, Nicholas Schiefer, Thomas I. Liao, KamilÄ— LukoÅ¡iÅ«tÄ—, Anna Chen, Anna Goldie, Azalia Mirhoseini, Catherine Olsson, Danny Hernandez, Dawn Drain, Dustin Li, Eli Tran-Johnson, Ethan Perez, Jackson Kernion, Jamie Kerr, Jared Mueller, Joshua Landau, Kamal Ndousse, Karina Nguyen, Liane Lovitt, Michael Sellitto, Nelson Elhage, Noemi Mercado, Nova DasSarma, Oliver Rausch, Robert Lasenby, Robin Larson, Sam Ringer, Sandipan Kundu, Saurav Kadavath, Scott Johnston, Shauna Kravec, Sheer El Showk, Tamera Lanham, Timothy Telleen-Lawton, Tom Henighan, Tristan Hume, Yuntao Bai, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam McCandlish, Tom Brown, Christopher Olah, Jack Clark, Samuel R. Bowman, Jared Kaplan",Unreleased,https://arxiv.org/abs/2302.07459,188.0,The Capacity for Moral Self-Correction in Large Language Models,2023-02-15,Anthropic,175000000000.0,175B,,,,,,,,,,Confident,"We test the hypothesis that language models trained with reinforcement learning from human feedback (RLHF) have the capability to ""morally self-correct"" -- to avoid producing harmful outputs -- if instructed to do so. We find strong evidence in support of this hypothesis across three different experiments, each of which reveal different facets of moral self-correction. We find that the capability for moral self-correction emerges at 22B model parameters, and typically improves with increasing model size and RLHF training. We believe that at this level of scale, language models obtain two capabilities that they can use for moral self-correction: (1) they can follow instructions and (2) they can learn complex normative concepts of harm like stereotyping, bias, and discrimination. As such, they can follow instructions to avoid certain kinds of morally harmful outputs. We believe our results are cause for cautious optimism regarding the ability to train language models to abide by ethical principles.",United States of America,,,,,,Unreleased,,,,
ViT-22B,Vision,"Object detection,Image classification","Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, Rodolphe Jenatton, Lucas Beyer, Michael Tschannen, Anurag Arnab, Xiao Wang, Carlos Riquelme, Matthias Minderer, Joan Puigcerver, Utku Evci, Manoj Kumar, Sjoerd van Steenkiste, Gamaleldin F. Elsayed, Aravindh Mahendran, Fisher Yu, Avital Oliver, Fantine Huot, Jasmijn Bastings, Mark Patrick Collier, Alexey Gritsenko, Vighnesh Birodkar, Cristina Vasconcelos, Yi Tay, Thomas Mensink, Alexander Kolesnikov, Filip PavetiÄ‡, Dustin Tran, Thomas Kipf, Mario LuÄiÄ‡, Xiaohua Zhai, Daniel Keysers, Jeremiah Harmsen, Neil Houlsby",Unreleased,https://arxiv.org/abs/2302.05442v1,757.0,Scaling Vision Transformers to 22 Billion Parameters,2023-02-10,Google,21743000000.0,"21.743B, Table 1",1.93248e+23,"""ViT-22B was trained using 256 visual tokens per image, where each token represents a 14 Ã— 14 patch extracted from 224 Ã— 224 sized images. ViT-22B is trained for 177k steps with batch size of 65k: approximately 3 epochs""

""ViT-22B was trained on 1024 TPU V4 chips for 177K steps""

""Using these techniques, ViT-22B processes 1.15k tokens per second per core during training (forward and backward pass) on TPUv4 [...] ViT-22Bâ€™s model flops utilization (MFU) is 54.9%""

256 * 177k * 65k = 2.945T tokens

So training time is 2.945T tokens / (1.15k * 2 * 1024) tokens/s = 1.25M seconds = 347.4 hours

So 1024 TPUv4 chips for 1.25M seconds at 54.9% MFU:
1024 * 2.75e14 * 1.25M * 0.549 = 1.93248e23",JFT-4B,"""Dataset. ViT-22B is trained on a version of JFT (Sun et al., 2017), extended to around 4B images (Zhai et al.,
2022a). These images have been semi-automatically annotated with a class-hierarchy of 30k labels""",4000000000.0,"""Dataset. ViT-22B is trained on a version of JFT (Sun et al., 2017), extended to around 4B images (Zhai et al.,
2022a). These images have been semi-automatically annotated with a class-hierarchy of 30k labels""",347.4,"""Using these techniques, ViT-22B processes 1.15k tokens per second per core during training (forward and backward pass)""
From model card we know they trained with 1024 TPUv4 chips, and there are 2 cores per chip. Total number of tokens was 177K steps * 65k images/step * 256 tokens/image = 2.945T tokens

So training time is 2.945T tokens / (1.15k * 2 * 1024) tokens/s = 1.25M seconds = 347.4 hours",Google TPU v4,Confident,"The scaling of Transformers has driven breakthrough capabilities for language models. At present, the largest large language models (LLMs) contain upwards of 100B parameters. Vision Transformers (ViT) have introduced the same architecture to image and video modelling, but these have not yet been successfully scaled to nearly the same degree; the largest dense ViT contains 4B parameters (Chen et al., 2022). We present a recipe for highly efficient and stable training of a 22B-parameter ViT (ViT-22B) and perform a wide variety of experiments on the resulting model. When evaluated on downstream tasks (often with a lightweight linear model on frozen features), ViT-22B demonstrates increasing performance with scale. We further observe other interesting benefits of scale, including an improved tradeoff between fairness and performance, state-of-the-art alignment to human visual perception in terms of shape/texture bias, and improved robustness. ViT-22B demonstrates the potential for ""LLM-like"" scaling in vision, and provides key steps towards getting there.",United States of America,,,,1024.0,0.549,Unreleased,don't see it here: https://github.com/google-research/vision_transformer?tab=readme-ov-file#available-vit-models ,,,
Gen-1,Video,"Video generation,Text-to-video","Patrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Granskog, Anastasis Germanidis",API access,https://arxiv.org/abs/2302.03011,656.0,Structure and Content-Guided Video Synthesis with Diffusion Models,2023-02-06,Runway,,,,,,,,,,,,Unknown,"Text-guided generative diffusion models unlock powerful image creation and editing tools. While these have been extended to video generation, current approaches that edit the content of existing footage while retaining structure require expensive re-training for every input or rely on error-prone propagation of image edits across frames. In this work, we present a structure and content-guided video diffusion model that edits videos based on visual or textual descriptions of the desired output. Conflicts between user-provided content edits and structure representations occur due to insufficient disentanglement between the two aspects. As a solution, we show that training on monocular depth estimates with varying levels of detail provides control over structure and content fidelity. Our model is trained jointly on images and videos which also exposes explicit control of temporal consistency through a novel guidance method. Our experiments demonstrate a wide variety of successes; fine-grained control over output characteristics, customization based on a few reference images, and a strong user preference towards results by our model.",United States of America,,,,,,Unreleased,,,,
Eleven Monolingual v1,Speech,"Text-to-speech (TTS),Speech synthesis",,API access,https://elevenlabs.io/docs/models#older-models,,First generation TTS model (outclassed by v2 models),2023-01-16,ElevenLabs,,,,,Unspecified unreleased,,,,,,,Unknown,,United States of America,,,,,,Unreleased,,,,
Whisper v2,Speech,Speech recognition (ASR),"Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, Ilya Sutskever",Open weights (unrestricted),"https://huggingface.co/openai/whisper-large-v2

https://arxiv.org/abs/2212.04356",5597.0,Robust Speech Recognition via Large-Scale Weak Supervision,2022-12-05,OpenAI,1550000000.0,1550M,1.1e+23,"""Compared to the Whisper large model, the large-v2 model is trained for 2.5x more epochs with added regularization for improved performance.""

We (roughly) estimated Whisper v1 as 4.65e22. 2.5x that is 1.16e23 or ~1.1e23",Unspecified unreleased,"""The models are trained on 680,000 hours of audio and the corresponding transcripts collected from the internet. 65% of this data (or 438,000 hours) represents English-language audio and matched English transcripts, roughly 18% (or 126,000 hours) represents non-English audio and English transcripts, while the final 17% (or 117,000 hours) represents non-English audio and the corresponding transcript. This non-English data represents 98 different languages.""",9302400000.0,"""When scaled to 680,000 hours of multilingual and multitask
supervision, the resulting models generalize well
to standard benchmarks and are often competitive
with prior fully supervised results but in a zeroshot transfer setting without the need for any finetuning.""


13,680 words/h (estimate) * 680,000h = 9,302,400,000 words",,,,Likely,"Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours of labelled data, Whisper models demonstrate a strong ability to generalise to many datasets and domains without the need for fine-tuning.

Whisper was proposed in the paper Robust Speech Recognition via Large-Scale Weak Supervision by Alec Radford et al. from OpenAI. The original code repository can be found here.

Compared to the Whisper large model, the large-v2 model is trained for 2.5x more epochs with added regularization for improved performance.",United States of America,,,,,,Unreleased,"Apache 2.0 for weights

code for v1 is MIT: https://github.com/openai/whisper",,,
GPT-3.5 Turbo,Language,"Language modeling/generation,Question answering,Chat","John Schulman, Barret Zoph, Christina Kim, Jacob Hilton, Jacob Menick, Jiayi Weng, Juan Felipe Ceron Uribe, Liam Fedus, Luke Metz, Michael Pokorny, Rapha Gontijo Lopes, Shengjia Zhao, Arun Vijayvergiya, Eric Sigler, Adam Perelman, Chelsea Voss, Mike Heaton, Joel Parish, Dave Cummings, Rajeev Nayak, Valerie Balcom, David Schnurr, Tomer Kaftan, Chris Hallacy, Nicholas Turley, Noah Deutsch, Vik Goel, Jonathan Ward, Aris Konstantinidis, Wojciech Zaremba, Long Ouyang, Leonard Bogdonoff, Joshua Gross, David Medina, Sarah Yoo, Teddy Lee, Ryan Lowe, Dan Mossing, Joost Huizinga, Roger Jiang, Carroll Wainwright, Diogo Almeida, Steph Lin, Marvin Zhang, Kai Xiao, Katarina Slama, Steven Bills, Alex Gray, Jan Leike, Jakub Pachocki, Phil Tillet, Shantanu Jain, Greg Brockman, Nick Ryder, Alex Paino, Qiming Yuan, Clemens Winter, Ben Wang, Mo Bavarian, Igor Babuschkin, Szymon Sidor, Ingmar Kanitscheider, Mikhail Pavlov, Matthias Plappert, Nik Tezak, Heewoo Jun, William Zhuk, Vitchyr Pong, Lukasz Kaiser, Jerry Tworek, Andrew Carr, Lilian Weng, Sandhini Agarwal, Karl Cobbe, Vineet Kosaraju, Alethea Power, Stanislas Polu, Jesse Han, Raul Puri, Shawn Jain, Benjamin Chess, Christian Gibson, Oleg Boiko, Emy Parparita, Amin Tootoonchian, Kyle Kosic, Christopher Hesse",API access,https://platform.openai.com/docs/models/gpt-3.5-turbo,,"A fast, inexpensive model for simple tasks",2022-11-30,OpenAI,20000000000.0,20B parameters according to Table 1 in Microsoft's CODEFUSION paper: https://arxiv.org/pdf/2310.17680.pdf,,,Unspecified unreleased,"Knowledge cutoff date was originally September 2021 - which I assume means September 1, 2021 - according to https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models?tabs=global-standard%2Cstandard-chat-completions#gpt-35.",,,,,,Speculative,"GPT-3.5 Turbo models can understand and generate natural language or code and have been optimized for chat using the Chat Completions API but work well for non-chat tasks as well. As of July 2024, use gpt-4o-mini in place of GPT-3.5 Turbo, as it is cheaper, more capable, multimodal, and just as fast. GPT-3.5 Turbo is still available for use in the API.",United States of America,,,,,,Unreleased,available on API: https://platform.openai.com/docs/models/gpt-3-5-turbo ,,,
"GPT-3.5 (davinci-002)
",Language,Language modeling,,API access,https://platform.openai.com/docs/models/gpt-3-5,,,2022-11-28,OpenAI,,"Parameter count may be 175B based on OpenAI's statements that text-davinci-003 is in the GPT-3.5 series of models. It was also stated to be 175B in the Microsoft CODEFUSION paper, but the paper was reportedly retracted because the authors did not know the parameter count.",2.578e+24,https://colab.research.google.com/drive/1QSxa8YCWjEBQU7mrXLhw6TP1VX5oqgdW#scrollTo=Gt6Z6oZ26clI,,"Knowledge cutoff date was originally September 2021 - which I assume means September 1, 2021 - according to https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models?tabs=global-standard%2Cstandard-chat-completions#gpt-35.",,,,,NVIDIA A100 SXM4 40 GB,Speculative,,United States of America,,,,,,Unreleased,,,,
Galactica,"Language,Biology","Language modeling,Question answering,Mathematical reasoning,Medical diagnosis,Language modeling/generation","Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, Robert Stojnic",Open weights (non-commercial),https://arxiv.org/abs/2211.09085,917.0,Galactica: A Large Language Model for Science,2022-11-16,Meta AI,120000000000.0,"""The largest 120B model we train runs on a single NVIDIA A100 node""",3.24e+23,"Authors state the model is trained on 450b tokens. Using 6 FLOP/token/parameter, this is 6*120b*450b = 3.24e23",Galactica Corpus,"""Our corpus consists of 106 billion tokens from papers, reference material, encyclopedias and other scientific sources. We combine natural language sources, such as papers and textbooks, and natural sequences, such as protein sequences and chemical formulae. We process LATEX where we can capture it, and also include academic code to capture computational science""",106000000000.0,"""Total dataset size = 106 billion tokens""",,,NVIDIA A100 SXM4 80 GB,Likely,"Information overload is a major obstacle to scientific progress. The explosive growth in scientific literature and data has made it ever harder to discover useful insights in a large mass of information. Today scientific knowledge is accessed through search engines, but they are unable to organize scientific knowledge alone. In this paper we introduce Galactica: a large language model that can store, combine and reason about scientific knowledge. We train on a large scientific corpus of papers, reference material, knowledge bases and many other sources. We outperform existing models on a range of scientific tasks. On technical knowledge probes such as LaTeX equations, Galactica outperforms the latest GPT-3 by 68.2% versus 49.0%. Galactica also performs well on reasoning, outperforming Chinchilla on mathematical MMLU by 41.3% to 35.7%, and PaLM 540B on MATH with a score of 20.4% versus 8.8%. It also sets a new state-of-the-art on downstream tasks such as PubMedQA and MedMCQA dev of 77.6% and 52.9%. And despite not being trained on a general corpus, Galactica outperforms BLOOM and OPT-175B on BIG-bench. We believe these results demonstrate the potential for language models as a new interface for science. We open source the model for the benefit of the scientific community.",United States of America,,,,128.0,,Unreleased,"cc-by-nc (non-commercial): https://huggingface.co/facebook/galactica-120b 

repo but no training code: https://github.com/paperswithcode/galai/blob/main/README.md ",,,
Imagen Video,Video,"Video generation,Text-to-video","Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P. Kingma, Ben Poole, Mohammad Norouzi, David J. Fleet, Tim Salimans",Unreleased,https://arxiv.org/abs/2210.02303,1834.0,Imagen Video: High Definition Video Generation with Diffusion Models,2022-10-05,Google Brain,11600000000.0,"Figure 6 summarizes the entire cascading pipeline of Imagen Video. In total, we have 1 frozen text encoder, 1 base video diffusion model, 3 SSR (spatial super-resolution), and 3 TSR (temporal superresolution) models â€“ for a total of 7 video diffusion models, with a total of 11.6B diffusion model parameters",,,"LAION-400M,Unspecified unreleased",,,"We train our models on a combination of an internal dataset consisting of 14 million video-text pairs and 60 million image-text pairs, and the publicly available LAION-400M image-text dataset.",,,,Confident,"We present Imagen Video, a text-conditional video generation system based on a cascade of video diffusion models. Given a text prompt, Imagen Video generates high definition videos using a base video generation model and a sequence of interleaved spatial and temporal video super-resolution models. We describe how we scale up the system as a high definition text-to-video model including design decisions such as the choice of fully-convolutional temporal and spatial super-resolution models at certain resolutions, and the choice of the v-parameterization of diffusion models. In addition, we confirm and transfer findings from previous work on diffusion-based image generation to the video generation setting. Finally, we apply progressive distillation to our video models with classifier-free guidance for fast, high quality sampling. We find Imagen Video not only capable of generating videos of high fidelity, but also having a high degree of controllability and world knowledge, including the ability to generate diverse videos and text animations in various artistic styles and with 3D object understanding. See this https URL for samples.",United States of America,,,,,,Unreleased,,,,
Make-A-Video,Video,"Video generation,Text-to-video","Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, Devi Parikh, Sonal Gupta, Yaniv Taigman",Hosted access (no API),https://arxiv.org/abs/2209.14792,1763.0,Make-A-Video: Text-to-Video Generation without Text-Video Data,2022-09-29,Meta AI,,,,,"LAION,WebVid-10M,HD-VILA-100M",,,,,,,Unknown,"We propose Make-A-Video -- an approach for directly translating the tremendous recent progress in Text-to-Image (T2I) generation to Text-to-Video (T2V). Our intuition is simple: learn what the world looks like and how it is described from paired text-image data, and learn how the world moves from unsupervised video footage. Make-A-Video has three advantages: (1) it accelerates training of the T2V model (it does not need to learn visual and multimodal representations from scratch), (2) it does not require paired text-video data, and (3) the generated videos inherit the vastness (diversity in aesthetic, fantastical depictions, etc.) of today's image generation models. We design a simple yet effective way to build on T2I models with novel and effective spatial-temporal modules. First, we decompose the full temporal U-Net and attention tensors and approximate them in space and time. Second, we design a spatial temporal pipeline to generate high resolution and frame rate videos with a video decoder, interpolation model and two super resolution models that can enable various applications besides T2V. In all aspects, spatial and temporal resolution, faithfulness to text, and quality, Make-A-Video sets the new state-of-the-art in text-to-video generation, as determined by both qualitative and quantitative measures.",United States of America,,,,,,Unreleased,,,,
PaLI,"Language,Vision,Multimodal","Visual question answering,Language modeling/generation,Image captioning","Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, Alexander Kolesnikov, Joan Puigcerver, Nan Ding, Keran Rong, Hassan Akbari, Gaurav Mishra, Linting Xue, Ashish Thapliyal, James Bradbury, Weicheng Kuo, Mojtaba Seyedhosseini, Chao Jia, Burcu Karagol Ayan, Carlos Riquelme, Andreas Steiner, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, Radu Soricut",Unreleased,https://arxiv.org/abs/2209.06794v4,897.0,PaLI: A Jointly-Scaled Multilingual Language-Image Model,2022-09-14,Google,16900000000.0,"3.9b Image Encoder, 
14b Multimodal Encoder-Decoder",1.69e+23,"Pre-training the ViT component involved 1.1 million steps (they train over 1M steps but run the last 100k twice and then average the two resulting models). Batch size is 16384 and the inputs are 224x224. Table 8 indicates a forward pass with ViT-e/14 on a 224 image takes 1980 GFLOPs, so total training compute for the ViT-e/14 model is:
1980e9 * 16384 * 1.1 million * 3 (account for backward passes) = 1.07e23

In the ""overal model"" section, they then say: ""The largest model, PaLI-17B, is pretrained using 1,024 GCP-TPUv4 chips for 7 days"". It is then trained for another 3 days on 512 chips at higher resolution. 

I assume the stated TPUv4 training does not include the ViT pretraining, since it amounts to fewer FLOPs than we estimate above for the ViT.

275 teraFLOP/s * ((1024 * 7) + (512 * 3)) * 24 * 3600 * 0.3 (utilization assumption) = 6.2e22

Total: 1.07e23 + 6.2e22 = 1.69e23",WebLI,"""we introduce WebLI, a multilingual imagelanguage dataset built from images and texts available on the public web... Due to the abundance of multilingual content on the internet, the collection process for the WebLI dataset can be scaled to cover 10 billion images and 12 billion alt-texts. In addition to annotation with web text, we use publicly available automatic service to extract OCR annotations on all images, resulting in 29 billion image-OCR pairs. To balance quality and retain scale, we filter the dataset to the highest quality subset retaining only the top 10% scoring of the original WebLI image-text pairs (about 1B examples), which we use to train PaLI""",1600000000.0,"""During training, the model passes over 1.6B images, one epoch over the entire pretraining dataset""",240.0,10,Google TPU v4,Likely,"Effective scaling and a flexible task interface enable large language models to excel at many tasks. We present PaLI (Pathways Language and Image model), a model that extends this approach to the joint modeling of language and vision. PaLI generates text based on visual and textual inputs, and with this interface performs many vision, language, and multimodal tasks, in many languages. To train PaLI, we make use of large pre-trained encoder-decoder language models and Vision Transformers (ViTs). This allows us to capitalize on their existing capabilities and leverage the substantial cost of training them. We find that joint scaling of the vision and language components is important. Since existing Transformers for language are much larger than their vision counterparts, we train a large, 4-billion parameter ViT (ViT-e) to quantify the benefits from even larger-capacity vision models. To train PaLI, we create a large multilingual mix of pretraining tasks, based on a new image-text training set containing 10B images and texts in over 100 languages. PaLI achieves state-of-the-art in multiple vision and language tasks (such as captioning, visual question-answering, scene-text understanding), while retaining a simple, modular, and scalable design.",United States of America,,,,1024.0,,Unreleased,,,,
Luminous-supreme,Language,Language generation,,API access,https://docs.aleph-alpha.com/docs/introduction/model-card/,,Model Card Luminous,2022-08-15,Aleph Alpha,70000000000.0,"""~70B""",3.5461e+23,"""~839000h"" GPU-hours on A100s, per Environmental Impact section of model card.

312 trillion * 839000 * 3600 * 0.3 = 2.8e23

6ND = 6*70B*1069.30B = 4.49106e+23

sqrt(2.8e23*4.49106e+23) = 3.54612... Ã— 10^23

reported here:
167TFLOPS
https://docs.aleph-alpha.com/docs/Deprecated%20Luminous/Deprecated-Luminous/model-card/",,"""The Luminous family has been trained on a dataset compiled of sources in English, German, French, Spanish and Italian...""

more details in model card 
https://docs.aleph-alpha.com/docs/introduction/model-card/",1069300000000.0,"from the table

Total Size:
2.77 + 0.79 + 0.18 + 0.07 + 0.06 + 0.02 = 3.89 TB
Tokens:
761.41B + 217.15B + 49.47B + 19.29B + 16.49B + 5.49B = 1069.30B tokens",2016.0,Approximately 12 weeks = 2016 hours,"NVIDIA A100 SXM4 40 GB,NVIDIA A100 SXM4 80 GB",Confident,"The Luminous series is a family of large language models. Large language models are powerful technological tools that can process and produce text. These capabilities emerge during model â€œtrainingâ€ where the model is exposed to significant amounts of human text data. Similar to a person who deliberately absorbs information while reading a whole library and half of the internet, large language models acquire structural understanding (and not necessarily also knowledge) of language and accumulated information about the world.

The Luminous family currently consists of three vanilla models, which differ in complexity and ability. They are, from the smallest to the largest, luminous-base, luminous-extended and luminous-supreme. All Luminous models are trained in the five most commonly spoken European languages: English, German, French, Italian and Spanish.",Germany,,,,512.0,,Unreleased,,,,
Luminous-extended,Language,Language modeling/generation,,API access,https://docs.aleph-alpha.com/docs/Deprecated%20Luminous/Deprecated-Luminous/model-card/,,,2022-08-15,Aleph Alpha,30000000000.0,~30B (~42B with multi-modality),1.0019457e+23,"311840000000000*360000*3600*0.3 = 1.2124339e+23

6ND = 6*30*10^9*460000000000 = 8.28e+22

sqrt(8.28e+22*1.2124339e+23) = 1.0019457e+23",,"""""""The Luminous family has been trained on a dataset compiled of sources in English, German, French, Spanish and Italian...""""

more details in model card 
https://docs.aleph-alpha.com/docs/introduction/model-card/""",460000000000.0,"~460B tokens
230000 iterations",1344.0,~ 8 weeks = 56*24 = 1344 hours,NVIDIA A100 SXM4 40 GB,Confident,"Aleph Alpha luminous-extended is the second largest model which is faster and cheaper than Luminous-supreme. the model can perform information extraction, language simplification and has multi-capable image description capability. You can try Aleph Alpha models with predefined examples for free. Go to at the Jumpstart page on their site and click through the examples on Classification and Labelling, Generation, Information Extraction, Translation and Conversion and Multimodal. Aleph Alpha are based in Europe, which allows customers with sensitive data to process their information in compliance with European regulations for data protection and security on a sovereign, European computing infrastructure.",Germany,,,,512.0,,Unreleased,,,,
GLM-130B,Language,"Language modeling/generation,Translation","Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan Ma, Yufei Xue, Jidong Zhai, Wenguang Chen, Peng Zhang, Yuxiao Dong, Jie Tang",Open weights (non-commercial),https://arxiv.org/abs/2210.02414,1207.0,GLM-130B: An Open Bilingual Pre-trained Model,2022-08-04,Tsinghua University,130000000000.0,Dense model,3.5490054945e+23,"""96 NVIDIA A100 (40G * 8) servers for 2 months""

312 TFLOPS/GPU * 96 servers * 8 GPU/server * 2 months * 32.5% utilization = 4.037e23

utilization rate - citation from the paper: ""we report hardware FLOPs utilization (HFU) of 43.3% and model FLOPs utilization (MFU) of 32.5% due to re-materialization.""

Aligns pretty well with 6ND:
6 * 400B * 130B = 3.12E23

Geometric mean: sqrt(4.037e23 * 3.12e23) = 3.549e23","The Pile,WuDao Corpora","""The pre-training data includes 1.2T Pile (train split) (Gao et al., 2020) English, 1.0T Chinese WudaoCorpora (Yuan et al., 2021), and 250G Chinese corpora (including online forums, encyclopedia, and
QA) we crawl from the web, which form a balanced composition of English and Chinese contents""",400000000000.0,"400B ""We completed the 400B-token training and evaluation of GLM-130B in July, and subsequently released the model and pre-training details in August 2022. ""  from https://arxiv.org/pdf/2406.12793

""As of July 3rd, 2022, GLM-130B has been trained on over 400 billion text tokens (200B each for Chinese and English)""",1440.0,"""During the 60-day access to the cluster, we manage to train GLM-130B for 400 billion tokens""
60 days * 24 = 1,440 hours",NVIDIA A100 SXM4 40 GB,Confident,"GLM-130B (ICLR 2023) is an open bilingual (English & Chinese) bidirectional dense model with 130 billion parameters, pre-trained using the General Language Model (GLM) algorithm1. It is designed to support inference tasks with the 130B parameters on a single A100 (40G * 8) or V100 (32G * 8) server. As of July 3rd, 2022, GLM-130B has been trained on over 400 billion text tokens (200B each for Chinese and English) ",China,,,,768.0,0.325,Unreleased,non commercial license. looks like inference but not training code: https://github.com/THUDM/GLM-130B/blob/main/MODEL_LICENSE,0.433,,
AlexaTM 20B,Language,"Language modeling,Translation,Question answering","Saleh Soltan, Shankar Ananthakrishnan, Jack FitzGerald, Rahul Gupta, Wael Hamza, Haidar Khan, Charith Peris, Stephen Rawls, Andy Rosenbaum, Anna Rumshisky, Chandana Satya Prakash, Mukund Sridhar, Fabian Triefenbach, Apurv Verma, Gokhan Tur, Prem Natarajan",API access,https://arxiv.org/abs/2208.01448,89.0,AlexaTM 20B: Few-Shot Learning Using a Large-Scale Multilingual Seq2Seq Model,2022-08-02,Amazon,19750000000.0,See Table 1 on p.3 of the paper,2.04374016e+23,"Training throughput is reported as 154 TFLOP/s - see p.5 of the paper.
""We relied on an internal and optimized version of DeepSpeed that we have since open-sourced (Chiu & Zheng, 2022) to obtain training throughput of up to 154 TFLOPS/GPU on 16 AWS p4d.24xlarge compute instances.""

Accelerator compute days are reported as 15,360 days - see Table 17 on p.18 of the paper.","mC4,Wikipedia",,1319000000000.0,"See Table 2 on p.3 of the paper.

119B Wikipedia tokens + 1.2T mC4 tokens = 1319000000000 tokens",2880.0,"See p.5 of the paper: ""We trained AlexaTM 20B for 120 days on 128 A100 GPUs...""",NVIDIA A100,Confident,"In this work, we demonstrate that multilingual large-scale sequence-to-sequence (seq2seq) models, pre-trained on a mixture of denoising and Causal Language Modeling (CLM) tasks, are more efficient few-shot learners than decoder-only models on various tasks. In particular, we train a 20 billion parameter multilingual seq2seq model called Alexa Teacher Model (AlexaTM 20B) and show that it achieves state-of-the-art (SOTA) performance on 1-shot summarization tasks, outperforming a much larger 540B PaLM decoder model. AlexaTM 20B also achieves SOTA in 1-shot machine translation, especially for low-resource languages, across almost all language pairs supported by the model (Arabic, English, French, German, Hindi, Italian, Japanese, Marathi, Portuguese, Spanish, Tamil, and Telugu) on Flores-101 dataset. We also show in zero-shot setting, AlexaTM 20B outperforms GPT3 (175B) on SuperGLUE and SQuADv2 datasets and provides SOTA performance on multilingual tasks such as XNLI, XCOPA, Paws-X, and XWinograd. Overall, our results present a compelling case for seq2seq models as a powerful alternative to decoder-only models for Large-scale Language Model (LLM) training. ",United States of America,,,,128.0,,Unreleased,https://aws.amazon.com/about-aws/whats-new/2022/11/alexatm-20b-model-available-sagemaker-jumpstart/?nc1=h_ls,0.4935,,
YuYan 11B,Language,Language modeling/generation,"Gongzheng Li, Yadong Xi, Jingzhen Ding, Duan Wang, Ziyang Luo, Rongsheng Zhang, Bai Liu, Changjie Fan, Xiaoxi Mao, Zeng Zhao",,https://arxiv.org/abs/2104.12470,11.0,Easy and Efficient Transformer: Scalable Inference Solution For Large NLP Model,2022-07-15,"Hong Kong Baptist University,NetEase",11000000000.0,https://huggingface.co/FUXI/yuyan-11b,,,,,,,,,"NVIDIA A100 PCIe,NVIDIA GeForce RTX 2080 Ti 11GB",Confident,"Recently, large-scale transformer-based models have been proven to be effective over various tasks across many domains. Nevertheless, applying them in industrial production requires tedious and heavy works to reduce inference costs. To fill such a gap, we introduce a scalable inference solution: Easy and Efficient Transformer (EET), including a series of transformer inference optimization at the algorithm and implementation levels. First, we design highly optimized kernels for long inputs and large hidden sizes. Second, we propose a flexible CUDA memory manager to reduce the memory footprint when deploying a large model. Compared with the state-of-the-art transformer inference library (Faster Transformer v4.0), EET can achieve an average of 1.40-4.20x speedup on the transformer decoder layer with an A100 GPU","Hong Kong,China",,,,,,,,,,
BLOOM-176B,Language,"Language modeling,Translation,Code generation","Margaret Mitchell, Giada Pistilli, Yacine Jernite, Ezinwanne Ozoani, Marissa Gerchick, Nazneen Rajani, Sasha Luccioni, Irene Solaiman, Maraim Masoud, Somaieh Nikpoor, Carlos MuÃ±oz Ferrandis, Stas Bekman, Christopher Akiki, Danish Contractor, David Lansky, Angelina McMillan-Major, Tristan Thrush, Suzana IliÄ‡, GÃ©rard Dupont, Shayne Longpre, Manan Dey, Stella Biderman, Douwe Kiela, Emi Baylor, Teven Le Scao, Aaron Gokaslan, Julien Launay, Niklas Muennighoff",Open weights (restricted use),https://arxiv.org/abs/2211.05100,2720.0,BLOOM: A 176B-Parameter Open-Access Multilingual Language Model,2022-07-11,"Hugging Face,BigScience",176247271424.0,"See ""Technical Specifications"" on Hugging Face:
https://huggingface.co/bigscience/bloom",3.65664e+23,"https://bigscience.huggingface.co/blog/bloom Blog post says 117 days.

384 A100 GPUs * 314 TFLOPS throughput per GPU * 117 days * 0.3 (utilization assumption) = 3.65664e23
https://www.wolframalpha.com/input?i=384+*+314+TFLOPS+*+117+days+*+0.3",BigScience ROOTS Corpus,"In total, 1.6 terabytes of pre-processed text was converted into 350 billion unique tokens as BLOOM's training datasets.
arXiv:2210.15424

""BLOOM was trained on the ROOTS corpus (LaurenÂ¸con et al., 2022), a composite collection
of 498 Hugging Face datasets (Lhoest et al., 2021) amounting to 1.61 terabytes of text that
span 46 natural languages and 13 programming languages. A high-level overview of this
dataset can be seen in Figure 3, while a detailed itemized list of every language along with
its linguistic genus, family and macroarea is presented in Table 1""",379000000000.0,"Table 3.5 https://arxiv.org/pdf/2211.05100

366B (pretrain) + 13B (finetune) = 379B  tokens total ",2808.0,117 days * 24 hours/day,NVIDIA A100 SXM4 80 GB,Confident,"Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.","United States of America,France",,,,384.0,,Unreleased,responsible use restrictions: https://bigscience.huggingface.co/blog/the-bigscience-rail-license,0.5,,
Minerva (540B),Language,"Quantitative reasoning,Mathematical reasoning,Language modeling/generation,Question answering","Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, Vedant Misra",Unreleased,https://arxiv.org/abs/2206.14858,1275.0,Solving Quantitative Reasoning Problems with Language Models,2022-06-29,Google,540350000000.0,"""To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model (PaLM).""

Our approach is to start with the PaLM pretrained decoder-only transformer language models Chowdhery et al. (2022), and further train (finetune) them on our mathematical dataset using an autoregressive objective.
Table 2 contains the main model and training hyperparameters.

See Table 2",2.7415e+24,"Minerva was fine-tuned from PaLM using the same hardware. Assume the same model FLOPs utilization rate for pre-training and fine-tuning.
""the 540B model was trained for 29 days on a v4-1024""

PaLM pretraining time: 6144 TPU for 1200 hours + 3072 TPU for 336 hours = @8404992 TPU-hours
Minerva finetuning time: 1024 TPU for 696 hours = 712704 TPU-hours
So fine-tuning added 8.5% more compute.

Minerva total compute = PaLM pretraining compute * (712704+8404992)/(8404992) = 2.7415*10^24 FLOP
https://www.wolframalpha.com/input?i=%28712704%2B8404992%29%2F%288404992%29+*+2.5272*10%5E24

Palm pretraining: 2.5272e+24",arXiv,"PaLM, finetuned on arxiv",26000000000.0,"""Our models were trained on a dataset of 38.5B tokens"" + PaLM

upd 38.5B tokens - sie of the dataset, the model saw 26B tokens in 399k steps (see Table 2)",696.0,,Google TPU v4,Confident,"Language models have achieved remarkable performance on a wide range of tasks that require natural language understanding. Nevertheless, state-of-the-art models have generally struggled with tasks that require quantitative reasoning, such as solving mathematics, science, and engineering problems at the college level. To help close this gap, we introduce Minerva, a large language model pretrained on general natural language data and further trained on technical content. The model achieves state-of-the-art performance on technical benchmarks without the use of external tools. We also evaluate our model on over two hundred undergraduate-level problems in physics, biology, chemistry, economics, and other sciences that require quantitative reasoning, and find that the model can correctly answer nearly a third of them.",United States of America,PaLM (540B),2.1429e+23,,1024.0,,Unreleased,,,,
CodeWhisperer,Language,Code generation,,Hosted access (no API),https://aws.amazon.com/blogs/machine-learning/introducing-amazon-codewhisperer-the-ml-powered-coding-companion/,,"Introducing Amazon CodeWhisperer, the ML-powered coding companion",2022-06-24,Amazon,,,,,Unspecified unreleased,,,,,,,Unknown,"We are excited to announce Amazon CodeWhisperer, a machine learning (ML)-powered service that helps improve developer productivity by providing code recommendations based on developersâ€™ natural comments and prior code. With CodeWhisperer, developers can simply write a comment that outlines a specific task in plain English, such as â€œupload a file to S3.â€ Based on this, CodeWhisperer automatically determines which cloud services and public libraries are best suited for the specified task, builds the specific code on the fly, and recommends the generated code snippets directly in the IDE.",United States of America,,,,,,Unreleased,,,,
YaLM,Language,"Language modeling,Chat","Mikhail Khrushchev, Ruslan Vasilev, Alexey Petrov, Nikolay Zinov",Open weights (unrestricted),https://medium.com/yandex/yandex-publishes-yalm-100b-its-the-largest-gpt-like-neural-network-in-open-source-d1df53d0e9a6,,Yandex Publishes YaLM 100B. Itâ€™s the Largest GPT-Like Neural Network in Open Source,2022-06-23,Yandex,100000000000.0,100B,2.2e+23,"""It took us 65 days to train the model on a pool of 800 A100 graphics cards and 1.7 TB of online texts, books, and countless other sources.""","The Pile,YaLM Russian Dataset","""25% The Pile â€” open English dataset by Eleuther AI team

75% Texts in Russian collected by our team (percentages of the whole dataset are given)""

https://github.com/yandex/YaLM-100B?tab=readme-ov-file",300000000000.0,"1.7TB of data 300B tokens â€“ from github https://github.com/yandex/YaLM-100B
I've assumed that 1 token correspond to 1 word in russian language.",1560.0,65 days,NVIDIA A100,Likely,,Russia,,,,800.0,,Unreleased,"Apache 2.0 for weights.

training details, but no code: https://medium.com/yandex/yandex-publishes-yalm-100b-its-the-largest-gpt-like-neural-network-in-open-source-d1df53d0e9a6 ",,,
Parti,Image generation,"Text-to-image,Image generation","Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, Ben Hutchinson, Wei Han, Zarana Parekh, Xin Li, Han Zhang, Jason Baldridge, Yonghui Wu",Unreleased,https://arxiv.org/abs/2206.10789v1,1349.0,Scaling Autoregressive Models for Content-Rich Text-to-Image Generation,2022-06-22,Google Research,20000000000.0,"Abstract: ""we achieve consistent quality improvements
by scaling the encoder-decoder Transformer model up to 20B parameters""",5.09607936e+23,"Calculated from architecture. Does not take into account the encoding and decoding of text and images, only the transformer stack.

Table 1 shows for the 20B model
16 encoder layers
64 decoder layers
Dmodel = 4096
Dhidden = 16384
Num heads = 64

Just below table 1:
""We use a maximum length of text tokens of 128, and the length of image tokens are fixed to 1024""

I take the length of the sequence to be 100 for the encoder stack and 1024 for the decoder stack.

Section 3, Training: ""a total
of 450,000 steps and final ratio of 0.025. We use a global batch size of 8192 during training.""

6* 20B parameters * (1024+128) sequence length*450000 steps*8192 batch size= 5.096079e+23","LAION-400M,FIT400M,JFT-4B",,4800000000.0,,,,Google TPU v4,Confident,"We present the Pathways Autoregressive Text-to-Image (Parti) model, which generates high-fidelity photorealistic images and supports content-rich synthesis involving complex compositions and world knowledge. Parti treats text-to-image generation as a sequence-to-sequence modeling problem, akin to machine translation, with sequences of image tokens as the target outputs rather than text tokens in another language. This strategy can naturally tap into the rich body of prior work on large language models, which have seen continued advances in capabilities and performance through scaling data and model sizes. Our approach is simple: First, Parti uses a Transformer-based image tokenizer, ViT-VQGAN, to encode images as sequences of discrete tokens. Second, we achieve consistent quality improvements by scaling the encoder-decoder Transformer model up to 20B parameters, with a new state-of-the-art zero-shot FID score of 7.23 and finetuned FID score of 3.22 on MS-COCO. Our detailed analysis on Localized Narratives as well as PartiPrompts (P2), a new holistic benchmark of over 1600 English prompts, demonstrate the effectiveness of Parti across a wide variety of categories and difficulty aspects. We also explore and highlight limitations of our models in order to define and exemplify key areas of focus for further improvements. See https://parti.research.google/ for high-resolution images.",United States of America,,,,,,Unreleased,"""For these reasons, we have decided not to release our Parti models, code, or data for public use without further safeguards in place""
https://sites.research.google/parti/",,,
OPT-66B,Language,"Language modeling,Chat,Language modeling/generation,Question answering","Susan Zhangâˆ— , Stephen Rollerâˆ— , Naman Goyalâˆ— , Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ottâ€  , Sam Shleiferâ€  , Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, Luke Zettlemoyer",Open weights (non-commercial),https://arxiv.org/abs/2205.01068,4334.0,OPT: Open Pre-trained Transformer Language Models,2022-06-21,Meta AI,66000000000.0,,1.100000000001e+23,"OPT-66B was trained for 140k steps, using a batch size of 2M tokens (see the OPT baselines logbook and Table 1 in Zhang et al. (2022), respectively), so training took 140e3 âˆ— 2e6 âˆ— 66e9 âˆ— 6 = 1.1e23 FLOP","The Pile,BookCorpus (BooksCorpus, Toronto Book Corpus),CC-Stories,Pushshift Reddit","C.2 Composition section:
â€“ BookCorpus (Zhu et al., 2015) consists of more than 10K unpublished books
â€“ CC-Stories (Trinh and Le, 2018) contains a subset of CommonCrawl data filtered to match the story-like style of Winograd schemas
â€“ The Pile (Gao et al., 2021a) from which the following was included:
* Pile-CC
* OpenWebText2
* USPTO
* Project Gutenberg
* OpenSubtitles
* Wikipedia
* DM Mathematics
* HackerNews
â€“ Pushshift.io Reddit dataset that was developed in Baumgartner et al. (2020) and processed in Roller et al. (2021).
â€“ CCNewsV2 containing an updated version of the English portion of the CommonCrawl News dataset that was used in RoBERTa (Liu et al., 2019b)",180000000000.0,"""Our final corpus contains roughly 180B tokens.""",,,NVIDIA A100 SXM4 80 GB,Confident,"Large language models, which are often trained for hundreds of thousands of compute days, have shown remarkable capabilities for zero- and few-shot learning. Given their computational cost, these models are difficult to replicate without significant capital. For the few that are available through APIs, no access is granted to the full model weights, making them difficult to study. We present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers. We show that OPT-175B is comparable to GPT-3, while requiring only 1/7th the carbon footprint to develop. We are also releasing our logbook detailing the infrastructure challenges we faced, along with code for experimenting with all of the released models.",United States of America,,,,,,Open source,"non-commercial for weights:
https://ai.meta.com/blog/democratizing-access-to-large-scale-language-models-with-opt-175b/

training code (MIT) https://github.com/facebookresearch/metaseq/blob/main/docs/training.md ",,,
MetaLM,"Multimodal,Language,Vision","Language modeling,Visual question answering,Language modeling/generation,Image captioning","Yaru Hao, Haoyu Song, Li Dong, Shaohan Huang, Zewen Chi, Wenhui Wang, Shuming Ma, Furu Wei",Unreleased,https://arxiv.org/abs/2206.06336v1,108.0,Language Models are General-Purpose Interfaces,2022-06-13,Microsoft Research,,,,,The Pile,,,,,,,Speculative,"Foundation models have received much attention due to their effectiveness across a broad range of downstream applications. Though there is a big convergence in terms of architecture, most pretrained models are typically still developed for specific tasks or modalities. In this work, we propose to use language models as a general-purpose interface to various foundation models. A collection of pretrained encoders perceive diverse modalities (such as vision, and language), and they dock with a language model that plays the role of a universal task layer. We propose a semi-causal language modeling objective to jointly pretrain the interface and the modular encoders. We subsume the advantages and capabilities from both causal and non-causal modeling, thereby combining the best of two worlds. Specifically, the proposed method not only inherits the capabilities of in-context learning and open-ended generation from causal language modeling, but also is conducive to finetuning because of the bidirectional encoders. More importantly, our approach seamlessly unlocks the combinations of the above capabilities, e.g., enabling in-context learning or instruction following with finetuned encoders. Experimental results across various language-only and vision-language benchmarks show that our model outperforms or is competitive with specialized models on finetuning, zero-shot generalization, and few-shot learning.",United States of America,,,,,,Unreleased,"I don't see neither code nor weights here
https://github.com/microsoft/unilm/tree/master/metalm",,,
BIG-G 137B,Language,Language modeling/generation,"Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R. Brown, Adam Santoro, Aditya Gupta, AdriÃ  Garriga-Alonso, Agnieszka Kluska, Aitor Lewkowycz, Akshat Agarwal, Alethea Power, Alex Ray, Alex Warstadt, Alexander W. Kocurek, Ali Safaya, Ali Tazarv, Alice Xiang, Alicia Parrish, Allen Nie, Aman Hussain, Amanda Askell, Amanda Dsouza, Ambrose Slone, Ameet Rahane, Anantharaman S. Iyer, Anders Andreassen, Andrea Madotto, Andrea Santilli, Andreas StuhlmÃ¼ller, Andrew Dai, Andrew La, Andrew Lampinen, Andy Zou, Angela Jiang, Angelica Chen, Anh Vuong, Animesh Gupta, Anna Gottardi, Antonio Norelli, Anu Venkatesh, Arash Gholamidavoodi, Arfa Tabassum, Arul Menezes, Arun Kirubarajan, Asher Mullokandov, Ashish Sabharwal, Austin Herrick, Avia Efrat, Aykut Erdem, Ayla KarakaÅŸ, B. Ryan Roberts, Bao Sheng Loe, Barret Zoph, BartÅ‚omiej Bojanowski, Batuhan Ã–zyurt, Behnam Hedayatnia, Behnam Neyshabur, Benjamin Inden, Benno Stein, Berk Ekmekci, Bill Yuchen Lin, Blake Howald, Bryan Orinion, Cameron Diao, Cameron Dour, Catherine Stinson, Cedrick Argueta, CÃ©sar Ferri RamÃ­rez, Chandan Singh, Charles Rathkopf, Chenlin Meng, Chitta Baral, Chiyu Wu, Chris Callison-Burch, Chris Waites, Christian Voigt, Christopher D. Manning, Christopher Potts, Cindy Ramirez, Clara E. Rivera, Clemencia Siro, Colin Raffel, Courtney Ashcraft, Cristina Garbacea, Damien Sileo, Dan Garrette, Dan Hendrycks, Dan Kilman, Dan Roth, Daniel Freeman, Daniel Khashabi, Daniel Levy, Daniel MoseguÃ­ GonzÃ¡lez, Danielle Perszyk, Danny Hernandez, Danqi Chen, Daphne Ippolito et al. (351 additional authors not shown)",Unreleased,https://arxiv.org/abs/2206.04615,2114.0,Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models,2022-06-09,Google,137000000000.0,"137B. Table App.1
",5.6e+23,"""BIG-G models were trained at Google. We use 13 dense decoder-only Transformer models (Vaswani
et al., 2017) with gated activation layers (Dauphin et al., 2017) and GELU activations based on the LaMDA
architectures (Thoppilan et al., 2022). These models were trained on a dataset consisting of a mixture of web
documents, code, dialog, and Wikipedia data, with approximately three billion documents tokenized to 2.8
trillion BPE tokens using a 32k-token SentencePiece vocabulary""

Appendix:

""We use a pre-training batch size of 262k tokens for all models...""

2.6M steps for 137B, per Table App.1. So trained on 2.6M * 262k = 681B tokens (~0.25 epochs)
681B * 137B * 6 = 5.6e23",GLaM dataset,"""These models were trained on a dataset consisting of a mixture of web
documents, code, dialog, and Wikipedia data, with approximately three billion documents tokenized to 2.8
trillion BPE tokens using a 32k-token SentencePiece vocabulary""",681000000000.0,"Full dataset is comprised of 2.8 trillion tokens, but calculation based on batch size and steps suggests model was trained on only 681 billion tokens.",,,,Confident,"Language models demonstrate both quantitative improvement and new qualitative capabilities with increasing scale. Despite their potentially transformative impact, these new capabilities are as yet poorly characterized. In order to inform future research, prepare for disruptive new model capabilities, and ameliorate socially harmful effects, it is vital that we understand the present and near-future capabilities and limitations of language models. To address this challenge, we introduce the Beyond the Imitation Game benchmark (BIG-bench). BIG-bench currently consists of 204 tasks, contributed by 450 authors across 132 institutions. Task topics are diverse, drawing problems from linguistics, childhood development, math, common-sense reasoning, biology, physics, social bias, software development, and beyond. BIG-bench focuses on tasks that are believed to be beyond the capabilities of current language models. We evaluate the behavior of OpenAI's GPT models, Google-internal dense transformer architectures, and Switch-style sparse transformers on BIG-bench, across model sizes spanning millions to hundreds of billions of parameters. In addition, a team of human expert raters performed all tasks in order to provide a strong baseline. Findings include: model performance and calibration both improve with scale, but are poor in absolute terms (and when compared with rater performance); performance is remarkably similar across model classes, though with benefits from sparsity; tasks that improve gradually and predictably commonly involve a large knowledge or memorization component, whereas tasks that exhibit ""breakthrough"" behavior at a critical scale often involve multiple steps or components, or brittle metrics; social bias typically increases with scale in settings with ambiguous context, but this can be improved with prompting.",United States of America,,,,,,Unreleased,,,,
UL2,Language,"Language modeling/generation,Question answering,Text summarization","Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng, Neil Houlsby, Donald Metzler",Open weights (unrestricted),https://arxiv.org/abs/2205.05131v1,357.0,Unifying Language Learning Paradigms,2022-05-10,"Google Research,Google Brain",20000000000.0,Taken from Directory of LLMs,1.2e+23,"Trained on 1T tokens
20B * 1T * 6 = 1.2e23 

Second source: Section 5.1 says model was trained on 512 TPUv4 chips, and took slightly over 1 month
512 * 2.75e14 * 31 * 24 * 3600 * 0.3 = 1.13e23",C4,'The model is trained on a total of 1 trillion tokens on C4 (2 million steps).',1000000000000.0,1T tokens,744.0,"around 31 days from 'Pre-training took approximately slight more than one month for about 1 trillion
tokens.' from section 5.1
so around 31*24 = 744
",Google TPU v4,Confident,"Existing pre-trained models are generally geared towards a particular class of problems. To date, there seems to be still no consensus on what the right architecture and pre-training setup should be. This paper presents a unified framework for pre-training models that are universally effective across datasets and setups. We begin by disentangling architectural archetypes with pre-training objectives -- two concepts that are commonly conflated. Next, we present a generalized and unified perspective for self-supervision in NLP and show how different pre-training objectives can be cast as one another and how interpolating between different objectives can be effective. We then propose Mixture-of-Denoisers (MoD), a pre-training objective that combines diverse pre-training paradigms together. We furthermore introduce a notion of mode switching, wherein downstream fine-tuning is associated with specific pre-training schemes. We conduct extensive ablative experiments to compare multiple pre-training objectives and find that our method pushes the Pareto-frontier by outperforming T5 and/or GPT-like models across multiple diverse setups. Finally, by scaling our model up to 20B parameters, we achieve SOTA performance on 50 well-established supervised NLP tasks ranging from language generation (with automated and human evaluation), language understanding, text classification, question answering, commonsense reasoning, long text reasoning, structured knowledge grounding and information retrieval. Our model also achieve strong results at in-context learning, outperforming 175B GPT-3 on zero-shot SuperGLUE and tripling the performance of T5-XXL on one-shot summarization. We release Flax-based T5X model checkpoints for the 20B model at \url{this https URL}.","United States of America,United States of America",,,,512.0,0.2992518703,Unreleased,"Apache 2.0

https://huggingface.co/google/ul2",,,
OPT-175B,Language,"Language modeling,Chat,Language modeling/generation,Question answering","Susan Zhangâˆ— , Stephen Rollerâˆ— , Naman Goyalâˆ— , Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ottâ€  , Sam Shleiferâ€  , Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, Luke Zettlemoyer",Open weights (non-commercial),https://arxiv.org/abs/2205.01068,4334.0,OPT: Open Pre-trained Transformer Language Models,2022-05-02,Meta AI,175000000000.0,"""In line with Meta AIâ€™s commitment to open science, we are sharing Open Pretrained Transformer (OPT-175B), a language model with 175 billion parameters trained on publicly available data sets""",4.3e+23,"https://github.com/facebookresearch/metaseq/blob/main/projects/OPT/chronicles/final_update.md

""As of yesterday, at 12:46pm PST on January 6, our 175B model finally completed its training run on 300B tokens. This required ~4.30E+23 FLOPs of compute""","The Pile,BookCorpus (BooksCorpus, Toronto Book Corpus),CC-Stories,Pushshift Reddit","""The pre-training corpus contains a concatenation
of datasets used in RoBERTa (Liu et al., 2019b),
the Pile (Gao et al., 2021a), and PushShift.io Reddit (Baumgartner et al., 2020; Roller et al., 2021)""
...
""RoBERTa We included the BookCorpus (Zhu et al., 2015) and Stories (Trinh and Le, 2018) subsets of the RoBERTa corpus and utilized an updated version of CCNews, containing news stories crawled through September 28, 2021. This CCNews v2 corpus was preprocessed the same way as the original RoBERTa CCNews (Liu et al., 2019b).

The Pile We included a subset of the Pile (Gao et al., 2021a), including: CommonCrawl, DM Mathematics, Project Gutenberg, HackerNews, OpenSubtitles, OpenWebText2, USPTO and Wikipedia. Other subsets of the Pile were eliminated
...

PushShift.io Reddit We included a subset of the Pushshift.io corpus produced by Baumgartner et al. (2020) and previously used by Roller et al. (2021). To convert the conversational trees
into language-model-accessible documents, we extracted the longest chain of comments in each thread and discarded all other paths in the tree. This reduced the corpus by about 66%.",180000000000.0,"""The training data contains 180B tokens corresponding to 800 GB of data""

1 token ~ 0.75 words",793.5,"4.3*10^23 FLOP / (147 TFLOPS) = 813000 A100-hours
https://www.wolframalpha.com/input?i=4.3*10%5E23+FLOP+%2F+%28147+TFLOPS%29

""As of yesterday, at 12:46pm PST on January 6, our 175B model finally completed its training run on 300B tokens. This required ~4.30E+23 FLOPs of compute, or roughly ~33 days of continuous training on 1024 80GB A100s (assuming no hardware issues, no numerical instabilities, etc.).""",NVIDIA A100 SXM4 80 GB,Confident,"Large language models, which are often trained for hundreds of thousands of compute days, have shown remarkable capabilities for zero- and few-shot learning. Given their computational cost, these models are difficult to replicate without significant capital. For the few that are available through APIs, no access is granted to the full model weights, making them difficult to study. We present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers. We show that OPT-175B is comparable to GPT-3,1 while requiring only 1/7th the carbon footprint to develop. We are also releasing our logbook detailing the infrastructure challenges we faced, along with code for experimenting with all of the released models",United States of America,,,,1024.0,,Open source,"non-commercial for weights:
https://github.com/facebookresearch/metaseq/blob/main/projects/OPT/MODEL_LICENSE.md

training code (MIT) https://github.com/facebookresearch/metaseq/blob/main/docs/training.md ",0.47115,,
OPT-13B,,,"Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, Luke Zettlemoyer",,https://arxiv.org/abs/2205.01068,,,2022-05-02,Meta AI,13000000000.0,,3.53e+23,2851200*992*312e12*0.40 = 3.53e23 assuming 40% utilization,,,,,,,,Likely,,United States of America,,,,,,,,,,
VLM-4,Language,"Language modeling/generation,Translation,Text summarization,Text classification",,API access,https://web.archive.org/web/20220506095925/https://lighton.ai/blog/lighton-publicly-launches-muse-an-api-to-vlm-4-large-language-models-trained-natively-in-five-european-languages/,,"LightOn publicly launches Muse, an API to VLM-4 Large Language Models trained natively in five European Languages",2022-04-12,LightOn,,,,,,,,,,,,Unknown,,France,,,,,,Unreleased,,,,
DALLÂ·E 2,Image generation,"Text-to-image,Image generation","Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, Mark Chen",API access,https://cdn.openai.com/papers/dall-e-2.pdf,8200.0,Hierarchical Text-Conditional Image Generation with CLIP Latents,2022-04-06,OpenAI,3500000000.0,"""Our decoder architecture is the 3.5 billion parameter GLIDE model""",3.3695784e+23,"Decoder architecture is similar to Imagen (1.46E+22), but trained on 1.6e9 datapoints (Table 3) rather than Imagen's 5.1e9 datapoints.

DALL-E 2 uses two models as priors. I estimate the prior model's FLOP as 6*N*D = 6 * 1e9 * 4096 * 1e6 = 2.5e19 FLOP. However, this seems low compared to CLIP.

So it may be possible to estimate DALL-E 2's compute by analogy to Imagen, but there is a lot of uncertainty and more research would be needed.

here (https://arxiv.org/pdf/2407.15811) they claim the DALL-E.2 model was trained on the equivalent of 5208.3 days on 8*A100 GPUs:

312000000000000 FLOP / sec / GPU * 8 GPUs * 5208.3 days * 24 hours / day * 3600 sec / hour * 0.3 [assumed utilization] = 3.3695784e+23 FLOP","CLIP,DALL-E","https://aimlapi.com/models/openai-dall-e-2-api says the model's knowledge cutoff date is September 2021, which I assume means September 1, 2021.",650000000.0,"""When training the encoder, we sample from the CLIP [39] and DALL-E [40] datasets (approximately 650M images in total) with equal probability""",,,,Speculative,"Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.",United States of America,,,,,,Unreleased,,,,
PaLM (540B),Language,"Language modeling,Code generation,Translation","Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev,, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta ,Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, Noah Fiedel",Unreleased,https://arxiv.org/abs/2204.02311,7351.0,PaLM: Scaling Language Modeling with Pathways,2022-04-04,Google Research,540350000000.0,"""To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model (PaLM).""",2.5272e+24,"See Table 20.

6144 TPUv4 for 1200 hours + 3072 TPUv4 for 336 hours.
Equivalent to 6144 TPUv4 for 1368 hours.

46.2% model FLOPs utilization

""The 540B-parameter PaLM model sustained a remarkable 57.8% of the peak hardware floating point performance over 50 days while training on TPU v4 supercomputers."" https://cloud.google.com/blog/topics/systems/tpu-v4-enables-performance-energy-and-co2e-efficiency-gains","Wikipedia,GLaM dataset,LaMBDA dataset,GitHub",,585000000000.0,"""The PaLM pretraining dataset consists of a high-quality corpus of 780 billion tokens that represent a wide range of natural language use cases.""

1 token ~ 0.75 words",1536.0,"6144 TPUv4 for 1200 hours + 3072 TPUv4 for 336 hours.

Equivalent to 6144 TPUv4 for 1368 hours.",Google TPU v4,Confident,"Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.",United States of America,,,,6144.0,0.462,Unreleased,,0.578,14451957.906571811,67883583.58684365
Chinchilla,Language,Language modeling,"Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, Laurent Sifre",Unreleased,https://arxiv.org/abs/2203.15556,2585.0,Training Compute-Optimal Large Language Models,2022-03-29,DeepMind,70000000000.0,"""We test this hypothesis by training a predicted compute-optimal model, \chinchilla, that uses the same compute budget as \gopher but with 70B parameters and 4Ã— more more data. \chinchilla uniformly and significantly outperforms \Gopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks.""",5.76e+23,"""Both Chinchilla and Gopher have been trained for the same number of FLOPs but differ in the size of the model and the number of training tokens.""

We see the number of flops in table 3","MassiveWeb,C4,GitHub,Wikipedia,books","MassiveWeb, Books, C4, News, Github, Wikipedia (Table A1)",1400000000000.0,"Table 1 shows Chinchilla was training on 1.4 trillion tokens

1 token ~ 0.75 words",,,"Google TPU v4,Google TPU v3",Confident,"We investigate the optimal model size and number of tokens for training a transformer language model under a given compute budget. We find that current large language models are significantly undertrained, a consequence of the recent focus on scaling language models whilst keeping the amount of training data constant. By training over \nummodels language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, we find that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled. We test this hypothesis by training a predicted compute-optimal model, \chinchilla, that uses the same compute budget as \gopher but with 70B parameters and 4Ã— more more data. \chinchilla uniformly and significantly outperforms \Gopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks. This also means that \chinchilla uses substantially less compute for fine-tuning and inference, greatly facilitating downstream usage. As a highlight, \chinchilla reaches a state-of-the-art average accuracy of 67.5\% on the MMLU benchmark, greater than a 7\% improvement over \gopher.",United Kingdom of Great Britain and Northern Ireland,,,,,,Unreleased,,,,
ST-MoE,Language,Language modeling/generation,"Barret Zoph, Irwan Bello, Sameer Kumar, Nan Du, Yanping Huang, Jeff Dean, Noam Shazeer, William Fedus",Unreleased,https://arxiv.org/abs/2202.08906v2,295.0,ST-MoE: Designing Stable and Transferable Sparse Expert Models,2022-02-17,"Google,Google Brain,Google Research",269000000000.0,269B. it's called ST-MoE-32B because it's equivalent to a 32B dense model.,2.9e+23,"The paper claims ""scaling a sparse model to 269B parameters, with a computational cost comparable to a 32B dense encoder-decoder"". If this is true for training cost, then 6*32e9*1.5e12 = 2.9e23",C4,"""The pre-training dataset used to train our Sparse 32B model is a mix of C4 (Raffel et al., 2019) and the dataset introduced in GLaM (Du et al., 2021).""",1500000000000.0,"""We pre-train for 1.5T tokens on a mixture of English-only C4 dataset (Raffel et al., 2019) and the dataset from GLaM (Du et al., 2021) summarized in Appendix E""
",,,,Likely,"Scale has opened new frontiers in natural language processing -- but at a high cost. In response, Mixture-of-Experts (MoE) and Switch Transformers have been proposed as an energy efficient path to even larger and more capable language models. But advancing the state-of-the-art across a broad set of natural language tasks has been hindered by training instabilities and uncertain quality during fine-tuning. Our work focuses on these issues and acts as a design guide. We conclude by scaling a sparse model to 269B parameters, with a computational cost comparable to a 32B dense encoder-decoder Transformer (Stable and Transferable Mixture-of-Experts or ST-MoE-32B). For the first time, a sparse model achieves state-of-the-art performance in transfer learning, across a diverse set of tasks including reasoning (SuperGLUE, ARC Easy, ARC Challenge), summarization (XSum, CNN-DM), closed book question answering (WebQA, Natural Questions), and adversarially constructed tasks (Winogrande, ANLI R3).","United States of America,United States of America,United States of America",,,,,,Open source,"Apache License 2.0
Code for our models is available at https://github.com/tensorflow/mesh/blob/master/mesh_tensorflow/transformer/moe.py",,,
Midjourney V1,Image generation,Image generation,,Hosted access (no API),,,,2022-02-15,Midjourney,,,,,Unspecified unreleased,,,,,,,Unknown,,United States of America,,,,,,Unreleased,,,,
LaMDA,Language,Language modeling,"Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, YaGuang Li, Hongrae Lee, Huaixiu Steven Zheng, Amin Ghafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Yanqi Zhou, Chung-Ching Chang, Igor Krivokon, Will Rusch, Marc Pickett, Kathleen Meier-Hellstern, Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos, Toju Duke, Johnny Soraker, Ben Zevenbergen, Vinodkumar Prabhakaran, Mark Diaz, Ben Hutchinson, Kristen Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui, Marian Croak, Ed Chi, Quoc Le",Unreleased,https://arxiv.org/abs/2201.08239,1769.0,LaMDA: Language Models for Dialog Applications,2022-02-10,Google,137000000000.0,"""LaMDA is a family of Transformer-based neural language models specialized for dialog, which have up to 137B parameters""",3.55e+23,"""The total FLOPS is 56.5% * 123 TFLOPS/s * 1024 chips * 57.7 days
= 3.55E+23""
From https://arxiv.org/pdf/2201.08239.pdf p.18
",Infiniset,"LaMDA's underlying dataset is called 'Infiniset', and besides the dialogue also involves common crawl, wikipedia, a mixture of english and non-english web documents, and data from programming-related sites (so LaMDA models can also dabble in code).",1560000000000.0,"""and are pre-trained on 1.56T words of public dialog data and web text""",1385.0,57.7 days * 24,Google TPU v3,Confident,"We present LaMDA: Language Models for Dialog Applications. LaMDA is a family of Transformer-based neural language models specialized for dialog, which have up to 137B parameters and are pre-trained on 1.56T words of public dialog data and web text. While model scaling alone can improve quality, it shows less improvements on safety and factual grounding. We demonstrate that fine-tuning with annotated data and enabling the model to consult external knowledge sources can lead to significant improvements towards the two key challenges of safety and factual grounding. The first challenge, safety, involves ensuring that the model's responses are consistent with a set of human values, such as preventing harmful suggestions and unfair bias. We quantify safety using a metric based on an illustrative set of human values, and we find that filtering candidate responses using a LaMDA classifier fine-tuned with a small amount of crowdworker-annotated data offers a promising approach to improving model safety. The second challenge, factual grounding, involves enabling the model to consult external knowledge sources, such as an information retrieval system, a language translator, and a calculator. We quantify factuality using a groundedness metric, and we find that our approach enables the model to generate responses grounded in known sources, rather than responses that merely sound plausible. Finally, we explore the use of LaMDA in the domains of education and content recommendations, and analyze their helpfulness and role consistency.",United States of America,,,,1024.0,,Unreleased,,0.565,,
AlphaCode,Language,Code generation,"Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, RÃ©mi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de Masson d'Autume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel J. Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu, Oriol Vinyals",Unreleased,https://arxiv.org/abs/2203.07814,1819.0,Competition-Level Code Generation with AlphaCode,2022-02-02,DeepMind,41100000000.0,41.1B. Table 3,2.38010000000001e+23,"Using C=6ND, we have C = 6 FLOP/token/param * 41.1B params * 967B tokens = 2.38e23 FLOP.

Figure 7 (a) shows a maximum training compute budget of approx 23000 TPU-days per model. This matches the operation-counting estimate at 44% utilization.","CodeContests,Unspecified unreleased","Looks like evaluation data is released but not pretraining data:

""We use large transformer language models to generate code, pre-training them
on selected GitHub code and fine-tuning on our curated set of competitive programming problems...
A core part of developing our system was ensuring that submissions are rigorously evaluated and
that evaluation problems are truly unseen during training, so difficult problems cannot be solved
by copying from the training set. Towards this goal, we release a new training and evaluation
competitive programming dataset, CodeContests""",,Appendix part A has answers for pretraining.,147.2,"Figure 7 (a) shows that the models were trained for around 23000 TPU-days. We know they trained on TPUv4s, and in appendix D.1 they say they have 3750 TPUv4 and TPUv4i. Assuming they trained only on the 3750 TPUv4s, that suggests 23000 / 3750 = 6.13 days, or 147.2 hours.",Google TPU v4,Confident,"Programming is a powerful and ubiquitous problem-solving tool. Developing systems that can assist programmers or even generate programs independently could make programming more productive and accessible, yet so far incorporating innovations in AI has proven challenging. Recent large-scale language models have demonstrated an impressive ability to generate code, and are now able to complete simple programming tasks. However, these models still perform poorly when evaluated on more complex, unseen problems that require problem-solving skills beyond simply translating instructions into code. For example, competitive programming problems which require an understanding of algorithms and complex natural language remain extremely challenging. To address this gap, we introduce AlphaCode, a system for code generation that can create novel solutions to these problems that require deeper reasoning. In simulated evaluations on recent programming competitions on the Codeforces platform, AlphaCode achieved on average a ranking of top 54.3% in competitions with more than 5,000 participants. We found that three key components were critical to achieve good and reliable performance: (1) an extensive and clean competitive programming dataset for training and evaluation, (2) large and efficient-to-sample transformer-based architectures, and (3) large-scale model sampling to explore the search space, followed by filtering based on program behavior to a small set of submissions.
",United Kingdom of Great Britain and Northern Ireland,,,,3750.0,0.4364,Unreleased,,,,
ERNIE 3.0 Titan,Language,"Language modeling,Language modeling/generation,Relation extraction,Sentiment classification,Text classification","Shuohuan Wang, Yu Sun, Yang Xiang, Zhihua Wu, Siyu Ding, Weibao Gong, Shikun Feng",Hosted access (no API),https://arxiv.org/abs/2112.12731,84.0,ERNIE 3.0 Titan: Exploring Larger-scale Knowledge Enhanced Pre-training for Language Understanding and Generation,2021-12-23,"Baidu,Peng Cheng Laboratory",260000000000.0,"""[We] developed... distributed training technology, including fine-grained parallelism, heterogeneous hardware-aware training, and fault tolerance mechanism to train the 260B model on both Nvidia V100 GPU and Ascend 910 NPU clusters.""
See also:
https://twitter.com/BaiduResearch/status/1468633977242243078?t=6q4zuLNdTSc4GUBe9OM5Aw&s=19",1.0421e+24,"The paper suggests that ERNIE 3.0 Titan uses more compute than GPT-3. This is consistent with the 6ND approximation.

C = 6ND = 6 (FLOP/param/token) * (260B params) * (668B tokens) = 1.0421*10^24 FLOP",ERNIE 3.0 Corpus,,668000000000.0,"""To ensure the success of the pre-training of ERNIE 3.0 Titan, we utilize the ERNIE 3.0 Corpus [ 2 ], a large-scale, wide-variety, and high-quality Chinese text corpora amounting to 4TB""

Assuming 167M words/tokens per GB",,,"NVIDIA Tesla V100 DGXS 32 GB,Huawei Ascend 910",Confident,"Pre-trained language models have achieved state-of-the-art results in various Natural Language Processing (NLP) tasks. GPT-3 has shown that scaling up pre-trained language models can further exploit their enormous potential. A unified framework named ERNIE 3.0 was recently proposed for pre-training large-scale knowledge enhanced models and trained a model with 10 billion parameters. ERNIE 3.0 outperformed the state-of-the-art models on various NLP tasks. In order to explore the performance of scaling up ERNIE 3.0, we train a hundred-billion-parameter model called ERNIE 3.0 Titan with up to 260 billion parameters on the PaddlePaddle platform. Furthermore, we design a self-supervised adversarial loss and a controllable language modeling loss to make ERNIE 3.0 Titan generate credible and controllable texts. To reduce the computation overhead and carbon emission, we propose an online distillation framework for ERNIE 3.0 Titan, where the teacher model will teach students and train itself simultaneously. ERNIE 3.0 Titan is the largest Chinese dense pre-trained model so far. Empirical results show that the ERNIE 3.0 Titan outperforms the state-of-the-art models on 68 NLP datasets.","China,China",,,,1920.0,,Unreleased,"The Ernie 3.0 Titan model was used in Ernie bot. Today, ERNIE has been widely deployed across finance, healthcare, insurance, equity, Internet, logistics, and other fields.

http://research.baidu.com/Blog/index-view?id=165",,,
EXAONE 1.0,"Multimodal,Language,Vision","Translation,Language modeling/generation,Visual question answering",,Unreleased,"https://www.lgcorp.com/media/release/27387#:~:text=LG%20AI%20Research%20proposes%20EXAONE,performance%20while%20learning%20fewer%20parameters.",,,2021-12-14,LG,300000000000.0,,1.6956e+24,"No indication of how images are processed. Supposing they used something like ViT-H/14, and training images were 512x512 (they state ""EXAONE shows remarkable performance such as [...] offering 1024x1024 sized image output"", but typically this size of image training would only be done during a relatively short, final stage of pre-training), there would be 37x37 = 1,369 patches per image
1,369 * 250 million = around 342 billion image patch embeddings.

300M parameters * (342 billion + 600 billion) image tokens * 6 = 1.6956e24",Unspecified unreleased,"""To create multi-modal AI, LG AI Research Institute learned from 600 billion corpora, the world's largest, and more than 250 million high-resolution images combining language and images. It is also differentiated in that it is a bilingual AI that understands and speaks Korean and English at the level of a native speaker.""

600000000000+250000000=600250000000",600250000000.0,,,,,Speculative,"[Dec 2021]  EXAONE is a bilingual artificial intelligence that has learned the characteristics of both Korean and English languages at the same time. Since the initial development last June, it has completed learning of 1.3 billion, 13 billion, 39 billion, and 175 billion parameter models, and it is currently learning 300 billion parametric models. EXAONE shows remarkable performance such as obtaining the highest FID score, offering 1024x1024 sized image output, and achieving purpose conversation in the language domain as well as the highest level in the emotional classification domain. ",Korea (Republic of),,,,,,Unreleased,,,,
GLaM,Language,"Language modeling/generation,Question answering","Nan Du, Yanping Huang, Andrew M. Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, Barret Zoph, Liam Fedus, Maarten Bosma, Zongwei Zhou, Tao Wang, Yu Emma Wang, Kellie Webster, Marie Pellat, Kevin Robinson, Kathleen Meier-Hellstern, Toju Duke, Lucas Dixon, Kun Zhang, Quoc V Le, Yonghui Wu, Zhifeng Chen, Claire Cui",Unreleased,https://arxiv.org/abs/2112.06905,1034.0,GLaM: Efficient Scaling of Language Models with Mixture-of-Experts,2021-12-13,Google,1200000000000.0,1.2 trillion parameters,3.6363112434e+23,"The network activates 96.6 billion parameters per token and trained for 600B tokens.

6 * 600B * 96.6B = 3.478e23

Digitizing figure 4 (d) indicates 139.67 TPU-years of training. 
2.75e14 * 139.67 * 365.25 * 24 * 3600 * 0.3 = 3.636e23

Since these are close, we will use the 6NC estimate and derive hardware utilization from the training time information.

Later they say they measured 326W power usage per chip, which could maybe be used to estimate utilization.","Wikipedia,GLaM dataset","""To train our model, we build a high-quality dataset of 1.6 trillion tokens that are representative of a wide range of natural language use cases. Web pages constitute the vast quantity of data in our unlabeled dataset. However, their quality ranges from professional writing to low-quality comment and forum pages.""",600000000000.0,"The dataset is made of 1.6 trillion tokens, but later in the paper they say they only train the largest model for 600b tokens. 600b / 0.75 words/token = 800b words.

""The complete GLaM training using 600B tokens consumes only 456 MWh and emits 40.2 net tCO2e.""",1366.0,"Note that they give several energy estimates. Use the complete training figures for 600B tokens, not the GPT-3 comparison values with 280B tokens.

""326W measured system power per TPU-v4 chip""
""The complete GLaM training using 600B tokens consumes only
456 MWh""
1024 TPU v4 chips
(456 MWh) / (326W/chip * 1024 chips) = 1366 hours",Google TPU v4,Confident,"Scaling language models with more data, compute and parameters has driven significant progress in natural language processing. For example, thanks to scaling, GPT-3 was able to achieve strong results on in-context learning tasks. However, training these large dense models requires significant amounts of computing resources. In this paper, we propose and develop a family of language models named GLaM (Generalist Language Model), which uses a sparsely activated mixture-of-experts architecture to scale the model capacity while also incurring substantially less training cost compared to dense variants. The largest GLaM has 1.2 trillion parameters, which is approximately 7x larger than GPT-3. It consumes only 1/3 of the energy used to train GPT-3 and requires half of the computation flops for inference, while still achieving better overall zero-shot and one-shot performance across 29 NLP tasks.",United States of America,,,,1024.0,0.2869636964,Unreleased,,,,
Gopher (280B),Language,"Language modeling,Question answering","Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson d'Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew Johnson, Blake Hechtman, Laura Weidinger, Iason Gabriel, William Isaac, Ed Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu, Geoffrey Irving",Unreleased,https://arxiv.org/abs/2112.11446,1501.0,"""Scaling Language Models: Methods, Analysis & Insights from Training Gopher""",2021-12-08,DeepMind,280000000000.0,"Language modelling provides a step towards intelligent communication systems by harnessing large repositories of written human knowledge to better predict and understand the world. In this paper, we present an analysis of Transformer-based language model performance across a wide range of model scales -- from models with tens of millions of parameters up to a 280 billion parameter model called Gopher. These models are evaluated on 152 diverse tasks, achieving state-of-the-art performance across the majority. Gains from scale are largest in areas such as reading comprehension, fact-checking, and the identification of toxic language, but logical and mathematical reasoning see less benefit. We provide a holistic analysis of the training dataset and model's behaviour, covering the intersection of model scale with bias and toxicity. Finally we discuss the application of language models to AI safety and the mitigation of downstream harms.",6.31e+23,"Table A26
6.31E+08 Train PFLOPs",MassiveTex,,300000000000.0,"""We train all models for 300 billion tokens with a 2048 token context window, using the Adam (Kingma and Ba, 2014) optimiser.""

1 token ~ 0.75 words",920.0,"""We trained Gopher for 920 hours in November and December 2020 in Googleâ€™s Georgia datacentre. The PUE of the datacenter at this time was 1.08; the net tCO2e per MWh in October 2020 was 0.33. Using an estimate of 283W drawn per chip, this leads to a total of 380 net tCO2e""",Google TPU v3,Confident,"We enhance auto-regressive language models by conditioning on document chunks retrieved from a large corpus, based on local similarity with preceding tokens. With a 2 trillion token database, our Retrieval-Enhanced Transformer (RETRO) obtains comparable performance to GPT-3 and Jurassic-1 on the Pile, despite using 25Ã— fewer parameters. After fine-tuning, RETRO performance translates to downstream knowledge-intensive tasks such as question answering. RETRO combines a frozen Bert retriever, a differentiable encoder and a chunked cross-attention mechanism to predict tokens based on an order of magnitude more data than what is typically consumed during training. We typically train RETRO from scratch, yet can also rapidly RETROfit pre-trained transformers with retrieval and still achieve good performance. Our work opens up new avenues for improving language models through explicit memory at unprecedented scale.",United Kingdom of Great Britain and Northern Ireland,,,,4096.0,0.378,Unreleased,,,3636030.5562741663,46909499.95890752
Yuan 1.0,Language,Language modeling,"Shaohua Wu, Xudong Zhao, Tong Yu, Rongguo Zhang, Chong Shen, Hongli Liu, Feng Li, Hong Zhu, Jiangang Luo, Liang Xu, Xuanwei Zhang, Jun Liu",API access,https://arxiv.org/abs/2110.04725,67.0,Yuan 1.0: Large-Scale Pre-trained Language Model in Zero-Shot and Few-Shot Learning,2021-10-12,Inspur,245730000000.0,"Table 2: Parameters of Yuan models.
""Parameters (billion)""",3.5380000000001e+23,"Table 9: 4095 petaFLOPS-days which equals 3.538*10^23 FLOP

https://www.wolframalpha.com/input?i=4095+petaFLOPS+*+1+day
","Common Crawl,Wikipedia,Sogue News","""A Chinese corpus with 5TB high-quality text is built, which is sufficient to train Yuan 245B model without sampling the dataset twice.""

In order to obtain the high-quality dataset, we develop a Massive Data Filtering System (MDFS) built on Spark to clean and filter the raw data, and train a Bert-based model to select high quality samples. MDFS is consisted of three parts, data collection, coarse filtering and fine filtering (Fig. 5). The raw data is collected from Common Crawl, Sogou News, SogouT, Encyclopedia, and Books (Table 3). To process these raw data, we run MDFS system on a high performance cluster with 36 nodes.",1000000000000.0,"""Yuan 1.0 was trained on a new Chinese dataset of 5TB high-quality text that was built on 850TB raw data from Internet.""

1 GB ~ 167M words in English or 333M words in Chinese. For a mixed dataset of mostly Chinese, 5TB may be equivalent to around 1T words.

Table 2: 180B training tokens",,,,Confident,"Recent work like GPT-3 has demonstrated excellent performance of Zero-Shot and Few-Shot learning on many natural language processing (NLP) tasks by scaling up model size, dataset size and the amount of computation. However, training a model like GPT-3 requires huge amount of computational resources which makes it challengeable to researchers. In this work, we propose a method that incorporates large-scale distributed training performance into model architecture design. With this method, Yuan 1.0, the current largest singleton language model with 245B parameters, achieves excellent performance on thousands GPUs during training, and the state-of-the-art results on NLP tasks. A data processing method is designed to efficiently filter massive amount of raw data. The current largest high-quality Chinese corpus with 5TB high quality texts is built based on this method. In addition, a calibration and label expansion method is proposed to improve the Zero-Shot and Few-Shot performance, and steady improvement is observed on the accuracy of various tasks. Yuan 1.0 presents strong capacity of natural language generation, and the generated articles are difficult to distinguish from the human-written ones.",China,,,,2128.0,,Unreleased,https://github.com/Shawn-IEITSystems/Yuan-1.0,0.45,,48571462.85398424
Megatron-Turing NLG 530B,Language,"Language modeling,Language modeling/generation,Question answering","Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, Elton Zhang, Rewon Child, Reza Yazdani Aminabadi, Julie Bernauer, Xia Song, Mohammad Shoeybi, Yuxiong He, Michael Houston, Saurabh Tiwary, Bryan Catanzaro",Unreleased,https://arxiv.org/abs/2201.11990,805.0,"Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model",2021-10-11,"Microsoft,NVIDIA",530000000000.0,,8.586e+23,"https://www.lesswrong.com/posts/bGuMrzhJdENCo8BxX/nvidia-and-microsoft-releases-530b-parameter-transformer?commentId=HSJSNspKp94tFcSCx

source: https://lair.lighton.ai/akronomicon/
9938 PF-days * 3600 * 24 * 10^15  = 8.586432e+23

6ND estimate: 6 * 530B * 270B = 8.586000e+23","Common Crawl,The Pile,CC-Stories,Realnews"," In addition to Common Crawl data, we leveraged a number of other previously generated datasets. From The Pile, we selected Books3, OpenWebText2, Stack Exchange, PubMed Abstracts,
Wikipedia, Gutenberg (PG-19), BookCorpus2, NIH ExPorter, and Pile-CC datasets. We also included the
CC-Stories and RealNews datasets used to train Megatron",270000000000.0,"""Our training dataset consists of 339 billion tokens and we
trained MT-NLG on 270 billions tokens by blending the 15 training datasets as described above. We also set aside 2% of our data for validation.""

1 token ~ 0.75 words",770.0,"Total compute was 1.17*10^24 FLOP.
They don't directly report the utilization and training speed when using the full Selene supercomputer with 560 DGX * 8 A100/DGX = 4480 GPUs. See section 2.3 Hardware Setup.

At 280 DGX, the utilization is 126/312 = 40% and a batch takes 60 seconds; at 350, it is 39% for 50 seconds; at 420, it is 36% for 44 seconds.

The overall utilization was 30.2% and the full cluster has 560 DGX. Dividing the total compute by the total performance of 4480 A100 at 30.2% utilization gives 770 hours.",NVIDIA A100 SXM4 80 GB,Confident,"Pretrained general-purpose language models can achieve state-of-the-art accuracies in various natural language processing domains by adapting to downstream tasks via zero-shot, few-shot and fine-tuning techniques. Because of their success, the size of these models has increased rapidly, requiring high-performance hardware, software, and algorithmic techniques to enable training such large models. As the result of a joint effort between Microsoft and NVIDIA, we present details on the training of the largest monolithic transformer based language model, Megatron-Turing NLG 530B (MT-NLG), with 530 billion parameters. In this paper, we first focus on the infrastructure as well as the 3D parallelism methodology used to train this model using DeepSpeed and Megatron. Next, we detail the training process, the design of our training corpus, and our data curation techniques, which we believe is a key ingredient to the success of the model. Finally, we discuss various evaluation results, as well as other interesting observations and new properties exhibited by MT-NLG. We demonstrate that MT-NLG achieves superior zero-, one-, and few-shot learning accuracies on several NLP benchmarks and establishes new state-of-the-art results. We believe that our contributions will help further the development of large-scale training infrastructures, large-scale language models, and natural language generations.","United States of America,United States of America",,,,4480.0,,Unreleased,,0.302,6663667.97106352,147455699.58722904
HyperCLOVA 82B,Language,"Language modeling/generation,Chat,Translation,Text classification","Boseop Kim, HyoungSeok Kim, Sang-Woo Lee, Gichang Lee, Donghyun Kwak, Dong Hyeon Jeon, Sunghyun Park, Sungju Kim, Seonhoon Kim, Dongpil Seo, Heungsub Lee, Minyoung Jeong, Sungjae Lee, Minsub Kim, Suk Hyun Ko, Seokhun Kim, Taeyong Park, Jinuk Kim, Soyoung Kang, Na-Hyeon Ryu, Kang Min Yoo, Minsuk Chang, Soobin Suh, Sookyo In, Jinseong Park, Kyungduk Kim, Hiun Kim, Jisu Jeong, Yong Goo Yeo, Donghoon Ham, Dongju Park, Min Young Lee, Jaewook Kang, Inho Kang, Jung-Woo Ha, Woomyoung Park, Nako Sung",API access,https://arxiv.org/abs/2109.04650,129.0,What Changes Can Large-scale Language Models Bring? Intensive Study on HyperCLOVA: Billions-scale Korean Generative Pretrained Transformers,2021-09-10,"NAVER,Search Solutions",82000000000.0,"""We introduce a Korean in-context large-scale LM with 82B parameters, i.e., HyperCLOVA. This is the first discovery on near
100B-scale non-English LM.""

According to media reports, HyperCLOVA has 204B parameters (i.e. a different version than in the paper)
https://m.koreaherald.com/view.php?ud=20210525000824 ",1.476e+23,"""For experiments in Section 4, the model trained with 150B is used for fair comparison, because not all models are finished training at the same iteration. However, experiments in Section 5.2 use the model trained with 300B tokens, as HyperCLOVA Studio provided the 39B and 82B models trained with 300B tokens.""

82e9 connections * 2 FLOP/connection * 300e9 tokens * 3 backward pass = 1.476e23 FLOP

Calculation using GPU time corroborates this:
- ""Our model is based on megatron-LM (Shoeybi et al., 2019) and trained on the NVIDIA Superpod, which includes 128 strongly clustered DGX servers with 1,024 A100 GPUs.""
- ""It takes 13.4 days to train a model with 82B parameters with 150B tokens."" Assume 300B tokens takes twice as long, 26.8 days.
- Assume the default of 30% utilization rate for large language models.

1024 A100 GPUs * 312e12 FLOP/second * 0.3 utilization * 26.8 days * 24 * 60 * 60 seconds/day = 2.219e+23 FLOP",Unspecified unreleased,"Blog corpus: 273.6 billion tokens
Cafe corpus (online community): 83.3 billion tokens
News corpus: 73.8 billion tokens
Comments (crawled from various platforms): 41.1 billion tokens
KiN (Korean QnA website): 27.3 billion tokens
Modu (collection of five datasets): 6.0 billion tokens
WikiEn, WikiJp (Foreign Wikipedia): 5.2 billion tokens
Other unspecified sources: 51.5 billion tokens",300000000000.0,"""However, experiments in Section 5.2 use the model trained with 300B tokens, as HyperCLOVA Studio provided the 39B and 82B models trained with 300B tokens.""

""We introduce HyperCLOVA, a large-scale
Korean in-context learning-based LM with
nearly 100B parameters, by constructing a
large Korean-centric corpus of 560B tokens.""

Based on tokenizing the Hyperclova article itself using OpenAI's tiktoken BPE tokenizer (https://github.com/openai/tiktoken), there are 3285 tokens for 1069 words - about 3 tokens per word.

This work uses a special tokenizer, but based on Figure 5 in Appendix E, the number of tokens seems similar between different tokenization methods.

Based on that, 5.6e11 Korean tokens ~= 1.9e11 words",643.2,see compute notes,NVIDIA A100,Confident,"GPT-3 shows remarkable in-context learning ability of large-scale language models (LMs) trained on hundreds of billion scale data. Here we address some remaining issues less reported by the GPT-3 paper, such as a non-English LM, the performances of different sized models, and the effect of recently introduced prompt optimization on in-context learning. To achieve this, we introduce HyperCLOVA, a Korean variant of 82B GPT-3 trained on a Korean-centric corpus of 560B tokens. Enhanced by our Korean-specific tokenization, HyperCLOVA with our training configuration shows state-of-the-art in-context zero-shot and few-shot learning performances on various downstream tasks in Korean. Also, we show the performance benefits of prompt-based learning and demonstrate how it can be integrated into the prompt engineering pipeline. Then we discuss the possibility of materializing the No Code AI paradigm by providing AI prototyping capabilities to non-experts of ML by introducing HyperCLOVA studio, an interactive prompt engineering interface. Lastly, we demonstrate the potential of our methods with three successful in-house applications.","Korea (Republic of),Korea (Republic of)",,,,1024.0,0.1995,Unreleased,"""We introduce HyperCLOVA Studio, an interactive prompt engineering interface which provides GUI and API interfaces like the OpenAI
playground1""",,,
HyperCLOVA 204B,Language,Language modeling/generation,,Hosted access (no API),https://aibusiness.com/nlp/south-korea-s-naver-unveils-hyperscale-ai-platform-language-model-with-more-parameters-than-gpt-3,92.0,"South Korea's Naver unveils 'hyperscale AI' platform, language model with more parameters than GPT-3",2021-09-10,NAVER,204000000000.0,https://www.navercorp.com/navercorp_/ir/announce/2023/NAVER_CEO%20letter%20to%20shareholders_Aug%202023_Eng.pdf,2.0000000001e+23,"Estimations for 82B model (marked as lower bound estimations)

""For experiments in Section 4, the model trained with 150B is used for fair comparison, because not all models are finished training at the same iteration. However, experiments in Section 5.2 use the model trained with 300B tokens, as HyperCLOVA Studio provided the 39B and 82B models trained with 300B tokens.""

82e9 connections * 2 FLOP/connection * 300e9 tokens * 3 backward pass = 1.476e23 FLOP

Calculation using GPU time corroborates this:
- ""Our model is based on megatron-LM (Shoeybi et al., 2019) and trained on the NVIDIA Superpod, which includes 128 strongly clustered DGX servers with 1,024 A100 GPUs.""
- ""It takes 13.4 days to train a model with 82B parameters with 150B tokens."" Assume 300B tokens takes twice as long, 26.8 days.
- Assume the default of 30% utilization rate for large language models.

1024 A100 GPUs * 312e12 FLOP/second * 0.3 utilization * 26.8 days * 24 * 60 * 60 seconds/day = 2.219e+23 FLOP",Unspecified unreleased,,560000000000.0,"https://twitter.com/arankomatsuzaki/status/1397583304610783238
https://venturebeat.com/ai/naver-trained-a-gpt-3-like-korean-language-model/",,,NVIDIA A100,Speculative,,Korea (Republic of),,,,,,Unreleased,,,,
Jurassic-1-Jumbo,Language,"Language modeling/generation,Chat","Opher Lieber, Or Sharir, Barak Lenz, Yoav Shoham",API access,https://uploads-ssl.webflow.com/60fd4503684b466578c0d307/61138924626a6981ee09caf6_jurassic_tech_paper.pdf,55.0,Jurassic-1: Technical Details and Evaluation,2021-08-11,AI21 Labs,178000000000.0,"""Jurassic-1 models come in two sizes, where the Jumbo version, at 178B parameters, is the largest and most sophisticated language model ever released for general use by developers.""",3.7e+23,"see here https://docs.google.com/document/d/1B8x6XYcmB1u6Tmq3VcbAtj5bzhDaj2TcIPyK6Wpupx4/edit

6 * 178B * 300B = 3.204000e+23",,,225000000000.0,"""Our model was trained with the conventional self-supervised auto-regressive training objective on 300B tokens drawn from publicly available resources""

1 token ~ 0.75 words",,,NVIDIA A100,Confident,"Jurassic-1 is a pair of auto-regressive language models recently released by AI21 Labs, consisting of J1-Jumbo, a 178B-parameter model, and J1-Large, a 7B-parameter model. We describe their architecture and training, and evaluate their performance relative to GPT-3. The evaluation is in terms of perplexity, as well as zero-shot and few-shot learning. To that end, we developed a zeroshot and few-shot test suite, which we made publicly available (https://github.com/ai21labs/ lm-evaluation) as a shared resource for the evaluation of mega language models.",Israel,,,,,,Unreleased,,,,
Wu Dao 2.0,"Multimodal,Language,Vision,Image generation","Image captioning,Chat,Image generation,Text-to-image,Language modeling/generation,Question answering,Visual question answering","Tang Jie, Zhai Jidong, Yang Hongxia, Chen Wenguang, Zheng Weimin, Ma Zixuan, He Jiaao, Qiu Jiezhong, Cao Huanqi, Wang Yuanwei, Sun Zhenbo, Zheng Liyan, Wang Haojie, Tang Shizhi, Feng Guanyu, Zeng Aohan, Zhong Runxin, Shi Tianhui, Du Zhengxiao,
Ding Ming, Tiago Antunes, Peng Jinjun, Lin Junyang Zhang Jianwei ",API access,https://www.engadget.com/chinas-gigantic-multi-modal-ai-is-no-one-trick-pony-211414388.html,,China's gigantic multi-modal AI is no one-trick pony,2021-05-31,Beijing Academy of Artificial Intelligence / BAAI,1750000000000.0,"""It's been trained on 1.75 trillion parameters""

MoE architecture, ""tens of thousands of experts""
https://keg.cs.tsinghua.edu.cn/jietang/publications/wudao-3.0-meta-en.pdf",1.54350000000001e+24,"It's a mixture-of-experts model, so all 1.75 trillion params were most likely not trained on each token.

""The parameter scale of Enlightenment 2.0 reached a record-breaking 1.75 trillion. According to reports, the new generation FastMoE technology is the key to the realization of the ""Trillion Model"" cornerstone of Enlightenment 2.0.""

Speculatively assuming 3% of parameters are active per forward pass and the model was trained for one epoch: 
6 FLOP / token / parameter * 1.75 * 10^12 parameters * 0.03  * 4.9 * 10^12 tokens [see dataset size notes] = 1.54e+24 FLOP",WuDao Corpora,"WuDao Corpora, as of version 2.0, was a large dataset constructed for training Wu Dao 2.0. It contains 3 terabytes of text scraped from web data, 90 terabytes of graphical data (incorporating 630 million text/image pairs), and 181 gigabytes of Chinese dialogue (incorporating 1.4 billion dialogue rounds).
https://en.wikipedia.org/wiki/Wu_Dao#WuDao_Corpora",4900000000000.0,"[tokens assumed[
""Bilingual (Cn and En) data: 4.9T text and images"" 

https://keg.cs.tsinghua.edu.cn/jietang/publications/wudao-3.0-meta-en.pdf",,,,Speculative,"Sporting 1.75 trillion parameters, Wu Dao 2.0 is roughly ten times the size of Open AI's GPT-3.",China,,,,,,,"based on this presentation weights and codes could be open sourced but links are unreachable from the US:
https://keg.cs.tsinghua.edu.cn/jietang/publications/wudao-3.0-meta-en.pdf",,,
GPT-3 175B (davinci),Language,"Text autocompletion,Language modeling/generation","Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei",API access,https://arxiv.org/abs/2005.14165,51687.0,Language Models are Few-Shot Learners,2020-05-28,OpenAI,174600000000.0,"""we train GPT-3, an autoregressive language model with 175 billion parameters""
Rather, it's 174.6 billion.
Table D.1: 174,600 million parameters",3.14e+23,"Table D.1
https://arxiv.org/abs/2005.14165","Common Crawl,WebText2,Wikipedia,Books1,Books2","Table 2.2 (other datasets also used)

Page 8 of the paper linked above says the model was trained â€œ41 shards of monthly CommonCrawl covering 2016 to 2019â€. The paper was published on May 28, 2020, so I assume based on the paperâ€™s description of the training data that it goes up to December 31, 2019.",374000000000.0,"From table 2.2, we determine that there are 410 + 19 + 12 + 55 + 3 = 499 billion tokens. 

We multiply this by 0.75 to give 374B words. 

3.74e11

========================
[Anson: I think the calculation below doesn't look at all the data, the CommonCrawl data only constitutes 60% of the data. Multiplying by 5/3 gives 4.75e11]

""The CommonCrawl data was downloaded from 41 shards of monthly CommonCrawl covering 2016 to 2019, constituting 45TB of compressed plaintext before filtering and 570GB after filtering, roughly equivalent to 400 billion byte-pair-encoded tokens. ""

Converted to words using 
http://extraconversion.com/data-storage/gigabits/gigabits-to-words.html

2.85e11",355.2,14.8 days according to https://arxiv.org/ftp/arxiv/papers/2104/2104.10350.pdf,NVIDIA Tesla V100 DGXS 32 GB,Confident,"Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.",United States of America,,,,10000.0,0.1966,Unreleased,"https://openai.com/blog/openai-api
",,4493219.860317459,232236674.505193
Meena,Language,"Text autocompletion,Chat","Dongling Xiao, Han Zhang, Yukun Li, Yu Sun, Hao Tian, Hua Wu, Haifeng Wang",Unreleased,https://arxiv.org/abs/2001.09977,991.0,Towards a Human-like Open-Domain Chatbot,2020-01-28,Google Brain,2600000000.0,"""We present Meena, a multi-turn open-domain chatbot trained end-to-end on data mined and filtered from public domain social media conversations. This 2.6B parameter neural network is simply trained to minimize perplexity of the next token.""",1.12e+23,"https://arxiv.org/ftp/arxiv/papers/2104/2104.10350.pdf
Table 4

In the paper: ""We trained our best model for 30 days on a TPUv3 Pod (2,048 TPU cores) on the Meena dataset containing 40B words (or 61B BPE tokens) [...] by the end of training, the model had traversed the full
training set 164 times (or epochs) and observed a total of about 10T tokens""

Hardware: 30 * 24 * 3600 * (2048/2) * 1.23e14 * 0.3 = 9.794e22
Ops counting: 6 * 10T * 2.6B = 1.56E23
Geometric mean: sqrt(9.79e22*1.56E23) = 1.24e23, very close to the figure in the link above.",,,40000000000.0,"""The final Meena dataset contains 341GB of text
(40B words)""

Converting from GB to words yields 6.8e10, which is in the same OOM",720.0,"We trained our best model for 30 days on a TPUv3 Pod (2,048 TPU cores)",Google TPU v3,Confident,"We present Meena, a multi-turn open-domain chatbot trained end-to-end on data mined and filtered from public domain social media conversations. This 2.6B parameter neural network is simply trained to minimize perplexity of the next token. We also propose a human evaluation metric called Sensibleness and Specificity Average (SSA), which captures key elements of a human-like multi-turn conversation. Our experiments show strong correlation between perplexity and SSA. The fact that the best perplexity end-to-end trained Meena scores high on SSA (72% on multi-turn evaluation) suggests that a human-level SSA of 86% is potentially within reach if we can better optimize perplexity. Additionally, the full version of Meena (with a filtering mechanism and tuned decoding) scores 79% SSA, 23% higher in absolute SSA than the existing chatbots we evaluated.",United States of America,,,,1024.0,,Unreleased,,0.34306622,724243.8744571933,11939150.88759962
AlphaStar,Games,StarCraft,"Oriol Vinyals,Igor Babuschkin,Wojciech M. Czarnecki,MichaÃ«l Mathieu,Andrew Dudzik,Junyoung Chung,David H. Choi,Richard Powell,Timo Ewalds,Petko Georgiev,Junhyuk Oh,Dan Horgan,Manuel Kroiss,Ivo Danihelka,Aja Huang,Laurent Sifre,Trevor Cai,John P. Agapiou,Max Jaderberg,Alexander S. Vezhnevets,RÃ©mi Leblond,Tobias Pohlen,Valentin Dalibard,David Budden,Yury Sulsky,James Molloy,Tom L. Paine,Caglar Gulcehre,Ziyu Wang,Tobias Pfaff,Yuhuai Wu,Roman Ring,Dani Yogatama,Dario WÃ¼nsch,Katrina McKinney,Oliver Smith,Tom Schaul,Timothy Lillicrap,Koray Kavukcuoglu,Demis Hassabis,Chris Apps,David Silver",Unreleased,https://www.deepmind.com/blog/alphastar-grandmaster-level-in-starcraft-ii-using-multi-agent-reinforcement-learning,4093.0,Grandmaster level in StarCraft II using multi-agent reinforcement learning,2019-10-30,DeepMind,139000000.0,"AlphaStar has 139 million weights, but only 55 million weights are required during inference.",1.0773400001e+23,"(Estimate by James Sanders, checked by Robi Rahman)
Fig 6 indicates that the learner uses 16 TPU ""devices"" which are 128 TPU cores total, which matches 4 TPUs per device, and 2 cores per TPU. Fig 6 indicates that 64 TPUs are used for training, and 64 are used for inference. (128 TPUs)*(12 agents)*(44 days)*(123 TFLOPS)*(0.3 utilization) = 2.15e23 FLOP (total), of which 1.07e23 FLOP are for training.",,,,"Multiple data types. First supervised learning, then other stuff",1056.0,"""Each agent was trained using 32 third-generation tensor
processing units (TPUs) over 44 days""",Google TPU v3,Confident,"Many real-world applications require artificial agents to compete and coordinate with other agents in complex environments. As a stepping stone to this goal, the domain of StarCraft has emerged as an important challenge for artificial intelligence research, owing to its iconic and enduring status among the most difficult professional esports and its relevance to the real world in terms of its raw complexity and multi-agent challenges. Over the course of a decade and numerous competitions1â€“3, the strongest agents have simplified important aspects of the game, utilized superhuman capabilities, or employed hand-crafted sub-systems4. Despite these advantages, no previous agent has come close to matching the overall skill of top StarCraft players. We chose to address the challenge of StarCraft using generalpurpose learning methods that are in principle applicable to other complex domains: a multi-agent reinforcement learning algorithm that uses data from both human and agent games within a diverse league of continually adapting strategies and counter-strategies, each represented by deep neural networks5,6. We evaluated our agent, AlphaStar, in the full game of StarCraft II, through a series of online games against human players. AlphaStar was rated at Grandmaster level for all three StarCraft races and above 99.8% of officially ranked human players.",United Kingdom of Great Britain and Northern Ireland,,,,384.0,,Open source,"Apache 2.0, training tools: https://github.com/google-deepmind/alphastar

training instructions here: https://github.com/google-deepmind/alphastar/blob/main/alphastar/unplugged/README.md ",,395061.2373658012,4541313.100598825
