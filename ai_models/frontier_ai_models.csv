Model,Domain,Task,Authors,Notability criteria,Notability criteria notes,Model accessibility,Link,Citations,Reference,Publication date,Organization,Parameters,Parameters notes,Training compute (FLOP),Training compute notes,Training dataset,Training dataset notes,Dataset size notes,Epochs,Inference compute (FLOP),Inference compute notes,Training time (hours),Training time notes,Training hardware,Approach,Compute cost notes,Compute sponsor categorization,Confidence,Abstract,Last modified,Created By,WikiText and Penn Treebank data,Exclude,Country (of organization),Base model,Finetune compute (FLOP),Finetune compute notes,Hardware quantity,Hardware utilization (MFU),Training cloud compute vendor,Training data center,Archived links,Batch size,Batch size notes,Organization categorization,Foundation model,Training compute lower bound,Training compute upper bound,Training chip-hours,Training code accessibility,Accessibility notes,Possibly over 1e23 FLOP,Training compute cost (2023 USD),Training dataset size,Sparsity,Utilization notes,Estimated over 1e25 FLOP,Power per GPU,Base model compute,Total compute - (base + finetune),API prices,Created,Inference code accessibility,Numerical format,Frontier model,Training power draw (W),FLOP/$,Hardware release date,Hardware age,Hardware FP32,Hardware TF32,Hardware count,Hardware TF16,Hardware FP16,Assumed precision,Assumed hardware performance (FLOP/s),Training compute estimation method,Hugging Face developer id,Post-training compute (FLOP),Post-training compute notes,Hardware maker,Model versions (benchmarks),Maybe over 1e25 FLOP,Updated dataset size,Distillation or synthetic data,Distillation or synthetic data compute,Distillation or synthetic data compute notes,Knowledge cutoff,Context window,Documents,Hardware utilization (HFU),Training compute cost (cloud),Training compute cost (upfront),Training dataset size (gradients)
Theseus,Robotics,Maze solving,Claude Shannon,Historical significance,,,https://www.technologyreview.com/2018/12/19/138508/mighty-mouse/,,Mighty Mouse,1950-07-02,Bell Laboratories,40.0,"The learned part is the maze configuration. There are 25 squares of the maze. The 16 squares to the left top corner have each one adjacent square down and one adjacent square up, for a total of 16*2 walls. We only need to count the 8 spare walls connecting the squares in the right side and the bottom side. In total there are 16*2+8 walls.",40,"The ""training"" consists on the mouse running around and checking each wall (assuming each relay switch is one operation).",,,Each wall Theseus bumps into is a datapoint,,,,,,,,,Industry,Confident,,2025-11-28 11:53:59+00:00,Robi Rahman,,,United States of America,,,,,,,,,,,Industry,,,,,,,,,Theseus,,,,,,,,2023-05-29T21:06:54.000Z,,,True,,Infinity,,NaN,,,0,,,FP32,,Other,,,,,,,True,,,,,,,,,,40
Perceptron Mark I,Other,Binary classification,F Rosenblatt,"Historical significance,Highly cited",First modern neural network ,,https://blogs.umass.edu/brain-wars/files/2016/03/rosenblatt-1957.pdf,1610.0,The Perceptron—a perceiving and recognizing automaton,1957-01-01,"Cornell Aeronautical Laboratory,Cornell University",1000.0,"""Figure 4.8 Illustration of the Mark 1 perceptron hardware. The photograph on the left shows how the inputs were obtained using a simple camera system in which an input scene, in this case a printed character, was illuminated by powerful lights, and an image focussed onto a 20 × 20 array of cadmium sulphide photocells, giving a primitive 400 pixel image. The perceptron also had a patch board, shown in the middle photograph, which allowed different configurations of input features to be tried. Often these were wired up at random to demonstrate the ability of the perceptron to learn without the need for precise wiring, in contrast to a modern digital computer. The photograph on the right shows one of the racks of adaptive weights. Each weight was implemented using a rotary variable resistor, also called a potentiometer, driven by an electric motor thereby allowing the value of the weight to be adjusted automatically by the learning algorithm.""

source: Bishop, Christopher M. (2006). Pattern Recognition and Machine Learning

The Perceptron had a 400-pixel visual input and 1000 neurons in the hidden layer. https://twitter.com/DiegoKuonen/status/1130352233223262208",694894.9377361819,"Extracted from AI and Compute (https://openai.com/blog/ai-and-compute/) charts by using https://automeris.io/WebPlotDigitizer/.

Additional experiment described in https://babel.hathitrust.org/cgi/pt?id=coo.31924004657973&seq=70 
- 400 input (20x20) - 512 hidden with 40 fixed connections each (not learned) - 1 output (learned)
Parameters: 512*1=512
Forward flop: 41*512=20992
Forward + “backward flop”: 43*512=22016 (only last layer was adjusted)
100*22016=2201600
",,,"Appendix II describes an experiment with 6 stimulus patterns

https://babel.hathitrust.org/cgi/pt?id=coo.31924004657973&seq=47 describes simulation experiments with ""X"" and ""E"" patterns using 100 total training stimuli",,,,,,,,,Industry,Likely,,2025-11-28 11:53:59+00:00,Robi Rahman,,,"United States of America,United States of America",,,,,,,,,,,"Academia,Academia",,,,,,,,,Perceptron Mark I,,,,,,,,2023-05-29T21:06:54.000Z,,,True,,Infinity,,NaN,,,0,,,FP32,,Third-party estimation,,,,,,,True,,,,,,,,,,100
Pandemonium (morse),Language,Morse translation,OG Selfridge,"Highly cited,Historical significance",,,https://aitopics.org/doc/classics:504E1BAC/,1453.0,Pandemonium: A Paradigm for Learning,1959-02-01,Massachusetts Institute of Technology (MIT),,"The paper mentions 11 function types. Unclear how many times they are called (number of ""demons"" in their Pandemonium implementation).",600000000,"The paper mentions using an IBM 704, which can execute up to 12,000 floating-point additions per second (https://wikiless.org/wiki/IBM_704). My best guess as to how long it ran for ranges between 1h to 2 days, which when plugged into guesstimate (https://www.getguesstimate.com/models/19625), i.e., taking the log mean, gives a mean estimate of 600M",,,??? Might need to make a guesstimate here.,,,,,,,,,Academia,Speculative,,2025-11-28 11:53:59+00:00,Robi Rahman,,,United States of America,,,,,,,,,,,Academia,,,,,,,,,Pandemonium (morse),,,,,,,,2023-05-29T21:06:54.000Z,,,True,,Infinity,,NaN,,,0,,,FP32,,Hardware,,,,,,,True,,,,,,,,,,
Samuel Neural Checkers,Games,Checkers,Arthur L. Samuel,"Highly cited,Historical significance",,,https://ieeexplore.ieee.org/abstract/document/5392560,5063.0,Some studies in machine learning using the game of checkers,1959-07-01,IBM,16.0,"""with 16 terms for generalization learning""

""Mention has been made several times of the procedure
for replacing terms in the scoring polynomial. The program, as it is currently running, contains 38 different
terms (in addition to the piece-advantage term), 16 of
these being included in the scoring polynomial at anyone
time and the remaining 22 being kept in reserve.""",428400000,"""it can learn to do this in a remarkably short period of time 8 or 10 hours of machine-playing time)""

""The availability of a larger and faster machine (the IBM 704), coupled with many detailed changes in the programming procedure, leads to a fairly interesting game being played, even without any learning.""

""The Type 704 is the first large-scale, commercially available computer to employ fully automatic floating point arithmetic commands. [...]. Floating point addition or subtraction operations require 84 microseconds.""

source: https://www.ibm.com/ibm/history/exhibits/mainframe/mainframe_PP704.html

""An idea of the learning ability of this procedure can be gained by analyzing an initial test series of 28 games""

""Each game averaged 68 moves (34 to a side), of which approximately 20 caused changes to be made in the scoring polynomial.""

10*3600*1000000/84=428571428",,,"Based on number of board positions

At the present time the memory tape contains something over 53,000 board positions (averaging 3.8 word search) which have been selected from a much larger
number of positions by means of the culling techniques
described. While this is still far from the number which
would tax the listing and searching procedures used in
the program, rough estimates, based on the frequency
with which the saved boards are utilized during normal
play (these figures being tabulated automatically), indicate that a library tape containing at least 20 times the
present number of board positions would be needed to
improve the midgame play significantly. At the present
rate of acquisition of new positions this would require
an inordinate amount of play and, consequently, of
machine time.",,,,,,,,,Industry,Likely,,2026-01-01 14:03:17+00:00,Robi Rahman,,,United States of America,,,,,,,,,,,Industry,,,,,,,,,Samuel Neural Checkers,,,,,,,,2023-05-29T21:06:54.000Z,,,True,,Infinity,,NaN,,,0,,,FP32,,,,,,,,,True,,,,,,,,,,
Perceptron (1960),Vision,Image classification,Frank Rosenblatt,Historical significance,,,https://www.semanticscholar.org/paper/Perceptron-Simulation-Experiments-Rosenblatt/ae76ce1ba27ac29addce4aab93b927e9bc7f7c67,394.0,Perceptron Simulation Experiments,1960-03-30,Cornell Aeronautical Laboratory,1000.0,""" The first program was designed to handle
up to 1000 A units, and a 72 by 72 sensory mosaic. It
was found that this large sensory system presented
stimuli with a fineness of grain considerably better than
the limits of discrimination of a thousand-unit percep-
tron, and at the same time, required an excessive
amount of time for stimulus transformations, since each
illuminated point in the stimulus must be transformed
individually into its image point.""",720000000,"4000 * 12000 * 15
from the text ""This program uses the IBM 704 computer to simulate per-ceptual learning, recognition, and spontaneous classification of visual stimuli in the perceptron,""
from https://en.wikipedia.org/wiki/IBM_704 The 704 can execute up to 12,000 floating-point additions per second.
"" For the first system, the computing time averaged about 15 seconds per stimulus cycle, ""
In Fig 10 we see up to 4000 stimuli",,,"from the text ""The two main simulation programs total about 5000 words each.""",,,,,,,,,,Speculative,"An experimental simulation program, which has been in progress at the Cornell Aeronautical Laboratory since 1957, is described. This program uses the IBM 704 computer to simulate perceptual learning, recognition, and spontaneous classification of visual stimuli in the perceptron, a theoretical brain model which has been described elsewhere. The paper includes a brief review of the organization of simple perceptrons, and theoretically predicted performance curves are compared with those obtained from the simulation programs, in several types of experiments, designed to study ""forced"" and ""spontaneous"" learning of pattern discriminations.",2025-11-28 11:53:59+00:00,Anonymous,,,United States of America,,,,,,,,,,,Academia,,,,,,,,,Perceptron (1960),,,,,,,,2024-01-26T16:39:23.000Z,,,True,,Infinity,,NaN,,,0,,,FP32,,Hardware,,,,,,,True,,,,,,,,,,100
ADALINE,Vision,Pattern recognition,Widrow and Hoff,Highly cited,,,https://isl.stanford.edu/~widrow/papers/c1960adaptiveswitching.pdf,6329.0,Adaptive switching circuits,1960-06-30,Stanford University,17.0,"""The machine's total experience is stored in the values of the weights a0,...,a16""",6600,"""The method of searching that has proven most useful is the method of steepest descent""

Apparently each pattern was only shown once to the system.

So the training compute is (forward pass compute) * (3 for backprop) * dataset size

This is a single layer (and single neuron) which does not require gradients w.r.t. inputs - 1:1 forward-backward ratio

",,,"""The best system, arrived at by slow precise adaptation on the full body of 100 noisy patterns, was able to classify these patterns as desired except for twelve errors.""

https://isl.stanford.edu/~widrow/papers/c1960adaptiveswitching.pdf",,33.0,We have 16 weights and a bias parameter. So 16 multadds and an add. The result is then thresholded to produce a binary output.,,,,,,Academia,Confident,,2025-11-28 11:53:59+00:00,Robi Rahman,,,United States of America,,,,,,,,,,,Academia,,,,,,,,,ADALINE,,,,,,,,2023-05-29T21:06:54.000Z,,,True,,Infinity,,NaN,,,0,,,FP32,,Operation counting,,,,,,,True,,,,,,,,,,100
Linear Decision Functions,Mathematics,Binary classification,W. Highleyman,"Historical significance,Highly cited",,,https://ieeexplore.ieee.org/document/4066882?denied=,,"Linear Decision Functions, with Application to Pattern Recognition",1962-06-01,Bell Laboratories,,,1559250,"0.5*45*35*1980 = 1559250 = 1.56e6
Trained using IBM punched cards, computation took 45 * 35s for all 10 digits (Section Estimating the Linear Decision Function).
Multiplications per second estimate based on publication year: 1.98e3 (regression on Nordhaus data).
Assumed utilization of 0.5",,,"""Fifty different people were asked, resulting in a sample size of 50 for each of the ten pattern classes. """,,,,0.4375,"""Forty-five hyperplanes are required in the complete linear decision function""
""About 35 seconds, on the average, was required to determine a hyperplane, given an initial position.""",,,,,Speculative,"Many pattern recognition machines may be considered to consist of two principal parts, a receptor and a categorizer. The receptor makes certain measurements on the unknown pattern to be recognized; the categorizer determines from these measurements the particular allowable pattern class to which the unknown pattern belongs. This paper is concerned with the study of a particular class of categorizers, the linear decision function. The optimum linear decision function is the best linear approximation to the optimum decision function in the following sense: 1) ""Optimum"" is taken to mean minimum loss (which includes minimum error systems). 2) ""Linear"" is taken to mean that each pair of pattern classes is separated by one and only one hyperplane in the measurement space. This class of categorizers is of practical interest for two reasons: 1) It can be empirically designed without making any assumptions whatsoever about either the distribution of the receptor measurements or the a priori probabilities of occurrence of the pattern classes, providing an appropriate pattern source is available. 2) Its implementation is quite simple and inexpensive. Various properties of linear decision functions are discussed. One such property is that a linear decision function is guaranteed to perform at least as well as a minimum distance categorizer. Procedures are then developed for the estimation (or design) of the optimum linear decision function based upon an appropriate sampling from the pattern classes to be categorized.",2025-11-28 11:53:59+00:00,Anonymous,,0,United States of America,,,,,,,,,,,Industry,,,,,,,,,Linear Decision Functions,,,,,,,,2024-05-05T11:36:53.000Z,,,True,,Infinity,,NaN,,,0,,,FP32,,Hardware,,,,,,,True,,,,,,,,,,500
Print Recognition Logic,Vision,Character recognition (OCR),"L. Kamentsky, Chao-Ning Liu",Historical significance,,,https://ieeexplore.ieee.org/document/5392331,,Computer-Automated Design of Multifont Print Recognition Logic,1963-01-01,IBM,,,22500000,"0.5*2.5*60*60*5000 = 22500000 = 2.25e7
Assumed utilization of 0.5
Trained for 2-3h on an IBM 7090 (from Introduction)
Estimated IBM 7090 at 5000 FLOP/s based on multiplications per second (Nordhaus, 2007)
Note: the Nordhaus estimate is very different from Wikipedia's estimate of 100000 FLOP/s, which cites a PowerPoint as source.",,,,,,,2.5,2-3h (from Introduction),,,,,Speculative,"A computer program has been written to design character recognition logic based on the processing of data samples. This program consists of two subroutines: (1) to search for logic circuits having certain constraints on hardware design, and (2) to evaluate these logics in terms of their discriminating ability over samples of the character set they are expected to recognize. An executive routine is used to apply these subroutines to select a complete logic with a given performance and complexity. This logic consists of 39 to 96 and gates connected to a shift register and a table look-up or resistance network comparison system.",2025-11-28 11:53:59+00:00,Anonymous,,0,United States of America,,,,,,,,,,,Industry,,,,,,,,,Print Recognition Logic,,,,,,,,2024-05-05T11:16:29.000Z,,,True,,Infinity,,NaN,,,0,,,FP32,,Hardware,,,,,,,True,,,,,,,,,,
LTE speaker verification system,Speech,Speech recognition (ASR),K. P. Li; J. E. Dammann; W. D. Chapman,Historical significance,,,https://pubs.aip.org/asa/jasa/article-abstract/40/5/966/754180/Experimental-Studies-in-Speaker-Verification-Using?redirectedFrom=fulltext,,"Experimental Studies in Speaker Verification, Using an Adaptive System",1966-11-01,IBM,2061.0,"2 connected systems, 1st level LTE and 2nd level LTE.
1st Level: 1810 parameters (""Thus, every 20 msec after the beginning of the utterance, the 15 filter amplitudes were each represented by a 12-bit code, resulting in a 180-bit time sample of the spectrum for that interval. Each time sample was fed to the first-level LTE's, which reduced it to a 10-bit code"")
2nd Level: 251 parameters (""This resulted in a 250-bit input pattern to the second level for the first half-second of each utterance. Each 250-bit pattern was then classified by the LTE into one of two classes"")",105917060,"1st and 2nd level system are trained separately, multiple versions of both are trained, I chose the largest clearly described training runs.

1st level LTE compute: 2*1810*28700=103894000=1.04e8
1st level steps: 28700 (""Only 287 samples were selected to train the 10 LTE's. The same algorithm was used as that used with the 100-class gain. Two LTE's
converged before 100 training passes."")

2nd level LTE compute: 2*251*4030=2023060=2e6
2nd level steps: 4030 (31 epochs, 130 training examples, see Table 3)

Total compute: 103894000+2023060=105917060=1.06e8 (assuming no backward pass since they didn't use backpropagation)
",,,"Split between both systems, 287 for 1st level, 130 for 2nd level.",131.0,,,,,,,,,Likely,"This paper describes an investigation of the capability of a two‐level adaptive linear threshold element (LTE) system to perform speaker discriminations. The study also includes an investigation of discriminating a speaker from an unknown population. The problem has been confined to the verification of an utterance as that of an expected informant. The environment of the experiments is discussed, and the experimental system is described. At the first level LTE, four different kinds of training have been developed for effective transformation and data reduction. At the second‐level LTE, different training conditions and different decision processes are investigated and evaluated. Over 90% accuracy is obtained in separating a known speaker from impostors.",2025-11-28 11:53:59+00:00,Lovis Heindrich,,,United States of America,,,,,,,,,,,Industry,,,,,,,,,LTE speaker verification system,,,,,,,,2024-05-22T06:51:51.000Z,,,True,,Infinity,,NaN,,,0,,,FP32,,Operation counting,,,,,,,True,,,,,,,,,,417
Neocognitron,Vision,Character recognition (OCR),"K Fukushima, S Miyake",Highly cited,,,https://link.springer.com/article/10.1007/BF00344251,5782.0,Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position,1980-04-01,NHK Broadcasting Science Research Laboratories,1140576.0,"""The synaptic connections from S-layers to C-layers
are fixed and unmodifiable. [...]
The numbers of excitatory cells in these seven layers are: 16x16 in U0, 16x16x24 in Us1, 10x10x 24 in Uc1, 8x8x24 in Us2, 6x 6x 24 in Uc2, 2x2x24 in Us3, and 24 in Uc3 
[...]
 the number of input synapses to each S-cell is 5 x 5 in layer Us1 and 5x5x24 in layers Us2 and Us3
[...]
The number of excitatory input synapses to each C-cell is 5x5 in layers Uc1 and Uc2, and is 2x2 in
layer Uc3
""

The number of synapses into each S-layer is:

S1: (16*16*24)*(5*5) 
S2: (8*8*24)*(5*5*24)
S3: (2*2*24)*(5*5*24)

We assume one parameter a per synapse into each cell in a S-layer, and one parameter b per each cell in a S-layer.",273738240,"""It does not necessarily mean that all of these input synapses are
always fully reinforced. In usual situations, only some of these input synapses are reinforced, and the rest of them remains in small values [...] Each of the five stimulus patterns has been presented 20 times to the network. By that time, self organization of the network has almost been completed.""

We multiply by 2 to account for multadds in the forward pass. 
There is no real backward pass, weights are only updated sparsely. Estimating 20% additional weight update compute compared to the forward pass:
2*1,140,576.0*1.2*100=273738240
",,,"""In order to self-organize the network, we have presented five stimulus patterns ""0"", ""1"", ""2"", ""3"", and ""4"", which are shown in Fig. 6""",,2281152.0,"2*1,140,576.0",,,,,,Industry,Confident,,2025-11-28 11:53:59+00:00,Robi Rahman,,,Japan,,,,,,,,,,,Industry,,,,,,,,,Neocognitron,,,,,,,,2023-05-29T21:06:54.000Z,,,True,,Infinity,,NaN,,,0,,,FP32,,Operation counting,,,,,,,True,,,,,,,,,,5
ASE+ACE,Robotics,Pole balancing,"Andrew G. Barto, Richard S. Sutton, and Charles W. Anderson","Highly cited,Historical significance",,,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6313077,4296.0,Neuronlike adaptive elements that can solve difficult learning control problems,1983-09-01,University of Massachusetts Amherst,324.0,"The system consists of two parts: ACE and ASE, each with 162 weights (=324 parameters). Found in Figures 2 and 3.",324000000,"324 * 2 * 500000 = 324000000 = 3.24e8. The calculation assumes ""compute per forward pass"" = ""number of parameters"" = ""compute per backward pass"". Their model only has a single layer and is trained with simple update rules instead of gradient descent. Training details are described in Section IX.

Note that this is the compute for a single run; they appear to have repeated training 10 times for the ASE+ACE system.",,,"""Runs consisted of 100 trials unless the run's duration exceeded 500 000 time steps (approximately 2.8 h of simulated real time)"" 
""Almost all runs of the ASE/ACE system [...], were terminated after 500 000"" (Section IX)",,,,2.8,"""Runs consisted of 100 trials unless the run's duration exceeded 500 000 time steps (approximately 2.8 h of simulated real time)"" 
""Almost all runs of the ASE/ACE system [...], were terminated after 500 000"" (Section IX)",,,,Academia,Likely,"It is shown how a system consisting of two neuronlike adaptive elements can solve a difficult learning control problem. The task is to balance a pole that is hinged to a movable cart by applying forces to the cart's base. It is argued that the learning problems faced by adaptive elements that are components of adaptive networks are at least as difficult as this version of the pole-balancing problem. The learning system consists of a single associative search element (ASE) and a single adaptive critic element (ACE). In the course of learning to balance the pole, the ASE constructs associations between input and output by searching under the influence of reinforcement feedback, and the ACE constructs a more informative evaluation function than reinforcement feedback alone can provide. The differences between this approach and other attempts to solve problems using neurolike elements are discussed, as is the relation of this work to classical and instrumental conditioning in animal learning studies and its possible implications for research in the neurosciences.",2025-11-28 11:53:59+00:00,Robi Rahman,,,United States of America,,,,,,,,,,,Academia,,,,,,,,,ASE+ACE,,,,,,,,2023-05-29T21:06:54.000Z,,,True,,Infinity,,NaN,,,0,,,FP32,,Operation counting,,,,,,,True,,,,,,,,,,500000
Distributed representation NN,Other,Representation learning,Geoffrey E. Hinton,"Historical significance,Highly cited",,,https://www.cs.toronto.edu/~hinton/absps/families.pdf,,Learning distributed representations of concepts.,1986-08-15,Carnegie Mellon University (CMU),432.0,"Parameters: 24*6 + 12*6 + 12*12 + 12*6 =432
""Figure 5: The activity levels in a five-layer network after it has learned. The bottom layer has 24 input units on the left for representing person 1 and 12 units on the right for representing the relationship. The white squares inside these two groups show the activity levels of the units. There is one active unit in the first group (representing Colin) and one in the second group (representing has-aunt). Each of the two groups of input units is totally connected to its own group of 6 units in the second layer. These two groups of 6 must learn to encode the input terms as distributed patterns of activity. The second layer is totally connected to the central layer of 12 units, and this layer is connected to the penultimate layer of 6 units.""
",388800000," 2*432*3*1500*100=388800000=3.9e8
""After 1500 sweeps through all 100 training examples the weights were very stable """,,,"""After 1500 sweeps through all 100 training examples the weights were very stable """,1500.0,,,,,,,,,Confident,,2025-11-28 11:53:59+00:00,Lovis Heindrich,,,United States of America,,,,,,,,,,,Academia,,,,,,,,,Distributed representation NN,,,,,,,,2024-05-28T09:44:15.000Z,,,True,,Infinity,,NaN,,,0,,,FP32,,Operation counting,,,,,,,True,,,,,,,,,,100
MLP with back-propagation,Mathematics,Triplet completion,"Rumelhart, David E.; Hinton, Geoffrey E.; Williams, Ronald J.",Highly cited,,,https://www.semanticscholar.org/paper/Learning-representations-by-back-propagating-errors-Rumelhart-Hinton/052b1d8ce63b07fec3de9dbb583772d860b7c769,29621.0,Learning representations by back-propagating errors,1986-10-01,"University of California San Diego,Carnegie Mellon University (CMU)",720.0,"Architecture in Figure 3: 

24+12 input -> 6 + 6 hidden -> 12 hidden -> 6 hidden -> 24 output

Parameters: 6*24+6*12+12*12+6*12+24*12=720",673920000,"We assume that the number of mult-adds per pass is equal to the number of parameters -> 2*720=1440 FLOP per forward pass.

""We trained the network for 1500 sweeps""
There are 104 relationship triplets (""[...] of the 104 possible triplets"")

FLOP: 1500*104*3*1440=673920000

",,,"There are 104 relationship triplets (""[...] of the 104 possible triplets"")",,1440.0,We assume that the number of mult-adds per pass is equal to the number of parameters -> 2*720=1440 FLOP per forward pass.,,,,Unsupervised,,Academia,Confident,,2026-01-01 14:03:17+00:00,Robi Rahman,,,"United States of America,United States of America",,,,,,,,,,,"Academia,Academia",,,,,,,,,Back-propagation,,,,,,,,2023-05-29T21:06:54.000Z,,,True,,Infinity,,NaN,,,0,,,FP32,,Operation counting,,,,,,,True,,,,,,,,,,104
NetTalk (transcription),Speech,Speech synthesis,"TJ Sejnowski, CR Rosenberg","Highly cited,Historical significance",,,http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=03A3D3EDF0BAF35405ABCF083411B55E?doi=10.1.1.154.7012&rep=rep1&type=pdf,2558.0,Parallel Networks that Learn to Pronounce English Text,1987-06-06,Princeton University,18629.0,"""The connections in the network are specified by a total of 18629
weight parameters (including a variable threshold for each unit)""",28328002560,18629 params * 2 FLOP/param * (3 for forward + backward pass) * 55 epochs * 1024 words/epoch * 4.5 letters/word,,,"We used the first two pages of transcriptions, which contained 1024 words from a child in firstgrade",55.0,37258.0,18629 params * 2 FLOP/param = 37258,,,,,,Academia,Confident,,2025-11-28 11:53:59+00:00,Robi Rahman,,,United States of America,,,,,,,,,,,Academia,,,,,,,,,NetTalk (transcription),,,,,,,,2023-05-29T21:06:54.000Z,,,True,,Infinity,,NaN,,,0,,,FP32,,Operation counting,,,,,,,True,,,,,,,,,,5120
NetTalk (dictionary),Speech,Speech synthesis,"TJ Sejnowski, CR Rosenberg","Highly cited,Historical significance",,,http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=03A3D3EDF0BAF35405ABCF083411B55E?doi=10.1.1.154.7012&rep=rep1&type=pdf,2558.0,Parallel Networks that Learn to Pronounce English Text,1987-06-06,Princeton University,18629.0,"""The connections in the network are specified by a total of 18629 weight parameters (including a variable threshold for each unit)""",27664065000,18629 params * 2 FLOP/param * (3 for forward + backward pass) * 55 epochs * 1000 words/epoch * 4.5 letters/word (estimated number of letters),,,"""A subset of the 1000 most commonly occurring words was selected from this dictionary based on frequency counts in the Brown corpus""",55.0,37258.0,18629 params * 2 FLOP/param = 37258,,,,,,,Confident,,2025-11-28 11:53:59+00:00,Anonymous,,,United States of America,,,,,,,,,,,Academia,,,,,,,,,NetTalk (dictionary),,,,,,,,2023-10-17T15:18:03.000Z,,,True,,Infinity,,NaN,,,0,,,FP32,,Operation counting,,,,,,,True,,,,,,,,,,5000
Translation-invariant MLP,,Object recognition,Geoffrey E. Hinton,Historical significance,,,https://www.cs.toronto.edu/~hinton/absps/parle.pdf,,Learning Translation Invariant Recognition in a Massively Parallel Network,1987-06-15,Carnegie Mellon University (CMU),816.0,"Network: 12-60-6-16
Weights: 6*60+60*6+6*16=816
Layer 2 was only sparsely connected to input layer",18032947200,FLOPs: 2*816*3*160*23020=18032947200=1.8e10,,,,23020.0,,,,,,,,,Confident,,2025-11-28 11:53:59+00:00,Lovis Heindrich,,,United States of America,,,,,,,,,,,Academia,,,,,,,,,Translation-invariant MLP,,,,,,,,2024-10-03T10:55:55.000Z,,,True,,Infinity,,NaN,,,0,,,FP32,,Operation counting,,,,,,,True,,,,,,"[{'id': 'attUkDbjCY0y1OnD9', 'url': 'https://v5.airtableusercontent.com/v3/u/49/49/1769875200000/XvidAEIyKSYcBT5q6lLKUA/90Mr4NhX836BH5CbGyVseY6o-qy-Qgo5ZghWutjKc5Yozmyhei1yE5gDefVntO49Pw5D2e46-qhRaCajgfmi6-rBaepTawPykco_6wMxqIAFekbELDzOfkBxMzgMUC9OeD06oEAjCMnLad3Kufceqcf_IiFhSVbb2XlOTJuibxafg7BUq3dRu4YNOa-ogs2ToQW5YgKMt6-dTHkxx_EcS-RXdllPZg6UhyQnv7WpAKbpWO14abWsR517dNw2jUv6/2vahduWwV2i-Hje8RxEMFIjPcymHsOPPUI6Nk_Jl9O0', 'filename': 'Learning translation invariant recognition in a massively parallel networks.pdf', 'size': 1150574, 'type': 'application/pdf', 'thumbnails': {'small': {'url': 'https://v5.airtableusercontent.com/v3/u/49/49/1769875200000/zuUBPTstIH4Aff6tnuFGFw/48aFsTFAMHNoiwaFatiqNKod7tNLTbVAxPCp-1i020QORBO1ONHRGtV8v3ef_GeaU-EFwKYMX-A6spgKYv0r67rZxS6bOtSGi7-612lHVOwPMyZVmee86VuNjh9aCl--DQpzqht5iHsEG32TrEzgrA/9bK67QqRBYwjXqa4U-i5sBmE39K8WumHyA9nv9OAGMk', 'width': 25, 'height': 36}, 'large': {'url': 'https://v5.airtableusercontent.com/v3/u/49/49/1769875200000/gv0tc5aWILOGiy6vCRSaBA/rYhYq-TaWDiyDtou5cUb7zQXzzhkkYtnO0LUudFofOTx5QCXW7kFB57ZwkVnbPCJ25lTIy5UljBaFOwLO_99AvFeADLhbqc2PiZfJK-XR7MSGOzNToU01S45WS-ry0OMcVEen8pP_N6AckvQveEg1A/NM9rvbUm2qGRX4ZHepWQZZCAuYzM7vKFmUjAb4-9Jo0', 'width': 512, 'height': 748}}}]",,,,160
Invariant image recognition,Vision,Representation learning,"V. Cruz, G. Cristóbal, T. Michaux, S. Barquin",Historical significance,,,https://ieeexplore.ieee.org/document/118669,,Invariant image recognition using a multi-network neural model,1989-06-18,Complutense University of Madrid,,,27000000000,"0.5*6*60*60*2.5e6 = 27000000000 = 2.7e10
Trained for 6h on a SUN-4 (section 4)
Assumed utilization of 0.5
SUN-4 is estimated at 2.5e6 FLOP/s (Nordhaus, 2007)",,,,,,,6.0,Section 4,,,,,Confident,"A new model which permits visual patterns to be invariant to affine transforms (translations, rotations, and dimensions) is presented. A training multilayer fully connected network of ADALINE neurons is proposed as a preprocessing step for invariant image extraction. A second neural network has been trained by the popular backpropagation algorithm for recovering the real image without distortions. First, the sample invariants are obtained by the preprocessing network. In the second step, the general invariant that includes all the sample invariants is computed. Afterward, the reordered sample invariants are input to a multilayer neural network trained by the backpropagation algorithm. The original image, without distortions, is obtained in the output of this system. Several test images have been computed, and evaluation of the results shows that in the case of images with intrinsic perceptual similarity, the learning procedure leads to a global invariant extraction that requires less computational effort in comparison with an arbitrary training selection. After the training process, this system is able to extract the generalized invariant image from an arbitrary picture recovering the input image without distortions.<<ETX>>",2025-11-28 11:53:59+00:00,Anonymous,,0,Spain,,,,,,,,,,,Academia,,,,,,,,,Invariant image recognition,,,,,,,,2024-05-05T10:44:24.000Z,,,True,,Infinity,,NaN,,,0,,,FP32,,Hardware,,,,,,,True,,,,,,,,,,
Handwritten digit recognition network,Vision,Digit recognition,"Yann LeCun, B. Boser, J. Denker, D. Henderson, R. Howard, W. Hubbard, L. Jackel",Historical significance,,,https://www.semanticscholar.org/paper/Handwritten-Digit-Recognition-with-a-Network-LeCun-Boser/86ab4cae682fbd49c5a5bedb630e5a40fa7529f6,,Handwritten Digit Recognition with a Back-Propagation Network,1989-11-27,AT&T,2578.0,"""In summary, the network has 4635 units, 98442 connections, and 2578 independent parameters.“",181440000000,"1.4e6 * 3 * 24 * 60* 60 * 0.5 = 181440000000 = 1.81e11
""A complete training session (30 passes through the training set plus test) takes about 3 days on a SUN SPARCstation 1""
Sparcstation 1 has an estimated compute of 1.4 MFLOPS (source: https://ieeexplore.ieee.org/document/63671 )",,,"""After 30 training passes the error rate on training set (7291 handwritten plus 2549 printed digits)""",30.0,,,72.0,"""A complete training session (30 passes through the training set plus test) takes about 3 days on a SUN SPARCstation 1""",,,,,Confident,"We present an application of back-propagation networks to handwritten digit recognition. Minimal preprocessing of the data was required, but architecture of the network was highly constrained and specifically designed for the task. The input of the network consists of normalized images of isolated digits. The method has 1% error rate and about a 9% reject rate on zipcode digits provided by the U.S. Postal Service.",2025-11-28 11:53:59+00:00,Lovis Heindrich,,,United States of America,,,,,,,,,,,Industry,,,,,,,,,Handwritten Digit Recognition System,,,,,,,,2024-05-15T11:30:09.000Z,,,True,,Infinity,,NaN,,,0,,,FP32,,Hardware,,,,,,,True,,,,,,,,,,9840
Zip CNN,Vision,Character recognition (OCR),Y. LeCun B. Boser J. S. Denker D. Henderson R. E. Howard W. Hubbard L. D. Jackel,Highly cited,,,https://ieeexplore.ieee.org/document/6795724,11725.0,Backpropagation applied to handwritten zip code recognition,1989-12-01,"AT&T,Bell Laboratories",9760.0,"""In summary, the network has 1256 units, 64,660 connections, and 9760 independent parameters""",1496338054440,"Its a deep CNN so we assume a backward-forward ratio of 2:1
 2*64660*3*23*167693=1496338054440
""The network was trained for 23
passes through the training set (167,693 pattern presentations).""",Buffalo zips,"""The data base used to train and test the network consists of 9298 segmented numerals digitized from handwritten zip codes
that appeared on U.S. mail passing through the Buffalo, NY post office.
Examples of such images are shown in Figure 1. The digits were written
by many different people, using a great variety of sizes, writing styles,
and instruments, with widely varying amounts of care; 7291 examples
are used for training the network and 2007 are used for testing the generalization performance. One important feature of this data base is that
both the training set and the testing set contain numerous examples that
are ambiguous, unclassifiable, or even misclassified. ""","The digits were written
by many different people, using a great variety of sizes, writing styles,
and instruments, with widely varying amounts of care; 7291 examples
are used for training the network and 2007 are used for testing the generalization performance",,129320.0,Roughly twice the number of connections,,,,,,Industry,Confident,,2026-01-01 14:02:55+00:00,Robi Rahman,,,"United States of America,United States of America",,,,,,,,,,,"Industry,Industry",,,,,,,,,Zip CNN,,,,,,,,2023-05-29T21:06:54.000Z,,,True,,Infinity,,NaN,,,0,,,FP32,,Operation counting,,,,,,,True,,,,,,,,,,7291
NETtalk reimplementation,Speech,Text-to-speech (TTS),"Thomas G. Dietterich, Hermann Hild, Ghulum Bakiri","Historical significance,Training cost",,,https://www.sciencedirect.com/science/article/abs/pii/B9781558601413500079,,A Comparative Study of ID3 and Backpropagation for English Text-to-speech Mapping,1990-06-01,Oregon State University,27480.0,"203*120+120*26=27480
“203 input units, 120 hidden units, and 26 output units”",35811936000,Updated FLOP estimate: 2*27480*3*1000*7.24*30=35811936000=3.6e10,,,"“This training set was further subdivided to extract smaller training sets of 1000, 800, 400, 200, 100, and 50 words“",30.0,,,,,,,,,Confident,"The performance of the error backpropagation (BP) and ID3 learning algorithms was compared on the task of mapping English text to phonemes and stresses. Under the distributed output code developed by Sejnowski and Rosenberg, it is shown that BP consistently out-performs ID3 on this task by several percentage points. Three hypotheses explaining this difference were explored: (a) ID3 is overfitting the training data, (b) BP is able to share hidden units across several output units and hence can learn the output units better, and (c) BP captures statistical information that ID3 does not. We conclude that only hypothesis (c) is correct. By augmenting ID3 with a simple statistical learning procedure, the performance of BP can be approached but not matched. More complex statistical procedures can improve the performance of both BP and ID3 substantially. A study of the residual errors suggests that there is still substantial room for improvement in learning methods for text-to-speech mapping.",2025-11-28 11:53:59+00:00,Lovis Heindrich,,,United States of America,,,,,,,,,,,Academia,,,,,,,,,NETtalk reimplementation,,,,,,,,2024-10-03T09:52:42.000Z,,,True,,Infinity,,NaN,,,0,,,FP32,,Operation counting,,,,,,,True,,,,,,,,,,7242
SexNet compression,Vision,Image representation,"B. Golomb, D. T. Lawrence, T. Sejnowski",Historical significance,,,https://www.semanticscholar.org/paper/SEXNET%3A-A-Neural-Network-Identifies-Sex-From-Human-Golomb-Lawrence/cbf90aa78fea0c8a1028705d92bc4bc7808ddeeb,,SEXNET: A Neural Network Identifies Sex From Human Faces,1990-10-01,,72940.0,"900*40*2+40+900=72940
“Images sampled at 30x30 were compressed using a 900x40x900 fully connected back-propagation network”",78775200000,"2*72940*3*90*2000=78775200000
“The compression network trained for 2000 runs on each of 90 faces”",,,“The compression network trained for 2000 runs on each of 90 faces”,2000.0,,,,,,Supervised,,,Confident,"Sex identification in animals has biological importance. Humans are good at making this determination visually, but machines have not matched this ability. A neural network was trained to discriminate sex in human faces, and performed as well as humans on a set of 90 exemplars. Images sampled at 30×30 were compressed using a 900×40×900 fully-connected back-propagation network; activities of hidden units served as input to a back-propagation ""SexNet"" trained to produce values of 1 for male and 0 for female faces. The network's average error rate of 8.1% compared favorably to humans, who averaged 11.6%. Some SexNet errors mimicked those of humans.",2025-11-28 11:53:59+00:00,Lovis Heindrich,,,,,,,,,,,,,,,,,,,,,,,SexNet compression,,,,,,,,2024-06-04T12:20:15.000Z,,,True,,Infinity,,NaN,,,0,,,FP32,,Operation counting,,,,,,,True,,,,,,,,,,81000
Weight Decay,Speech,Speech synthesis,"A. Krogh, J. Hertz","Highly cited,Historical significance",,,https://www.semanticscholar.org/paper/A-Simple-Weight-Decay-Can-Improve-Generalization-Krogh-Hertz/48e1de7d085808004d5f0493d486669a3d2930b5,,A Simple Weight Decay Can Improve Generalization,1991-12-02,,8386.0," 7*26*40+40+40*26+26=8386
""The network had 7 x 26 input units, 40 hidden units and 26 output units""",75474000000,"2*8386*3*1500000=75474000000=7.55e10
""It was trained on 400 to 5000 random words from the data base of around 20.000 words,""
""The top full line corresponds to the generalization error after 300 epochs""
",,NetTalk dataset of 20000 words,"""It was trained on 400 to 5000 random words from the data base of around 20.000 words,""",300.0,,,,,,,,,Confident,"It has been observed in numerical simulations that a weight decay can improve generalization in a feed-forward neural network. This paper explains why. It is proven that a weight decay has two effects in a linear network. First, it suppresses any irrelevant components of the weight vector by choosing the smallest vector that solves the learning problem. Second, if the size is chosen right, a weight decay can suppress some of the effects of static noise on the targets, which improves generalization quite a lot. It is then shown how to extend these results to networks with hidden layers and non-linear units. Finally the theory is confirmed by some numerical simulations using the data from NetTalk.",2025-11-28 11:53:59+00:00,Lovis Heindrich,,,,,,,,,,,,,,,,,,,,,,,Weight Decay,,,,,,,,2024-06-09T07:26:21.000Z,,,True,,Infinity,,NaN,,,0,,,FP32,,Operation counting,,,,,,,True,,,,,,,,,,25000
TD-Gammon,Games,Backgammon,G Tesauro,"Highly cited,Historical significance",,,https://papers.nips.cc/paper/1991/file/68ce199ec2c5517597ce0a4d89620f55-Paper.pdf,1344.0,Practical Issues in Temporal Difference Learning,1992-05-01,IBM,25000.0,"""The best performance was obtained with a network containing 80 hidden units and over 25,000 weights.""",18232157622832.703,"Extracted from AI and Compute (https://openai.com/blog/ai-and-compute/) charts by using https://automeris.io/WebPlotDigitizer/.

OpenAI estimate: 1.8e13

Hardware estimate (likely overestimates due to simulation effort)
""on an IBM RS/6000 workstation, the smallest network was trained in several hours, while the largest net required two weeks of simulation time.""
IBM RS/6000 achieves 1.5 GFLOPS on Linpack (https://link.springer.com/rwe/10.1007/978-0-387-09766-4_232) 
14*24*60*60*0.5*1500000000=9.072e+14

Operation counting estimate: 
Forward FLOP: 50000
Each legal move had to be evaluated separately, assuming an average of 10 move options (+2 for backward passes of the chosen move):
50000*12*300000*21=3.78e+12

Keeping the OpenAI estimate as the median estimate. ",,,"""This network was trained
for over 300,000 training games""

Each backgammon game has an avg of around 21 movements
https://www.bkgm.com/rgb/rgb.cgi?view+712",,50000.0,,,,,,,Industry,Speculative,,2025-11-28 11:53:59+00:00,Robi Rahman,,,United States of America,,,,,,,,,,,Industry,,,,,,,,,TD-Gammon,,,,,,,,2023-05-29T21:06:54.000Z,,,True,,Infinity,,NaN,,,0,,,FP32,,"Third-party estimation,Operation counting,Hardware",,,,,,,True,,,,,,,,,,6300000
Siamese-TDNN,Vision,Image classification,"J. Bromley, James W. Bentz, L. Bottou, Isabelle M Guyon, Yann LeCun, C. Moore, Eduard Säckinger, Roopak Shah","Historical significance,Highly cited",,,https://www.semanticscholar.org/paper/Signature-Verification-Using-A-%22Siamese%22-Time-Delay-Bromley-Bentz/997dc5d9a058753f034422afe7bd0cc0b8ad808b,,"Signature Verification using a ""Siamese"" Time Delay Neural Network",1993-08-01,Bell Laboratories,744.0,"""The input is 8 by 200 units, the first convolutional layer is 6 by 192 units with each unit's receptive field covering 8 by 9 units of the input. The first averaging layer is 6 by 64 units, the second convolution layer is 4 by 57 with 6 by 8 receptive fields and the second averaging layer is 4 by 19""
""Two separate sub-networks based on Time Delay Neural Networks (Lang and Hinton, 1988, Guyon et al. 1990) act on each input pattern to extract features,""
""All weights could be learnt, but the two sub-networks were constrained to have identical weights.""
L1: H=1, W=200, C=8, K=9, D=6
L2: H=1, W=64, C=6, K=8, D=4
Parameters:  7*9*8+5*8*6=744",12869570138112,"8073216*3*7701*69=12869570138112=1.29e13
Forward pass flop: 2*(2*200*200*8*6+2*64*64*6*4)=8073216
""We used up to 7,701 signature pairs""
Epochs: 69 (Table 1)
",,"""In total, 219 people signed between 10 and 20 signatures each, 145 signed genuines, 74 signed forgeries."" ""We used up to 7,701 signature pairs""","""We used up to 7,701 signature pairs""",69.0,,,,,,,,,Likely,"This paper describes an algorithm for verification of signatures written on a pen-input tablet. The algorithm is based on a novel, artificial neural network, called a ""Siamese"" neural network. This network consists of two identical sub-networks joined at their outputs. During training the two sub-networks extract features from two signatures, while the joining neuron measures the distance between the two feature vectors. Verification consists of comparing an extracted feature vector with a stored feature vector for the signer. Signatures closer to this stored representation than a chosen threshold are accepted, all other signatures are rejected as forgeries.",2025-11-28 11:53:59+00:00,Lovis Heindrich,,,United States of America,,,,,,,,,,,Industry,,,,,,,,,Siamese-TDNN,,,,,,,,2024-06-12T11:57:30.000Z,,,True,,Infinity,,NaN,,,0,,,FP32,,Operation counting,,,,,,,True,,,,,,,,,,7701
Predictive Coding NN,Language,Language modeling,"J. Schmidhuber, Stefan Heil",Historical significance,,,https://proceedings.neurips.cc/paper/1994/file/5705e1164a8394aace6018e27d20d237-Paper.pdf,,Predictive Coding with Neural Nets: Application to Text Compression,1994-12-02,Technical University of Munich,206910.0,"5*80*430+430+430*80+80=206910
""P has nk input units and k output units. n is called the ""time-window size""
""Note that the time-window was quite small (n = 5).""
""alphabet consisted of k = 80 possible characters""
""P had 430 hidden units""",18621900000000,"2*206910*3*15000000=18621900000000=1.86e13
""The training phase consisted of 25 sweeps through the training set""",,,"Training dataset: 15000*40=600000
""The training set for the predictor was given by a set of 40 articles from the newspaper Miinchner M erkur, each containing between 10000 and 20000 characters.""",25.0,,,,,,,,,Confident,"To compress text files, a neural predictor network P is used to approximate the conditional probability distribution of possible ""next characters"", given n previous characters. P's outputs are fed into standard coding algorithms that generate short codes for characters with high predicted probability and long codes for highly unpredictable characters. Tested on short German newspaper articles, our method outperforms widely used Lempel-Ziv algorithms (used in UNIX functions such as ""compress"" and ""gzip"").",2025-11-28 11:53:59+00:00,Lovis Heindrich,,,Germany,,,,,,,,,,,Academia,,,,,,,,,Predictive Coding NN,,,,,,,,2024-06-12T08:03:47.000Z,,,True,,Infinity,,NaN,,,0,,,FP32,,Operation counting,,,,,,,True,,,,,,,,,,600000
NeuroChess,Games,Chess,S. Thrun,"Historical significance,Highly cited",,,https://www.semanticscholar.org/paper/Learning-to-Play-the-Game-of-Chess-Thrun/4bc7a6dcb9e0e6c7a26800532e2a00f5572eea47,,Learning to Play the Game of Chess,1994-12-02,,72251.0,"""Prior to learning an evaluation function, the model M (175 input, 165 hidden, and 175 output units)"" = 58,090 parameters
""NeuroChess then learns an evaluation network V (175 input units, 0 to 80 hidden units, and one output units)."" = 14,161 parameters
Total: 58,090 + 14,161 = 72,251",858730812676,"Lower bound: 0.3*2*24*60*60*1400000=72576000000=7.26e10
Upper bound: 0.3*14*24*60*60*1400000*20=10160640000000=1.02e13
Geometric mean: 858730812676=8.59e11 (speculative)
""Thus far, experiments lasted for 2 days to 2 weeks on I to 20 SUN Sparc Stations. ""
SparcStation has 1.4 MFLOPS (https://ieeexplore.ieee.org/document/63671)
",,,"""is trained using a database of 120,000 expert games.""",,,,,,,,,,Speculative,"This paper presents NeuroChess, a program which learns to play chess from the final outcome of games. NeuroChess learns chess board evaluation functions, represented by artificial neural networks. It integrates inductive neural network learning, temporal differencing, and a variant of explanation-based learning. Performance results illustrate some of the strengths and weaknesses of this approach.",2025-11-28 11:53:59+00:00,Lovis Heindrich,,,,,,,,,,,,,,,,,,,,,,,NeuroChess,,,,,,,,2024-06-12T14:26:02.000Z,,,True,,Infinity,,NaN,,,0,,,FP32,,Hardware,,,,,,,True,,,,,,,,,,9600000
MUSIC perceptron,Vision,Image completion,M. Kocheisen; U.A. Muller; G. Troster,Historical significance,,,https://ieeexplore.ieee.org/document/549237,,A neural network for grey level and color correction used in photofinishing,1996-06-03,,13607.0,230*55+56*15+16*6+7*3=13607 (Figure 2),881733600000,"2*13607*3*10800000=881733600000=8.8e11
Training steps: 400*27000=10800000
""After 400 epochs the error of the network""",,,"“The training experiments were carried out on a database of 30,000 photos. Therefor the database was split into ten sets. Nine of them were used for the training and one for the testing.”",400.0,,,,,,,,,Confident,"The application of a multilayer perceptron for color and gray level correction in the field of photofinishing is presented. It is shown, that a neural network can improve the overall performance of a state of the art photo printer. The improved correction ability will reduce the number of unsalable pictures and thus lowers the production costs for the photo laboratory. The training experiments were carried out on a database of 30,000 photos using the MUSIC parallel supercomputer. The MUSIC system made it possible, for the first time, to process this large database in a reasonable time.",2025-11-28 11:53:59+00:00,Lovis Heindrich,,,,,,,,,,,,,,,,,,,,,,,MUSIC perceptron,,,,,,,,2024-06-11T13:07:09.000Z,,,True,,Infinity,,NaN,,,0,,,FP32,,Operation counting,,,,,,,True,,,,,,"[{'id': 'attWAYTYN7O3Oyh3T', 'url': 'https://v5.airtableusercontent.com/v3/u/49/49/1769875200000/WlLPQ9xkEompSK5StqEjSA/HQ-rF2f5nWs13kfJ-nsoULrbSu_DeW86BPWdb9I9CXswscfGCs2dEiWBxa9YVN_iUqQLRWZerZNpPWQ3381wtaG1279XBGR8hp33FvP1PCbnDOiBnAHBI2eLyflKVTmYXoaJOOyAZerl5rpfeWJwTKRUXhYGuEdnt6ik0824_U4BxwvTZiYGUR16vqnqdxiTf9KTzUKkoHhkNegCx8-yJDLH9hWDb08u-GO87oh84hU/jHkXg4GULRxriz99VrT0n57w_BZSN65gLCu7XVTTX0M', 'filename': 'A neural network for grey level and color correction used in photofinishing.pdf', 'size': 298032, 'type': 'application/pdf', 'thumbnails': {'small': {'url': 'https://v5.airtableusercontent.com/v3/u/49/49/1769875200000/tAR-InFtC4ahGam7V958lg/SEZsEs0ukyYiRd4tHetHE1laN3P4lnH4WwCi3GlVXhDDSjzQxxf3zy5EPuv7As1NzC8a2ljdGSsf_u1tXSUIkfh4zWeBv-3QAbdLCCidRcqia13R7w5NY590L99-0RgdHrWJz2e5kA89dtt1sqAKwg/nEv8vQ4sKc8HbWf9YHl7W88Gepf5X15jPAsaLYom7v4', 'width': 28, 'height': 36}, 'large': {'url': 'https://v5.airtableusercontent.com/v3/u/49/49/1769875200000/fhLueTvStSssOfSGBWXe7g/JZsupER77L5RF7YcGk3kpc55OCgcZmqsRroTnxR1cFejLFaahwiKwFvnWxGcD_gfdnmmmu0UpqFnx9Vwl2B8wSVAC8n0B80vyLSd78JZJjFngJPO3V4RTpxVuEteGg9jA7Kc7zBcGQYd6jAivjIuPg/tnKULaHLG73SiIGezjo4YVs9fI2fvnlFFgCmeMoXOuQ', 'width': 512, 'height': 662}}}]",,,,81000
LSTM,Language,Language modeling,Sepp Hochreiter ; Jurgen Schmidhuber,Highly cited,,,https://direct.mit.edu/neco/article-abstract/9/8/1735/6109/Long-Short-Term-Memory?redirectedFrom=fulltext,98595.0,Long short-term memory,1997-11-15,Technical University of Munich,10504.0,"Table 2

http://www.bioinf.jku.at/publications/older/2604.pdf",31512000000000,"""Due to limited computation time, training is stopped after 5 million sequence presentations""

Each sequence has p=100 elements in the long-delay setting.

COMPUTE = PRESENTATIONS * PRESENTATION LENGTH * UPDATE COMPUTE PER TOKEN

5000000*100*6*10,504.0=",,,"Table 8. The rightmost column lists numbers of training sequences required to achieve the stopping
criterion.

This applies to experiment 5 (multiplication)

Sequences have random lengths, on the order of 100-1000 (table 7 )",,21008.0,"Appendix A.1
""LSTM's update complexity per time is [...] K + 2KH + KC + 2KSC + H I + C I + 4CSI steps [...] where K is the number of output units, C is the number of memory cell blocks, S > 0 is the size of the memory cell blocks, H is the number of hidden units, I is the (maximal) number of units forward-connected to memory cells, gate units and hidden units""

""W = KH + KCS + CSI + 2C is the number of weights""

So the update complexity is roughly twice the number of weights.

The authors take 1 FMA = 1 step, so this is roughly 4*W FLOP

Backward (update complexity): 4*num_params
Forward pass: 2*num_params",,,,,,Academia,Confident,,2026-01-01 14:03:17+00:00,Robi Rahman,,,Germany,,,,,,,,,,,Academia,,,,,,,,,LSTM,,,,,,,,2023-05-29T21:06:54.000Z,,,True,,Infinity,,NaN,,,0,,,FP32,,Operation counting,,,,,,,True,,,,,,,,,,853000
LeNet-5,Vision,Character recognition (OCR),"Yann LeCun, Léon Bottou, Yoshua Bengio, Patrick Haffner","Historical significance,Highly cited",,,http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf,57900.0,Gradient-based Learning Applied to Document Recognition,1998-11-01,AT&T,60000.0,"""[LeNet5] contains 390408 connections, but only 60000 trainable free parameters because of the weight sharing""",2810937600000,"""[LeNet5] contains 390408 connections"" = multiply-adds
MNIST - 60,000 data points
20 epochs
390408*60000*6*20=2.810938e+12",MNIST,,"The MNIST database contains 60,000 training images and 10,000 testing images (Wikipedia)",,780816.0,,,,,,,Industry,Confident,,2026-01-01 14:03:16+00:00,Robi Rahman,,,United States of America,,,,,,,,,,,Industry,,,,,,,,,LeNet-5,,,,,,,,2023-05-29T21:06:54.000Z,,,True,,Infinity,,NaN,,,0,,,FP32,,Operation counting,,,,,,,True,,,,,,,,,,60000
RECONTRA-categorized,Language,Translation,"M. A. Castaño, F. Casacuberta",Training cost,,,https://www.semanticscholar.org/paper/Text-to-Text-Machine-Translation-Using-the-RECONTRA-Casta%C3%B1o-Casacuberta/47ee046b416d9258952cb8f4b0e2b6e65f334fad,,Text-to-text machine translation using the RECONTRA connectionist model,1999-06-02,,66780.0,"6*50*140+140*140+140*37=66780
Table 1

",8013600000000,"2*66780*3*500*5000*8=8013600000000=8e12
""The number of words of the categorized sentences ranged from 3 to 13 for the Spanish ones and from 3 to 12 for the English ones.""
""was trained up to 500 epochs using the 5,000 categorized pairs""",,,"5000*8=40000 words
The number of words of the categorized sentences ranged from 3 to 13 for the Spanish ones and from 3 to 12 for the English ones.
",500.0,,,,,,Supervised,,,Likely,"Encouragingly accurate translations have recently been obtained using a connectionist translator called RECONTRA (Recurrent Connectionist Translator). In contrast to traditional Knowledge-Based systems, this model is built from training data resulting in an Example-Based approach. It directly carries out the translation between the source and target language and employs a simple (recurrent) connectionist topology and a simple training scheme. This paper extends previous work exploring the capabilities of this RECONTRA model to perform text-to-text translations in limited-domain tasks.",2025-11-28 11:53:59+00:00,Lovis Heindrich,,,,,,,,,,,,,,,,,,,,,,,RECONTRA-categorized,,,,,,,,2024-10-03T12:55:31.000Z,,,True,,Infinity,,NaN,,,0,,,FP32,,Operation counting,,,,,,,True,,,,,,,,,,40000
PoE MNIST,Vision,Digit recognition,"Guy Mayraz, Geoffrey E. Hinton",Historical significance,,,https://proceedings.neurips.cc/paper_files/paper/2000/file/1f1baa5b8edac74eb4eaa329f14a0361-Paper.pdf,,Recognizing Hand-written Digits Using Hierarchical Products of Experts,2000-11-28,University College London (UCL),3925310.0,"10 models, one for each digit. Largest models: 500 epochs, 500 hidden units (Table 2)
""The largest network was the best, even though each digit model contains 392,500 parameters trained on only 4,400 images""
""the classification network had 30 inputs and therefore 300 weights and 10 output biases.""

Total: 392500*10 + 310 = 3,925,310",51810000000000,"Each model was trained on 4400 examples: ""The largest network was the best, even though each digit model contains 392,500 parameters trained on only 4,400 images.""

Table 2, largest network trained 500 epochs.
10 * 6 * 392500 * 4400 * 500 = 51,810,000,000,000",MNIST,,Total training data size is 60000 but the subnetworks were trained on smaller subsets.,500.0,,,,,,,,,Confident,"The product of experts learning procedure [1] can discover a set of stochastic binary features that constitute a non-linear generative model of handwritten images of digits. The quality of generative models learned in this way can be assessed by learning a separate model for each class of digit and then comparing the unnormalized probabilities of test images under the 10 different class-specific models. To improve discriminative performance, it is helpful to learn a hierarchy of separate models for each digit class. Each model in the hierarchy has one layer of hidden units and the nth level model is trained on data that consists of the activities of the hidden units in the already trained (n - 1)th level model. After training, each level produces a separate, unnormalized log probabilty score. With a three-level hierarchy for each of the 10 digit classes, a test image produces 30 scores which can be used as inputs to a supervised, logistic classification network that is trained on separate data. On the MNIST database, our system is comparable with current state-of-the-art discriminative methods, demonstrating that the product of experts learning procedure can produce effective generative models of high-dimensional data.",2025-11-28 11:53:59+00:00,Lovis Heindrich,,,United Kingdom of Great Britain and Northern Ireland,,,,,,,,,,,Academia,,,,,,,,,PoE MNIST,,,,,,,,2024-06-30T07:58:31.000Z,,,True,,Infinity,,NaN,,,0,,,FP32,,Operation counting,,,,,,,True,,,,,,,,,,54000
Neural LM,Language,Language modeling,"Yoshua Bengio, Réjean Ducharme, Pascal Vincent, Christian Janvin","Training cost,Historical significance,Highly cited",,,https://papers.nips.cc/paper_files/paper/2000/file/728f206c2a01bf572b5940d7d9a8fa4c-Paper.pdf,7637.0,A Neural Probabilistic Language Model,2000-11-28,University of Montreal / Université de Montréal,6906980.0,"(30959*100) + (8*100*120) + (120*30959) = 6,906,980
""This is obtained with a network with the direct architecture, 100 randomly initialized words features, 120 hidden units, and n = 8 words of context.""
""The Hansard corpus (Canadian parliament proceedings, French version) is a stream of about 34 million words, of which 32 millions (set A) was used for training, 1.1 million (set B) was used for validation, and 1.2 million (set C) was used for out-of-sample tests. The original data has 106, 936 different words, and those with frequency <= 10 were merged into a single token, yielding IVI = 30,959 different words.""
",6339000000000000,"The authors use a trick to avoid having to calculate the final layer for all possible words in the vocabulary. They precompute a ""short list"" of the most common word following any 2 precursor words with a smoothed trigram model, and then only calculate the softmax over words on the short list. This means only a negligible fraction of the unembedding parameters get used, so the effective number of parameters appears to be (30959*100) + (8*100*120) = 3,191,900

""Apparent convergence of the stochastic gradient descent procedure was obtained after around 10 epochs for Hansard""

6ND:
6*3191900*33100000*10=6.339e15
",Hansard Corpus,,"The Hansard corpus (Canadian parliament proceedings, French version) is a stream of about 34 million words, of which 32 millions (set A) was used for training, 1.1 million (set B) was used for validation, and 1.2 million (set C) was used for out-of-sample tests.",10.0,,,,,,,,,Confident,"A goal of statistical language modeling is to learn the joint probability function of sequences of words. This is intrinsically difficult because of the curse of dimensionality: we propose to fight it with its own weapons. In the proposed approach one learns simultaneously (1) a distributed representation for each word (i.e. a similarity between words) along with (2) the probability function for word sequences, expressed with these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar to words forming an already seen sentence. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach very significantly improves on a state-of-the-art trigram model.",2026-01-01 14:03:48+00:00,Lovis Heindrich,,,Canada,,,,,,,,,,,Academia,,,,,,,,,Neural LM,,,,,,,,2024-06-30T08:15:48.000Z,,,True,,Infinity,,NaN,,,0,,,FP32,,Operation counting,,,,,,,True,,,,,,,,,,32000000
Decision tree (classification),Vision,Face recognition,"P. Viola, M. Jones",Highly cited,,,https://www.cs.cmu.edu/~efros/courses/LBMV07/Papers/viola-cvpr-01.pdf,23449.0,Rapid object detection using a boosted cascade of simple features,2001-12-08,"Mitsubishi Electric Research Labs,Compaq CRL",12000.0,"Iteratively learns decision tree features, where each feature has 2 parameters (threshold and parity). They learn 6000 features total: ""The complete face detection cascade has 38 stages with over
6000 features""

Additional weights (see Table 1) are a temporary variable of the optimization and not part of the final model. ",63000000000000,"
The training compute can be tediously worked out from the pseudocode. I think for dataset size D, number of filters T, the training compute is roughly 180k * D * 3 * T = 6.3e13 FLOPs

The training evaluates each of 180k candidate features at each step, repeated for 6000 steps (as 1 feature is selected per round). 
The operations for one feature evaluation are unclear, but should be low (they only compare specific integer positions in the image). Estimated at 10op. 
180000*6000*10*14460=1.56168e+14",,They scraped the dataset personally for training,Section 5: 4916 hand labeled faces  + 9544 non-face images = 14460,,840000000.0,"They claim to take about 0.067s to classify an image using a 700MHz Pentium III processor. Pentium 3 performs 2 FLOP per cycle: 2*7.0e+8*0.067=9.4e+7

Using the 10op per feature evaluation estimate. One test image is split into 75M subwindows and each subwindow requires 10 evaluations on average. ""On a difficult dataset, con-
taining 507 faces and 75 million sub-windows, faces are
detected using an average of 10 feature evaluations per sub-
window.""

75000000*10*10=7500000000

Geometric mean: gmean(9.4e7, 7.5e9)=8.4e+8

",,,,,,Industry,Likely,,2025-11-28 11:53:59+00:00,Robi Rahman,,,"United States of America,United States of America",,,,,,,,,,,"Industry,Industry",,,,,,,,,Decision tree (classification),,,,,,,,2023-05-29T21:06:54.000Z,,,True,,Infinity,,NaN,,,0,,,FP32,,Operation counting,,,,,,,True,,,,,,,,,,753616
NPLM (Brown),Language,Text autocompletion,"Yoshua Bengio, Réjean Ducharme, Pascal Vincent, Christian Jauvin",Highly cited,,,https://dl.acm.org/doi/10.5555/944919.944966,7637.0,A Neural Probabilistic Language Model,2003-03-15,University of Montreal / Université de Montréal,4124233.0,"""The number of free parameters is |V|(1 + nm + h) + h(1 + (n − 1)m) [...] For example, consider the following architecture used in the experiments on the AP (Associated Press) news data: the vocabulary size is |V| = 17,964, the number of hidden units is h = 60, the order of the model is n = 6, the number of word features is m = 100""

Brown corpus: n=5, h=100, m=30, |V|=16383
16383*(1+5*30+100)+100*(1+(5-1)*30)=4124233",132076260000000,"""For example, consider the following architecture used in the experiments on the AP (Associated Press) news data: the vocabulary size is |V| = 17,964, the number of hidden units is h = 60, the order of the model is n = 6, the number of word features is m = 100. The total number of numerical operations to process a single training example is approximately |V|(1+nm+h)+h(1+nm)+nm""

Brown corpus: n=5, h=100, m=30, |V|=16383, dataset size = 800000, epochs=20
Forward FLOP: 16383*(1+5*30+100)+100*(1+5*30)+5*30=4127383
Adjusting for backward pass with 1:1 ratio, as the by far largest layer (embedding) doesn't require gradients w.r.t. inputs.
Total FLOP: 2*4127383*800000*20=1.3207626e+14",Brown corpus,,"""Comparative experiments were performed on the Brown corpus which is a stream of 1,181,041 words, from a large variety of English texts and books. The first 800,000 words were used for training, the following 200,000 for validation (model selection, weight decay, early stopping) and the remaining 181,041 for testing. The number of different words is 47,578 (including punctuation, distinguishing between upper and lower case, and including the syntactical marks used to separate texts and paragraphs). Rare words with frequency ≤ 3 were merged into a single symbol, reducing the vocabulary size to |V| = 16,383.""",,4127383.0,,,,,,,Academia,Confident,,2026-01-01 14:03:48+00:00,Robi Rahman,,,Canada,,,,,,,,,,,Academia,,,,,,,,,NPLM,,,,,,,,2023-05-29T21:06:54.000Z,,,True,,Infinity,,NaN,,,0,,,FP32,,Operation counting,,,,,,,True,,,,,,,,,,13994528
NPLM (AP News),Language,Text autocompletion,"Yoshua Bengio, Réjean Ducharme, Pascal Vincent, Christian Jauvin",Highly cited,,,https://dl.acm.org/doi/10.5555/944919.944966,7637.0,A Neural Probabilistic Language Model,2003-03-15,University of Montreal / Université de Montréal,11904264.0,"""The number of free parameters is |V|(1 + nm + h) + h(1 + (n − 1)m) [...] For example, consider the following architecture used in the experiments on the AP (Associated Press) news data: the vocabulary size is |V| = 17,964, the number of hidden units is h = 60, the order of the model is n = 6, the number of word features is m = 100""

AP News: n=6, h=60, m=100, |V|=17964
17964*(1+6*100+60)+60*(1+(6-1)*100)=11904264",1666869200000000,"""For example, consider the following architecture used in the experiments on the AP (Associated Press) news data: the vocabulary size is |V| = 17,964, the number of hidden units is h = 60, the order of the model is n = 6, the number of word features is m = 100. The total number of numerical operations to process a single training example is approximately |V|(1+nm+h)+h(1+nm)+nm""

AP News: n=6, h=60, m=100, |V|=17964, dataset=13994528, epochs=5
Forward FLOP: 17964*(1+6*100+60)+60*(1+6*100)+6*100=11910864
Adjusting for backward pass with 1:1 ratio, as the by far largest layer (embedding) doesn't require gradients w.r.t. inputs.
Total FLOP: 2*11910864*13994528*5=1.6668692e+15",Brown corpus,,"""Comparative experiments were performed on the Brown corpus which is a stream of 1,181,041 words, from a large variety of English texts and books. The first 800,000 words were used for training, the following 200,000 for validation (model selection, weight decay, early stopping) and the remaining 181,041 for testing. The number of different words is 47,578 (including punctuation, distinguishing between upper and lower case, and including the syntactical marks used to separate texts and paragraphs). Rare words with frequency ≤ 3 were merged into a single symbol, reducing the vocabulary size to |V| = 16,383.""",,11910864.0,,,,,,,Academia,Confident,,2026-01-01 14:03:48+00:00,Lovis Heindrich,,,Canada,,,,,,,,,,,Academia,,,,,,,,,NPLM,,,,,,,,2025-06-16T15:33:34.000Z,,,True,,Infinity,,NaN,,,0,,,FP32,,Operation counting,,,,,,,True,,,,,,,,,,13994528
LMICA,Vision,Object detection,"Yoshitatsu Matsuda, K. Yamaguchi","Training cost,Historical significance",,,https://www.semanticscholar.org/paper/Linear-Multilayer-Independent-Component-Analysis-Matsuda-Yamaguchi/7061b01572fbff2e223ce3abb59f397895b1ebf1,,"Linear Multilayer Independent Component Analysis for Large Natural Scenes
",2004-12-01,,4096000.0,"64*64*1000=4096000
""100000 samples of natural scenes of 64 × 64 pixels were given as X""
""LMICA was carried out in 1000 layers""
",2782080000000000,"69*60*60*8*2800000000*0.5=2782080000000000=2.78e15
""it consumed about 69 hours with Intel 2.8GHz CPU""
- Assuming they used an Intel Pentium 4 processor with 8 FLOP/cycle (https://en.wikipedia.org/wiki/FLOPS)",,,"""100000 samples of natural scenes of 64 × 64 pixels were given as X""",,,,,,,,,,Confident,"In this paper, linear multilayer ICA (LMICA) is proposed for extracting independent components from quite high-dimensional observed signals such as large-size natural scenes. There are two phases in each layer of LMICA. One is the mapping phase, where a one-dimensional mapping is formed by a stochastic gradient algorithm which makes more highly-correlated (non-independent) signals be nearer incrementally. Another is the local-ICA phase, where each neighbor (namely, highly-correlated) pair of signals in the mapping is separated by the MaxKurt algorithm. Because LMICA separates only the highly-correlated pairs instead of all ones, it can extract independent components quite efficiently from appropriate observed signals. In addition, it is proved that LMICA always converges. Some numerical experiments verify that LMICA is quite efficient and effective in large-size natural image processing.",2025-11-28 11:53:59+00:00,Lovis Heindrich,,,,,,,,,,,,,,,,,,,,,,,LMICA,,,,,,,,2024-06-30T11:38:31.000Z,,,True,,Infinity,,NaN,,,0,,,FP32,,Hardware,,,,,,,True,,,,,,,,,,100000
Hierarchical LM,Language,Language modeling,"Frederic Morin, Yoshua Bengio",Highly cited,,,https://www.semanticscholar.org/paper/Hierarchical-Probabilistic-Neural-Network-Language-Morin-Bengio/c19fbefdeead6a4154a22a9c8551a18b1530033a,,Hierarchical Probabilistic Neural Network Language Model,2005-01-06,,,,115848000000000,"""The computations were performed on Athlon processors with a 1.2 GHz clock""
FP32 per cycle: 4 (""The bottom line is that the Athlon is capable of delivering as many as four 32-bit, single-precision floating-point results per clock cycle"", https://www.pctechguide.com/amd-technology/amd-athlon) 
Training time per epoch: 1609s (table 1)
Epochs: 30 ""Training is performed over about 20 to 30 epochs according to validation set perplexity (early stopping).""
Assumed utilization: 0.5 
Compute estimate: 0.5*1200000000*4*30*1609=115848000000000=1.16e14",Brown corpus,"""The experiments were performed on the Brown corpus, with a reduced vocabulary size of 10,000 words""","""The corpus has 1,105,515 occurrences of words, split into 3 sets: 900,000 for training, 100,000 for validation (model selection), and 105,515 for testing""",30.0,,,13.4,"Training time per epoch: 1609s (table 1)
Total training time 30*1609/60/60=13.408h
",,,,,Confident,"In recent years, variants of a neural network architecture for statistical language modeling have been proposed and successfully applied, e.g. in the language modeling component of speech recognizers. The main advantage of these architectures is that they learn an embedding for words (or other symbols) in a continuous space that helps to smooth the language model and provide good generalization even when the number of training examples is insufficient. However, these models are extremely slow in comparison to the more commonly used n-gram models, both for training and recognition. As an alternative to an importance sampling method proposed to speed-up training, we introduce a hierarchical decomposition of the conditional probabilities that yields a speed-up of about 200 both during training and recognition. The hierarchical decomposition is a binary hierarchical clustering constrained by the prior knowledge extracted from the WordNet semantic hierarchy.",2025-11-28 11:53:59+00:00,Lovis Heindrich,,,,,,,1.0,,,,,,,,,,,,,,,,Hierarchical LM,,,,,,,,2024-05-28T09:20:38.000Z,,,True,,Infinity,,NaN,,,0,,,FP32,,Hardware,,,,,,,True,,,,,,,,,,900000
SVM-CNN,Vision,Image classification,"Fu Jie Huang, Yann LeCun",Historical significance,,Unreleased,https://www.semanticscholar.org/paper/Large-scale-Learning-with-SVM-and-Convolutional-for-Huang-LeCun/cf03fdf52dd6e4249cbbdbd0bffbbbe5ca389feb,,Large-scale Learning with SVM and Convolutional for Generic Object Categorization,2006-06-17,New York University (NYU),90857.0,,745200000000000,"Training time: 5880+330=6210 minutes (Table 1)
“The timing is normalized to hypothetical 1GHz single CPU.”
Assuming utilization of 0.5 and AMD Athlon 64 Processor with 4 FLOP/cycle
Compute: 6210*60*4*1000000000*0.5=745200000000000 = 7.4e14
",NORB,,"To generate the training set, each image was perturbed with 10 different configurations of the above parameters, which makes up 291,600 image pairs of size 108×108. The testing set has 2 drawings of perturbations, and have ",,,,,Only normalized training times are reported,,Supervised,,,Confident,"The detection and recognition of generic object categories with invariance to viewpoint, illumination, and clutter requires the combination of a feature extractor and a classifier. We show that architectures such as convolutional networks are good at learning invariant features, but not always optimal for classification, while Support Vector Machines are good at producing decision surfaces from wellbehaved feature vectors, but cannot learn complicated invariances. We present a hybrid system where a convolutional network is trained to detect and recognize generic objects, and a Gaussian-kernel SVM is trained from the features learned by the convolutional network. Results are given on a large generic object recognition task with six categories (human figures, four-legged animals, airplanes, trucks, cars, and ""none of the above""), with multiple instances of each object category under various poses, illuminations, and backgrounds. On the test set, which contains different object instances than the training set, an SVM alone yields a 43.3% error rate, a convolutional net alone yields 7.2% and an SVM on top of features produced by the convolutional net yields 5.9%.",2025-11-28 11:53:59+00:00,Lovis Heindrich,,,United States of America,,,,1.0,,,,,,,Academia,,,,,,,,,SVM-CNN,,,,,,,,2024-10-22T07:37:34.000Z,,,True,,Infinity,,NaN,,,0,,,FP32,,Hardware,,,,,,,True,,,,,,,,,,583200
SB-LM,Language,Language modeling,"T. Brants, Ashok Popat, P. Xu, F. Och, J. Dean","Training cost,Highly cited",,,https://www.semanticscholar.org/paper/Large-Language-Models-in-Machine-Translation-Brants-Popat/ba786c46373892554b98df42df7af6f5da343c9d,,Large Language Models in Machine Translation,2007-06-22,Google,300000000000.0,Table 2,1449446400000000000,"Assuming a Nehalem based processor with 8 FLOP/cycle (https://www.agner.org/optimize/microarchitecture.pdf#page=105.06) , 2 cores and 2.33 GHz clock speed: 8*2*2330000000=37280000000 FLOP/s
Trained for 1 day on 1500 machines (Table 2)
Compute: 1500*37280000000*1*24*60*60*0.3=1449446400000000000=1.4e18
",,"Largest training set is dubbed ""web"", and is described as ""general web data, which was collected in January 2006 (2 trillion tokens)""
Table 2 indicates 1.8T tokens",Table 2,,,,24.0,Table 2,,,,,Likely,"Systems, methods, and computer program products for machine translation are provided. In some implementations a system is provided. The system includes a language model including a collection of n-grams from a corpus, each n-gram having a corresponding relative frequency in the corpus and an order n corresponding to a number of tokens in the n-gram, each n-gram corresponding to a backoff n-gram having an order of n-1 and a collection of backoff scores, each backoff score associated with an n-gram, the backoff score determined as a function of a backoff factor and a relative frequency of a corresponding backoff n-gram in the corpus.",2025-11-28 11:53:44+00:00,Lovis Heindrich,,,United States of America,,,,1500.0,,,,,,,Industry,,,,,,,,,SB-LM,,,,,,,,2024-06-30T07:13:52.000Z,,,True,,Infinity,,NaN,,,0,,,FP32,,Hardware,,,,,,,True,,,,,,,,,,1800000000000
KN-LM,Language,Language modeling,"T. Brants, Ashok Popat, P. Xu, F. Och, J. Dean","Training cost,Highly cited",,,https://www.semanticscholar.org/paper/Large-Language-Models-in-Machine-Translation-Brants-Popat/ba786c46373892554b98df42df7af6f5da343c9d,,Large Language Models in Machine Translation,2007-06-22,Google,21000000000.0,Table 2,773038080000000000,"Trained for 2 days on 400 machines (Table 2)
Assuming a Nehalem based processor with 8 FLOP/cycle (https://www.agner.org/optimize/microarchitecture.pdf#page=105.06) , 2 cores and 2.33 GHz clock speed: 8*2*2330000000=37280000000 FLOP/s
Compute: 400*37280000000*2*24*60*60*0.3=773038080000000000=7.7e17",,"Largest dataset (""web"") was deemed to expensive to train with the KN methodology. Largest actually used was ""webnews"", described as ""data collected over several years, up to December 2005, from web pages containing predominantly English news articles (31 billion tokens).""",Table 2,,,,48.0,Table 2,,,,,Likely,"Systems, methods, and computer program products for machine translation are provided. In some implementations a system is provided. The system includes a language model including a collection of n-grams from a corpus, each n-gram having a corresponding relative frequency in the corpus and an order n corresponding to a number of tokens in the n-gram, each n-gram corresponding to a backoff n-gram having an order of n-1 and a collection of backoff scores, each backoff score associated with an n-gram, the backoff score determined as a function of a backoff factor and a relative frequency of a corresponding backoff n-gram in the corpus.",2025-11-28 11:53:44+00:00,Lovis Heindrich,,,United States of America,,,,400.0,,,,,,,Industry,,,,,,,,,KN-LM,,,,,,,,2024-06-30T07:17:01.000Z,,,True,,Infinity,,NaN,,,0,,,FP32,,Hardware,,,,,,,True,,,,,,,,,,31000000000
LCNP LabelMe,Vision,Object recognition,"Rafael Uetz, Sven Behnke",Historical significance,,,https://ieeexplore.ieee.org/document/5357786,,"Large-scale object recognition with CUDA-accelerated hierarchical neural networks
",2009-11-22,University of Bonn,13729792.0,"Locally connected: 128*128*4*4*4*7 + 64*64*8*4*4*4 + 32*32*16*4*4*8 + 16*16*32*4*4*16=13631488
Classification head: 16*16*32*12=98304
Total: 13631488+98304=13729792
""five regular layers with the dimensions 256×256, 128×128, . . ., 16×16.""
""size of the receptive field to be 4 × 4 neurons""",3295150080000000,2*13729792*3*40000*1000=3295150080000000,,,Table 1,1000.0,,,,,,Supervised,,,Confident,"Robust recognition of arbitrary object classes in natural visual scenes is an aspiring goal with numerous practical applications, for instance, in the area of autonomous robotics and autonomous vehicles. One obstacle on the way towards human-like recognition performance is the limitation of computational power, restricting the size of the training and testing dataset as well as the complexity of the object recognition system. In this work, we present a hierarchical, locally-connected neural network model that is well-suited for large-scale, high-performance object recognition. By using the NVIDIA CUDA framework, we create a massively parallel implementation of the model which is executed on a state-of-the-art graphics card. This implementation is up to 82 times faster than a single-core CPU version of the system. This significant gain in computational performance allows us to evaluate the model on a very large, realistic, and challenging set of natural images which we extracted from the LabelMe dataset. To compare our model to other approaches, we also evaluate the recognition performance using the well-known MNIST and NORB datasets, achieving a testing error rate of 0.76% and 2.87%, respectively.",2025-11-28 11:53:44+00:00,Lovis Heindrich,,,Germany,,,,,,,,,,,Academia,,,,,,,,,LCNP LabelMe,,,,,,,,2024-10-08T16:19:40.000Z,,,True,,Infinity,,NaN,,,0,,,FP32,,Operation counting,,,,,,,True,,,,,,,,,,40000
LCNP MNIST,Vision,Object recognition,"Rafael Uetz, Sven Behnke",Historical significance,,,https://ieeexplore.ieee.org/document/5357786,,"Large-scale object recognition with CUDA-accelerated hierarchical neural networks
",2009-11-22,,11616256.0,"Locally connected: 128*128*4*4*4*5 + 64*64*8*4*4*4 + 32*32*16*4*4*8 + 16*16*32*4*4*16=11534336
Classification head: 16*16*32*10=81920
Total: 11534336+81920=11616256
""five regular layers with the dimensions 256×256, 128×128, . . ., 16×16.""
""size of the receptive field to be 4 × 4 neurons""",4181852160000000,2*11616256*3*60000*1000=4181852160000000,MNIST,"We tested our system with three datasets: (1) The MNIST dataset [9], which consists of 60,000 grayscale images (50,000 for training and 10,000 for testing)",,1000.0,,,,,,Supervised,,,Confident,"Robust recognition of arbitrary object classes in natural visual scenes is an aspiring goal with numerous practical applications, for instance, in the area of autonomous robotics and autonomous vehicles. One obstacle on the way towards human-like recognition performance is the limitation of computational power, restricting the size of the training and testing dataset as well as the complexity of the object recognition system. In this work, we present a hierarchical, locally-connected neural network model that is well-suited for large-scale, high-performance object recognition. By using the NVIDIA CUDA framework, we create a massively parallel implementation of the model which is executed on a state-of-the-art graphics card. This implementation is up to 82 times faster than a single-core CPU version of the system. This significant gain in computational performance allows us to evaluate the model on a very large, realistic, and challenging set of natural images which we extracted from the LabelMe dataset. To compare our model to other approaches, we also evaluate the recognition performance using the well-known MNIST and NORB datasets, achieving a testing error rate of 0.76% and 2.87%, respectively.",2025-11-28 11:53:44+00:00,Lovis Heindrich,,,,,,,,,,,,,,,,,,,,,,,LCNP MNIST,,,,,,,,2024-10-08T16:19:42.000Z,,,True,,Infinity,,NaN,,,0,,,FP32,,Operation counting,,,,,,,True,,,,,,,,,,50000
KN5 LM + RNN 400/10 (WSJ),Speech,Transcription,"T. Mikolov, M. Karafiat, L. Burget, J. Cernock ´ y, and S. Khudanpur",Highly cited,,,https://www.isca-archive.org/interspeech_2010/mikolov10_interspeech.html,6038.0,Recurrent neural network based language model,2010-09-26,"Brno University of Technology,Johns Hopkins University",22160000.0,"RNN 400/10 indicates that the model has 400 units in the hidden layer and merges words that occur less than 10 times in the training set.

The model is trained on a 6.4M word subset of the NYT subset of English Gigaword. In this colab notebook, I estimate the vocabulary after merging infrequent words using english word frequency data at around 27.5k: https://colab.research.google.com/drive/1K5qH0EqXtFwTLESNtp4oelCM28GpGXt6?usp=sharing

Then RNN 400/10 has:
(27.5k + 400) * 400 + (400 * 27.5k) = 22.16M parameters

The KN5 LM is effectively a lookup table over 5-grams in it's training data. In one sense, this means it has approximately as many parameters as unique 5-grams in the 37M words of training data. However, these aren't parameters in the same sense as neural networks, as they are accessed as a lookup table, so on each forward pass only one parameter is active. I choose not to include these parameters in our count.",17000000000000000,"""Convergence is usually achieved after 10-20 epochs.""

6 * 22,160,000 * 20 * 6.4M = 1.70e16",Gigaword,,"The training corpus consists of 37M words from NYT section of English Gigaword. As it is very time consuming to train RNN LM on large data, we have used only up to 6.4M words for training RNN models (300K sentences) - it takes several weeks to train the most complex models",,44320000.0,Roughly twice the number of parameters,504.0,"""it takes several weeks to train the most complex models""
Assume several weeks is around 3 weeks, or 504 hours",,,,Academia,Confident,,2026-01-01 14:02:54+00:00,Robi Rahman,,1,"Czechia,United States of America",,,,,,,,,,,"Academia,Academia",,,,,,,,,KN5 LM + RNN 400/10 (WSJ),,,,,,,,2023-05-29T21:06:54.000Z,,,True,,Infinity,,NaN,,,0,,,FP32,,Operation counting,,,,,,,True,,,,,,,,,,6400000
RNN LM,Language,Language modeling,"Tomas Mikolov, M. Karafiát, L. Burget, J. Černocký, S. Khudanpur",Highly cited,,,https://www.semanticscholar.org/paper/Recurrent-neural-network-based-language-model-Mikolov-Karafi%C3%A1t/9819b600a828a57e1cde047bbe710d3446b30da5,6038.0,Recurrent neural network based language model,2010-09-26,Johns Hopkins University,70265000.0,"This database entry refers to the 3xRNN rows in Table 2 (static and dynamic likely use the same model ensemble, but allow the model weights to update once when testing the dynamic version).

I assume the 3xRNN represents interpolation between the three largest models shown explicitly (RNN 250/5, RNN 250/2, and RNN 400/10). This seems likely, since smaller models do considerably worse on their own.

In the following colab notebook, I estimate vocabulary sizes for the NYT Gigaword data at around 54.4k, 41.4k, and 27.6k for merge thresholds of 2, 5, and 10, respectively: https://colab.research.google.com/drive/1K5qH0EqXtFwTLESNtp4oelCM28GpGXt6#scrollTo=tedUkbgklNJ3

So the total number of parameters in each constituent model is:
- RNN 250/2: (250 + 54.4k) * 250 + (250 * 54.4k) = 27,262,500
- RNN 250/5: (250 + 41.4k) * 250 + (250 * 41.4k) = 20,762,500
- RNN 400/10: (400 + 27.6k) * 400 + (400 * 27.6k) = 22,240,000

In total: 70,265,000 parameters",53960000000000000,"""Convergence is usually achieved after 10-20 epochs.""
Training was done over a 6.4M subset of the NYT section of English Gigaword.

6 * 70,265,000 * 20 * 6.4M = 5.396e16",WSJ,The training corpus consists of 37M words from NYT section of English Gigaword,"As it is very time consuming to train RNN LM on large data, we have used only up to 6.4M words for training RNN models.",1.0,,,504.0,"""it takes several weeks to train the most complex models.""
Rough guess, 3 weeks = 504 hours

Assuming these models trained at the same time on different machines.",,,,,Speculative,"A new recurrent neural network based language model (RNN LM) with applications to speech recognition is presented. Results indicate that it is possible to obtain around 50% reduction of perplexity by using mixture of several RNN LMs, compared to a state of the art backoff language model. Speech recognition experiments show around 18% reduction of word error rate on the Wall Street Journal task when comparing models trained on the same amount of data, and around 5% on the much harder NIST RT05 task, even when the backoff model is trained on much more data than the RNN LM. We provide ample empirical evidence to suggest that connectionist language models are superior to standard n-gram techniques, except their high computational (training) complexity. Index Terms: language modeling, recurrent neural networks, speech recognition",2026-01-01 14:02:54+00:00,Lovis Heindrich,,,United States of America,,,,,,,,,,,Academia,,,,,,,,,RNN LM,,,,,,,,2024-05-28T08:51:14.000Z,,,True,,Infinity,,NaN,,,0,,,FP32,,Operation counting,,,,,,,True,,,,,,,,,,6400000
RNN 1000/5 + RT09 LM (NIST RT05),Speech,"Speech recognition (ASR),Transcription","T. Mikolov, M. Karafiat, L. Burget, J. Cernocky, and S. Khudanpur",Highly cited,,Unreleased,https://www.isca-archive.org/interspeech_2010/mikolov10_interspeech.html,6038.0,Recurrent neural network based language model,2010-09-26,"Brno University of Technology,Johns Hopkins University",77039000.0,"""The acoustic HMMs are based on cross-word tied-states triphones trained discriminatively using MPE criteria. Feature extraction use 13 Mel-PLP’s features with deltas, double and triple deltas reduced by HLDA to 39-dimension feature vector""

RNN 1000/5 indicates that the model has a 1000 units in the hidden layer and merges words that occur less than 5 times in the training set.

Section 4.2 in this paper appears to indicate that NIST RT05 has a vocabulary of 50k words: https://www.fit.vut.cz/person/imikolov/public/rnnlm/char.pdf

Using this dataset of english word frequencies ...
https://www.kaggle.com/datasets/rtatman/english-word-frequency
... we can estimate that the number of words in the final vocabulary would have been around 38k when merging with threshold 5:
https://colab.research.google.com/drive/1K5qH0EqXtFwTLESNtp4oelCM28GpGXt6?usp=sharing

Thus the model in question would have:

(39 + 1000 + 38k) * 1000 + 1000 * 38k = 77,039,000 parameters

The RT09 LM they interpolate with appears to be a 4-gram model. In one sense, n-gram models have parameters roughly equal to the number of unique n-grams in the training data. However, these aren't parameters in the same sense as neural networks as they are accessed as a lookup table, so on each forward pass only one parameter is active. I choose not to include these parameters in our count.",50000000000000000,"""Convergence is usually achieved after 10-20 epochs.""

Assuming a backward-forward ratio of 2:1, since this is a shallow network

6 * 77039000 * 5.4M * 20 = 5.00e16",NIST RT05,"""Table 4: Comparison of very large back-off LMs and RNN LMs
trained only on limited in-domain data (5.4M words).""","""Table 4: Comparison of very large back-off LMs and RNN LMs
trained only on limited in-domain data (5.4M words).""",20.0,154078000.0,2x parameters,1200.0,"The biggest individual models evaluated on WSJ (probably RNN 250/2 at 27M parameters) took about 2.1e16 FLOPs to train, and training took ""several weeks"". Assuming a linear scaleup and several = about 3:
3 weeks * 7 days/week * 24 hr/day * 5e16/2.1e16 = 1,200 hours",,Supervised,,,Likely,"A new recurrent neural network based language model (RNN LM) with applications to speech recognition is presented. Results indicate that it is possible to obtain around 50% reduction of perplexity by using mixture of several RNN LMs, compared to a state of the art backoff language model. Speech recognition experiments show around 18% reduction of word error rate on the Wall Street Journal task when comparing models trained on the same amount of data, and around 5% on the much harder NIST RT05 task, even when the backoff model is trained on much more data than the RNN LM. We provide ample empirical evidence to suggest that connectionist language models are superior to standard n-gram techniques, except their high computational (training) complexity",2026-01-01 14:02:54+00:00,Luke Frymire,,1,"Czechia,United States of America",,,,,,,,,,,"Academia,Academia",,,,,,,,,RNN 1000/5 + RT09 LM (NIST RT05),,,,,,,,2024-10-08T00:27:17.000Z,,,True,,Infinity,,NaN,,,0,,,FP32,,Operation counting,,,,,,,True,,,,,,,,,,5400000
Deep Autoencoders,Vision,Image representation,"A. Krizhevsky, Geoffrey E. Hinton",Historical significance,,,https://www.semanticscholar.org/paper/Using-very-deep-autoencoders-for-content-based-Krizhevsky-Hinton/88080d28536f36588740737f3b7a1f6c1a409654,,Using very deep autoencoders for content-based image retrieval,2011-04-29,University of Toronto,139808256.0,"2*(3072*8192+8192*4096+4096*2048+2048*1024+1024*512+512*256+256*128+128*64+64*28)=139808256
""n each autoencoder, the hidden layers halve in size until they reach the desired size, except that we use 28 instead of 32""",36728640000000000,"48*60*60*708500000000*0.3=36728640000000000=3.7e16
GTX 285 with 708.5 GFLOP/s",,"""We tested our models on two subsets of the 80 million tiny images datase""","""We train on 1.6 million 32 × 32 color images""",85.0,,,48.0,"""The entire training procedure for each autoencoder took about 2 days on an Nvidia GTX 285 GPU.""",NVIDIA GeForce GTX 285,,,,Confident,"We show how to learn many layers of features on color images and we use these features to initialize deep autoencoders. We then use the autoencoders to map images to short binary codes. Using semantic hashing [6], 28-bit codes can be used to retrieve images that are similar to a query image in a time that is independent of the size of the database. This extremely fast retrieval makes it possible to search using multiple di erent transformations of the query image. 256-bit binary codes allow much more accurate matching and can be used to prune the set of images found using the 28-bit codes.",2025-11-28 11:53:44+00:00,Lovis Heindrich,,,Canada,,,,1.0,,,,,,,Academia,,,,,,,,,Deep Autoencoders,,,,204,,,,2024-10-08T14:54:29.000Z,,,True,246.22598104391827,Infinity,2008-12-23,2.346338124572211,708500000000,,1,,,FP32,708500000000,Hardware,,,,NVIDIA,,,True,,,,,,,,,,4915200000
CNN Committee (MNIST),Vision,Image classification,"D. Ciresan, U. Meier, L. Gambardella, J. Schmidhuber",Historical significance,,Unreleased,https://www.semanticscholar.org/paper/Convolutional-Neural-Network-Committees-for-Ciresan-Meier/260a7615bbffec052d67dffde5bcf9b4687b50ee,,Convolutional Neural Network Committees for Handwritten Character Classification,2011-09-18,IDSIA,120620.0,"1st CNN layer: 20*4*4=320
2nd CNN layer: 20*40*9*9=64800
1st FC layer: 40*3*3*150=54000
2nd FC layer: 150*10=1500
Total: 320+64800+54000+1500=120620",52000000000000000,"GPU hour estimate:
800 epochs, 14 hours per network, 7 networks total
7*14*60*60*(2*1580000000000+2*1345000000000)*0.3=6.19e17

Counting estimate:
50000 examples, 800 epochs
1st CNN layer: 2*20*26*26*4*4=432640
2nd CNN layer: 2*20*6*6*9*9*40=4665600
1st FC layer: 2*40*3*3*150=108000
2nd FC layer: 2*150*10=3000
Total: 432640+4665600+108000+3000=5209240
5209240*3*50000*800*7=4375761600000000=4.4e15

Geometric mean: sqrt(4.4e15*6.19e17)=5.2e16",MNIST,,"[images]
""Our CNNs are trained for around 800 epochs""",800.0,,,98.0,,"NVIDIA GeForce GTX 580,NVIDIA GeForce GTX 480",Supervised,,,Likely,"In 2010, after many years of stagnation, the MNIST handwriting recognition benchmark record dropped from 0.40% error rate to 0.35%. Here we report 0.27% for a committee of seven deep CNNs trained on graphics cards, narrowing the gap to human performance. We also apply the same architecture to NIST SD 19, a more challenging dataset including lower and upper case letters. A committee of seven CNNs obtains the best results published so far for both NIST digits and NIST letters. The robustness of our method is verified by analyzing 78125 different 7-net committees.",2025-11-28 11:53:44+00:00,Lovis Heindrich,,,Switzerland,,,,4.0,,,,,,,Academia,,,,,Unreleased,,,,CNN Committee (MNIST),,,,"244,250",,,,2024-10-17T13:03:15.000Z,,,True,,Infinity,"2010-11-09,2010-03-26",NaN,"1581000000000,1344960000000",,2,,,FP32,"1581000000000,1344960000000","Hardware,Operation counting",,,,"NVIDIA,NVIDIA",,,True,,,,,,,,,,420000
Dropout (ImageNet),Vision,Image classification,"GE Hinton, N Srivastava, A Krizhevsky",Highly cited,,Unreleased,https://arxiv.org/abs/1207.0580,7899.0,Improving neural networks by preventing co-adaptation of feature detectors,2012-06-03,University of Toronto,,"""We achieved comparable performance of 48.6% error using a single neural network with five convolutional hidden layers interleaved with “max-pooling” layer followed by two globally
connected layers and a final 1000-way softmax layer""",273196800000000000,"""a single NVIDIA GTX 580 GPU... Training on ImageNet takes
roughly four days with dropout and two days without.""
1.581 TFLOP/s * 4 day * 86400 s/day * 0.5 utilization",ImageNet,,"In 2010, a subset of 1000 classes
with roughly 1000 examples per class was the basis of an object recognition competition...",,,,96.0,4 days with dropout; 2 days without dropout,NVIDIA GeForce GTX 580,,,Academia,Confident,"Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different thinned networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.",2026-01-01 14:02:54+00:00,Robi Rahman,,,Canada,,,,,,,,,,,Academia,,,,,Unreleased,,,7.952555832759744,Dropout (ImageNet),,,,244,,,,2023-05-29T21:06:54.000Z,Unreleased,FP32,True,,34353333160465670,2010-11-09,1.566050650239562,1581000000000,,1,,,FP32,1581000000000,Hardware,,,,NVIDIA,,,True,,,,,,,,,,2600000
Unsupervised High-level Feature Learner,Vision,Image classification,"Quoc V. Le, Marc'Aurelio Ranzato, Rajat Monga, Matthieu Devin, Kai Chen, Greg S. Corrado, Jeff Dean, Andrew Y. Ng","Highly cited,SOTA improvement","""we trained our network to obtain 15.8% accuracy in recognizing 20,000 object categories from ImageNet, a leap of 70% relative improvement over the previous state-of-the-art""",,https://arxiv.org/abs/1112.6209,2909.0,Building High-level Features Using Large Scale Unsupervised Learning,2012-07-12,Google,1000000000.0,"""To answer this, we train a 9-layered locally connected sparse autoencoder with pooling and local contrast normalization on a large dataset of images (the model has 1 billion connections, the dataset has 10 million 200x200 pixel images downloaded from the Internet)""",600000000000000000,"Assuming 1 epoch, 10 million images and 1 billion parameters, 6*N*D = 6*10^17 FLOP",,10 million 200x200 images extracted from Youtube videos,10 million 200x200 images extracted from Youtube videos,,,,72.0,"""We train this network using model parallelism and asynchronous SGD on a cluster with 1,000 machines (16,000 cores) for three days. """,,Unsupervised,Hardware not reported,Industry,Likely,"We consider the problem of building high-level, class-specific feature detectors from only unlabeled data. For example, is it possible to learn a face detector using only unlabeled images using unlabeled images? To answer this, we train a 9-layered locally connected sparse autoencoder with pooling and local contrast normalization on a large dataset of images (the model has 1 billion connections, the dataset has 10 million 200×200 pixel images downloaded from the Internet). We train this network using model parallelism and asynchronous SGD on a cluster with 1,000 machines (16,000 cores) for three days. Contrary to what appears to be a widely-held intuition, our experimental results reveal that it is possible to train a face detector without having to label images as containing a face or not. Control experiments show that this feature detector is robust not only to translation but also to scaling and out-of-plane rotation. We also find that the same network is sensitive to other high-level concepts such as cat faces and human bodies. Starting with these learned features, we trained our network to obtain 15.8% accuracy in recognizing 20,000 object categories from ImageNet, a leap of 70% relative improvement over the previous state-of-the-art.",2025-11-28 11:53:44+00:00,Anonymous,,,United States of America,,,,,,,,,,,Industry,,,,,,,,16.029257461780738,Unsupervised High-level Feature Learner,,,,,,,,2023-06-15T15:50:10.000Z,,,True,,37431552985570690,,NaN,,,0,,,FP32,,Operation counting,,,,,,,True,,,,,,,,,,1200000000000
AlexNet,Vision,Image classification,"Alex Krizhevsky, Ilya Sutskever, Geoffrey Hinton","Highly cited,Historical significance",,,https://proceedings.neurips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html,125497.0,ImageNet Classification with Deep Convolutional Neural Networks,2012-09-30,University of Toronto,60000000.0,"""Our neural network architecture has 60 million parameters.""",470000000000000000,"1.2M images * 90 epochs * 0.75 GFLOP * (2 add-multiply) * (3 backward pass) 
= 470 PF = 0.0054 pfs-days

Source: https://openai.com/blog/ai-and-compute/

Hardware method:
2 GTX 580 3GB GPUs for ""between five and six days"". Assuming 5.5 days and 32-bit training:
1.581 TFLOPS * 5.5 days * 2 = 1.5e18 FLOP
Comparing to the operation counting method, this implies around 31% MFU.

Authors of ""AI and Memory Wall"" (https://github.com/amirgholami/ai_and_memory_wall) estimated model's training compute as 460 PFLOP = 4.6*10^17 FLOP",ImageNet,,"""ImageNet is a dataset of over 15 million labeled high-resolution images belonging to roughly 22,000 categories. The images were collected from the web and labeled by human labelers using Amazon’s Mechanical Turk crowd-sourcing tool. Starting in 2010, as part of the Pascal Visual Object Challenge, an annual competition called the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC) has been held. ILSVRC uses a subset of ImageNet with roughly 1000 images in each of 1000 categories. In all, there are roughly 1.2 million training images, 50,000 validation images, and 150,000 testing images.""",,,,132.0,"""Our network takes between five and six days to train on two GTX 580 3GB GPUs.""",NVIDIA GeForce GTX 580,,,Academia,Confident,"We trained a large, deep convolutional neural network to classify the 1.3 million high-resolution images in the LSVRC-2010 ImageNet training set into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 39.7\% and 18.9\% which is considerably better than the previous state-of-the-art results. The neural network, which has 60 million parameters and 500,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and two globally connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of convolutional nets. To reduce overfitting in the globally connected layers we employed a new regularization method that proved to be very effective.",2026-01-01 14:02:54+00:00,Robi Rahman,,,Canada,,,,,0.3133,,,,,,Academia,,,,,,,,15.56975299843643,AlexNet,,"Operation counting approach for compute estimate implies 470 PFLOP.
Their hardware, 2 GTX 580 GPUs, would perform 1500 PFLOPs if operating in 32-bit for 5.5 days.
This implies a model FLOP utilization rate of 31.3%.

MFU = 470 PFLOP / 1500 PFLOP = 0.3133",,244,,,,2023-05-29T21:06:54.000Z,,FP32,True,,30186734500360990,2010-11-09,1.891854893908282,1581000000000,,1,,,FP32,1581000000000,"Operation counting,Hardware,Third-party estimation",,,,NVIDIA,,,True,,,,,,,,,,2457600000
DNN EM segmentation,Vision,Image segmentation,"D. Ciresan, A. Giusti, L. Gambardella, J. Schmidhuber","Historical significance,Highly cited",,Unreleased,https://www.semanticscholar.org/paper/Deep-Neural-Networks-Segment-Neuronal-Membranes-in-Ciresan-Giusti/09193e19b59fc8f05bee9d6efbfb1607ca5b6501,,Deep Neural Networks Segment Neuronal Membranes in Electron Microscopy Images,2012-12-03,"IDSIA,SUPSI",218896.0,"Table 1 shows architecture
1st layer CNN: 48*4*4=768
2nd layer CNN: 48*48*5*5=57600
3rd layer CNN: 48*48*4*4=36864
4th layer CNN: 48*48*4*4=36864
1st layer FC: 48*3*3*200=86400
2nd layer FC: 200*2=400
Total parameters: 768+57600+36864+36864+86400+400=218896",478000000000000000,"""This amounts to 3 million training examples in total, in which both classes are equally represented. [...] We take advantage of this property, and synthetically augment the training set at the beginning of each epoch by randomly mirroring each training instance, and/or rotating it by ±90◦""
Assuming 3 x training examples per epoch due to data augmentation

Counting estimate:
1st layer CNN: 2*92*92*48*4*4=13000704
2nd layer CNN: 2*42*42*48*48*5*5=203212800
3rd layer CNN: 2*18*18*48*48*4*4=23887872
4th layer CNN: 2*6*6*4*4*48*48=2654208
1st layer FC: 2*48*3*3*200=172800
2nd layer FC: 2*200*2=800
Total forward flop: 13000704+203212800+23887872+2654208+172800+800=242929184
Training compute: 242929184*3*9000000*30=196772639040000000=1.9e17

GPU hour estimate
340 minutes per epoch, 30 epochs
4 GTX 580 with 1580000000000 FLOPs
Assumed utilization of 0.3
30*340*60*1580000000000*4*0.3=1160352000000000000=1.16e18

Geometric mean: sqrt(196772639040000000*1160352000000000000)=4.78e17
",ISBI 2012 EM Segmentation Challenge,ISBI 2012 EM Segmentation Challenge,"[images]
""This amounts to 3 million training examples in total, in which both classes are equally represented.""",30.0,,,17.0,"Training time for one epoch varies from approximately 170 minutes for N1 (w = 65) to 340 minutes for N4 (w = 95). All nets are trained for 30 epochs, which leads to a total training time of several days.",NVIDIA GeForce GTX 580,Supervised,,,Confident,"We address a central problem of neuroanatomy, namely, the automatic segmentation of neuronal structures depicted in stacks of electron microscopy (EM) images. This is necessary to efficiently map 3D brain structure and connectivity. To segment biological neuron membranes, we use a special type of deep artificial neural network as a pixel classifier. The label of each pixel (membrane or non-membrane) is predicted from raw pixel values in a square window centered on it. The input layer maps each window pixel to a neuron. It is followed by a succession of convolutional and max-pooling layers which preserve 2D information and extract features with increasing levels of abstraction. The output layer produces a calibrated probability for each class. The classifier is trained by plain gradient descent on a 512 × 512 × 30 stack with known ground truth, and tested on a stack of the same size (ground truth unknown to the authors) by the organizers of the ISBI 2012 EM Segmentation Challenge. Even without problem-specific postprocessing, our approach outperforms competing techniques by a large margin in all three considered metrics, i.e. rand error, warping error and pixel error. For pixel error, our approach is the only one outperforming a second human observer.",2025-11-28 11:53:44+00:00,Lovis Heindrich,,,"Switzerland,Switzerland",,,,4.0,,,,,,,"Academia,Academia",,,,,Unreleased,,,3.9045853984413377,DNN EM segmentation,,,,244,,,,2024-10-17T12:14:30.000Z,,,True,2116.2977934480996,122420167885382060,2010-11-09,2.0670773442847366,1581000000000,,1,,,FP32,1581000000000,"Operation counting,Hardware",,,,NVIDIA,,,True,,,,,,,,,4710.247852829266,3000000
DistBelief NNLM,Language,Semantic embedding,"Tomas Mikolov, Kai Chen, G. Corrado, J. Dean","Highly cited,SOTA improvement","they seem to use their own evaluation protocol, I don't see any standard benchmarks",,https://arxiv.org/abs/1301.3781,41000.0,Efficient Estimation of Word Representations in Vector Space,2013-01-16,Google,,,2612736000000000000,"Trained for 14 days on 180 CPU cores (Table 6)
Roughly estimating the performance of CPUs in a HPC around 2013: 16 FP32 operations per cycle, 2.5GHz, 0.3 utilization
Time: 14*24*60*60=1209600s
FLOPs: 0.3*180*16*2500000000=2160000000000
Training compute: 1209600s * 2160000000000 = 2612736000000000000 = 2.61e18
https://www.wolframalpha.com/input?i=16+FLOP+*+2.5+GHz+*+180+*+14+days+*+0.3",Google news,,Largest system is trained on 6B words (Table 6),,,,336.0,Trained for 14 days (Table 6),,,,,Likely,"We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.",2025-11-28 11:53:44+00:00,Lovis Heindrich,,,United States of America,,,,180.0,,,,,,,Industry,,,,,,,,3255.1839637787602,DistBelief NNLM,,,,,,,,2024-05-28T07:27:17.000Z,,,True,,802638508014466.1,,NaN,,,0,,,FP32,,Hardware,,,,,,,True,,,,,,,,,211961.15337731695,6000000000
Visualizing CNNs,Vision,Image classification,"MD Zeiler, R Fergus",Highly cited,,,https://arxiv.org/abs/1311.2901,16599.0,Visualizing and Understanding Convolutional Networks,2013-11-12,New York University (NYU),,,532000000000000000,"1 GPU * 12 days * 1.54 TFLOPS/GTX 580 * 0.33 utilization 
= 532 PF = 0.0062 pfs-days

Source: https://openai.com/blog/ai-and-compute

""We stopped training after 70 epochs,
which took around 12 days on a single GTX580 GPU""",,,,,,,,,NVIDIA GeForce GTX 580,,,Academia,Confident,,2026-01-01 14:02:40+00:00,Robi Rahman,,,United States of America,,,,,,,,,,,Academia,,,,,,,,12.994002368456531,Visualizing CNNs,,,,244,,,,2023-05-29T21:06:54.000Z,,FP32,True,,40941965755789890,2010-11-09,3.0088980150581794,1581000000000,,1,,,FP32,1581000000000,"Hardware,Third-party estimation",,,,NVIDIA,,,True,,,,,,,,,,7680000
TransE,Language,Entity embedding,"Antoine Bordes, Nicolas Usunier, Alberto Garcia- Duran, Jason Weston, and Oksana Yakhnenko",Highly cited,,,https://papers.nips.cc/paper/2013/hash/1cecc7a77928ca8133fa24680a88d2f9-Abstract.html,8347.0,Translating Embeddings for Modeling Multi- relational Data,2013-12-05,"Universite de Technologie de Compiègne – CNRS,Google",942000000.0,"Based on the TransE architecture, the authors give a formula for how the model size scales with the dimensionality of the dataset. The model scale is proportional to: k*(n_e+n_r) where k is the embeddings dimension, n_e is the number of entities, and n_r is the number of relationships.

They studied using the TransE model for two datasets: FB15k and FB1M. The FB15k model has 810000 parameters.

FB15k has 14951 entities and 1345 relationships. FB1M has 1000000 entities and 23382 relationships. Therefore, the FB1M model will be bigger than the FB15k model by a factor of (23382e6)/(14951*1345) => N = 8.1e5 * (23382e6)/(14951*1345) = 942e6.",1340928000000000000,"8 GPUs (they don't specify which, so I used the average for FP32 for 2017 from the write-up table)
8 hours 
0.33 util rate",,,"""it can be successfully trained on a large scale data set with 1M
entities, 25k relationships and more than 17M training samples""",,,,,,,,,Industry,Speculative,,2026-01-01 14:02:39+00:00,Robi Rahman,,,"France,United States of America",,,,,,,,,,,"Academia,Industry",,,,,,,,30.028097313317172,TransE,,,,,,,,2023-05-29T21:06:54.000Z,,,True,,44655776421948370,,NaN,,,0,,,FP32,,Hardware,,,,,,,True,,,,,,,,,,17500000
SPPNet,Vision,Image classification,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun",Highly cited,,,https://arxiv.org/abs/1406.4729,12164.0,Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition,2014-06-18,"Microsoft,Xi’an Jiaotong University,University of Science and Technology of China (USTC)",,,3411072000000000000,"""All networks in this paper can be
trained on a single GeForce GTX Titan GPU (6 GB memory) within two to four weeks.""
4.7e12 FLOP/s * 4* 7*24*60*60 seconds * 0.3 utilisation",ImageNet-1k,,"Section 3.1: ""We train the networks on the 1000-category training
set of ImageNet 2012.""",,,,672.0,"""All networks in this paper can be trained on a single GeForce GTX Titan GPU (6 GB memory) within two to four weeks.""",NVIDIA GeForce GTX TITAN,,,Industry,Confident,,2026-01-01 14:02:39+00:00,Robi Rahman,,,"United States of America,China,China",,,,,,,,,,,"Industry,Academia,Academia",,,,,,,,51.65209223027738,SPPNet,,,,250,,,,2023-05-29T21:06:54.000Z,,FP32,True,,66039377161967130,2013-02-19,1.325119780971937,4709000000000,,1,,,FP32,4709000000000,Hardware,,,,NVIDIA,,,True,,,,,,,,,,1280000
RNNsearch-50*,Language,Translation,"D Bahdanau, K Cho, Y Bengio",Highly cited,,,https://arxiv.org/abs/1409.0473,28579.0,Neural Machine Translation by Jointly Learning to Align and Translate,2014-09-01,"Jacobs University Bremen,University of Montreal / Université de Montréal",,,1555200000000000000,"From https://openai.com/blog/ai-and-compute/ Appendix.

0.018 pfs-days
(86400*10^15*0.018)

252 hours in a Quadro K-6000 GPU (assumed utilization: 0.33)

5196000000000 FLOP/s *252 hours * 3600 second/hour * 0.33 utilization = 1555200000000000000 FLOP",WMT'14 + selection,,"[WORDS]
""WMT ’14 contains the following English-French parallel corpora: Europarl (61M words), news
commentary (5.5M), UN (421M) and two crawled corpora of 90M and 272.5M words respectively,
totaling 850M words. Following the procedure described in Cho et al. (2014a), we reduce the size of
the combined corpus to have 348M words using the data selection method by Axelrod et al. (2011).""",,,,,,NVIDIA Quadro K6000,,,Academia,Confident,,2026-01-01 14:02:40+00:00,Robi Rahman,,,"Germany,Canada",,,,,,,,,,,"Academia,Academia",,,,,,,,,RNNsearch-50*,,,,225,,,,2023-05-29T21:06:54.000Z,,,True,,Infinity,2013-07-23,1.108829568788501,5196000000000,,1,,,FP32,5196000000000,Third-party estimation,,,,NVIDIA,,,True,,,,,,,,,,232000000
VGG16,Vision,Image classification,Karen Simonyan; Andrew Zisserman,Highly cited,,,https://arxiv.org/abs/1409.1556,107801.0,Very Deep Convolutional Networks for Large-Scale Image Recognition,2014-09-04,University of Oxford,138000000.0,"Source: Table 2
https://arxiv.org/abs/1409.1556",12291000000000000000,"3 weeks * 4 Titan Black GPUs * 0.30 utilization

Section 3.3: ""On a system equipped with four NVIDIA Titan Black GPUs, training a single net took 2–3 weeks depending on the architecture.""

Titan Black performance: 5.645 TFLOPS (assuming FP32)

https://www.wolframalpha.com/input?i=5.645+TFLOPS+*+3+weeks+*+4+*+0.3


",ILSVRC 2012 subset of ImageNet,,"""In this section, we present the image classification results achieved by the described ConvNet architectures on the ILSVRC-2012 dataset (which was used for ILSVRC 2012–2014 challenges). The dataset includes images of 1000 classes, and is split into three sets: training (1.3M images), validation (50K images), and testing (100K images with held-out class labels).""

This is confirmed by section 3.1 Training:
""The batch size was set to 256""
""In total, the learning rate was decreased 3 times, and the learning was stopped after 370K iterations (74 epochs)""
256 * 370k/74 = 1.3M",74.0,15300000000.0,"""Remarkably, although the depth is significantly increased, the 152-layer ResNet (11.3 billion FLOPs) still has lower complexity than VGG-16/19 nets (15.3/19.6 billion FLOPs).""

Source: https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf",504.0,,NVIDIA GeForce GTX Titan Black,,,Academia,Confident,,2026-01-01 14:02:40+00:00,Robi Rahman,,,United Kingdom of Great Britain and Northern Ireland,,,,4.0,,,,,256.0,,Academia,,,,,,,,239.2376655010467,VGG16,,,,250,,,,2023-05-29T21:06:54.000Z,,FP32,True,2137.653074766455,51375689418546950,2014-02-18,0.5420944558521561,5645000000000,,1,,,FP32,5645000000000,Hardware,,,,NVIDIA,,,True,,,,,,,,,9359.364939195508,1300000
VGG19,Vision,Image classification,"Karen Simonyan, Andrew Zisserman",Highly cited,,,https://arxiv.org/abs/1409.1556,107801.0,Very Deep Convolutional Networks for Large-Scale Image Recognition,2014-09-04,University of Oxford,144000000.0,"Source: Table 2
https://arxiv.org/abs/1409.1556",11000000000000000000,"Authors of ""AI and Memory Wall"" (https://github.com/amirgholami/ai_and_memory_wall) estimated model's training compute as 11,000 PFLOP = 1.1*10^19 FLOP",ILSVRC 2012 subset of ImageNet,,"""In this section, we present the image classification results achieved by the described
ConvNet architectures on the ILSVRC-2012 dataset (which was used for ILSVRC 2012–2014 challenges). The dataset includes images of 1000 classes, and is split into three sets: training (1.3M images), validation (50K images), and testing (100K images with held-out class labels).""",,19600000000.0,"""Remarkably, although the depth is significantly increased, the 152-layer ResNet (11.3 billion FLOPs) still has lower complexity than VGG-16/19 nets (15.3/19.6 billion FLOPs).""

Source: https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf",,,,,,Academia,Likely,"In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.",2026-01-01 14:02:40+00:00,Robi Rahman,,,United Kingdom of Great Britain and Northern Ireland,,,,,,,,,,,Academia,,,,,,,,,VGG19,,,,,,,,2023-05-29T21:06:54.000Z,,FP32,True,,Infinity,,NaN,,,0,,,FP32,,Third-party estimation,,,,,,,True,,,,,,,,,,1300000
Seq2Seq LSTM,Language,Translation,"Ilya Sutskever, Oriol Vinyals, Quoc V. Le",Highly cited,,,https://arxiv.org/abs/1409.3215,21520.0,Sequence to Sequence Learning with Neural Networks,2014-09-10,Google,1920000000.0,"The resulting LSTM has 384M parameters of which 64M are pure recurrent connections (32M for the “encoder” LSTM and 32M
for the “decoder” LSTM).
The paper uses an ensemble of 5 LSTMs.",56000000000000000000,"384E+6 parameters * 2 FLOP/parameter * (348E+6 + 304E+6 points per epoch) * 7.5 epochs * 3 FLOP/point ~= 1.126656e+19 FLOP
Times 5 independent models in ensemble => 5.6E+19 FLOP

If we assume NVIDIA K40 (in use at the time): 10 days * 24 * 60 * 60 seconds/day * 8 GPUs * 33% * 5e12 FLOP/s * 5 models in ensemble ~= 5.7E+19 FLOP

Authors of ""AI and Memory Wall"" estimated model's training compute as 11,000 PFLOPS = 1.1*10^19 FLOPS
(https://github.com/amirgholami/ai_and_memory_wall)",WMT14,,"[WORDS]
""We used the WMT’14 English to French dataset. We trained our models on a subset of 12M sentences consisting of 348M French words and 304M English words, which is a clean “selected”
subset from [29].""",7.5,,,240.0,Training took about 10 days,,,,Industry,Confident,"Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT'14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.",2026-01-01 14:02:40+00:00,Robi Rahman,,,United States of America,,,,,,,,,,,Industry,,,,,,,,,Seq2Seq LSTM,,,,,,,,2023-05-29T21:06:54.000Z,,,True,,Infinity,,NaN,,,0,,,FP32,,"Operation counting,Hardware,Third-party estimation",,,,,,,True,,,,,,,,,,870000000
SNM-skip,Language,Language modeling,"Noam Shazeer, Joris Pelemans, Ciprian Chelba",SOTA improvement,'When using skip-gram features the models are able to match the state-of-the-art recurrent neural network (RNN) LMs; combining the two modeling techniques yields the best known result on the benchmark. ' - from abstract,,https://arxiv.org/abs/1412.1454,14.0,Skip-gram Language Modeling Using Sparse Non-negative Matrix Probability Estimation,2014-12-03,Google,62000000000.0,62B from Table 2,297600000001000000000,https://www.wolframalpha.com/input?i=0.8+billion+*+62+billion+*+6+FLOP,One Billion Word benchmark,'Our experimental setup used the One Billion Word Benchmark corpus' from section 4.1 - 'Total number of training tokens is about 0.8 billion',1B from 'Our experimental setup used the One Billion Word Benchmark corpus' from section 4.1 - 'Total number of training tokens is about 0.8 billion',,,,,,,,,,Speculative,"We present a novel family of language model (LM) estimation techniques named Sparse Non-negative Matrix (SNM) estimation. A first set of experiments empirically evaluating it on the One Billion Word Benchmark shows that SNM n-gram LMs perform almost as well as the well-established Kneser-Ney (KN) models. When using skip-gram features the models are able to match the state-of-the-art recurrent neural network (RNN) LMs; combining the two modeling techniques yields the best known result on the benchmark. The computational advantages of SNM over both maximum entropy and RNN LM estimation are probably its main strength, promising an approach that has the same flexibility in combining arbitrary features effectively and yet should scale to very large amounts of data as gracefully as n-gram LMs do. ",2025-11-28 11:53:44+00:00,Bartosz Podkanowicz,,,United States of America,,,,,,,,,,,Industry,,,,,,,,,SNM-skip,,,,,,,,2024-03-03T18:18:12.000Z,,,True,,Infinity,,NaN,,,0,,,FP32,,Operation counting,,,,,,,True,,,,,,,,,,800000000
"MSRA (C, PReLU)",Vision,Image classification,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun",Highly cited,,,https://arxiv.org/abs/1502.01852,20078.0,Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification,2015-02-06,Microsoft Research,87048800.0,"I used the architecture in table 3
I ignored biases, and assumed a SPP bin size of 256

3*7*7*96+96*3*3*384+384*3*3*384*5+384*3*3*768+768*3*3*768*5+768*3*3*896+896*3*3*896*5+896*(7*7+3*3+2*2+1)*4096+4096*4096+4096*1000=330581792



",23974030080000000000,"""training C on eight K40 GPUs, takes about 3-4 weeks""
0.33 util rate
(From Imagenet paper-data, Besiroglu et al., forthcoming) ",,"They won ILSVRC 2015, but actually the classification dataset is the same as 2012","""We perform the experiments on the 1000-class ImageNet 2012 dataset"", paper; ImageNet 2012 train set size from https://huggingface.co/datasets/imagenet-1k",,,,588.0,,NVIDIA Tesla K40t,,,Industry,Confident,"Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on our PReLU networks (PReLU-nets), we achieve 4.94% top-5 test error on the ImageNet 2012 classification dataset. This is a 26% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66%). To our knowledge, our result is the first to surpass human-level performance (5.1%, Russakovsky et al.) on this visual recognition challenge.",2025-11-28 11:53:44+00:00,Robi Rahman,,,United States of America,,,,,,,,,,,Industry,,,,,,,,1394.115708804708,"MSRA (C, PReLU)",,,,245,,,,2023-05-29T21:06:54.000Z,,FP32,True,,17196585569324760,2013-11-22,1.2073921971252566,5046000000000,,1,,,FP32,5046000000000,Hardware,,,,NVIDIA,,,True,,,,,,,,,,1280000
AlphaGo Fan,Games,Go,"David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, Demis Hassabis","Highly cited,SOTA improvement",,Unreleased,https://www.nature.com/articles/nature16961,18175.0,Mastering the game of Go with deep neural networks and tree search,2015-10-01,DeepMind,8209984.0,"The input to the policy network is a 19 × 19 × 48 image stack consisting of 48 feature planes. The first hidden layer zero pads the input into a 23 × 23 image, then convolves k filters of kernel size 5 × 5 with stride 1 with the input image and applies a rectifier nonlinearity. Each of the subsequent hidden layers 2 to 12 zero pads the respective previous hidden layer into a 21 × 21 image, then convolves k filters of kernel size 3 × 3 with stride 1, again followed by a rectifier nonlinearity. The final layer convolves 1 filter of kernel size 1 × 1 with stride 1, with a different bias for each position, and applies a softmax function. The match version of AlphaGo used k = 192 filters; Fig. 2b and Extended Data Table 3 additionally show the results of training with k = 128, 256 and 384 filters.

The input to the value network is also a 19 × 19 × 48 image stack, with an additional binary feature plane describing the current colour to play. Hidden layers 2 to 11 are identical to the policy network, hidden layer 12 is an additional convolution layer, hidden layer 13 convolves 1 filter of kernel size 1 × 1 with stride 1, and hidden layer 14 is a fully connected linear layer with 256 rectifier units. The output layer is a fully connected linear layer with a single tanh unit.",380000000000000000000,"Assume 0.3 utilisation rate, 1e13 GPU FLOP/s [single precision]. Trained in three stages using 50 GPUs over 3 weeks + 1 day + 1 week

Training compute = (50 GPUs)(29 days)(86400s/day)(0.3 utilisation rate)(1e13 FLOP/s) = 3.8e20 FLOPs
",,,Supervised learning + self-play,,,"Distributed: 176 GPUs + 1202 PUs + 40 search threads
Single machine: 8 GPUs + 48 CPUs 

https://www.nature.com/articles/nature16961",,,,,,Industry,Likely,"A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, superhuman proficiency in challenging domains. Recently, AlphaGo became the first program to defeat a world champion in the game of Go. The tree search in AlphaGo evaluated positions and selected moves using deep neural networks. These neural networks were trained by supervised learning from human expert moves, and by reinforcement learning from self-play. Here we introduce an algorithm based solely on reinforcement learning, without human data, guidance or domain knowledge beyond game rules. AlphaGo becomes its own teacher: a neural network is trained to predict AlphaGo’s own move selections and also the winner of AlphaGo’s games. This neural network improves the strength of the tree search, resulting in higher quality move selection and stronger self-play in the next iteration. Starting tabula rasa, our new program AlphaGo Zero achieved superhuman performance, winning 100–0 against the previously published, champion-defeating AlphaGo.",2026-01-01 14:02:04+00:00,Robi Rahman,,,United Kingdom of Great Britain and Northern Ireland,,,,,,,,,,,Industry,,,,,Unreleased,,,4828.322276803044,AlphaGo Fan,,,,,,,,2023-05-29T21:06:54.000Z,,,True,,78702285848990130,,NaN,,,0,,,FP32,,Hardware,,,,,,,True,,,,,,,,,,12697600000
SAF R-CNN,Vision,Object detection,"Jianan Li, Xiaodan Liang, ShengMei Shen, Tingfa Xu, Jiashi Feng, Shuicheng Yan",SOTA improvement,"Pedestrian Detection on Caltech benchmark

""our method achieves state-of-the-art performance on Caltech, INRIA, and ETH, and obtains competitive results on KITTI. """,Unreleased,https://arxiv.org/abs/1510.08160,,Scale-aware Fast R-CNN for Pedestrian Detection,2015-10-28,"Beijing Institute of Technology,Sun Yat-sen University,Panasonic R&D,National University of Singapore",138000000.0,"Taken from VGG16 base model, ignoring minor architectural changes",12311081250000001000,"Base model: 12291000000000002000
Base model inference FLOP: 15300000000
Finetune for 7 epochs with 62500 training examples
15300000000*3*62500*7=20081250000000000=2e16

Total compute: 12291000000000002000+20081250000000000=12311081250000002000",,Caltech pedestrian dataset,"""There are totally 350,000 bounding boxes of about 2,300 unique pedestrians labeled in 250,000 frames""
""We use dense sampling of the training data (every 4th frame""",7.0,15300000000.0,From VGG16 base model,,,NVIDIA GeForce GTX TITAN X,Supervised,,,Likely,"In this work, we consider the problem of pedestrian detection in natural scenes. Intuitively, instances of pedestrians with different spatial scales may exhibit dramatically different features. Thus, large variance in instance scales, which results in undesirable large intra-category variance in features, may severely hurt the performance of modern object instance detection methods. We argue that this issue can be substantially alleviated by the divide-and-conquer philosophy. Taking pedestrian detection as an example, we illustrate how we can leverage this philosophy to develop a Scale-Aware Fast R-CNN (SAF R-CNN) framework. The model introduces multiple built-in sub-networks which detect pedestrians with scales from disjoint ranges. Outputs from all the sub-networks are then adaptively combined to generate the final detection results that are shown to be robust to large variance in instance scales, via a gate function defined over the sizes of object proposals. Extensive evaluations on several challenging pedestrian detection datasets well demonstrate the effectiveness of the proposed SAF R-CNN. Particularly, our method achieves state-of-the-art performance on Caltech, INRIA, and ETH, and obtains competitive results on KITTI. ",2025-11-28 11:53:44+00:00,Lovis Heindrich,,,"China,China,Singapore,Singapore",VGG16,2.008125e+16,15300000000*3*62500*7=20081250000000000=2e16,1.0,,,,,,,"Academia,Academia,Industry,Academia",,,,,,,,,SAF R-CNN,,,,250,12291000000000000000,2048.0,,2025-01-22T10:34:39.000Z,,,True,290.9065505304121,Infinity,2015-03-17,0.6160164271047228,6691000000000,,1,,,FP32,6691000000000,Operation counting,,,,NVIDIA,,,True,,,,,,,,,,350000
Inception v3,Vision,Image classification,"Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, Zbigniew Wojna",Highly cited,,,https://arxiv.org/abs/1512.00567,29696.0,Rethinking the inception architecture for computer vision.,2015-12-02,"Google,University College London (UCL)",23626728.0,Table 3 from Xception paper,100000000000000000000,"Authors of ""AI and Memory Wall"" (https://github.com/amirgholami/ai_and_memory_wall) estimated model's training compute as 100,000 PFLOP = 1*10^20 FLOP",ILSVRC 2012 subset of ImageNet,,"The full dataset is a lot larger and has far more categories. When people say ""ImageNet"" they're usually referring to the subset of the full dataset with 1000 categories and 1.2million images, found here: https://image-net.org/challenges/LSVRC/2012/",,114830000000.0,"Rados (FLOPs)
https://drive.google.com/drive/folders/1bhy5z6hh1n3wCHx6528Xb7xB1KhYdAL1",,,,,,Industry,Likely,"Convolutional networks are at the core of most state-of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we explore ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21.2% top-1 and 5.6% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3.5% top-5 error on the validation set (3.6% error on the test set) and 17.3% top-1 error on the validation set.",2026-01-01 14:02:04+00:00,Robi Rahman,,,"United States of America,United Kingdom of Great Britain and Northern Ireland",,,,,,,,,,,"Industry,Academia",,,,,,,,1218.1810104797394,Inception v3,,,,,,,,2023-05-29T21:06:54.000Z,,,True,,82089606667418320,,NaN,,,0,,,FP32,,Third-party estimation,,,,,,,True,,,,,,,,,,1200000
DeepSpeech2 (English),Speech,Speech recognition (ASR),"Dario Amodei, Rishita Anubhai, Eric Battenberg, Carl Case, Jared Casper, Bryan Catanzaro, Jingdong Chen, Mike Chrzanowski, Adam Coates, Greg Diamos, Erich Elsen, Jesse Engel, Linxi Fan, Christopher Fougner, Tony Han, Awni Hannun, Billy Jun, Patrick LeGresley, Libby Lin, Sharan Narang, Andrew Ng, Sherjil Ozair, Ryan Prenger, Jonathan Raiman, Sanjeev Satheesh, David Seetapun, Shubho Sengupta, Yi Wang, Zhiqian Wang, Chong Wang, Bo Xiao, Dani Yogatama, Jun Zhan, Zhenyao Zhu",,,,https://arxiv.org/abs/1512.02595,3094.0,Deep Speech 2: End-to-End Speech Recognition in English and Mandarin,2015-12-08,Baidu Research - Silicon Valley AI Lab,38000000.0,All networks have 38 million parameters.,26000000000000000000,"1 timestep = (1280 hidden units)^2 * (7 RNN layers * 4 matrices for bidirectional + 2 DNN layers) * (2 for doubling parameters from 36M to 72M) = 98 MFLOP
20 epochs * 12,000 hours * 3600 seconds/hour * 50 samples/sec * 98 MFLOP * 3 add-multiply * 2 backprop 
= 26,000 PF = 0.30 pfs-days

See also AI and Compute by Dario Amodei and OpenAI https://openai.com/research/ai-and-compute",,,"""Our English speech system is trained on 11,940 hours of speech, while the Mandarin system is trained on 9,400 hours.""

11,940 * 13,680 = 163339200",,1800000000.0,,120.0,"""5 days"" from AI and Compute https://openai.com/index/ai-and-compute/",NVIDIA GeForce GTX TITAN X,,,Industry,Confident,"We show that an end-to-end deep learning approach can be used to recognize either English or Mandarin Chinese speech--two vastly different languages. Because it replaces entire pipelines of hand-engineered components with neural networks, end-to-end learning allows us to handle a diverse variety of speech including noisy environments, accents and different languages. Key to our approach is our application of HPC techniques, resulting in a 7x speedup over our previous system. Because of this efficiency, experiments that previously took weeks now run in days. This enables us to iterate more quickly to identify superior architectures and algorithms. As a result, in several cases, our system is competitive with the transcription of human workers when benchmarked on standard datasets. Finally, using a technique called Batch Dispatch with GPUs in the data center, we show that our system can be inexpensively deployed in an online setting, delivering low latency when serving users at scale.",2026-01-01 14:02:03+00:00,Robi Rahman,,,United States of America,,,,16.0,,,,,,,Industry,,,,1920.0,,,,214.33761025036443,DeepSpeech2 (English),,"""Overall the system sustains approximately 50 teraFLOP/second when training on 16 GPUs. This amounts to 3 teraFLOP/second per GPU which is about 50% of peak theoretical performance""
HFU = (50/16) TFLOP / 6.69 TFLOP = 0.4670",,250,,,,2023-05-29T21:06:54.000Z,,FP32,True,8463.467702301677,121303955799590200,2015-03-17,0.7282683093771389,6691000000000,,1,,,FP32,6691000000000,"Operation counting,Third-party estimation",,,,NVIDIA,,,True,,,,,,,0.467,,37402.47147663551,716400000
AlphaGo Lee,Games,Go,"David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, Demis Hassabis",Highly cited,,Unreleased,https://www.nature.com/articles/nature16961,18175.0,Mastering the game of Go with deep neural networks and tree search,2016-01-27,DeepMind,,,1.9e+21,"This number is pretty uncertain. I expect it to be right to around a factor of 3, at least compared to AlphaGo Fan.

The architecture used was pretty much the same as AlphaGo Fan, but it was ""trained for longer"" and had around 5.33x the number of convolutional layers of AlphaGo Fan (256/48 = 5.33). 

The convolutional layers are the major contributor to the training compute, so I somewhat arbitrarily just multiply the compute for AlphaGo Fan by 5. Thus 3.8e20 * 5 = 1.9e21

Otherwise there has been little said about this model specifically - I've mainly relied on the source for AlphaGo Zero and AlphaGo Fan, linked below

AlphaGo Fan: https://www.nature.com/articles/nature16961

AlphaGo Zero: https://www.nature.com/articles/nature24270.epdf?author_access_token=VJXbVjaSHxFoctQQ4p2k4tRgN0jAjWel9jnR3ZoTv0PVW4gB86EEpGqTRDtpIz-2rmo8-KG06gqVobU5NSCFeHILHcVFUeMsbvwS-lxjqQGg98faovwjxeTUgZAUMnRQ",,,"We trained the policy network pσ to classify positions according to expert moves played in the KGS data set. This data set contains 29.4 million positions from 160,000 games played by KGS 6 to 9 dan human players; 35.4% of the games are handicap games.",,,,696.0,"Training times are given for several components:
- Policy network classifier: 3 weeks
- Policy network RL: 1 day
- Value network regression: 1 week
- Rollout policy: ""Similar to the policy network, the weights π of the rollout policy are trained from 8 million positions from human games on the Tygem server to maximize log likelihood by stochastic gradient descent. Rollouts execute at approximately 1,000 simulations per second per CPU thread on an empty board."" could suggest (8M sims / 1000 sims/sec) / 3600 sec/hr = 2.2 hours, if each position corresponds to only one simulation (unclear)

Total: 29 days or 696 hours",,,,Industry,Speculative,"The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses ‘value networks’ to evaluate board positions and ‘policy networks’ to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8% winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.",2026-01-01 14:02:05+00:00,Robi Rahman,,,United Kingdom of Great Britain and Northern Ireland,,,,,,,,,,,Industry,,,,,Unreleased,,,22206.80954117433,AlphaGo Lee,,,,,,,,2023-05-29T21:06:54.000Z,,,True,,85559341447818130,,NaN,,,0,,,FP32,,Comparison with other models,,,,,,,True,,,,,,,,,,300000000
BIG LSTM+CNN INPUTS ,Language,Language modeling,"Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, Yonghui Wu",,,,https://arxiv.org/abs/1602.02410,,Exploring the Limits of Language Modeling,2016-02-11,Google Brain,1040000000.0,1.04B (Table 1),102208830000000000000,"assuming (!) 10 days similarly to another model in the paper
240*3600*5046000000000*32*0.3 = 4.1853542e+19

assuming (!) 50 epochs similarly to another model in the paper
6*1.04*10^9*0.8*10^9*50 = 2.496e+20

sqrt(4.1853542e+19*2.496e+20) = 1.0220883e+20",One Billion Word benchmark,,"""The experiments are performed on the 1B Word Benchmark data set introduced by (Chelba et al., 2013), which is
a publicly available benchmark for measuring progress of
statistical language modeling. The data set contains about
0.8B words with a vocabulary of 793471 words, including
sentence boundary markers.""",,,,,"*I assumed Tesla K40s though they stated it was just one of Tesla K40

""The best model needs about 5 days to get to 35 perplexity and 10 days to 32.5.""

10*24 = 240

This model is not the biggest but has the best performance, I assume it is *likely* to have comparativeley similar training time",NVIDIA Tesla K40s,,,,Likely,"In this work we explore recent advances in Recurrent Neural Networks for large scale Language Modeling, a task central to language understanding. We extend current models to deal with two key challenges present in this task: corpora and vocabulary sizes, and complex, long term structure of language. We perform an exhaustive study on techniques such as character Convolutional Neural Networks or Long-Short Term Memory, on the One Billion Word Benchmark. Our best single model significantly improves state-of-the-art perplexity from 51.3 down to 30.0 (whilst reducing the number of parameters by a factor of 20), while an ensemble of models sets a new record by improving perplexity from 41.0 down to 23.7. We also release these models for the NLP and ML community to study and improve upon.",2025-11-28 11:53:44+00:00,Natalia Martemianova,,,United States of America,,,,32.0,,,,,,,Industry,,,,240.0,,,,71.21404931614963,BIG LSTM+CNN INPUTS,,,,245,,,,2025-01-13T16:41:55.000Z,,,True,16564.40221110092,1435234072229923300,2013-11-22,2.220396988364134,5046000000000,,1,,,FP32,5046000000000,"Hardware,Operation counting",,,,NVIDIA,,,,,,,,,,,,405868.30769230763,
GNMT,Language,Translation,"Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, Łukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes, Jeffrey Dean",Highly cited,,Hosted access (no API),https://arxiv.org/abs/1609.08144,7072.0,Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation,2016-09-26,Google,278000000.0,"Table 5 in 'Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer'

https://arxiv.org/abs/1701.06538",6.620000000001e+21,"From AI and Compute:
""sqrt(10 * 100) factor added because production model used 2-3 orders of magnitude more data, but only 1 epoch rather than 10.
96 K80 GPU’s * 9 days * 8.5 TFLOPS * 0.33 utilization * sqrt(10 * 100)  
= 6.9e6 PF = 79 pfs-days""
source: https://openai.com/blog/ai-and-compute/

https://www.wolframalpha.com/input?i=96+*+9+days+*+8.5+TFLOPS+*+0.33+*+sqrt%281000%29
",,,"[WORDS]
"" On WMT En→Fr, the training set contains 36M sentence pairs. On WMT En→De, the training set contains 5M sentence pairs.""
""we also test GNMT on Google’s translation production corpora, which are two to three decimal orders of magnitudes bigger than the WMT corpora for a given language pair.""

41M sentence pairs * 2 sentences per pair * 15 words/sentence * 10^2.5",1.0,,,,"Test model used 96 K80 for 9 days, then this was scaled up by 31x for the production model, but unclear how many GPUs were used or how long it was trained for. The production run used 96 * 9 days * sqrt(1000) ~= 655730 chip-hours.",NVIDIA Tesla K80,Reinforcement learning,,Industry,Likely,"Neural Machine Translation (NMT) is an end-to-end learning approach for automated translation, with the potential to overcome many of the weaknesses of conventional phrase-based translation systems. Unfortunately, NMT systems are known to be computationally expensive both in training and in translation inference. Also, most NMT systems have difficulty with rare words. These issues have hindered NMT's use in practical deployments and services, where both accuracy and speed are essential. In this work, we present GNMT, Google's Neural Machine Translation system, which attempts to address many of these issues. Our model consists of a deep LSTM network with 8 encoder and 8 decoder layers using attention and residual connections. To improve parallelism and therefore decrease training time, our attention mechanism connects the bottom layer of the decoder to the top layer of the encoder. To accelerate the final translation speed, we employ low-precision arithmetic during inference computations. To improve handling of rare words, we divide words into a limited set of common sub-word units (""wordpieces"") for both input and output. This method provides a good balance between the flexibility of ""character""-delimited models and the efficiency of ""word""-delimited models, naturally handles translation of rare words, and ultimately improves the overall accuracy of the system. Our beam search technique employs a length-normalization procedure and uses a coverage penalty, which encourages generation of an output sentence that is most likely to cover all the words in the source sentence. On the WMT'14 English-to-French and English-to-German benchmarks, GNMT achieves competitive results to state-of-the-art. Using a human side-by-side evaluation on a set of isolated simple sentences, it reduces translation errors by an average of 60% compared to Google's phrase-based production system.",2026-01-01 14:01:38+00:00,Robi Rahman,,0,United States of America,,,,,,,,,,,Industry,,,,655730.0,Unreleased,presumably deployed via Google translate,,201331.569937687,GNMT,,,,300,,,,2023-05-29T21:06:54.000Z,,FP32,True,,32881082693836440,2014-11-17,1.8590006844626967,8126000000000,,1,,,FP32,8126000000000,"Hardware,Third-party estimation",,,,NVIDIA,,,True,,,,,,,,,,720000000
Xception,Vision,Image classification,François Chollet,Highly cited,,,https://arxiv.org/abs/1610.02357,16557.0,Xception: Deep Learning with Depthwise Separable Convolutions,2016-10-07,Google,22855952.0,Table 3,436000000000000000000,"60 K80 GPUs * 30 days * 8.5 TFLOPS/GPU * 0.33 utilization  = 4.36e20

Authors of ""AI and Memory Wall"" (https://github.com/amirgholami/ai_and_memory_wall) estimated model's training compute as 450,000 PFLOP = 4.5*10^20 FLOP",JFT,"Also ImageNet, but JFT is significantly larger","""JFT is an internal Google dataset for large-scale image classification dataset, first introduced by Hinton et al. in [5], which comprises over 350 million high-resolution images annotated with labels from a set of 17,000 classes. To evaluate the performance of a model trained on JFT, we use an auxiliary dataset, FastEval14k""",,16800000000.0,"Rados (FLOPs)
https://drive.google.com/drive/folders/1bhy5z6hh1n3wCHx6528Xb7xB1KhYdAL1",720.0,"""while the JFT experiments took over one month each.""",NVIDIA Tesla K80,,,Industry,Confident,"We present an interpretation of Inception modules in convolutional neural networks as being an intermediate step in-between regular convolution and the depthwise separable convolution operation (a depthwise convolution followed by a pointwise convolution). In this light, a depthwise separable convolution can be understood as an Inception module with a maximally large number of towers. This observation leads us to propose a novel deep convolutional neural network architecture inspired by Inception, where Inception modules have been replaced with depthwise separable convolutions. We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for), and significantly outperforms Inception V3 on a larger image classification dataset comprising 350 million images and 17,000 classes. Since the Xception architecture has the same number of parameters as Inception V3, the performance gains are not due to increased capacity but rather to a more efficient use of model parameters.",2026-01-01 14:01:39+00:00,Robi Rahman,,,United States of America,,,,60.0,,,,,,,Industry,,,,43200.0,,,,13108.852355728606,Xception,,,,300,,,,2023-05-29T21:06:54.000Z,,,True,37828.64014138488,33259967247206560,2014-11-17,1.88911704312115,8126000000000,,1,,,FP32,8126000000000,"Hardware,Third-party estimation",,,,NVIDIA,,,True,,,,,,,,20829.84110091743,689117.635066259,350000000
NASv3 (CIFAR-10),Vision,"Image classification,Neural Architecture Search - NAS","Barret Zoph, Quoc V. Le",Highly cited,,,https://arxiv.org/abs/1611.01578,5683.0,Neural Architecture Search with Reinforcement Learning,2016-11-05,Google Brain,37400000.0,Table 1,2.2e+21,"50 epochs * 50,000 images * 10.0 GFLOPSs * 12800 networks * 2 add-multiply * 3 backward pass 
= 1.9e6 PF = 22 pfs-days

source: https://openai.com/blog/ai-and-compute/",,,CIFAR-10 (does not factor in augmentation procedures),,,,,,,,,Industry,Likely,"Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than the previous state-of-the-art model that used a similar architectural scheme. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines. Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of-the-art model. The cell can also be transferred to the character language modeling task on PTB and achieves a state-of-the-art perplexity of 1.214.",2026-01-01 14:01:39+00:00,Robi Rahman,,0,United States of America,,,,800.0,,,,,,,Industry,,,,,,,,21184.441090423978,NASv3 (CIFAR-10),,,,,,,,2023-05-29T21:06:54.000Z,,,True,,103849801399502960,,NaN,,,0,,,FP32,,"Third-party estimation,Operation counting",,,,,,,True,,,,,,,,,1835809.3798165137,45000
JFT,Vision,"Image classification,Object detection,Semantic segmentation,Pose estimation","Chen Sun, Abhinav Shrivastava, Saurabh Singh, Abhinav Gupta",SOTA improvement,SOTA on COCO and PASCAL VOC,,https://arxiv.org/abs/1707.02968,2605.0,Revisiting Unreasonable Effectiveness of Data in Deep Learning Era.,2017-07-10,"Google Research,Carnegie Mellon University (CMU)",44654504.0,"Uses ResNet-101 architecture, which has 44,654,504 parameters:
https://resources.wolframcloud.com/NeuralNetRepository/resources/ResNet-101-Trained-on-ImageNet-Competition-Data/",843000000000000000000,"Tesla K80 performance: 8.13 TFLOP/s

Assume 40% utilization

60 days * 50 GPUs * 40% utilization * 8.13 TFLOP/s/GPU = 8.43*10^20 FLOP",JFT-300M,,,4.0,,,1440.0,,NVIDIA Tesla K80,,,Industry,Confident,"The success of deep learning in vision can be attributed to: (a) models with high capacity; (b) increased computational power; and (c) availability of large-scale labeled data. Since 2012, there have been significant advances in representation capabilities of the models and computational capabilities of GPUs. But the size of the biggest dataset has surprisingly remained constant. What will happen if we increase the dataset size by 10x or 100x? This paper takes a step towards clearing the clouds of mystery surrounding the relationship between `enormous data' and visual deep learning. By exploiting the JFT-300M dataset which has more than 375M noisy labels for 300M images, we investigate how the performance of current vision tasks would change if this data was used for representation learning. Our paper delivers some surprising (and some expected) findings. First, we find that the performance on vision tasks increases logarithmically based on volume of training data size. Second, we show that representation learning (or pre-training) still holds a lot of promise. One can improve performance on many vision tasks by just training a better base model. Finally, as expected, we present new state-of-the-art results for different vision tasks including image classification, object detection, semantic segmentation and human pose estimation. Our sincere hope is that this inspires vision community to not undervalue the data and develop collective efforts in building larger datasets.",2026-01-01 14:01:08+00:00,Robi Rahman,,,"United States of America,United States of America",,,,50.0,,,,,32.0,,"Industry,Academia",,,,72000.0,,,,17910.759507843868,JFT,,,,300,,,,2023-05-29T21:06:54.000Z,,FP32,True,31330.704406642573,47066680764197360,2014-11-17,2.64476386036961,8126000000000,,1,,,FP32,8126000000000,Hardware,,,,NVIDIA,,,True,,,,,,,,34338.364791288564,568011.3598171673,5487300000000
OpenAI TI7 DOTA 1v1,Games,Dota 2,,"Historical significance,SOTA improvement",,,https://openai.com/research/dota-2,,Dota 2,2017-08-11,OpenAI,,"Section 4 states: ""we used a model with over 150 million parameters"" but this is for the 5v5 agent, not the 1v1.",604609522259200200000,Extracted from AI and Compute (https://openai.com/blog/ai-and-compute/) charts by using https://automeris.io/WebPlotDigitizer/.,,,,,,,,,,,,Industry,Confident,"We’ve created a bot which beats the world’s top professionals at 1v1 matches of Dota 2 under standard tournament rules. The bot learned the game from scratch by self-play, and does not use imitation learning or tree search. This is a step towards building AI systems which accomplish well-defined goals in messy, complicated situations involving real humans.",2025-11-28 11:53:44+00:00,Robi Rahman,,,United States of America,,,,,,,,,,,Industry,,,,,,,,,OpenAI TI7 DOTA 1v1,,,,,,,,2023-05-29T21:06:54.000Z,,,True,,Infinity,,NaN,,,0,,,FP32,,Third-party estimation,,,,,,,True,,,,,,,,,,
AlphaGo Zero,Games,Go,"D Silver, J Schrittwieser, K Simonyan, I Antonoglou",Highly cited,,Unreleased,https://www.nature.com/articles/nature24270,10021.0,Mastering the game of Go without human knowledge,2017-10-18,DeepMind,46400244.0,"Input size: 19*19*17=6137, internal dimension 19*19
1 conv block
17*3*3*256=39168
39 residual blocks
2*3*3*256*256=1179648
Policy head
256*2+2*19*19*(19*19+2)=262598
Value head
256*1+1*19*19*256+256*1=92928
Total: 39168+39*1179648+262598+92928=46400966",649439910290227000000,"Updating this compute estimate to only account for direct training compute, not synthetic data generation compute.

Number of connections (to calculate forward FLOP)
Input size: 19*19*17, Assuming internal dimension stays at 19*19
1 conv block
19*19*17*3*3*256=14139648
40 residual blocks
19*19*2*3*3*256*256=425852928
Policy head
19*19*256*2+2*19*19*(19*19+2)=446918
Value head
19*19*256*1+1*19*19*256+256*1=185088
Total: 14139648+40*425852928+446918+185088=17048888774
Forward FLOP: 2*17048888774=34097777548
Parameter updates “Parameters were updated from 3.1 million mini-batches of 2,048 positions each.”
Total updates: 3100000*2048=6348800000

Training compute: 3 * 34097777548 FLOP * 6348800000 = 6.5e20 FLOP",,,"""Over the course of training, 29 million games of self-play were generated""

Approx 200 moves per Go game on average

https://homepages.cwi.nl/~aeb/go/misc/gostat.html

Thus 200 * 29e6 = 5.8e9",,,,480.0,,Google TPU v1,Self-supervised learning,,Industry,Confident,"A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, superhuman proficiency in challenging domains. Recently, AlphaGo became the first program to defeat a world champion in the game of Go. The tree search in AlphaGo evaluated positions and selected moves using deep neural networks. These neural networks were trained by supervised learning from human expert moves, and by reinforcement learning from self-play. Here we introduce an algorithm based solely on reinforcement learning, without human data, guidance or domain knowledge beyond game rules. AlphaGo becomes its own teacher: a neural network is trained to predict AlphaGo's own move selections and also the winner of AlphaGo's games. This neural network improves the strength of the tree search, resulting in higher quality move selection and stronger self-play in the next iteration. Starting tabula rasa, our new program AlphaGo Zero achieved superhuman performance, winning 100-0 against the previously published, champion-defeating AlphaGo. © 2017 Macmillan Publishers Limited, part of Springer Nature. All rights reserved.",2026-01-01 14:01:09+00:00,Robi Rahman,,0,United Kingdom of Great Britain and Northern Ireland,,,,,,,,,,,Industry,,,,,Unreleased,,,1213.8707315707268,AlphaGo Zero,,,,75,,,,2023-05-29T21:06:54.000Z,,,True,,535015709168523600,2015-05-15,2.428473648186174,,,1,,,FP32,,"Third-party estimation,Hardware",,,,Google,,,True,True,3.4100000000001e+23,"source: https://docs.google.com/spreadsheets/d/1Kj4Q5WADcDXtUJLIOfGTCE3tGvxNczEMwyy8QtgSkHk/edit#gid=54587040&fvid=1361937389


AGZ had two models, one of which was small and another of which was large. The compute for AGZ is for the large model, which has 40 residual blocks instead of 20.

A second way of looking at this... we believe multiple TPUs were used for training. 29 million games * 211 moves per game on average * 0.8 seconds per move = 4.8952E+09 seconds of player-time across all TPUs.

4.8952E+09 seconds of player-time / (40 days * 24 * 60 * 60 seconds of real time) ~= 1,416 players

4 TPUs per player => 4.8952E+09 * 4 = 1.95808E+10 TPU-seconds
Total compute = 1.95808E+10 TPU-seconds * 92E+12 FLOP/(TPU-second) * 0.4 = 7.2e23 FLOP

So similar to the Cotra and Davidson estimate (within a factor of 2).

Alternative operation counting estimate: 
Number of connections (to calculate forward FLOP)
Input size: 19*19*17, Assuming internal dimension stays at 19*19
1 conv block
19*19*17*3*3*256=14139648
40 residual blocks
19*19*2*3*3*256*256=425852928
Policy head
19*19*256*2+2*19*19*(19*19+2)=446918
Value head
19*19*256*1+1*19*19*256+256*1=185088
Total: 14139648+40*425852928+446918+185088=17048888774
Forward FLOP: 2*17048888774=34097777548
Parameter updates “Parameters were updated from 3.1 million mini-batches of 2,048 positions each.”
Total updates: 3100000*2048=6348800000
Paramter update FLOP: 3*6348800000*34097777548=649439910290227200000
MCTS move generation FLOP
“Over the course of training, 29 million games of selfplay were generated.”
From the main training details (not 40 block specific): “using 1,600 simulations for each MCTS”
Assuming each MCTS simulation requires 1 forward pass
Assuming 200 moves on average per game
MCTS FLOP: 34097777548*29000000*200*1600=3.1642737564544e+23
Total: 3.1642737564544e+23+649439910290227200000=3.170768e+23",,,"[{'id': 'attowK8CXVWqwyxQk', 'url': 'https://v5.airtableusercontent.com/v3/u/49/49/1769875200000/IWPCRwlUggh_LRUHH4yuYw/Ah7hJxP6HlXkcMZEiwfWvXJzlaMaOZk_zJwMeAbaTRZee8TXh6Y-24P0iyfnwu0nOpR9UVG6UM2FLZQgOCeZgqxXd0vRcjgjdkrccuORRbHMvQsgIyETUtVduHQuH3NqHUPcBSHG4NW-oMEh9sOa6_KoypdDHf-QEAc43qMBDaKj1u62SaRfIqoDwCANbTt26wYNJYCrY9scsC_HfYBAqQ/4-4-47Tfu9mkWgLlmiU98eTqDM04fPKLjMzlAaADvL4', 'filename': 'Mastering the game of Go without human knowledge.pdf', 'size': 4030887, 'type': 'application/pdf', 'thumbnails': {'small': {'url': 'https://v5.airtableusercontent.com/v3/u/49/49/1769875200000/sHh-WNcbP2VRGByrJJq2Jg/l3dl9VA9WDndqy3yi4Z_aM-2SdV-aVjtXACwxqjxghfpJAyfPFSPb1sRhpDK-UfMCGCBzF5oVtfnGaOq1dwbjcbpKz5FRF4x2Yqi6rhJBYWfYP8wAa1HEz7KcupCL5C8RVYWAXNtD1I5daWfKH2Q2w/dX4nQH_JSrYZj1BNyQ_a0xRte0gE74SiCPt8MVfSluA', 'width': 27, 'height': 35}, 'large': {'url': 'https://v5.airtableusercontent.com/v3/u/49/49/1769875200000/GmuhYQL3_VoNVNOVnYdS0w/KczfO6Q-fhK-xqA6qRY-R47HlefL1Ze5Go_o0tNzY0wx-AvOkIF8519cM4VSg1BIdodQOTovnblPKXonqxWVkd-g3993nUTdWzO672dxK4PTQ7AjVqxlsr--NJyiaohbqohQuIYQF9UjauXkiQlbQw/c6dyEDP3JNJxriw2y9vxspkJQPnQuMJDq6hTtLSxxsQ', 'width': 512, 'height': 673}}}]",,,,6348800000
ResNeXt-101 32x48d,Vision,Image classification,"Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li, Ashwin Bharambe, Laurens van der Maaten","Highly cited,SOTA improvement","""We show improvements on several image classification and object detection tasks, and report the highest ImageNet-1k single-crop, top-1 accuracy to date: 85.4%",Open weights (non-commercial),https://arxiv.org/abs/1805.00932,1425.0,Exploring the Limits of Weakly Supervised Pretraining,2018-05-02,Facebook,829000000.0,"Table 6
",8.74395e+21,"Table 6: 153e9 mult-adds.
Section 2.4: ""minibatches of 8,064 images"".

Compute = 2 * 3 * mult-adds * dataset size = 2 * 3 * 153e9 * 9525e6 = 8.74e21 FLOP

Likely trained on V100s, since Facebook had just upgraded their Big Basin GPU cluster to V100s as of March 2018. The previous iteration of Big Basin had 32 clusters of 8xP100s, while Big Basin v2 had 42 clusters of 8xV100s, which matches the 336 GPUs used in this paper.","ImageNet,Instagram","Instagram images, captioned with hashtags",Table 3: (300+1925+300+7000) million images,,30600000000.0,"Table 6: 153e9 mult-adds per image
153e8 * 2 = 3.06e10",496.0,"""Mahajan et al. (2018) required 19 GPU years to train their ResNeXt101-32x48d"" https://arxiv.org/abs/2103.00020
Models were trained on 336 GPUs, so that suggests 20.65 days or 496 hours",NVIDIA V100,,,Industry,Confident,"State-of-the-art visual perception models for a wide range of tasks rely on supervised pretraining. ImageNet classification is the de facto pretraining task for these models. Yet, ImageNet is now nearly ten years old and is by modern standards ""small"". Even so, relatively little is known about the behavior of pretraining with datasets that are multiple orders of magnitude larger. The reasons are obvious: such datasets are difficult to collect and annotate. In this paper, we present a unique study of transfer learning with large convolutional networks trained to predict hashtags on billions of social media images. Our experiments demonstrate that training for large-scale hashtag prediction leads to excellent results. We show improvements on several image classification and object detection tasks, and report the highest ImageNet-1k single-crop, top-1 accuracy to date: 85.4% (97.6% top-5). We also perform extensive experiments that provide novel empirical data on the relationship between large-scale pretraining and transfer learning performance.",2026-01-01 14:00:52+00:00,Robi Rahman,,,United States of America,,,,336.0,,,,,,,Industry,,,,,Unreleased,"models, non-commercial: https://github.com/facebookresearch/WSL-Images ",,134077.42688552555,ResNeXt-101 32x48d,,,,300,,,,2023-05-29T21:06:54.000Z,Open source,,True,209159.05867416784,65215675771176070,2017-06-21,0.8624229979466119,15670000000000,,1,125000000000000,31330000000000,TF16,125000000000000,Operation counting,,,,NVIDIA,,,True,,,,,,,,229613.15707826085,8015194.444444443,940000000
BigGAN-deep 512x512,Image generation,Image generation,"Andrew Brock, Jeff Donahue, Karen Simonyan",,,Open weights (unrestricted),https://arxiv.org/abs/1809.11096,5872.0,Large Scale GAN Training for High Fidelity Natural Image Synthesis,2018-09-28,"Heriot-Watt University,DeepMind",112694781.0,"I used the publicly available implementation available at [1]

There I loaded the biggan-deep512/1 model, and ran script [2] to compute the number of parameters

[1] https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/biggan_generation_with_tf_hub.ipynb

[2]
n_params = 0
for var in module.variables:
  n_params += np.prod(var.shape.as_list())
  pass

print(n_params)",1.8e+21,"3e21, estimate taken from:

https://www.lesswrong.com/posts/wfpdejMWog4vEDLDg/ai-and-compute-trend-isn-t-predictive-of-what-is-happening",JFT-300M,,"""To confirm that our design choices are effective for even larger and more complex and diverse datasets, we also present results of our system on a subset of JFT-300M (Sun et al., 2017). The full JFT-300M dataset contains 300M real-world images labeled with 18K categories. Since the category distribution is heavily long-tailed, we subsample the dataset to keep only images with the 8.5K most common labels. The resulting dataset contains 292M images – two orders of magnitude larger than ImageNet. """,,,,48.0,"""We train on a Google TPU v3 Pod, with the number of cores proportional to the resolution: 128 for 128×128, 256 for 256×256, and 512 for 512×512. Training takes between 24 and 48 hours for most models""",Google TPU v3,,,Industry,Likely,"Despite recent progress in generative image modeling, successfully generating high-resolution, diverse samples from complex datasets such as ImageNet remains an elusive goal. To this end, we train Generative Adversarial Networks at the largest scale yet attempted, and study the instabilities specific to such scale. We find that applying orthogonal regularization to the generator renders it amenable to a simple ""truncation trick,"" allowing fine control over the trade-off between sample fidelity and variety by reducing the variance of the Generator's input. Our modifications lead to models which set the new state of the art in class-conditional image synthesis. When trained on ImageNet at 128x128 resolution, our models (BigGANs) achieve an Inception Score (IS) of 166.5 and Frechet Inception Distance (FID) of 7.4, improving over the previous best IS of 52.52 and FID of 18.6.",2026-01-01 14:01:38+00:00,Robi Rahman,,0,"United Kingdom of Great Britain and Northern Ireland,United Kingdom of Great Britain and Northern Ireland",,,,256.0,,,,,,,"Academia,Industry",,,,12288.0,Unreleased,"repo license is Apache:

https://github.com/tensorflow/tfhub.dev/blob/master/assets/docs/deepmind/models/biggan-deep-512/1.md",,5371.7513177697365,BigGAN-deep 512x512,,,,450,,,,2023-05-29T21:06:54.000Z,,FP32,True,238247.0749212108,335086249068456640,2018-05-18,0.36413415468856947,,,1,123000000000000,,FP32,,Third-party estimation,,,,Google,,,True,,,,,,,,12101.560459004524,3060420.351957097,584000000
GPT-2 (774M),Language,Language modeling/generation,"Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever",Highly cited,,Open weights (unrestricted),https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf,26463.0,Language Models are Unsupervised Multitask Learners,2019-02-14,OpenAI,774000000.0,"Note that the initial paper release stated GPT-2 large had 762M parameters. The official github repo notes that this was due to an error:
https://github.com/openai/gpt-2?tab=readme-ov-file",4.9536e+21,"assuming 100 epochs (consistent with original GPT paper + reasoning from here: https://arxiv.org/pdf/1906.06669)

6 FLOP / token / parameter * 774*10^6 parameters * 10666666666.7 tokens * 100 epochs = 4.9536e+21 FLOP",,"This model was likely trained the same time as GPT-2 (1.5B), according to Figure 1 on page 2 of https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf. From page 3 of this paper, “we created a new web scrape which emphasizes document quality. To do this we only scraped web pages which have been curated/filtered by humans. Manually filtering a full web scrape would be exceptionally expensive so as a starting point, we scraped all outbound links from Reddit, a social media platform, which received at least 3 karma. This can be thought of as a heuristic indicator for whether other users found the link interesting, educational, or just funny.
The resulting dataset, WebText, contains the text subset of these 45 million links. To extract the text from HTML responses we use a combination of the Dragnet (Peters &
Lecocq, 2013) and Newspaper1 content extractors. All results presented in this paper use a preliminary version of WebText which does not include links created after Dec 2017.”",40GB * 200*10^6 words per GB * 4/3 tokens per word = 10666666666.7 tokens,100.0,,,,,,,,,Speculative,"Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically
approached with supervised learning on taskspecific datasets. We demonstrate that language
models begin to learn these tasks without any explicit supervision when trained on a new dataset
of millions of webpages called WebText. When
conditioned on a document plus questions, the answers generated by the language model reach 55
F1 on the CoQA dataset - matching or exceeding
the performance of 3 out of 4 baseline systems
without using the 127,000+ training examples.
The capacity of the language model is essential
to the success of zero-shot task transfer and increasing it improves performance in a log-linear
fashion across tasks. Our largest model, GPT-2,
is a 1.5B parameter Transformer that achieves
state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting
but still underfits WebText. Samples from the
model reflect these improvements and contain coherent paragraphs of text. These findings suggest
a promising path towards building language processing systems which learn to perform tasks from
their naturally occurring demonstrations.
",2026-01-01 14:01:38+00:00,Robi Rahman,GPT-2 (762M),1,United States of America,,,,,,,,,,,Industry,,,,,Unreleased,"modified MIT
https://github.com/openai/gpt-2?tab=License-1-ov-file#readme ",,18857.363172241257,GPT-2 (774M),,,,,,,,2023-11-02T04:05:42.000Z,,,True,,262687840009990600,,NaN,,,0,,,FP32,,,,,,,,,True,,,,2017-11-30,,,,,,10666666666.666666
GPT-2 (355M),Language,Language modeling/generation,"Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever",Highly cited,,Open weights (unrestricted),https://openai.com/blog/better-language-models/,26463.0,Language Models are Unsupervised Multitask Learners,2019-02-14,OpenAI,355000000.0,"Note that the initial paper release stated GPT-2 medium had 345M parameters. The official github repo notes that this was due to an error:
https://github.com/openai/gpt-2?tab=readme-ov-file",2.272000000071e+21,"assuming 100 epochs (consistent with original GPT paper + reasoning from here: https://arxiv.org/pdf/1906.06669)

6* 355000000*10666666667*100=2.272000e+21",,"This model was likely trained the same time as GPT-2 (1.5B), according to Figure 1 on page 2 of https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf. From page 3 of this paper, “we created a new web scrape which emphasizes document quality. To do this we only scraped web pages which have been curated/filtered by humans. Manually filtering a full web scrape would be exceptionally expensive so as a starting point, we scraped all outbound links from Reddit, a social media platform, which received at least 3 karma. This can be thought of as a heuristic indicator for whether other users found the link interesting, educational, or just funny.
The resulting dataset, WebText, contains the text subset of these 45 million links. To extract the text from HTML responses we use a combination of the Dragnet (Peters &
Lecocq, 2013) and Newspaper1 content extractors. All results presented in this paper use a preliminary version of WebText which does not include links created after Dec 2017.”","“All results presented in this paper use a preliminary version of WebText which does not include links created after Dec 2017 and which after de-duplication and some heuristic based cleaning contains slightly over 8 million documents for a total of 40 GB of text.”
40GB is approximately 3e9 words or 1.07e10 tokens.",100.0,,,,,,,,,Speculative,"Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically
approached with supervised learning on taskspecific datasets. We demonstrate that language
models begin to learn these tasks without any explicit supervision when trained on a new dataset
of millions of webpages called WebText. When
conditioned on a document plus questions, the answers generated by the language model reach 55
F1 on the CoQA dataset - matching or exceeding
the performance of 3 out of 4 baseline systems
without using the 127,000+ training examples.
The capacity of the language model is essential
to the success of zero-shot task transfer and increasing it improves performance in a log-linear
fashion across tasks. Our largest model, GPT-2,
is a 1.5B parameter Transformer that achieves
state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting
but still underfits WebText. Samples from the
model reflect these improvements and contain coherent paragraphs of text. These findings suggest
a promising path towards building language processing systems which learn to perform tasks from
their naturally occurring demonstrations.
",2026-01-01 14:01:38+00:00,Robi Rahman,GPT-2 (345M),1,United States of America,,,,,,,,,,,Industry,,,,,Unreleased,"modified MIT
https://github.com/openai/gpt-2?tab=License-1-ov-file#readme",,8649.049000458455,GPT-2 (355M),,,,,,,,2023-11-02T04:05:42.000Z,,,True,,262687840009990620,,NaN,,,0,,,FP32,,,,,,,,,True,,,,2017-11-30,,,,,,10666666666.666666
Grover-Mega,Language,Language modeling/generation,"R Zellers, A Holtzman, H Rashkin, Y Bisk",,,Open weights (unrestricted),https://arxiv.org/abs/1905.12616,1148.0,Defending Against Neural Fake News,2019-05-29,University of Washington,1500000000.0,"""Our largest model, Grover-Mega, has 48 layers and 1.5 billion parameters, on par with GPT2""",4.6386183e+21,"""We trained Grover-Mega for 800k iterations, using a batch size of 512 and 256 TPU v3 cores. Training time was two weeks.""

TPUv3 is 123 teraflops, but for 2 cores. 

So 256/2 * 123 teraflops * 14 days * 24 * 3600 * 0.3 (utilization assumption) = 5.7e21

6 FLOP / parameter / token * 1.5 * 10^9 parameters * 800000 steps * 512 [batch size] * 1024 [sequence length] =  3.7748736e+21 FLOP

sqrt(5.7e21*3.7748736e+21) = 4.6386183e+21
",Realnews,"""We present RealNews, a large corpus of news articles from Common Crawl. Training Grover requires a large corpus of news articles with metadata, but none currently exists. Thus, we construct one by scraping dumps from Common Crawl, limiting ourselves to the 5000 news domains indexed by Google News. We used the Newspaper Python library to extract the body and metadata from each article. News from Common Crawl dumps from December 2016 through March 2019 were used as training data; articles published in April 2019 from the April 2019 dump were used for evaluation. After deduplication, RealNews is 120 gigabytes without compression.""","""We trained Grover-Mega for 800k iterations, using a batch size of 512 and 256 TPU v3 cores.""

""We trained each Grover model on randomly-sampled sequences from RealNews with length 1024.""

""After deduplication, RealNews is 120 gigabytes without compression.""

120 GB * 200M English words per GB / 0.75 English words per token = 32000000000 tokens

800000 * 512 * 1024 / 32000000000 ~ 13 epochs",13.0,,,336.0,"""We trained Grover-Mega for 800k iterations, using a batch size of 512 and 256 TPU v3 cores. Training time was two weeks.""",Google TPU v3,,,Industry,Confident,"Recent progress in natural language generation has raised dual-use concerns. While applications like summarization and translation are positive, the underlying technology also might enable adversaries to generate neural fake news: targeted propaganda that closely mimics the style of real news.
Modern computer security relies on careful threat modeling: identifying potential threats and vulnerabilities from an adversary's point of view, and exploring potential mitigations to these threats. Likewise, developing robust defenses against neural fake news requires us first to carefully investigate and characterize the risks of these models. We thus present a model for controllable text generation called Grover. Given a headline like `Link Found Between Vaccines and Autism,' Grover can generate the rest of the article; humans find these generations to be more trustworthy than human-written disinformation.
Developing robust verification techniques against generators like Grover is critical. We find that best current discriminators can classify neural fake news from real, human-written, news with 73% accuracy, assuming access to a moderate level of training data. Counterintuitively, the best defense against Grover turns out to be Grover itself, with 92% accuracy, demonstrating the importance of public release of strong generators. We investigate these results further, showing that exposure bias -- and sampling strategies that alleviate its effects -- both leave artifacts that similar discriminators can pick up on. We conclude by discussing ethical issues regarding the technology, and plan to release Grover publicly, helping pave the way for better detection of neural fake news.",2026-01-01 14:00:37+00:00,Robi Rahman,,,United States of America,,,,128.0,,,,,,,Academia,,,,,Open source,"apache, code and weights: https://github.com/rowanz/grover 
train code here, inference code in main readme: https://github.com/rowanz/grover/tree/master/lm ",,15691.613136656455,Grover-Mega,,,,450,,,,2023-05-29T21:06:54.000Z,Open source,,True,118480.6465908319,295611309022393500,2018-05-18,1.0294318959616702,,,1,123000000000000,,TF16,123000000000000,"Hardware,Operation counting",,,,Google,,,True,,,,2019-03-15,,,,42470.76685589836,1534375.902410432,32000000000
XLNet,Language,"Language modeling/generation,Question answering,Sentiment classification","Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le",Highly cited,,Open weights (unrestricted),https://arxiv.org/abs/1906.08237,9044.0,XLNet: Generalized Autoregressive Pretraining for Language Understanding,2019-06-01,"Carnegie Mellon University (CMU),Google Brain",340000000.0,"Same size as BERT-Large, which was 340M",6.19e+21,"""Specifically, we train on 512 TPU v3 chips for 500K steps with an Adam weight decay optimizer, linear learning rate decay, and a batch size of 8192, which takes about 5.5 days.""

123 teraflops * 5.5 days * 24 * 3600 * 512 * 0.3 utilization (assumption) ~= 8977858560*10^12=8.9*10^21

Alternatively, 500k steps * batch size 8192 * sequence length 512 = 2.1T training passes. 340 million * 6 * 2 trillion = 4.3e21 FLOP. 

Geometric mean: sqrt(8.9e21 * 4.3e21) = 6.19e21","Wikipedia,BookCorpus (BooksCorpus, Toronto Book Corpus)","""Following BERT [10], we use the BooksCorpus [40] and English Wikipedia as part of our pretraining
data, which have 13GB plain text combined. In addition, we include Giga5 (16GB text) [26],
ClueWeb 2012-B (extended from [5]), and Common Crawl [6] for pretraining. We use heuristics
to aggressively filter out short or low-quality articles for ClueWeb 2012-B and Common Crawl,
which results in 19GB and 110GB text respectively. After tokenization with SentencePiece [17], we
obtain 2.78B, 1.09B, 4.75B, 4.30B, and 19.97B subword pieces for Wikipedia, BooksCorpus, Giga5,
ClueWeb, and Common Crawl respectively, which are 32.89B in total.""",,63.7626026148,,,,,Google TPU v3,,,Industry,Confident,"With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, under comparable experiment settings, XLNet outperforms BERT on 20 tasks, often by a large margin, including question answering, natural language inference, sentiment analysis, and document ranking.",2026-01-01 14:00:37+00:00,Robi Rahman,,,"United States of America,United States of America",,,,,,,,,8192.0,,"Academia,Industry",,,,,Open source,Apache 2.0 for code and weights: https://github.com/zihangdai/xlnet,,13644.959135438661,XLNet,,,,450,,,,2023-05-29T21:06:54.000Z,Open source,FP32,True,,453647382784998200,2018-05-18,1.0376454483230664,,,1,123000000000000,,FP32,,"Hardware,Operation counting",,,,Google,,,True,,,,,,,,,,32890000000
RoBERTa Large,Language,"Question answering,Language modeling/generation","Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov","Highly cited,SOTA improvement","""Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD""",Open weights (unrestricted),https://arxiv.org/abs/1907.11692,27640.0,RoBERTa: A Robustly Optimized BERT Pretraining Approach,2019-07-01,"Facebook,University of Washington",355000000.0,"355M 
https://github.com/facebookresearch/fairseq/blob/main/examples/roberta/README.md",8.5067e+21,"Section 5: We pretrain our model using 1024 V100 GPUs for approximately one day.

Note this is the base pretraining comparable to BERT, 100k steps. Subsequently they do more: ""increasing the number of pretraining steps
from 100K to 300K, and then further to 500K"".

So assume 5x the 1024 V100 GPUs for 1d estimate. Mixed precision tensor cores get 1.25e14 FLOP/s.

1024 * 1.25e14 * 5 * 24 * 3600 * 0.3 = 1.65888e22

6ND estimate: batches are 8k sequences of 512 tokens; 500k updates means the model saw 500k * 8k * 512 = 2.048T tokens
6 * 2.048T * 355M = 4.36224e21

geometric mean: sqrt(1.65888e22 * 4.36224e21) = 8.5067e21

Authors of ""AI and Memory Wall"" estimated model's training compute as 4,300,000 PFLOP = 4.3*10^21 FLOP
(https://github.com/amirgholami/ai_and_memory_wall)","CC-News,BookCorpus (BooksCorpus, Toronto Book Corpus),WebText2,Wikipedia","""We consider five English-language corpora of
varying sizes and domains, totaling over 160GB
of uncompressed text. We use the following text
corpora:
• BOOKCORPUS (Zhu et al., 2015) plus English
WIKIPEDIA. This is the original data used to
train BERT. (16GB).
• CC-NEWS, which we collected from the English portion of the CommonCrawl News
dataset (Nagel, 2016). The data contains 63
million English news articles crawled between
September 2016 and February 2019. (76GB after filtering).4
• OPENWEBTEXT (Gokaslan and Cohen, 2019),
an open-source recreation of the WebText corpus described in Radford et al. (2019). The text
is web content extracted from URLs shared on
Reddit with at least three upvotes. (38GB).5
• STORIES, a dataset introduced in Trinh and Le
(2018) containing a subset of CommonCrawl
data filtered to match the story-like style of
Winograd schemas. (31GB).""","160GB*200M words/GB * (4 tokens / 3 words) = 3.2e10 tokens

max steps 500k
batch size  8k
""We pretrain with sequences of at most T = 512 tokens.""

500000*8000*512 = 2.048e+12 tokens",48.0,,"Authors of KEPLER say their model has the same inference compute as RoBERTa, so if we calculate this we may use it for KEPLER, too

""It depends on the length of the input sequences. The inference computation of KEPLER is the same as RoBERTa (base) and you may estimate it with this.""",120.0,"First the model is pretrained for 100k steps on 1024 GPUs for 1 day, then pretraining is increased to 500k steps, so assuming they used the same number of GPUs, this would have taken 5 days.",NVIDIA Tesla V100 DGXS 32 GB,,,Industry,Confident,"Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.",2026-01-01 14:00:20+00:00,Robi Rahman,,0,"United States of America,United States of America",,,,1024.0,,,,,,,"Industry,Academia",,,,122880.0,Open source,"code and weights: https://github.com/facebookresearch/fairseq/blob/main/examples/roberta/README.md
pretrain code: https://github.com/facebookresearch/fairseq/blob/main/examples/roberta/README.pretraining.md 

repo is MIT license

",,85350.2350127364,RoBERTa Large,,"Hardware estimate: 100k steps took 1 day on 1024 V100; 500k steps would have taken about 5 days.
1024 * 1.25e14 * 5 * 24 * 3600 = 5.5296e22 at full utilization

6ND estimate: batches are 8k sequences of 512 tokens; 500k updates means the model saw 500k * 8k * 512 = 2.048T tokens
6 * 2.048T * 355M = 4.36224e21

Implies 4.36224e21 / 5.5296e22 = 0.0789 utilization
That seems on the edge of plausibility; our lowest confirmed utilization is 0.189.",,250,,,,2023-05-29T21:06:54.000Z,Open source,FP16,True,526193.8152119832,99668149697895800,2018-03-27,1.2621492128678986,15670000000000,,1,125000000000000,31330000000000,FP16,31330000000000,"Hardware,Operation counting,Third-party estimation",,,,NVIDIA,,,True,,,,,,,,166246.0074358047,24383087.000200927,42666666666
Megatron-BERT,Language,Language modeling/generation,"Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, Bryan Catanzaro","Highly cited,SOTA improvement","""Our BERT model achieves SOTA results on the RACE dataset""",Unreleased,https://arxiv.org/abs/1909.08053,2375.0,Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism,2019-09-17,NVIDIA,3900000000.0,Table 4,2.2e+22,"A third-party source: https://lair.lighton.ai/akronomicon/ claims 5.7e22

The authors report experimenting on 1 V100 GPU and achieving throughput of 39 TFLOPS which is 30% of the peak throughput. Therefore the GPU has a peak throughput of 130 TFLOPS so it is specifically the NVIDIA V100S PCIe.
https://images.nvidia.com/content/technologies/volta/pdf/volta-v100-datasheet-update-us-1165301-r5.pdf

Param-based calculation:
6ND = 6*3.9e9*(2e6+1e4)*1024*512 = 2.5e22 FLOP

1024 is the batch size, 512 is the sequence length (not explicitly stated but they say non-specified hyperparameters follow cited papers).

Time-based calculation:
The 8.3B GPT-like arch took 2.1 days per epoch on 512 GPUs, batch size 512. An epoch was 68.5k iterations with sequence length 1024.

Halving the model size should ~halve the iteration time.
Doubling the batch size should ~double the iteration time.
Halving the sequence length should ~quarter the iteration time (quadratic scaling).

Hence 3.1e-5 days/iteration * 2 * 1/2 * 1/4 = 7.8e-6 days/iteration.

2e6 iterations => seems like 15.6 days training.

On 512 GPUs they achieve a peak throughput of 15.1 PFLOPS.
C=15.1 PFLOPS * 58 days = 2.0e22 FLOP.

If we disregard the Akronomicon estimate and aggregate our two, geometric mean is 2.2e22 FLOP",,,"""The resulting aggregate corpus contains 174 GB of deduplicated text.""
174e9 bytes * (1 word / 5 bytes) * (4 tokens / 3 words) = 4.64e10 tokens",,,,374.0,"The 8.3B GPT-like arch took 2.1 days per epoch on 512 GPUs, batch size 512, sequence length 1024. An epoch was 68.5k iterations.

BERT: batch size 1024, sequence length 512, 2e6 iterations total.

Halving the model size should ~halve the iteration time.
Doubling the batch size should ~double the iteration time.
Halving the sequence length should ~quarter the iteration time (quadratic scaling).

Hence 3.1e-5 days/iteration * 2 * 1/2 * 1/4 = 7.8e-6 days/iteration.

2e6 iterations => seems like 15.6 days training.",NVIDIA Tesla V100S PCIe 32 GB,Self-supervised learning,,Industry,Confident,"Recent work in language modeling demonstrates that training large transformer models advances the state of the art in Natural Language Processing applications. However, very large models can be quite difficult to train due to memory constraints. In this work, we present our techniques for training very large transformer models and implement a simple, efficient intra-layer model parallel approach that enables training transformer models with billions of parameters. Our approach does not require a new compiler or library changes, is orthogonal and complimentary to pipeline model parallelism, and can be fully implemented with the insertion of a few communication operations in native PyTorch. We illustrate this approach by converging transformer based models up to 8.3 billion parameters using 512 GPUs. We sustain 15.1 PetaFLOPs across the entire application with 76% scaling efficiency when compared to a strong single GPU baseline that sustains 39 TeraFLOPs, which is 30% of peak FLOPs. To demonstrate that large language models can further advance the state of the art (SOTA), we train an 8.3 billion parameter transformer language model similar to GPT-2 and a 3.9 billion parameter model similar to BERT. We show that careful attention to the placement of layer normalization in BERT-like models is critical to achieving increased performance as the model size grows. Using the GPT-2 model we achieve SOTA results on the WikiText103 (10.8 compared to SOTA perplexity of 15.8) and LAMBADA (66.5% compared to SOTA accuracy of 63.2%) datasets. Our BERT model achieves SOTA results on the RACE dataset (90.9% compared to SOTA accuracy of 89.4%).",2026-01-01 14:00:20+00:00,Robi Rahman,,0,United States of America,,,,512.0,,,,,524288.0,"""we set the batch size to 1024 and use a learning rate of 1.0e4 warmed up over 10,000 iterations and decayed linearly
over 2 million iterations. Other training parameters are kept
the same as (Devlin et al., 2018).""

in Devlin et al (BERT), sequences are 512 tokens",Industry,,,,191488.0,Open source,"training code 

https://github.com/NVIDIA/Megatron-LM/blob/main/pretrain_bert.py 

MIT-like license:
https://github.com/NVIDIA/Megatron-LM/blob/main/LICENSE ",,171818.64196707975,Megatron-BERT,,"When training the 512 V100 GPT-2 style models, achieved 15.1 PetaFLOP/s vs theoretical maximum of 512 * 125e12 = 64 PetaFLOP/s
BERT model used same number of GPUs, likely had similar utilization (around 15.1/64=23.59%), probably slightly lower because they are splitting a model across the same number of GPUs but it is smaller so the GPUs are doing fewer matmuls.",,250,,,,2023-05-29T21:06:54.000Z,Unreleased,FP16,True,262640.30207328824,128041985131131310,2019-11-26,-0.19164955509924708,16350000000000,,1,130000000000000,32710000000000,FP16,32710000000000,"Operation counting,Third-party estimation",,,,NVIDIA,,,True,,,,,,,,260989.03287741935,12082300.278773397,6960000000
Megatron-LM (8.3B),Language,Language modeling/generation,"Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, Bryan Catanzaro","Highly cited,SOTA improvement","""Using the GPT-2 model we achieve SOTA results on the WikiText103 (10.8 compared to SOTA perplexity of 15.8) and LAMBADA"" 

GPT-2 model here meaning model similar to GPT-2",Unreleased,https://arxiv.org/abs/1909.08053,2375.0,Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism,2019-09-17,NVIDIA,8300000000.0,"Source: https://lair.lighton.ai/akronomicon/

Archived source: https://web.archive.org/web/20211220142906/https://lair.lighton.ai/akronomicon/

Data also available on GitHub: https://github.com/lightonai/akronomicon/blob/main/akrodb/NVIDIA/Megatron-LM.json",9.1e+21,"source: https://lair.lighton.ai/akronomicon/

archived: https://github.com/lightonai/akronomicon/tree/main/akrodb

other estimates:

8.3B is a GPT-2-based model (Table 2). ""For GPT-2 models, all training is performed with sequences of 1024 subword units at a batch size of 512 for 300k iterations"" 

I interpret the above as 1024*512*300k = 157B training tokens 

6 * 157 billion * 8.3 billion  = 7.8e21

Also, their training setup achieved 15.1 petaFLOPS or 1.5e16 FLOPS.
(512 V100s is 512 * 125 teraflops = 64 petaFLOPS so they had ~25% utilization)
2.1 days per epoch, ~4.4 epochs
2.1 * 4.4 * 24 * 3600 * 1.5e16 = 1.197e22

These are both close to the akronomicon estimate

Authors of ""AI and Memory Wall"" (https://github.com/amirgholami/ai_and_memory_wall) estimated model's training compute as 8,100,000 PFLOP = 8.1*10^21 FLOP",,"""we aggregate several of the largest language
modeling datasets. We create an aggregate dataset consisting of Wikipedia (Devlin et al., 2018), CC-Stories (Trinh &
Le, 2018), RealNews (Zellers et al., 2019), and OpenWebtext (Radford et al., 2019). To avoid training set leakage
into our downstream tasks we remove the Wikipedia articles
present in the WikiText103 test set (Merity et al., 2016).""","""The resulting aggregate
corpus contains 174 GB of deduplicated text.""",4.4,18000000000000.0,"Rados (FLOPs)
https://drive.google.com/drive/folders/1bhy5z6hh1n3wCHx6528Xb7xB1KhYdAL1",327.0,"Reported throughput is 15.1 teraFLOPS per GPU on 512 GPUs
Assume total compute is 9.1e21 FLOP.
Then training time is 327 hours.
https://www.wolframalpha.com/input?i=9.1*10%5E21+FLOP+%2F+%28512*15.1+teraFLOPS%29",NVIDIA Tesla V100 DGXS 32 GB,Self-supervised learning,"327 hours * 512 GPUs * $0.55/V100 GPU-hour = $92,083
Convert to 2020 dollars: $78,689",Industry,Likely,"Recent work in language modeling demonstrates that training large transformer models advances the state of the art in Natural Language Processing applications. However, very large models can be quite difficult to train due to memory constraints. In this work, we present our techniques for training very large transformer models and implement a simple, efficient intra-layer model parallel approach that enables training transformer models with billions of parameters. Our approach does not require a new compiler or library changes, is orthogonal and complimentary to pipeline model parallelism, and can be fully implemented with the insertion of a few communication operations in native PyTorch. We illustrate this approach by converging transformer based models up to 8.3 billion parameters using 512 GPUs. We sustain 15.1 PetaFLOPs across the entire application with 76% scaling efficiency when compared to a strong single GPU baseline that sustains 39 TeraFLOPs, which is 30% of peak FLOPs. To demonstrate that large language models can further advance the state of the art (SOTA), we train an 8.3 billion parameter transformer language model similar to GPT-2 and a 3.9 billion parameter model similar to BERT. We show that careful attention to the placement of layer normalization in BERT-like models is critical to achieving increased performance as the model size grows. Using the GPT-2 model we achieve SOTA results on the WikiText103 (10.8 compared to SOTA perplexity of 15.8) and LAMBADA (66.5% compared to SOTA accuracy of 63.2%) datasets. Our BERT model achieves SOTA results on the RACE dataset (90.9% compared to SOTA accuracy of 89.4%).",2026-01-01 14:00:19+00:00,Robi Rahman,Megatron-LM (8.3B),,United States of America,,,,512.0,,,,,,,Industry,,,,167424.0,Open source,"code (2.5B model is a GPT model): https://github.com/NVIDIA/Megatron-LM?tab=readme-ov-file#megatron-overview  
open license: https://github.com/NVIDIA/Megatron-LM?tab=License-1-ov-file#readme ",,109287.55758939347,Megatron-LM (8.3B),,"Achieved 15.1 PetaFLOP/s on 512 V100s, vs theoretical maximum of 512 * 125e12 = 64 PetaFLOP/s
HFU = 15.1 / 64 = 0.2359",,250,,,,2023-05-29T21:06:54.000Z,Unreleased,FP16,True,262640.30207328824,83266569413050620,2018-03-27,1.4757015742642026,15670000000000,,1,125000000000000,31330000000000,FP16,31330000000000,"Hardware,Operation counting,Third-party estimation",,,,NVIDIA,,,True,,,,,,,0.2359,224480.52397419358,12082300.278773397,46400000000
T5-3B,Language,Text autocompletion,"Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu",Highly cited,,Open weights (unrestricted),https://arxiv.org/abs/1910.10683,23572.0,Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,2019-10-23,Google,2800000000.0,"page 37, 3B and 11B. ""To further explore what kind of performance is possible when using larger models, we consider two additional variants. In both cases, we use d_model = 1024, a 24 layer encoder and decoder, and dkv = 128. For the “3B” variant, we use dff = 16,384 with 32-headed attention, which results in around 2.8 billion parameters; for “11B” we use dff = 65,536 with 128-headed attention producing a model with about 11 billion parameters""",9.0000000001e+21,"Akronomicon states 1.04e+22 FLOP. Archived source: https://github.com/lightonai/akronomicon/tree/main/akrodb
However, this seems dubiously high.

""We pre-train each model for 2^19 = 524,288 steps on C4 before fine-tuning.""
""In total, this batch size and number of steps corresponds to pre-training on 2^35 ≈ 34B tokens.""
""To compare these mixing strategies on equal footing with our baseline pre-train-then-fine-tune results, we train multi-task models for the same total number of steps: 2^19 + 2^18 = 786,432""
Using the 6DN approximation gives: 6 FLOP/token/param * 2^35 pretrain tokens * (1+1/2 finetune tokens per pretrain token) * 1 iteration of training data* 2.8 billion parameters = 8.659e20 FLOP
https://www.wolframalpha.com/input?i=6+*+2%5E35+*+2.8+billion+*+1.5

update: 9.0E+21 per FLAN paper from Google 
https://arxiv.org/pdf/2210.11416.pdf",C4,,"""This produces a collection of text that is not only orders of magnitude larger than most data sets used for pre-training (about 750 GB) but also
comprises reasonably clean and natural English text. We dub this data set the “Colossal Clean Crawled Corpus” (or C4 for short) and release it as part of TensorFlow Datasets""
750GB * 200M word/GB = 1.5e11

""In total, this batch size and number of steps corresponds to pre-training on 2^35 ≈ 34B tokens.""
""Note that 2^35 tokens only covers a fraction of the entire C4 data set, so we never repeat any data during pre-training.""
The fraction is 25.5 billion / 150 billion = 0.17 epochs.",,,,,,Google TPU v3,Self-supervised learning,,Industry,Confident,"Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.",2026-01-01 14:00:21+00:00,Robi Rahman,,,United States of America,,,,,,,,,,,Industry,,,,,Open source,"Apache for code and weights:
https://github.com/google-research/text-to-text-transfer-transformer

Data is C4 which is open
training script: https://github.com/google-research/text-to-text-transfer-transformer?tab=readme-ov-file#training ",,17420.709393371122,T5-3B,,,,450,,,,2023-05-29T21:06:54.000Z,Open source,,True,,516626493036193660,2018-05-18,1.431895961670089,,,1,123000000000000,,TF16,123000000000000,"Third-party estimation,Reported",,,,Google,T5-3B,,True,,,,,,,,,,5100000000
T5-11B,Language,"Text autocompletion,Language modeling/generation","Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu",Highly cited,,Open weights (unrestricted),https://arxiv.org/abs/1910.10683,23572.0,Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,2019-10-23,Google,11000000000.0,The full 11-billion parameter model,3.3e+22,"https://arxiv.org/ftp/arxiv/papers/2104/2104.10350.pdf
Table 4, 4.05e22

update: 3.3e22 per FLAN paper from Google 
https://arxiv.org/pdf/2210.11416.pdf

6ND rule suggests somewhat more FLOPs:
6 * 1T * 11B = 6.6e22",C4,,"""This produces a collection of text that is not only
orders of magnitude larger than most data sets used for pre-training (about 750 GB) but also comprises reasonably clean and natural English text. We dub this data set the “Colossal Clean Crawled Corpus” (or C4 for short) and release it as part of TensorFlow Datasets""

750GB * 200M word/GB * 4/3 tokens per word = 2e11.

Total tokens seen is about 1T:  ""We therefore pre-train our models for 1 million steps on a batch size of 2^11 sequences of length 512, corresponding to a total of about 1 trillion pre-training tokens""",,,,481.9,"4.05*10^22 FLOP at 37.073% utilization on 512 TPU v3 chips (123 TFLOPS) -> 482 hours
https://www.wolframalpha.com/input?i=4.05*10%5E22+seconds+%2F+%28512*123*10%5E12%29+*%28123%2F45.6%29",Google TPU v3,Self-supervised learning,,Industry,Confident,"Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.",2026-01-01 14:00:21+00:00,Robi Rahman,,,United States of America,,,,512.0,,,,,65536.0,"""We use a maximum sequence length of 512 and a batch size of 128 sequences. Whenever possible, we “pack” multiple sequences into each entry of the batch10 so that our batches contain roughly 2^16 = 65,536 tokens""",Industry,,,,246733.0,Open source,"Apache for code and weights:
https://github.com/google-research/text-to-text-transfer-transformer

Data is C4 which is open",,78464.68282814459,T5-11B,,Table 4 in https://arxiv.org/pdf/2104.10350 gives measured performance of  45.6 TFLOP/s out of maximum 123 TFLOP/s. Therefore HFU = 45.6/123 = 0.3707,,450,,,,2023-05-29T21:06:54.000Z,Open source,,True,472373.69114573154,420571380786403800,2018-05-18,1.431895961670089,,,1,123000000000000,,TF16,123000000000000,"Reported,Operation counting,Third-party estimation",,,,Google,T5-11B,,True,,,,,,,0.3707,240378.8008668934,6055084.134131768,34000000000
AlphaStar,Games,StarCraft,"Oriol Vinyals,Igor Babuschkin,Wojciech M. Czarnecki,Michaël Mathieu,Andrew Dudzik,Junyoung Chung,David H. Choi,Richard Powell,Timo Ewalds,Petko Georgiev,Junhyuk Oh,Dan Horgan,Manuel Kroiss,Ivo Danihelka,Aja Huang,Laurent Sifre,Trevor Cai,John P. Agapiou,Max Jaderberg,Alexander S. Vezhnevets,Rémi Leblond,Tobias Pohlen,Valentin Dalibard,David Budden,Yury Sulsky,James Molloy,Tom L. Paine,Caglar Gulcehre,Ziyu Wang,Tobias Pfaff,Yuhuai Wu,Roman Ring,Dani Yogatama,Dario Wünsch,Katrina McKinney,Oliver Smith,Tom Schaul,Timothy Lillicrap,Koray Kavukcuoglu,Demis Hassabis,Chris Apps,David Silver",Discretionary,,Unreleased,https://www.deepmind.com/blog/alphastar-grandmaster-level-in-starcraft-ii-using-multi-agent-reinforcement-learning,4093.0,Grandmaster level in StarCraft II using multi-agent reinforcement learning,2019-10-30,DeepMind,139000000.0,"AlphaStar has 139 million weights, but only 55 million weights are required during inference.",1.0773400001e+23,"(Estimate by James Sanders, checked by Robi Rahman)
Fig 6 indicates that the learner uses 16 TPU ""devices"" which are 128 TPU cores total, which matches 4 TPUs per device, and 2 cores per TPU. Fig 6 indicates that 64 TPUs are used for training, and 64 are used for inference. (128 TPUs)*(12 agents)*(44 days)*(123 TFLOPS)*(0.3 utilization) = 2.15e23 FLOP (total), of which 1.07e23 FLOP are for training.",,,"Multiple data types. First supervised learning, then other stuff",,,,1056.0,"""Each agent was trained using 32 third-generation tensor
processing units (TPUs) over 44 days""",Google TPU v3,,,Industry,Confident,"Many real-world applications require artificial agents to compete and coordinate with other agents in complex environments. As a stepping stone to this goal, the domain of StarCraft has emerged as an important challenge for artificial intelligence research, owing to its iconic and enduring status among the most difficult professional esports and its relevance to the real world in terms of its raw complexity and multi-agent challenges. Over the course of a decade and numerous competitions1–3, the strongest agents have simplified important aspects of the game, utilized superhuman capabilities, or employed hand-crafted sub-systems4. Despite these advantages, no previous agent has come close to matching the overall skill of top StarCraft players. We chose to address the challenge of StarCraft using generalpurpose learning methods that are in principle applicable to other complex domains: a multi-agent reinforcement learning algorithm that uses data from both human and agent games within a diverse league of continually adapting strategies and counter-strategies, each represented by deep neural networks5,6. We evaluated our agent, AlphaStar, in the full game of StarCraft II, through a series of online games against human players. AlphaStar was rated at Grandmaster level for all three StarCraft races and above 99.8% of officially ranked human players.",2026-01-01 14:00:20+00:00,Robi Rahman,,,United Kingdom of Great Britain and Northern Ireland,,,,384.0,,,,,,,Industry,,,,405504.0,Open source,"Apache 2.0, training tools: https://github.com/google-deepmind/alphastar

training instructions here: https://github.com/google-deepmind/alphastar/blob/main/alphastar/unplugged/README.md ",,130654.07330431018,AlphaStar,,,,450,,,,2023-05-29T21:06:54.000Z,Open source,,True,354225.04547181545,824574368677152800,2018-05-18,1.4510609171800137,,,1,123000000000000,,TF16,123000000000000,Hardware,,,,Google,,,True,,,,,,,,395061.2373658012,4541313.100598825,
XLM-RoBERTa,Language,"Named entity recognition (NER),Question answering,Text classification","Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, Veselin Stoyanov
","Highly cited,SOTA improvement","citation ""which obtains state-of-the-art performance on cross-lingual classification, sequence labeling and question answering""",Open weights (non-commercial),https://arxiv.org/abs/1911.02116,7541.0,Unsupervised Cross-lingual Representation Learning at Scale,2019-11-05,Facebook AI,550000000.0,"The number of parameters in the model is specified as ""550M params"" for XLM-R.",2.076e+22,"""We use the multilingual MLM loss and train our XLM-R model for
1.5 Million updates on five-hundred 32GB Nvidia
V100 GPUs with a batch size of 8192. ""

6ND:
Sequence length was probably 512, based on follow up paper (XLM-R XXL)
6 * 550e6 * 1.5e6 * 8192 * 512 = 2.076e22
",CC100,"The training dataset and size are mentioned as ""using more than two terabytes of filtered CommonCrawl data"" and the model being trained on ""100 languages"".",size of CC100 - copied from other rows,,,,,,NVIDIA Tesla V100 DGXS 32 GB,,,,Confident,"This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6% average accuracy on XNLI, +13% average F1 score on MLQA, and +2.4% F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7% in XNLI accuracy for Swahili and 11.4% for Urdu over previous XLM models. We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing per-language performance; XLM-R is very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We will make our code, data and models publicly available.",2026-01-01 14:00:20+00:00,Anonymous,,,United States of America,,,,500.0,,,,,,,Industry,,,,,Open (non-commercial),"non-commercial: https://github.com/facebookresearch/XLM?tab=License-1-ov-file#readme

data is wikipedia",,77861.31819548723,XLM-RoBERTa,,,,250,,,,2024-02-12T20:46:32.000Z,Open access (non-commercial),,True,256204.94677329363,266627903060639840,2018-03-27,1.6098562628336757,15670000000000,,1,125000000000000,31330000000000,TF16,125000000000000,Operation counting,,,,NVIDIA,,,True,,,,,,,,,11788558.141848203,167000000000
Noisy Student (L2),Vision,Image classification,"Q Xie, MT Luong, E Hovy, QV Lee","Highly cited,SOTA improvement","""Noisy Student Training achieves 88.4% top-1 accuracy on ImageNet, which is 2.0% better than the state-of-the-art model""",Unreleased,https://arxiv.org/abs/1911.04252v4,2600.0,Self-training with Noisy Student improves ImageNet classification,2019-11-11,"Carnegie Mellon University (CMU),Google",480000000.0,,2.612e+22,"""Our largest model, EfficientNet-L2, needs to be trained for 6 days on a Cloud TPU v3 Pod, which has 2048 cores, if the unlabeled batch size is 14x the labeled batch size""
TPU v3 gets 1.23e14 FLOP/s per chip, with 2 cores per chip

1024 * 1.23e14 * 6 * 24 * 3600 * 0.4 = 2.612e22","ImageNet,JFT",,"""Due to duplications, there are only 81M unique images among these 130M images.""",,1040000000000.0,"Rados (FLOPs)
https://drive.google.com/drive/folders/1bhy5z6hh1n3wCHx6528Xb7xB1KhYdAL1",144.0,6 days,Google TPU v3,,,Industry,Confident,"We present Noisy Student Training, a semi-supervised learning approach that works well even when labeled data is abundant. Noisy Student Training achieves 88.4% top-1 accuracy on ImageNet, which is 2.0% better than the state-of-the-art model that requires 3.5B weakly labeled Instagram images. On robustness test sets, it improves ImageNet-A top-1 accuracy from 61.0% to 83.7%, reduces ImageNet-C mean corruption error from 45.7 to 28.3, and reduces ImageNet-P mean flip rate from 27.8 to 12.2.
Noisy Student Training extends the idea of self-training and distillation with the use of equal-or-larger student models and noise added to the student during learning. On ImageNet, we first train an EfficientNet model on labeled images and use it as a teacher to generate pseudo labels for 300M unlabeled images. We then train a larger EfficientNet as a student model on the combination of labeled and pseudo labeled images. We iterate this process by putting back the student as the teacher. During the learning of the student, we inject noise such as dropout, stochastic depth, and data augmentation via RandAugment to the student so that the student generalizes better than the teacher. Models are available at this https URL. Code is available at this https URL.",2026-01-01 14:00:20+00:00,Robi Rahman,,,"United States of America,United States of America",,,,1024.0,,,,,,,"Academia,Industry",,,,,Open source,apache 2.0 license: https://github.com/google-research/noisystudent train script: https://github.com/google-research/noisystudent/blob/master/local_scripts/imagenet/train.sh ,,45609.73119622589,Noisy Student (L2),,,,450,,,,2023-05-29T21:06:54.000Z,Open source,FP32,True,944347.7271739373,572684804644526660,2018-05-18,1.483915126625599,,,1,123000000000000,,FP32,,Hardware,,,,Google,,,True,,,,,,,,146923.60067323185,12110168.268263536,81000000
OpenAI Five,Games,Dota 2,"Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemysław Dębiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, Rafal Józefowicz, Scott Gray, Catherine Olsson, Jakub Pachocki, Michael Petrov, Henrique P. d.O. Pinto, Jonathan Raiman, Tim Salimans, Jeremy Schlatter, Jonas Schneider, Szymon Sidor, Ilya Sutskever, Jie Tang, Filip Wolski, Susan Zhang","Highly cited,SOTA improvement","no standard benchmark results

""On April 13th, 2019, OpenAI Five became the first AI system to defeat the world champions at an esports game.""

"" it defeated the Dota 2 world champions
in a best-of-three match and 99.4% of human players during a multi-day online showcase.""

""we ran OpenAI Five Arena, in which we opened OpenAI Five to the public for
competitive online games from April 18-21, 2019. In total, Five played 3,193 teams in 7,257 total games, winning 99.4%""",Unreleased,https://arxiv.org/abs/1912.06680,2020.0,Dota 2 with Large Scale Deep Reinforcement Learning,2019-12-13,OpenAI,159000000.0,"""We define a policy (π) as a function from the history of observations to a probability distribution over actions, which we parameterize as a recurrent neural network with approximately 159 million parameters (θ)."" pg. 3 of paper

source: https://docs.google.com/spreadsheets/d/1Kj4Q5WADcDXtUJLIOfGTCE3tGvxNczEMwyy8QtgSkHk/edit#gid=54587040&fvid=1361937389",6.7e+22,"""770±50 PFlops/s·days of compute"" for the model that played against world champions. They did a single training run that took 10 months.

While the model was playing against world champions, they continued training for a few days, so that the resulting model used even more training compute: 820±50 PFlops/s·days.

Finally, they also trained a Rerun model with 150±5 PFlops/s·days of compute.

Source: Dota 2 with Large Scale Deep Reinforcement Learning
https://arxiv.org/abs/1912.06680

You cannot multiply the hardware quantity by training time to get the quantity of GPU-hours! Page 5: "" the number of GPUs (up to 1536 at the peak)""

From this NVIDIA blogpost, it appears they were using P100s:
https://developer.nvidia.com/blog/ai-learns-to-play-dota-2-with-human-precision/#:~:text=AI%20Learns%20to%20Play%20Dota,The%20neural",,,"""Although the Dota 2 engine runs at 30 frames per second, OpenAI Five only acts on every 4th
frame which we call a timestep""
--> 7.5 timesteps/s

""OpenAI Five is a single training run that ran from June 30th, 2018 to April 22nd, 2019. "" --> 296 days

296 * 24*3600 * 7.5 = 1.92e8

This number seems a little low? The DQN paper had 1e7 timesteps. Might be to do with sample efficiency?

EDIT 14/06/2022
Multiple copies of OpenAI Five were trained in parallel, so the total training time is much higher than 296 days.
Table 1 shows 220,000 GPU iterations, each iteration has a batch size of between 1M and 3M timesteps (Table 2), so the total number of episodes is on the order of 2e11",,,,7104.0,"""OpenAI Five is a single training run that ran from June 30th, 2018 to April 22nd, 2019. "" --> 296 days",NVIDIA P100,Self-supervised learning,"Cannot multiply the hardware quantity by training time to get the quantity of GPU-hours! Page 5: "" the number of GPUs (up to 1536 at the peak)""",Industry,Confident,"On April 13th, 2019, OpenAI Five became the first AI system to defeat the world champions at an esports game. The game of Dota 2 presents novel challenges for AI systems such as long time horizons, imperfect information, and complex, continuous state-action spaces, all challenges which will become increasingly central to more capable AI systems. OpenAI Five leveraged existing reinforcement learning techniques, scaled to learn from batches of approximately 2 million frames every 2 seconds. We developed a distributed training system and tools for continual training which allowed us to train OpenAI Five for 10 months. By defeating the Dota 2 world champion (Team OG), OpenAI Five demonstrates that self-play reinforcement learning can achieve superhuman performance on a difficult task.
",2026-01-01 14:00:20+00:00,Robi Rahman,,,United States of America,,,,1536.0,,,,,,,Industry,,,,10911744.0,Unreleased,,,4328311.470477086,OpenAI Five,,,,250,,,,2023-05-29T21:06:54.000Z,,,True,786395.8382790724,15479477495323360,2016-04-05,3.6878850102669403,9300000000000,,1,,18700000000000,FP16,18700000000000,Reported,,,,NVIDIA,,,True,,,,,,,,11072136.60189257,33761961.904761896,454164480000
Meena,Language,"Text autocompletion,Chat","Dongling Xiao, Han Zhang, Yukun Li, Yu Sun, Hao Tian, Hua Wu, Haifeng Wang",SOTA improvement,"""We also propose a human evaluation metric called Sensibleness and Specificity Average (SSA)... the full version of Meena (with a filtering mechanism and tuned decoding) scores 79% SSA, 23% higher in absolute SSA than the existing chatbots we evaluated""",Unreleased,https://arxiv.org/abs/2001.09977,991.0,Towards a Human-like Open-Domain Chatbot,2020-01-28,Google Brain,2600000000.0,"""We present Meena, a multi-turn open-domain chatbot trained end-to-end on data mined and filtered from public domain social media conversations. This 2.6B parameter neural network is simply trained to minimize perplexity of the next token.""",1.12e+23,"https://arxiv.org/ftp/arxiv/papers/2104/2104.10350.pdf
Table 4

In the paper: ""We trained our best model for 30 days on a TPUv3 Pod (2,048 TPU cores) on the Meena dataset containing 40B words (or 61B BPE tokens) [...] by the end of training, the model had traversed the full
training set 164 times (or epochs) and observed a total of about 10T tokens""

Hardware: 30 * 24 * 3600 * (2048/2) * 1.23e14 * 0.3 = 9.794e22
Ops counting: 6 * 10T * 2.6B = 1.56E23
Geometric mean: sqrt(9.79e22*1.56E23) = 1.24e23, very close to the figure in the link above.",,,"""The final Meena dataset contains 341GB of text
(40B words)""

Converting from GB to words yields 6.8e10, which is in the same OOM",164.0,,,720.0,"We trained our best model for 30 days on a TPUv3 Pod (2,048 TPU cores)",Google TPU v3,Self-supervised learning,,Industry,Confident,"We present Meena, a multi-turn open-domain chatbot trained end-to-end on data mined and filtered from public domain social media conversations. This 2.6B parameter neural network is simply trained to minimize perplexity of the next token. We also propose a human evaluation metric called Sensibleness and Specificity Average (SSA), which captures key elements of a human-like multi-turn conversation. Our experiments show strong correlation between perplexity and SSA. The fact that the best perplexity end-to-end trained Meena scores high on SSA (72% on multi-turn evaluation) suggests that a human-level SSA of 86% is potentially within reach if we can better optimize perplexity. Additionally, the full version of Meena (with a filtering mechanism and tuned decoding) scores 79% SSA, 23% higher in absolute SSA than the existing chatbots we evaluated.",2026-01-01 14:00:21+00:00,Robi Rahman,,0,United States of America,,,,1024.0,,,,,82655.0,"61B tokens over 738k training steps, or 82655 tokens per batch on average. Not certain about warmup, etc",Industry,,,,737280.0,Unreleased,,True,214809.91213508433,Meena,,"Per https://arxiv.org/ftp/arxiv/papers/2104/2104.10350.pdf
1.12e23 FLOPs used
1024 TPUv3s for 30 days: 30 * 24 * 3600 * 1024 * 1.23e14 = 3.2647e23
1.12e23 / 3.2647e23 

HFU = 0.3431",,450,,,,2023-05-29T21:06:54.000Z,,,True,942708.8086440804,521391209962267500,2018-05-18,1.6974674880219027,,,1,123000000000000,,TF16,123000000000000,"Hardware,Operation counting,Third-party estimation",,,,Google,,,True,,,,,,,0.34306622,724243.8744571933,11939150.88759962,53333333333.333336
GPT-3 175B (davinci),Language,"Text autocompletion,Language modeling/generation","Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei","Highly cited,Training cost",,API access,https://arxiv.org/abs/2005.14165,51687.0,Language Models are Few-Shot Learners,2020-05-28,OpenAI,174600000000.0,"""we train GPT-3, an autoregressive language model with 175 billion parameters""
Rather, it's 174.6 billion.
Table D.1: 174,600 million parameters",3.14e+23,"Table D.1
https://arxiv.org/abs/2005.14165","Common Crawl,WebText2,Wikipedia,Books1,Books2","Table 2.2 (other datasets also used)

Page 8 of the paper linked above says the model was trained “41 shards of monthly CommonCrawl covering 2016 to 2019”. The paper was published on May 28, 2020, so I assume based on the paper’s description of the training data that it goes up to December 31, 2019.","From table 2.2, we determine that there are 410 + 19 + 12 + 55 + 3 = 499 billion tokens. 

We multiply this by 0.75 to give 374B words. 

3.74e11

========================
[Anson: I think the calculation below doesn't look at all the data, the CommonCrawl data only constitutes 60% of the data. Multiplying by 5/3 gives 4.75e11]

""The CommonCrawl data was downloaded from 41 shards of monthly CommonCrawl covering 2016 to 2019, constituting 45TB of compressed plaintext before filtering and 570GB after filtering, roughly equivalent to 400 billion byte-pair-encoded tokens. ""

Converted to words using 
http://extraconversion.com/data-storage/gigabits/gigabits-to-words.html

2.85e11",0.6,740000000000000.0,"Rados (FLOPs)
https://drive.google.com/drive/folders/1bhy5z6hh1n3wCHx6528Xb7xB1KhYdAL1",355.2,14.8 days according to https://arxiv.org/ftp/arxiv/papers/2104/2104.10350.pdf,NVIDIA Tesla V100 DGXS 32 GB,Self-supervised learning,,Industry,Confident,"Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.",2026-01-01 14:00:37+00:00,Robi Rahman,GPT-3 175B (davinci),0,United States of America,,,,10000.0,0.1966,Microsoft,,,3200000.0,"3.2M, per table 2.1",Industry,True,,,3552000.0,Unreleased,"https://openai.com/blog/openai-api
",True,2116866.2428390156,GPT-3 175B (davinci),,"See table 4 of the carbon emissions paper: https://arxiv.org/pdf/2104.10350

MFU calculation from total compute:
(6 FLOP/token/param * 174.6B params * 300B tokens) / (10k V100 * 125 TFLOPS/V100 * 14.8 days) = 0.1966
https://www.wolframalpha.com/input?i=%286*174.6+billion+*+300+billion%29+FLOP+%2F+%281250000+TFLOPS+*+14.8+days%29

HFU calculation from GPU throughput:
The paper reports 24.6 TFLOPS throughput per V100, and reports the V100 peak performance of 125 TFLOPS (which is correct if training was done in tensor 16-bit format).
HFU = 24.6/125 = 0.1968

These are suspiciously close, so it's likely they did not actually measure the GPU throughput, but estimated it from the total compute and the training duration. Therefore, let's leave HFU blank because the true value is uncertain.

MFU = 0.1966",,250,,,"$12 / 1M input tokens, fine tuned model,$12 / 1M output tokens, fine tuned model,$6 / 1M training tokens,$60 / M tokens,$60 / M tokens,$20 / M tokens,$20 / M tokens",2023-05-29T21:06:54.000Z,,,True,5100759.605963441,148332470727523070,2018-03-27,2.1711156741957565,15670000000000,,1,125000000000000,31330000000000,TF16,125000000000000,Reported,,,,NVIDIA,"davinci,davinci-002",,True,,,,2019-12-31,,,,4493219.860317459,232236674.505193,238000000000
iGPT-XL,"Vision,Image generation",Image completion,"Mark Chen, Alec Radford, Rewon Child, Jeff Wu, Heewoo Jun, Prafulla Dhariwal, David Luan, Ilya Sutskever",,,Open weights (unrestricted),https://openai.com/research/image-gpt,1694.0,Generative Pretraining from Pixels,2020-06-17,OpenAI,6801000000.0,source: https://openai.com/blog/image-gpt/#rfref53,3.3e+22,"Taken from here
https://www.lesswrong.com/posts/wfpdejMWog4vEDLDg/ai-and-compute-trend-isn-t-predictive-of-what-is-happening (""There's no compute data for the largest model, iGPT-XL. But based on the FLOP/s increase from GPT-3 XL (same num of params as iGPT-L) to GPT-3 6.7B (same num of params as iGPT-XL), I think it required 5 times more compute: 3.3 * 10^22 FLOP."")",ILSVRC 2012 subset of ImageNet,,"""We use the ImageNet ILSVRC 2012 training dataset, splitting off 4% as our experimental validation set and report results on the ILSVRC 2012 validation set as our test set.""

https://image-net.org/challenges/LSVRC/2012/

""The goal of this competition is to estimate the content of photographs for the purpose of retrieval and automatic annotation using a subset of the large hand-labeled ImageNet dataset (10,000,000 labeled images depicting 10,000+ object categories) as training.""
",,,,,,NVIDIA Tesla V100 DGXS 32 GB,Self-supervised learning,,Industry,Likely,"Inspired by progress in unsupervised representation learning for natural language, we examine whether similar models can learn useful representations for images. We train a sequence Transformer to auto-regressively predict pixels, without incorporating knowledge of the 2D input structure. Despite training on low-resolution ImageNet without labels, we find that a GPT-2 scale model learns strong image representations as measured by linear probing, fine-tuning, and low-data classification. On CIFAR-10, we achieve 96.3% accuracy with a linear probe, outperforming a supervised Wide ResNet, and 99.0% accuracy with full finetuning, matching the top supervised pre-trained models. An even larger model trained on a mixture of ImageNet and web images is competitive with self-supervised benchmarks on ImageNet, achieving 72.0% top-1 accuracy on a linear probe of our features.",2026-01-01 14:00:38+00:00,Robi Rahman,,,United States of America,,,,,,,,,,,Industry,,,,,Open source,"Modified MIT, code and weights:

https://github.com/openai/image-gpt?tab=License-1-ov-file#readme

train code: https://github.com/openai/image-gpt/blob/master/src/run.py ",,100796.41431883824,iGPT-XL,,,,250,,,,2023-05-29T21:06:54.000Z,Open source,,True,,327392598466992260,2018-03-27,2.2258726899383983,15670000000000,,1,125000000000000,31330000000000,TF16,125000000000000,Third-party estimation,,,,NVIDIA,,,True,,,,,,,,,,4718592000
GShard (dense),Language,Translation,"Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, Zhifeng Chen",SOTA improvement,"""such a giant model can efficiently be trained on 2048 TPU v3 accelerators in 4 days to achieve far superior quality for translation from 100 languages to English compared to the prior art""

I don't see any standard benchmark that they claim SOTA on",Unreleased,https://arxiv.org/abs/2006.16668,1584.0,GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding,2020-06-30,Google,2300000000.0,"""Our best quality dense single Transformer model (2.3B parameters) achieving ∆BLEU of 6.1, was trained with GPipe [15] on 2048 TPU v3 cores for 6 weeks or total of 235.5 TPU v3 core-years.""",4.765e+22,"Trained for a total of 235.5 TPU v3 core-years.
Hardware estimate: 235.5 * 365.25 * 24 * 3600 * (1.23e14 / 2) * 0.3 = 1.371e23

Footnote 10 indicates 300k steps and 4M tokens/step -> 1.2T tokens
Arithmetic estimate: 6 * 2.3B * 1.2T = 1.656e22 FLOPs

Geometric mean: sqrt(1.371e23 * 1.656e22) = 4.765e22",,,"""We focus on improving the translation quality (measured in terms of BLEU score [48]) from all 100 languages to English. This resulted in approximately 13 billion training examples to be used for model training""

Each example is a sentence pair. Assuming 20 words per sentence and 4/3 tokens per word, that is 13*20*4/3 billion tokens",,,,1008.0,6 weeks = 1008 hours,Google TPU v3,Self-supervised learning,,Industry,Confident,"Neural network scaling has been critical for improving the model quality in many real-world machine learning applications with vast amounts of training data and compute. Although this trend of scaling is affirmed to be a sure-fire approach for better model quality, there are challenges on the path such as the computation cost, ease of programming, and efficient implementation on parallel devices. GShard is a module composed of a set of lightweight annotation APIs and an extension to the XLA compiler. It provides an elegant way to express a wide range of parallel computation patterns with minimal changes to the existing model code. GShard enabled us to scale up multilingual neural machine translation Transformer model with Sparsely-Gated Mixture-of-Experts beyond 600 billion parameters using automatic sharding. We demonstrate that such a giant model can efficiently be trained on 2048 TPU v3 accelerators in 4 days to achieve far superior quality for translation from 100 languages to English compared to the prior art.",2026-01-01 14:00:38+00:00,Robi Rahman,,0,United States of America,,,,1024.0,,,,,4000000.0,"Table 3, bolded row is best model",Industry,,,,1032192.0,Open source,"training code is open, Apache: https://github.com/tensorflow/lingvo/tree/master/lingvo/tasks/lm",,266200.0315979732,GShard (dense),GShard (dense),"Trained for a total of 235.5 TPU v3 core-years.
Hardware: 235.5 * 365.25 * 24 * 3600 * (1.23e14 / 2) = 4.571e23 at full utilization

Footnote 10 indicates 300k steps and 4M tokens/step -> 1.2T tokens
6ND: 6 * 2.3B * 1.2T = 1.656e22 FLOPs

Implied utilization: 1.656e22 / 4.571e23 = 0.036
This seems implausibly low, but it's not clear where a mistake could have entered the calculation.",,450,,,,2023-05-29T21:06:54.000Z,Unreleased,FP32,True,939481.3420790087,179000730067391940,2018-05-18,2.1190965092402463,,,1,123000000000000,,FP32,,"Operation counting,Hardware",,,,Google,,,True,,,,,,,,1006832.2819141106,11855440.802498134,346666666667
mT5-XXL,Language,"Language modeling,Translation,Language modeling/generation,Question answering","Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, Colin Raffel","Highly cited,SOTA improvement","""Table 2 presents our main results, with perlanguage breakdowns for each task given in Appendix B. Our largest model mT5-XXL exceeds state-of-the-art on all classification and QA tasks and is near SOTA on NER (69.2 vs. 70.1).""",Open weights (unrestricted),https://aclanthology.org/2021.naacl-main.41/,2892.0,mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer,2020-10-20,"Google,Google Research",13000000000.0,13 billion,8.2e+22,"""We pre-train our mT5 model variants for 1 million steps on batches of 1024 length-1024 input sequences, corresponding to roughly 1 trillion input tokens total.""

1 million steps * 1024 batchsize * 1024 length * 13 billion params * 6 = 8.2e22

Ignores fine-tuning compute; this is likely a small fraction of pre-training compute.",mC4,"""The C4 dataset was explicitly designed to be English only: any page that was not given a probability of at least 99% of being English by langdetect2 was discarded. In contrast, for mC4 we use cld33 to identify over 100 languages.
Since some of these languages are relatively scarce on the internet, we make use of all of the 71 monthly web scrapes released so far by Common Crawl. This is dramatically more source data than was used for C4, for which the April 2019 web scrape alone was enough to provide plenty of English-language data.""","The model was trained on a subset of 1 trillion tokens.
Full mC4 corpus has data ""totaling 6.6B pages and 6.3T tokens""
Distribution by language is in Appendix A.",,,,,,,,,,Confident,"The recent “Text-to-Text Transfer Transformer” (T5) leveraged a unified text-to-text format and scale to attain state-of-the-art results on a wide variety of English-language NLP tasks. In this paper, we introduce mT5, a multilingual variant of T5 that was pre-trained on a new Common Crawl-based dataset covering 101 languages. We detail the design and modified training of mT5 and demonstrate its state-of-the-art performance on many multilingual benchmarks. We also describe a simple technique to prevent “accidental translation” in the zero-shot setting, where a generative model chooses to (partially) translate its prediction into the wrong language. All of the code and model checkpoints used in this work are publicly available.",2026-01-06 00:38:03+00:00,Anonymous,,0,"United States of America,United States of America",,,,,,,,,1048576.0,"""We pre-train our mT5 model variants for 1 million steps on batches of 1024 length-1024 input sequences, corresponding to roughly 1 trillion input tokens total.""","Industry,Industry",,,,,Open source," Apache 2.0 license
training code: https://github.com/google-research/multilingual-t5",,179442.0291399545,mT5-XXL,,,,,,,,2023-11-29T20:28:58.000Z,Open source,,True,,456972095071688500,,NaN,,,0,,,FP32,,Operation counting,,,,,,,True,,,,,,,,,,1000000000000
Switch,Language,Text autocompletion,"William Fedus, Barret Zoph, Noam Shazeer","Highly cited,SOTA improvement",""" On ANLI (Nie et al., 2019), Switch XXL improves over the prior state-of-the-art to get a 65.7 accuracy versus the prior best of 49.4 (Yang et al., 2020)... Finally, we also conduct an early examination of the model’s knowledge with three closed-book knowledge-based tasks: Natural Questions, WebQuestions and TriviaQA, without additional pre-training using Salient Span Masking (Guu et al., 2020). In all three cases, we observe improvements over the prior stateof-the-art T5-XXL model (without SSM)""",Open weights (unrestricted),https://arxiv.org/abs/2101.03961,3008.0,Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity,2021-01-11,Google,1571000000000.0,"""Combining expert, model and data parallelism, we design two large Switch Transformer models, one with 395 billion and 1.6 trillion parameters""
Table 9 gives more precise count of 1571B parameters",8.22e+22,"Table 4
https://arxiv.org/ftp/arxiv/papers/2104/2104.10350.pdf",C4,,"""In our protocol we pre-train with 2^20 (1,048,576) tokens
per batch for 550k steps amounting to 576B total tokens.""

1 token ~ 0.75 words",,890000000000.0,Table 9 states 890B FLOP/seq for largest Switch-C,648.0,"see table 4 in https://arxiv.org/ftp/arxiv/papers/2104/2104.10350.pdf
",Google TPU v3,Self-supervised learning,,Industry,Confident,"In deep learning, models typically reuse the same parameters for all inputs. Mixture of Experts (MoE) defies this and instead selects different parameters for each incoming example. The result is a sparsely-activated model -- with outrageous numbers of parameters -- but a constant computational cost. However, despite several notable successes of MoE, widespread adoption has been hindered by complexity, communication costs and training instability -- we address these with the Switch Transformer. We simplify the MoE routing algorithm and design intuitive improved models with reduced communication and computational costs. Our proposed training techniques help wrangle the instabilities and we show large sparse models may be trained, for the first time, with lower precision (bfloat16) formats. We design models based off T5-Base and T5-Large to obtain up to 7x increases in pre-training speed with the same computational resources. These improvements extend into multilingual settings where we measure gains over the mT5-Base version across all 101 languages. Finally, we advance the current scale of language models by pre-training up to trillion parameter models on the ""Colossal Clean Crawled Corpus"" and achieve a 4x speedup over the T5-XXL model.
",2026-01-01 14:00:52+00:00,Robi Rahman,,,United States of America,,,,1024.0,,,,,,,Industry,,,,663552.0,Unreleased,"Apache 2 for weights: https://huggingface.co/google/switch-c-2048 

paper links to this repo but not clear that the training hyperparams for Switch are here:
https://github.com/google-research/t5x

",,145100.89767585773,Switch,Switch,"Table 4 in https://arxiv.org/pdf/2104.10350 gives measured performance of 34.4 TFLOP/s, vs. peak achievable FLOP/s of 123 TFLOP/s on the TPUv3 being used.

HFU = 34.4/123 = 0.27967",,450,,,,2023-05-29T21:06:54.000Z,Open source,BF16,True,935410.4639575825,566502353304714560,2018-05-18,2.652977412731006,,,1,123000000000000,,BF16,,Third-party estimation,,,,Google,,,True,,,,,,,0.2796747967,643863.538608544,11793424.547210434,86400000000
Wu Dao 2.0,"Multimodal,Language,Vision,Image generation","Image captioning,Chat,Image generation,Text-to-image,Language modeling/generation,Question answering,Visual question answering","Tang Jie, Zhai Jidong, Yang Hongxia, Chen Wenguang, Zheng Weimin, Ma Zixuan, He Jiaao, Qiu Jiezhong, Cao Huanqi, Wang Yuanwei, Sun Zhenbo, Zheng Liyan, Wang Haojie, Tang Shizhi, Feng Guanyu, Zeng Aohan, Zhong Runxin, Shi Tianhui, Du Zhengxiao,
Ding Ming, Tiago Antunes, Peng Jinjun, Lin Junyang Zhang Jianwei ",,,API access,https://www.engadget.com/chinas-gigantic-multi-modal-ai-is-no-one-trick-pony-211414388.html,,China's gigantic multi-modal AI is no one-trick pony,2021-05-31,Beijing Academy of Artificial Intelligence / BAAI,1750000000000.0,"""It's been trained on 1.75 trillion parameters""

MoE architecture, ""tens of thousands of experts""
https://keg.cs.tsinghua.edu.cn/jietang/publications/wudao-3.0-meta-en.pdf",1.54350000000001e+24,"It's a mixture-of-experts model, so all 1.75 trillion params were most likely not trained on each token.

""The parameter scale of Enlightenment 2.0 reached a record-breaking 1.75 trillion. According to reports, the new generation FastMoE technology is the key to the realization of the ""Trillion Model"" cornerstone of Enlightenment 2.0.""

Speculatively assuming 3% of parameters are active per forward pass and the model was trained for one epoch: 
6 FLOP / token / parameter * 1.75 * 10^12 parameters * 0.03  * 4.9 * 10^12 tokens [see dataset size notes] = 1.54e+24 FLOP",WuDao Corpora,"WuDao Corpora, as of version 2.0, was a large dataset constructed for training Wu Dao 2.0. It contains 3 terabytes of text scraped from web data, 90 terabytes of graphical data (incorporating 630 million text/image pairs), and 181 gigabytes of Chinese dialogue (incorporating 1.4 billion dialogue rounds).
https://en.wikipedia.org/wiki/Wu_Dao#WuDao_Corpora","[tokens assumed[
""Bilingual (Cn and En) data: 4.9T text and images"" 

https://keg.cs.tsinghua.edu.cn/jietang/publications/wudao-3.0-meta-en.pdf",,,,,,,,,Industry,Speculative,"Sporting 1.75 trillion parameters, Wu Dao 2.0 is roughly ten times the size of Open AI's GPT-3.",2025-12-01 14:03:25+00:00,Robi Rahman,,,China,,,,,,,,,,,Academia,True,,,,,"based on this presentation weights and codes could be open sourced but links are unreachable from the US:
https://keg.cs.tsinghua.edu.cn/jietang/publications/wudao-3.0-meta-en.pdf",,2870839.8244998017,Wu Dao 2.0,,,,,,,,2023-05-29T21:06:54.000Z,,,True,,537647550667143360,,NaN,,,0,,,FP32,,Operation counting,,,,,,,True,,,,,,,,,,4900000000000
Jurassic-1-Jumbo,Language,"Language modeling/generation,Chat","Opher Lieber, Or Sharir, Barak Lenz, Yoav Shoham",Training cost,"""Training such a large model, on over 800 GPUs over many months""

Lower-bound cost estimate:
800 GPUs * $1/GPU-hour * 4 months = $2.3M
True cost was probably substantially higher. ""many months"" implies more than 4, and the GPUs were probably more expensive than $1/hour.",API access,https://uploads-ssl.webflow.com/60fd4503684b466578c0d307/61138924626a6981ee09caf6_jurassic_tech_paper.pdf,55.0,Jurassic-1: Technical Details and Evaluation,2021-08-11,AI21 Labs,178000000000.0,"""Jurassic-1 models come in two sizes, where the Jumbo version, at 178B parameters, is the largest and most sophisticated language model ever released for general use by developers.""",3.7e+23,"see here https://docs.google.com/document/d/1B8x6XYcmB1u6Tmq3VcbAtj5bzhDaj2TcIPyK6Wpupx4/edit

6 * 178B * 300B = 3.204000e+23",,,"""Our model was trained with the conventional self-supervised auto-regressive training objective on 300B tokens drawn from publicly available resources""

1 token ~ 0.75 words",,,,,,NVIDIA A100,Self-supervised learning,,Industry,Confident,"Jurassic-1 is a pair of auto-regressive language models recently released by AI21 Labs, consisting of J1-Jumbo, a 178B-parameter model, and J1-Large, a 7B-parameter model. We describe their architecture and training, and evaluate their performance relative to GPT-3. The evaluation is in terms of perplexity, as well as zero-shot and few-shot learning. To that end, we developed a zeroshot and few-shot test suite, which we made publicly available (https://github.com/ai21labs/ lm-evaluation) as a shared resource for the evaluation of mega language models.",2025-11-28 11:53:44+00:00,Robi Rahman,,0,Israel,,,,,,,,,3200000.0,"""Namely, we used a base learning rate of 1.2 × 10−4 and 0.6 × 10−4 , and a batch size of 2M and 3.2M tokens, for J1-Large and J1-Jumbo, respectively. We also used a linear warm-up over roughly the first 375 million tokens, and gradually increased the batch size from 32K tokens up to its target value for the first few billion tokens.""",Industry,,,,,Unreleased,,True,836700.050829598,Jurassic-1-Jumbo,,,,400,,,,2023-05-29T21:06:54.000Z,,,True,,442213430766665600,2020-03-01,1.4455852156057496,19500000000000,156000000000000,1,312000000000000,77970000000000,TF16,312000000000000,Third-party estimation,,,,NVIDIA,,,True,,,,,,,,,,300000000000
FLAN 137B,Language,"Language modeling,Question answering,Language modeling/generation","Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le","Highly cited,SOTA improvement","Abstract: 
""FLAN substantially improves the performance of its unmodified counterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 datasets that we evaluate.""

""FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze""

SOTA is reported among unsupervised models",Unreleased,https://arxiv.org/abs/2109.01652,4557.0,Finetuned Language Models Are Zero-Shot Learners,2021-09-03,Google Research,137000000000.0,"Abstract:
""We take a 137B parameter pretrained language model and instruction tune it on over 60 NLP datasets verbalized via natural language instruction templates. We evaluate this instruction-tuned model, which we call FLAN, on unseen task types.""

Many models seem to be using the same 137B base transformer model?",2.047e+24,"From section 2.4: Pretraining was done over 2.49T tokens.
6 * 2.49T * 137B = 2.047e24 
Also, ""instruction tuning takes around 60 hours on a TPUv3 with 128 cores."" 128 TPUv3 cores = 64 TPUv3 chips. Environmental considerations section claims this took less than 2% of total time
1.23e14 * 64 * 60 * 3600 * 0.3 = 5.10e20","Wikipedia,Unspecified unreleased","Abstract: ""We take a 137B parameter pretrained language model and instruction tune it on over 60 NLP datasets""","""Model architecture and pretraining. In our experiments, we use LaMDA-PT, a dense left-to-right, decoder-only transformer language model of 137B parameters (Thoppilan et al., 2022). This model is pretrained on a collection of web documents (including those with computer code), dialog data, and Wikipedia, tokenized into 2.49T BPE tokens with a 32k vocabulary using the  SentencePiece library (Kudo & Richardson, 2018). Around 10% of the pretraining data was non-English. Note that LaMDA-PT only has language model pretraining (c.f. LaMDA, which was finetuned for dialog).""",1.0,,,,,Google TPU v3,Self-supervised learning,,Industry,Confident,"This paper explores a simple method for improving the zero-shot learning abilities of language models. We show that instruction tuning—finetuning language models on a collection of datasets described via instructions—substantially improves zeroshot performance on unseen tasks. We take a 137B parameter pretrained language model and instruction tune it on over 60 NLP datasets verbalized via natural language instruction templates. We evaluate this instruction-tuned model, which we call FLAN, on unseen task types. FLAN substantially improves the performance of its unmodified counterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 datasets that we evaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number of finetuning datasets, model scale, and natural language instructions are key to the success of instruction tuning.",2026-01-01 14:01:09+00:00,Robi Rahman,,,United States of America,LaMDA,,"""In our experiments, we use LaMDA-PT, a dense left-to-right,
decoder-only transformer language model of 137B parameters (Thoppilan et al., 2022) [...] Note that LaMDA-PT only has language model pretraining (c.f. LaMDA, which was finetuned for dialog)."" In our entry for LaMDA we only measured pre-training compute, so we just specify LaMDA as the base model of FLAN 137B.",,,,,,,just a finetune,Industry,,,,3840.0,Unreleased,,True,,FLAN 137B,,,,450,3.55e+23,1.6919999999999998e+24,,2023-05-29T21:06:54.000Z,,,True,,Infinity,2018-05-18,3.29637234770705,,,1,123000000000000,,TF16,123000000000000,Operation counting,,,,Google,,,True,,,,,,,,,,2490000000000
HyperCLOVA 204B,Language,Language modeling/generation,,SOTA improvement,"""HyperCLOVA with our training configuration shows state-of-the-art in-context zero-shot and few-shot learning performances on various downstream tasks in Korean""

no evaluations on any standard benchmarks are reported",Hosted access (no API),https://aibusiness.com/nlp/south-korea-s-naver-unveils-hyperscale-ai-platform-language-model-with-more-parameters-than-gpt-3,92.0,"South Korea's Naver unveils 'hyperscale AI' platform, language model with more parameters than GPT-3",2021-09-10,NAVER,204000000000.0,https://www.navercorp.com/navercorp_/ir/announce/2023/NAVER_CEO%20letter%20to%20shareholders_Aug%202023_Eng.pdf,2.0000000001e+23,"Estimations for 82B model (marked as lower bound estimations)

""For experiments in Section 4, the model trained with 150B is used for fair comparison, because not all models are finished training at the same iteration. However, experiments in Section 5.2 use the model trained with 300B tokens, as HyperCLOVA Studio provided the 39B and 82B models trained with 300B tokens.""

82e9 connections * 2 FLOP/connection * 300e9 tokens * 3 backward pass = 1.476e23 FLOP

Calculation using GPU time corroborates this:
- ""Our model is based on megatron-LM (Shoeybi et al., 2019) and trained on the NVIDIA Superpod, which includes 128 strongly clustered DGX servers with 1,024 A100 GPUs.""
- ""It takes 13.4 days to train a model with 82B parameters with 150B tokens."" Assume 300B tokens takes twice as long, 26.8 days.
- Assume the default of 30% utilization rate for large language models.

1024 A100 GPUs * 312e12 FLOP/second * 0.3 utilization * 26.8 days * 24 * 60 * 60 seconds/day = 2.219e+23 FLOP",Unspecified unreleased,,"https://twitter.com/arankomatsuzaki/status/1397583304610783238
https://venturebeat.com/ai/naver-trained-a-gpt-3-like-korean-language-model/",,,,,,NVIDIA A100,Self-supervised learning,,Industry,Speculative,,2025-11-28 11:53:44+00:00,Natalia Martemianova,,0,Korea (Republic of),,,,,,,,,,,Industry,True,1.476e+23,,,Unreleased,,True,441770.0038182093,HyperCLOVA 204B,,,,400,,,,2024-05-20T12:47:38.000Z,Unreleased,,True,,452724264394150850,2020-03-01,1.5277207392197125,19500000000000,156000000000000,1,312000000000000,77970000000000,TF16,312000000000000,,,,,NVIDIA,,,True,,,,,,,,,,560000000000
Megatron-Turing NLG 530B,Language,"Language modeling,Language modeling/generation,Question answering","Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, Elton Zhang, Rewon Child, Reza Yazdani Aminabadi, Julie Bernauer, Xia Song, Mohammad Shoeybi, Yuxiong He, Michael Houston, Saurabh Tiwary, Bryan Catanzaro","SOTA improvement,Training cost","The 105-layer, transformer-based MT-NLG improved upon the prior state-of-the-art models in zero-, one-, and few-shot settings",Unreleased,https://arxiv.org/abs/2201.11990,805.0,"Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model",2021-10-11,"Microsoft,NVIDIA",530000000000.0,,8.586e+23,"https://www.lesswrong.com/posts/bGuMrzhJdENCo8BxX/nvidia-and-microsoft-releases-530b-parameter-transformer?commentId=HSJSNspKp94tFcSCx

source: https://lair.lighton.ai/akronomicon/
9938 PF-days * 3600 * 24 * 10^15  = 8.586432e+23

6ND estimate: 6 * 530B * 270B = 8.586000e+23","Common Crawl,The Pile,CC-Stories,Realnews"," In addition to Common Crawl data, we leveraged a number of other previously generated datasets. From The Pile, we selected Books3, OpenWebText2, Stack Exchange, PubMed Abstracts,
Wikipedia, Gutenberg (PG-19), BookCorpus2, NIH ExPorter, and Pile-CC datasets. We also included the
CC-Stories and RealNews datasets used to train Megatron","""Our training dataset consists of 339 billion tokens and we
trained MT-NLG on 270 billions tokens by blending the 15 training datasets as described above. We also set aside 2% of our data for validation.""

1 token ~ 0.75 words",,,,770.0,"Total compute was 1.17*10^24 FLOP.
They don't directly report the utilization and training speed when using the full Selene supercomputer with 560 DGX * 8 A100/DGX = 4480 GPUs. See section 2.3 Hardware Setup.

At 280 DGX, the utilization is 126/312 = 40% and a batch takes 60 seconds; at 350, it is 39% for 50 seconds; at 420, it is 36% for 44 seconds.

The overall utilization was 30.2% and the full cluster has 560 DGX. Dividing the total compute by the total performance of 4480 A100 at 30.2% utilization gives 770 hours.",NVIDIA A100 SXM4 80 GB,Self-supervised learning,,Industry,Confident,"Pretrained general-purpose language models can achieve state-of-the-art accuracies in various natural language processing domains by adapting to downstream tasks via zero-shot, few-shot and fine-tuning techniques. Because of their success, the size of these models has increased rapidly, requiring high-performance hardware, software, and algorithmic techniques to enable training such large models. As the result of a joint effort between Microsoft and NVIDIA, we present details on the training of the largest monolithic transformer based language model, Megatron-Turing NLG 530B (MT-NLG), with 530 billion parameters. In this paper, we first focus on the infrastructure as well as the 3D parallelism methodology used to train this model using DeepSpeed and Megatron. Next, we detail the training process, the design of our training corpus, and our data curation techniques, which we believe is a key ingredient to the success of the model. Finally, we discuss various evaluation results, as well as other interesting observations and new properties exhibited by MT-NLG. We demonstrate that MT-NLG achieves superior zero-, one-, and few-shot learning accuracies on several NLP benchmarks and establishes new state-of-the-art results. We believe that our contributions will help further the development of large-scale training infrastructures, large-scale language models, and natural language generations.",2026-01-01 14:01:22+00:00,Robi Rahman,,0,"United States of America,United States of America",,,,4480.0,,,,,3932160.0,"""The sequence length is 2048 and the global batch size is 1920. We used 8-way tensor and 35-way pipeline parallelism. The learning rate is 5.0e −5 . We used one billion tokens for linear learning rate warmup. We used cosine decay for the learning rate targeting to reach 10% of its value over 340 billion tokens. Over the first 12 billion tokens, we started at a batch size of 32 and gradually increased the batch size in increments of 32, until we reach the final batch size of 1920"" 

Final batch size is 1920 * 2048 = 3932160","Industry,Industry",True,,,3449600.0,Unreleased,,True,3848505.6256471733,Megatron-Turing NLG 530B,,"They don't directly report the utilization and training speed when using the full Selene supercomputer with 560 DGX * 8 A100/DGX = 4480 GPUs. See section 2.3 Hardware Setup.

At 280 DGX, the utilization is 126/312 = 40% and a batch takes 60 seconds; at 350, it is 39% for 50 seconds; at 420, it is 36% for 44 seconds.

The overall utilization was 30.2% and the full cluster has 560 DGX. 

HFU = 0.3020
",,400,,,,2023-05-29T21:06:54.000Z,,BF16,True,3615658.868639838,223099582933730530,2020-11-16,0.9007529089664613,19490000000000,156000000000000,1,312000000000000,77970000000000,BF16,,Third-party estimation,,,,NVIDIA,Megatron-Turing NLG 530B,,True,,,,,,,0.302,6663667.97106352,147455699.58722904,270000000000
Yuan 1.0,Language,Language modeling,"Shaohua Wu, Xudong Zhao, Tong Yu, Rongguo Zhang, Chong Shen, Hongli Liu, Feng Li, Hong Zhu, Jiangang Luo, Liang Xu, Xuanwei Zhang, Jun Liu",SOTA improvement,"""The zero-shot average scores of both LM and PLM are superior to the SOTA one. On Csldcp, Tnews and Iflytek tasks, we surpass the zero-shot SOTA by a large margin""",API access,https://arxiv.org/abs/2110.04725,67.0,Yuan 1.0: Large-Scale Pre-trained Language Model in Zero-Shot and Few-Shot Learning,2021-10-12,Inspur,245730000000.0,"Table 2: Parameters of Yuan models.
""Parameters (billion)""",3.5380000000001e+23,"Table 9: 4095 petaFLOPS-days which equals 3.538*10^23 FLOP

https://www.wolframalpha.com/input?i=4095+petaFLOPS+*+1+day
","Common Crawl,Wikipedia,Sogue News","""A Chinese corpus with 5TB high-quality text is built, which is sufficient to train Yuan 245B model without sampling the dataset twice.""

In order to obtain the high-quality dataset, we develop a Massive Data Filtering System (MDFS) built on Spark to clean and filter the raw data, and train a Bert-based model to select high quality samples. MDFS is consisted of three parts, data collection, coarse filtering and fine filtering (Fig. 5). The raw data is collected from Common Crawl, Sogou News, SogouT, Encyclopedia, and Books (Table 3). To process these raw data, we run MDFS system on a high performance cluster with 36 nodes.","""Yuan 1.0 was trained on a new Chinese dataset of 5TB high-quality text that was built on 850TB raw data from Internet.""

1 GB ~ 167M words in English or 333M words in Chinese. For a mixed dataset of mostly Chinese, 5TB may be equivalent to around 1T words.

Table 2: 180B training tokens",,,,,,,Self-supervised learning,,Industry,Confident,"Recent work like GPT-3 has demonstrated excellent performance of Zero-Shot and Few-Shot learning on many natural language processing (NLP) tasks by scaling up model size, dataset size and the amount of computation. However, training a model like GPT-3 requires huge amount of computational resources which makes it challengeable to researchers. In this work, we propose a method that incorporates large-scale distributed training performance into model architecture design. With this method, Yuan 1.0, the current largest singleton language model with 245B parameters, achieves excellent performance on thousands GPUs during training, and the state-of-the-art results on NLP tasks. A data processing method is designed to efficiently filter massive amount of raw data. The current largest high-quality Chinese corpus with 5TB high quality texts is built based on this method. In addition, a calibration and label expansion method is proposed to improve the Zero-Shot and Few-Shot performance, and steady improvement is observed on the accuracy of various tasks. Yuan 1.0 presents strong capacity of natural language generation, and the generated articles are difficult to distinguish from the human-written ones.",2026-01-01 14:01:23+00:00,Robi Rahman,,0,China,,,,2128.0,,,,,6881280.0,"Table 2. Batch size 3360, sequence length 2048. 3360*2048 = 6881280",Industry,,,,,Unreleased,https://github.com/Shawn-IEITSystems/Yuan-1.0,True,591664.5798708292,Yuan 1.0,,"""The Yuan models are trained on a cluster with 2128 GPUs. A stable real performance of 45% of the theoretical peak performance is achieved on this cluster.""

HFU = 0.4500",,,,,,2023-05-29T21:06:54.000Z,,FP16,True,,597973940027389800,,NaN,,,0,,,FP16,,Reported,,,,,,,True,,,,,,,0.45,,48571462.85398424,180000000000
Gopher (280B),Language,"Language modeling,Question answering","Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson d'Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew Johnson, Blake Hechtman, Laura Weidinger, Iason Gabriel, William Isaac, Ed Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu, Geoffrey Irving",SOTA improvement,"""These models are evaluated on 152 diverse tasks, achieving state-of-the-art performance across the majority""",Unreleased,https://arxiv.org/abs/2112.11446,1501.0,"""Scaling Language Models: Methods, Analysis & Insights from Training Gopher""",2021-12-08,DeepMind,280000000000.0,"Language modelling provides a step towards intelligent communication systems by harnessing large repositories of written human knowledge to better predict and understand the world. In this paper, we present an analysis of Transformer-based language model performance across a wide range of model scales -- from models with tens of millions of parameters up to a 280 billion parameter model called Gopher. These models are evaluated on 152 diverse tasks, achieving state-of-the-art performance across the majority. Gains from scale are largest in areas such as reading comprehension, fact-checking, and the identification of toxic language, but logical and mathematical reasoning see less benefit. We provide a holistic analysis of the training dataset and model's behaviour, covering the intersection of model scale with bias and toxicity. Finally we discuss the application of language models to AI safety and the mitigation of downstream harms.",6.31e+23,"Table A26
6.31E+08 Train PFLOPs",MassiveTex,,"""We train all models for 300 billion tokens with a 2048 token context window, using the Adam (Kingma and Ba, 2014) optimiser.""

1 token ~ 0.75 words",1.0,,,920.0,"""We trained Gopher for 920 hours in November and December 2020 in Google’s Georgia datacentre. The PUE of the datacenter at this time was 1.08; the net tCO2e per MWh in October 2020 was 0.33. Using an estimate of 283W drawn per chip, this leads to a total of 380 net tCO2e""",Google TPU v3,Self-supervised learning,,Industry,Confident,"We enhance auto-regressive language models by conditioning on document chunks retrieved from a large corpus, based on local similarity with preceding tokens. With a 2 trillion token database, our Retrieval-Enhanced Transformer (RETRO) obtains comparable performance to GPT-3 and Jurassic-1 on the Pile, despite using 25× fewer parameters. After fine-tuning, RETRO performance translates to downstream knowledge-intensive tasks such as question answering. RETRO combines a frozen Bert retriever, a differentiable encoder and a chunked cross-attention mechanism to predict tokens based on an order of magnitude more data than what is typically consumed during training. We typically train RETRO from scratch, yet can also rapidly RETROfit pre-trained transformers with retrieval and still achieve good performance. Our work opens up new avenues for improving language models through explicit memory at unprecedented scale.",2026-01-01 14:01:22+00:00,Robi Rahman,Gopher (280B),0,United Kingdom of Great Britain and Northern Ireland,,,,4096.0,0.378,,,,6000000.0,"Table 1. ""Furthermore, we increase Gopher’s batch size from three to six million tokens per batch during training""",Industry,,,,3768320.0,Unreleased,,True,640616.8522346151,Gopher (280B),,"Compute used to train model (table A26): 6.31e23 
Maximum possble compute based on GPU-hours at 100% utilization: 920 * 4096 * 3600 * 1.23e14 =  1.669e24

Therefore, we can calculate utilization:
MFU = 6.31e23 / 1.669e24 = 0.3780",,450,,,,2023-05-29T21:06:54.000Z,,BF16,True,3714163.0079064844,984988137291316500,2018-05-18,3.5592060232717317,,,1,123000000000000,,BF16,,Reported,,,,Google,Gopher (280B),,True,,,,,,,,3636030.5562741663,46909499.95890752,300000000000
EXAONE 1.0,"Multimodal,Language,Vision","Translation,Language modeling/generation,Visual question answering",,Training cost,,Unreleased,"https://www.lgcorp.com/media/release/27387#:~:text=LG%20AI%20Research%20proposes%20EXAONE,performance%20while%20learning%20fewer%20parameters.",,,2021-12-14,LG,300000000000.0,,1.6956e+24,"No indication of how images are processed. Supposing they used something like ViT-H/14, and training images were 512x512 (they state ""EXAONE shows remarkable performance such as [...] offering 1024x1024 sized image output"", but typically this size of image training would only be done during a relatively short, final stage of pre-training), there would be 37x37 = 1,369 patches per image
1,369 * 250 million = around 342 billion image patch embeddings.

300M parameters * (342 billion + 600 billion) image tokens * 6 = 1.6956e24",Unspecified unreleased,"""To create multi-modal AI, LG AI Research Institute learned from 600 billion corpora, the world's largest, and more than 250 million high-resolution images combining language and images. It is also differentiated in that it is a bilingual AI that understands and speaks Korean and English at the level of a native speaker.""

600000000000+250000000=600250000000",,,,,,,,,,,Speculative,"[Dec 2021]  EXAONE is a bilingual artificial intelligence that has learned the characteristics of both Korean and English languages at the same time. Since the initial development last June, it has completed learning of 1.3 billion, 13 billion, 39 billion, and 175 billion parameter models, and it is currently learning 300 billion parametric models. EXAONE shows remarkable performance such as obtaining the highest FID score, offering 1024x1024 sized image output, and achieving purpose conversation in the language domain as well as the highest level in the emotional classification domain. ",2025-12-01 14:03:23+00:00,Natalia Martemianova,,1,Korea (Republic of),,,,,,,,,,,Industry,,,5.36e+25,,Unreleased,,True,2702636.2890818655,EXAONE 1.0,,,,,,,,2024-05-20T12:24:33.000Z,,,True,,627387416815906800,,NaN,,,0,,,FP32,,Operation counting,,,,,,True,True,,,,,,,,,,
ERNIE 3.0 Titan,Language,"Language modeling,Language modeling/generation,Relation extraction,Sentiment classification,Text classification","Shuohuan Wang, Yu Sun, Yang Xiang, Zhihua Wu, Siyu Ding, Weibao Gong, Shikun Feng",SOTA improvement,"""Empirical results show that the ERNIE 3.0 Titan outperforms the state-of-the-art models on 68 NLP datasets.""",Hosted access (no API),https://arxiv.org/abs/2112.12731,84.0,ERNIE 3.0 Titan: Exploring Larger-scale Knowledge Enhanced Pre-training for Language Understanding and Generation,2021-12-23,"Baidu,Peng Cheng Laboratory",260000000000.0,"""[We] developed... distributed training technology, including fine-grained parallelism, heterogeneous hardware-aware training, and fault tolerance mechanism to train the 260B model on both Nvidia V100 GPU and Ascend 910 NPU clusters.""
See also:
https://twitter.com/BaiduResearch/status/1468633977242243078?t=6q4zuLNdTSc4GUBe9OM5Aw&s=19",1.0421e+24,"The paper suggests that ERNIE 3.0 Titan uses more compute than GPT-3. This is consistent with the 6ND approximation.

C = 6ND = 6 (FLOP/param/token) * (260B params) * (668B tokens) = 1.0421*10^24 FLOP",ERNIE 3.0 Corpus,,"""To ensure the success of the pre-training of ERNIE 3.0 Titan, we utilize the ERNIE 3.0 Corpus [ 2 ], a large-scale, wide-variety, and high-quality Chinese text corpora amounting to 4TB""

Assuming 167M words/tokens per GB",,,,,,"NVIDIA Tesla V100 DGXS 32 GB,Huawei Ascend 910",,,Industry,Confident,"Pre-trained language models have achieved state-of-the-art results in various Natural Language Processing (NLP) tasks. GPT-3 has shown that scaling up pre-trained language models can further exploit their enormous potential. A unified framework named ERNIE 3.0 was recently proposed for pre-training large-scale knowledge enhanced models and trained a model with 10 billion parameters. ERNIE 3.0 outperformed the state-of-the-art models on various NLP tasks. In order to explore the performance of scaling up ERNIE 3.0, we train a hundred-billion-parameter model called ERNIE 3.0 Titan with up to 260 billion parameters on the PaddlePaddle platform. Furthermore, we design a self-supervised adversarial loss and a controllable language modeling loss to make ERNIE 3.0 Titan generate credible and controllable texts. To reduce the computation overhead and carbon emission, we propose an online distillation framework for ERNIE 3.0 Titan, where the teacher model will teach students and train itself simultaneously. ERNIE 3.0 Titan is the largest Chinese dense pre-trained model so far. Empirical results show that the ERNIE 3.0 Titan outperforms the state-of-the-art models on 68 NLP datasets.",2025-12-01 14:05:51+00:00,Robi Rahman,,0,"China,China",,,,1920.0,,,"Peng Cheng Cloud Brain II, Paper on Ernie 3.0",,1048576.0,"""The maximum sequence length of context and
the memory length of language generation is 512 and 128, respectively""

In table 1, they use a global batch size of 512 when data parallelism is ""1"" and 2048 when DP is ""4"". Not sure I fully understand this part but I guess they'd use parallelism as much as possible given how they talk about it.

2048 * 512 = 1048576.","Industry,Academia",True,,,,Unreleased,"The Ernie 3.0 Titan model was used in Ernie bot. Today, ERNIE has been widely deployed across finance, healthcare, insurance, equity, Internet, logistics, and other fields.

http://research.baidu.com/Blog/index-view?id=165",True,,ERNIE 3.0 Titan,,,,"250,310",,,,2023-05-29T21:06:54.000Z,,FP16,True,,Infinity,"2018-03-27,2019-08-23",NaN,15670000000000,,2,125000000000000,"31330000000000,256000000000000",FP16,"31330000000000,256000000000000",Operation counting,,,,"NVIDIA,Huawei",,,True,,,,,,,,,,668000000000
PaLM (540B),Language,"Language modeling,Code generation,Translation","Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev,, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta ,Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, Noah Fiedel","Highly cited,SOTA improvement,Training cost","Table 4
Demonstrates continued benefits of scaling, as well as discontinuous improvements in performance",Unreleased,https://arxiv.org/abs/2204.02311,7351.0,PaLM: Scaling Language Modeling with Pathways,2022-04-04,Google Research,540350000000.0,"""To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model (PaLM).""",2.5272e+24,"See Table 20.

6144 TPUv4 for 1200 hours + 3072 TPUv4 for 336 hours.
Equivalent to 6144 TPUv4 for 1368 hours.

46.2% model FLOPs utilization

""The 540B-parameter PaLM model sustained a remarkable 57.8% of the peak hardware floating point performance over 50 days while training on TPU v4 supercomputers."" https://cloud.google.com/blog/topics/systems/tpu-v4-enables-performance-energy-and-co2e-efficiency-gains","Wikipedia,GLaM dataset,LaMBDA dataset,GitHub",,"""The PaLM pretraining dataset consists of a high-quality corpus of 780 billion tokens that represent a wide range of natural language use cases.""

1 token ~ 0.75 words",1.0,,,1536.0,"6144 TPUv4 for 1200 hours + 3072 TPUv4 for 336 hours.

Equivalent to 6144 TPUv4 for 1368 hours.",Google TPU v4,Self-supervised learning,"Training compute and utilization rate exclude rematerialization FLOP, but cost should account for rematerialization.",Industry,Confident,"Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.",2026-01-01 14:01:38+00:00,Robi Rahman,,0,United States of America,,,,6144.0,0.462,,,,4000000.0,"""For the largest model, we use batch size 512 (1M tokens) until step 50k, then double it to 1024 (2M tokens) until step 115k, and finally double again it to 2048 (4M tokens) until training is complete at step 255k""",Industry,True,,,8404992.0,Unreleased,,True,3060364.5969860666,PaLM (540B),,"""In Section 4, we describe how we were able to scale pipeline-free training of PaLM 540B to 6144 chips across two TPU v4 Pods while achieving very high efficiency of 46.2% in model FLOPs utilization (observed throughput relative to theoretical max throughput) and 57.8% in hardware FLOPs utilization.""

MFU = 0.4620
HFU = 0.5780
",,340,,,,2023-05-29T21:06:54.000Z,,,True,4198431.396906301,825783961325672800,2021-05-20,0.8733744010951403,,,1,275000000000000,,TF16,275000000000000,Hardware,,,,Google,PaLM 540B,,True,,,,,,,0.578,14451957.906571811,67883583.58684365,780000000000
Minerva (540B),Language,"Quantitative reasoning,Mathematical reasoning,Language modeling/generation,Question answering","Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, Vedant Misra",SOTA improvement,"""We achieve state-of-the-art performance on MATH Hendrycks et al. (2021), GSM8k Cobbe et al. (2021), and a STEM subset of the MMLU Hendrycks et al. (2020) dataset""",Unreleased,https://arxiv.org/abs/2206.14858,1275.0,Solving Quantitative Reasoning Problems with Language Models,2022-06-29,Google,540350000000.0,"""To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model (PaLM).""

Our approach is to start with the PaLM pretrained decoder-only transformer language models Chowdhery et al. (2022), and further train (finetune) them on our mathematical dataset using an autoregressive objective.
Table 2 contains the main model and training hyperparameters.

See Table 2",2.7415e+24,"Minerva was fine-tuned from PaLM using the same hardware. Assume the same model FLOPs utilization rate for pre-training and fine-tuning.
""the 540B model was trained for 29 days on a v4-1024""

PaLM pretraining time: 6144 TPU for 1200 hours + 3072 TPU for 336 hours = @8404992 TPU-hours
Minerva finetuning time: 1024 TPU for 696 hours = 712704 TPU-hours
So fine-tuning added 8.5% more compute.

Minerva total compute = PaLM pretraining compute * (712704+8404992)/(8404992) = 2.7415*10^24 FLOP
https://www.wolframalpha.com/input?i=%28712704%2B8404992%29%2F%288404992%29+*+2.5272*10%5E24

Palm pretraining: 2.5272e+24",arXiv,"PaLM, finetuned on arxiv","""Our models were trained on a dataset of 38.5B tokens"" + PaLM

upd 38.5B tokens - sie of the dataset, the model saw 26B tokens in 399k steps (see Table 2)",,,,696.0,,Google TPU v4,Self-supervised learning,,Industry,Confident,"Language models have achieved remarkable performance on a wide range of tasks that require natural language understanding. Nevertheless, state-of-the-art models have generally struggled with tasks that require quantitative reasoning, such as solving mathematics, science, and engineering problems at the college level. To help close this gap, we introduce Minerva, a large language model pretrained on general natural language data and further trained on technical content. The model achieves state-of-the-art performance on technical benchmarks without the use of external tools. We also evaluate our model on over two hundred undergraduate-level problems in physics, biology, chemistry, economics, and other sciences that require quantitative reasoning, and find that the model can correctly answer nearly a third of them.",2026-01-01 14:02:05+00:00,Robi Rahman,,0,United States of America,PaLM (540B),2.1429e+23,,1024.0,,,,,,,Industry,True,,,712704.0,Unreleased,,True,,Minerva (540B),,,,340,2.5272e+24,9.9999999998338e+18,,2023-05-29T21:06:54.000Z,,,True,698399.733142376,Infinity,2021-05-20,1.108829568788501,,,1,275000000000000,,TF16,275000000000000,Hardware,,,,Google,,,True,,,,,,,,,,26000000000
U-PaLM (540B),Language,"Language generation,Language modeling/generation,Question answering,Mathematical reasoning,Quantitative reasoning","Yi Tay, Jason Wei, Hyung Won Chung, Vinh Q. Tran, David R. So, Siamak Shakeri, Xavier Garcia, Huaixiu Steven Zheng, Jinfeng Rao, Aakanksha Chowdhery, Denny Zhou, Donald Metzler, Slav Petrov, Neil Houlsby, Quoc V. Le, Mostafa Dehghani",SOTA improvement,"Figure 3
""We show that U-PaLM 540B outperforms PaLM 540B on 21 out of 26 tasks. Given that PaLM is the SOTA language model on these tasks, this makes U-PaLM the new state-of-the-art on these tasks.""

performance improvement equivalent to 2x training efficiency: ""Impressively, at 540B scale, we show an approximately 2x computational savings rate where U-PaLM achieves the same performance as the final PaLM 540B model at around half its computational budget """,Unreleased,https://arxiv.org/abs/2210.11399,73.0,Transcending Scaling Laws with 0.1% Extra Compute,2022-10-20,Google,540000000000.0,,2.53e+24,"""The total number of extra tokens we train on for the 540B
model is approximately 1.3 Billion which constitutes 0.16% extra computation... Training an U-PaLM 540B model only consumes 512 TPUv4 chips and finishes in about 5 days which is considered to be lightweight.""

original PaLM was 2.527e+24. adding 0.16% is ~2.53e24",,"""To keep things consistent, we train this model with the same data mixture as PaLM and do not rely on additional sources of data (labeled or unlabeled).""",,,,,120.0,5 days,Google TPU v4,,,,Confident,"Scaling language models improves performance but comes with significant computational costs. This paper proposes UL2R, a method that substantially improves existing language models and their scaling curves with a relatively tiny amount of extra compute. The key idea is to continue training a state-of-the-art large language model (e.g., PaLM) on a few more steps with UL2's mixture-of-denoiser objective. We show that, with almost negligible extra computational costs and no new sources of data, we are able to substantially improve the scaling properties of large language models on downstream metrics. In this paper, we continue training PaLM with UL2R, introducing a new set of models at 8B, 62B, and 540B scale which we call U-PaLM. Impressively, at 540B scale, we show an approximately 2x computational savings rate where U-PaLM achieves the same performance as the final PaLM 540B model at around half its computational budget (i.e., saving ∼4.4 million TPUv4 hours). We further show that this improved scaling curve leads to 'emergent abilities' on challenging BIG-Bench tasks -- for instance, U-PaLM does much better than PaLM on some tasks or demonstrates better quality at much smaller scale (62B as opposed to 540B). Overall, we show that U-PaLM outperforms PaLM on many few-shot setups, i.e., English NLP tasks (e.g., commonsense reasoning, question answering), reasoning tasks with chain-of-thought (e.g., GSM8K), multilingual tasks (MGSM, TydiQA), MMLU and challenging BIG-Bench tasks. Finally, we provide qualitative examples showing the new capabilities of U-PaLM for single and multi-span infilling.",2025-11-28 11:53:44+00:00,Anonymous,,0,United States of America,PaLM (540B),4e+21,"""The total number of extra tokens we train on for the 540B
model is approximately 1.3 Billion which constitutes 0.16% extra computation... Training an U-PaLM 540B model only consumes 512 TPUv4 chips and finishes in about 5 days which is considered to be lightweight.""

PaLM was 2.5e24
0.16% of that is 4e21",512.0,,,,,,,Industry,,,,61440.0,Unreleased,,True,,U-PaLM (540B),,,,340,2.5272e+24,-1.2000000000004572e+21,,2023-12-29T00:21:36.000Z,,,True,348322.23114529514,Infinity,2021-05-20,1.4182067077344285,,,1,275000000000000,,TF16,275000000000000,Comparison with other models,,,,Google,,,True,,,,,,,,,,1300000000
Flan-PaLM 540B,Language,"Language modeling/generation,Question answering,Mathematical reasoning","Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, Jason Wei","Highly cited,SOTA improvement",">1k cites

""Flan-PaLM 540B achieves state-of-the-art performance on several benchmarks, such as 75.2% on five-shot MMLU.""",Unreleased,https://arxiv.org/abs/2210.11416,3771.0,Scaling Instruction-Finetuned Language Models,2022-10-20,Google,540000000000.0,540B,2.540000000001e+24,"0.2% greater than Palm 540B, which used 2.5e24",Flan,"Various instruction examples for many tasks:

""Our final set of finetuning tasks is sourced from a combination of tasks from FLAN, T0, Natural Instructions, along with some dialog, program synthesis, and chain-of-thought reasoning tasks, as described in Figure 2. We provide specific pointers and citations in Table 24. All data sources are publicly
available. We also remove all MMLU tasks from Natural Instructions to preserve its role as a broad benchmark of 57 held-out tasks for evaluation. In total, there are 1,836 tasks."" ",,,,,37.0,"""we only use 0.2% of the pre-training compute to instruction-finetune Flan-PaLM 540B (approximately 512 v4 TPU chips for 37 hours)""",Google TPU v4,,,,Confident,"Finetuning language models on a collection of datasets phrased as instructions has been shown to improve model performance and generalization to unseen tasks. In this paper we explore instruction finetuning with a particular focus on (1) scaling the number of tasks, (2) scaling the model size, and (3) finetuning on chain-of-thought data. We find that instruction finetuning with the above aspects dramatically improves performance on a variety of model classes (PaLM, T5, U-PaLM), prompting setups (zero-shot, few-shot, CoT), and evaluation benchmarks (MMLU, BBH, TyDiQA, MGSM, open-ended generation). For instance, Flan-PaLM 540B instruction-finetuned on 1.8K tasks outperforms PALM 540B by a large margin (+9.4% on average). Flan-PaLM 540B achieves state-of-the-art performance on several benchmarks, such as 75.2% on five-shot MMLU. We also publicly release Flan-T5 checkpoints, which achieve strong few-shot performance even compared to much larger models, such as PaLM 62B. Overall, instruction finetuning is a general method for improving the performance and usability of pretrained language models.",2026-01-01 14:02:05+00:00,Anonymous,,0,United States of America,PaLM (540B),5.6e+21,"5.6e21 per Table 2

""we only use 0.2% of the pre-training compute to instruction-finetune Flan-PaLM 540B (approximately 512 v4 TPU chips for 37 hours)""

512 * 37 * 3600 * 275 teraflops * 0.3 = 5.6e21 (so 30% utilization was correct)",512.0,0.1892,,,,,,Industry,,,,18944.0,Unreleased,,True,,Flan-PaLM 540B,,"Estimated training compute: 2.5e24
FLOPs at 100% utilization, based on GPU-hours: 37 * 512 * 3600 * 3.12e14 = 2.128e22

Therefore, we can calculate utilization from known compute and maximum possible compute:
MFU = 3.76e24 / 1.987e25 = 0.1892",,340,2.5272e+24,7.200000001000249e+21,,2024-02-08T16:31:34.000Z,,,True,348322.23114529514,Infinity,2021-05-20,1.4182067077344285,,,1,275000000000000,,TF16,275000000000000,"Reported,Hardware",,,,Google,,,True,,,,,,,,,,1400000000
"GPT-3.5 (davinci-002)
",Language,Language modeling,,"Historical significance,Significant use,SOTA improvement,Training cost",,API access,https://platform.openai.com/docs/models/gpt-3-5,,,2022-11-28,OpenAI,,"Parameter count may be 175B based on OpenAI's statements that text-davinci-003 is in the GPT-3.5 series of models. It was also stated to be 175B in the Microsoft CODEFUSION paper, but the paper was reportedly retracted because the authors did not know the parameter count.",2.578e+24,https://colab.research.google.com/drive/1QSxa8YCWjEBQU7mrXLhw6TP1VX5oqgdW#scrollTo=Gt6Z6oZ26clI,,"Knowledge cutoff date was originally September 2021 - which I assume means September 1, 2021 - according to https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models?tabs=global-standard%2Cstandard-chat-completions#gpt-35.",,,,,,,NVIDIA A100 SXM4 40 GB,Reinforcement learning,,Industry,Speculative,,2026-01-02 07:11:56+00:00,Anonymous,,0,United States of America,,,,,,,,,,,Industry,,,9.9e+24,,Unreleased,,True,4805631.250163697,"GPT-3.5 (text-davinci-003),GPT-3.5",,,,400,,,$20 / M tokens,2023-10-02T01:09:41.000Z,,,True,,536453977802018050,2020-05-14,2.54072553045859,19490000000000,156000000000000,1,312000000000000,77970000000000,TF16,312000000000000,"Comparison with other models,Benchmarks",,,,NVIDIA,"text-davinci-003,text-davinci-002,code-davinci-002,text-davinci-edit-001,code-davinci-edit-001",,"True,True",,,,2021-09-01,,,,,,
GPT-4 (Mar 2023),"Multimodal,Language,Vision","Language modeling,Language modeling/generation,Question answering,Visual question answering","OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Simón Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain et al. (181 additional authors not shown)","Highly cited,SOTA improvement,Training cost","See the paper, p.1: ""On a suite of traditional NLP benchmarks, GPT-4 outperforms both previous large language models and most state-of-the-art systems (which often have benchmark-specific training or hand-engineering).""

""On the MMLU benchmark [35, 36], an English-language suite of multiple-choice questions covering 57 subjects, GPT-4 not only outperforms existing models by a considerable margin in English, but also demonstrates strong performance in other languages""",API access,https://arxiv.org/abs/2303.08774,20421.0,GPT-4 Technical Report,2023-03-15,OpenAI,1800000000000.0,"Rumored to be 1.8T parameter MoE with 280B activated on the forward pass, per https://www.semianalysis.com/p/gpt-4-architecture-infrastructure. Other sources estimate 1.76T with 220B per forward pass https://web.archive.org/web/20230712123915/https://the-decoder.com/gpt-4-architecture-datasets-costs-and-more-leaked/",2.1e+25,"90% CI: 8.2E+24 to 4.4E+25

NOTE: this is a rough estimate based on public information, much less information than most other systems in the database.

Calculation and confidence intervals here: https://colab.research.google.com/drive/1O99z9b1I5O66bT78r9ScslE_nOj5irN9?usp=sharing",Unspecified unreleased,"Assuming this is the earliest model in the family, the knowledge cutoff date was originally September 2021 - which I assume means September 1, 2021 - according to https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models?tabs=global-standard%2Cstandard-chat-completions#gpt-35.","Speculative. Reported secondhand by online sources such as Semianalysis, but not verified by OpenAI. If total number of tokens seen was 13T, text was repeated for 2 epochs, and text was the majority of tokens, then dataset size roughly is 13T*0.75/2 = 4.9T words.

Note this examines only the text dataset, since GPT-4 was first and foremost a language model. However, the vision component had its own vision dataset, which we believe accounted for a much smaller part of the compute budget.",2.0,,,2280.0,(Speculative) SemiAnalysis conjectures that GPT-4 training took 90-100 days with utilization of 32-36%.,NVIDIA A100 SXM4 40 GB,Self-supervised learning,,Industry,Likely,"We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.",2026-01-06 02:29:11+00:00,Robi Rahman,,0,United States of America,,,,25000.0,0.34,,,,60000000.0,not listed,Industry,True,,,57000000.0,Unreleased,,True,37334304.976625234,GPT-4,GPT-4,"CANNOT VERIFY, LIKELY HFU, PAYWALLED. 
(Speculative) SemiAnalysis conjectures that GPT-4 had utilization of 32-36%: https://www.semianalysis.com/p/gpt-4-architecture-infrastructure",True,400,,,"$30 / M input tokens (<= 8k context),$60 / M input tokens (<= 32k context),$60 / M output tokens (<= 8k context),$120 / M output tokens (<= 32k context)",2023-05-29T21:06:54.000Z,,,True,19944368.12599428,562485360666228100,2020-05-14,2.833675564681725,19490000000000,156000000000000,1,312000000000000,77970000000000,TF16,312000000000000,Hardware,,,,NVIDIA,"gpt-4-0314,gpt-4-32k-0314",,True,,,,2021-09-01,,,,81392748.68255694,806190976.8020828,5416666666666.667
GPT-4 (Jun 2023),"Multimodal,Language,Vision","Language modeling,Language modeling/generation,Question answering,Visual question answering","OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Simón Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain et al. (181 additional authors not shown)","Highly cited,SOTA improvement,Training cost","See the paper, p.1: ""On a suite of traditional NLP benchmarks, GPT-4 outperforms both previous large language models and most state-of-the-art systems (which often have benchmark-specific training or hand-engineering).""

""On the MMLU benchmark [35, 36], an English-language suite of multiple-choice questions covering 57 subjects, GPT-4 not only outperforms existing models by a considerable margin in English, but also demonstrates strong performance in other languages""",API access,https://arxiv.org/abs/2303.08774,20421.0,GPT-4 Technical Report,2023-03-15,OpenAI,1800000000000.0,"Rumored to be 1.8T parameter MoE with 280B activated on the forward pass, per https://www.semianalysis.com/p/gpt-4-architecture-infrastructure. Other sources estimate 1.76T with 220B per forward pass https://web.archive.org/web/20230712123915/https://the-decoder.com/gpt-4-architecture-datasets-costs-and-more-leaked/",2.1e+25,"90% CI: 8.2E+24 to 4.4E+25

NOTE: this is a rough estimate based on public information, much less information than most other systems in the database.

Calculation and confidence intervals here: https://colab.research.google.com/drive/1O99z9b1I5O66bT78r9ScslE_nOj5irN9?usp=sharing",Unspecified unreleased,"Assuming this is the earliest model in the family, the knowledge cutoff date was originally September 2021 - which I assume means September 1, 2021 - according to https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models?tabs=global-standard%2Cstandard-chat-completions#gpt-35.","Speculative. Reported secondhand by online sources such as Semianalysis, but not verified by OpenAI. If total number of tokens seen was 13T, text was repeated for 2 epochs, and text was the majority of tokens, then dataset size roughly is 13T*0.75/2 = 4.9T words.

Note this examines only the text dataset, since GPT-4 was first and foremost a language model. However, the vision component had its own vision dataset, which we believe accounted for a much smaller part of the compute budget.",2.0,,,2280.0,(Speculative) SemiAnalysis conjectures that GPT-4 training took 90-100 days with utilization of 32-36%.,NVIDIA A100 SXM4 40 GB,Self-supervised learning,,Industry,Likely,"We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.",2026-01-06 02:29:14+00:00,Luke Frymire,,0,United States of America,GPT-4 (Mar 2023),,,25000.0,0.34,,,,60000000.0,"MoE so i hesitate but 7.5 mill? https://www.reddit.com/r/mlscaling/comments/14wcy7m/gpt4s_details_are_leaked/#:~:text=There%20is%20millions%20of%20rows,get%20the%20real%20batch%20size.",Industry,True,,,57000000.0,Unreleased,,True,,GPT-4,GPT-4,"CANNOT VERIFY, LIKELY HFU, PAYWALLED. 
(Speculative) SemiAnalysis conjectures that GPT-4 had utilization of 32-36%: https://www.semianalysis.com/p/gpt-4-architecture-infrastructure",True,400,2.1e+25,0.0,"$30 / M input tokens (<= 8k context),$60 / M input tokens (<= 32k context),$60 / M output tokens (<= 8k context),$120 / M output tokens (<= 32k context)",2025-12-04T17:58:01.000Z,,,True,19944368.12599428,Infinity,2020-05-14,2.833675564681725,19490000000000,156000000000000,1,312000000000000,77970000000000,TF16,312000000000000,Hardware,,,,NVIDIA,"gpt-4-0613,gpt-4-32k-0613",,True,,,,2021-09-01,,,,,,5416666666666.667
SenseChat,Language,"Chat,Language modeling/generation",,Training cost,,API access,https://www.sensetime.com/en/news-detail/51166397?categoryId=1072,,"SenseTime Launches “SenseNova” Foundation Model Sets and AI Computing Systems, Advancing AGI Development",2023-04-10,SenseTime,180000000000.0,"https://www.thepaper.cn/newsDetail_forward_22639611
Translation:
""SenseTime launched the ""SenseNova"" large model system, which includes natural language generation, image generation services, pre-labeling for perception models, and model development. The ""SenseChat"" application platform, powered by a 180-billion parameter Chinese language model, supports ultra-long text comprehension and offers capabilities such as question answering, understanding, and generation in Chinese.""

Link says ""hundreds of billions"" but the more precise number above seems more credible.",3.89e+24,"“Over the course of five years, SenseTime has built SenseCore, a leading AI infrastructure with 27,000 GPUs, capable of delivering a total computational power of 5,000 petaflops”

Assuming they used this entire cluster with 30 days of training (rough average of frontier model training times since 2016), 30% utilization rate: 5000e15 * 0.3 * 30 * 24 * 60 * 60 = 3.89e24 FLOP.

Assuming the model is dense and trained Chinchilla-optimal: 20 tokens/parameter * (180e9 parameters)**2 * 6 = 3.89e24 FLOP. (The two estimates match by coincidence.)

The model seems more likely than not to be dense, given that news of SenseChat 5.0 makes a point of stating its MoE architecture, whereas SenseChat 1.0 does not mention architecture.

Given uncertainties (e.g. the model is possibly MoE, could have been overtrained or undertrained, could have trained longer or shorter), likely between 1e23 and 3e25 FLOP.",,,,,,,,,,,,,Speculative,"SenseTime hosted a Tech Day event, sharing their strategic plan for advancing AGI (Artificial General Intelligence) development through the combination of “foundation models + large-scale computing” systems. Under this strategy, SenseTime unveiled the “SenseNova” foundation model set, introducing a variety of foundation models and capabilities in natural language processing, content generation, automated data annotation, and custom model training. At the event, SenseTime not only showcased their large language model’s capabilities, but also demonstrated a series of generative AI models and applications, such as text-to-image creation, 2D/3D digital human generation, and complex scenario/detailed object generation. Additionally, they introduced their AGI research and development platform facilitated by the integration of “foundation models + large-scale computing” systems.",2025-11-28 11:53:44+00:00,Anonymous,,1,Hong Kong,,,,,,,SenseTime AI Computing Center,,,,Industry,,1.000001e+23,3e+25,,Unreleased,,True,5328550.47393551,SenseChat,,,,,,,,2023-12-06T22:16:50.000Z,,,True,,730029680497135500,,NaN,,,0,,,FP32,,Hardware,,,,,,True,True,,,,,,,,,,
PaLM 2,Language,"Language modeling,Language modeling/generation","Andrew M. Dai, David R. So, Dmitry Lepikhin, Jonathan H. Clark, Maxim Krikun, Melvin Johnson, Nan Du, Rohan Anil, Siamak Shakeri, Xavier Garcia, Yanping Huang, Yi Tay, Yong Cheng, Yonghui Wu, Yuanzhong Xu, Yujing Zhang, Zachary Nado, Bryan Richter, Alex Polozov, Andrew Nystrom, Fangxiaoyu Feng, Hanzhao Lin, Jacob Austin, Jacob Devlin, Kefan Xiao, Orhan Firat, Parker Riley, Steven Zheng, Yuhuai Wu, Zhongtao Liu, Jiahui Yu, Guy Gur-Ari, Weikang Zhou, Sneha Kudugunta, Sunipa Dev, Frederick Liu, Gustavo Hernandez Abrego, Kelvin Xu, Abe Ittycheriah, Daniel Sohn, John Nham, Le Hou, Siyuan Qiao, Pidong Wang, Zirui Wang, Laurent El Shafey, Hyeontaek Lim, Marcello Maggioni, Michael Isard, Paul Barham, Qiao Zhang, Tao Wang, Yash Katariya, Aurko Roy, Benjamin Lee, Brennan Saeta, Ce Zheng, Hadi Hashemi, Junwhan Ahn, Rajkumar Samuel, Steven Hand, Zhifeng Chen, Kiran Vodrahalli, Aakanksha Chowdhery, Ethan Dyer, Emanuel Taropa, Vlad Feinberg, James Bradbury, Reiner Pope, Wei Li, YaGuang Li, Eric Chu, Jeffrey Hui, Joshua Howland, Vlad Fienber, Aroma Mahendru, Michele Catasta, Vedant Misra, Kevin Robinson, Maysam Moussalem, Sebastian Ruder, Erica Moreira, Eric Ni, Paige Bailey, Lucas Gonzalez, Alexandre Passos, Slav Petrov, Gaurav Mishra, Mark Omernick, Ambrose Slone, Andrea Hu, Colin Cherry, Denny Zhou, Jan Botha, John Wieting, Joshua Maynez, Kathleen Kenealy, Kevin Brooks, Linting Xue, Markus Freitag, Martin Polacek, Pengcheng Yin, Sebastian Gehrmann, Xuezhi Wang, Kathy Meier-Hellstern, Christopher A. Choquette-Choo, Daniel Smilkov, Emily Reif, Alicia Parrish, Alex Castro Ros, Clément Crepy, Dasha Valter, Jeremy Hurwitz, Katherine Lee, Mark Díaz, Marie Pellat, Matthew Jagielski, Renee Shelby, Shachi Dave","SOTA improvement,Training cost,Significant use,Highly cited","Significant use: Gmail and Google Docs have millions of users.
SOTA performance: Table 5
""At I/O today, we announced over 25 new products and features powered by PaLM 2. That means that PaLM 2 is bringing the latest in advanced AI capabilities directly into our products and to people — including consumers, developers, and enterprises of all sizes around the world. Here are some examples:
PaLM 2’s improved multilingual capabilities are allowing us to expand Bard to new languages, starting today. Plus, it’s powering our recently announced coding update.
Workspace features to help you write in Gmail and Google Docs, and help you organize in Google Sheets are all tapping into the capabilities of PaLM 2 at a speed that helps people get work done better, and faster.""
https://blog.google/technology/ai/google-palm-2-ai-large-language-model/",API access,https://arxiv.org/abs/2305.10403,1734.0,PaLM 2 Technical Report,2023-05-10,Google,340000000000.0,"Model Architecture: ""PaLM-2 is a new state-of-the-art language model. We have small, medium, and large variants that use stacked layers based on the Transformer architecture, with varying parameters depending on model size. Further details of model size and architecture are withheld from external publication.""
However, the parameter count was leaked to CNBC: https://www.cnbc.com/2023/05/16/googles-palm-2-uses-nearly-five-times-more-text-data-than-predecessor.html",7.34e+24,"Compute Requirements ""Not reported.""
Paper suggests heuristic of  C=6ND. Based on 340B parameters and 3.6T tokens, training compute would be around 7.3*10^24 FLOP.",,"""The PaLM 2 pre-training corpus is composed of a diverse set of sources: web documents, books, code, mathematics, and conversational data. The pre-training corpus is significantly larger than the corpus used to train PaLM (Chowdhery et al., 2022). PaLM 2 is trained on a dataset that includes a higher percentage of non-English data than previous large language models, which is beneficial for multilingual tasks"" (page 9)

“PaLM 2's knowledge cutoff time is mid-2021. Knowledge about events after that time is limited,” according to https://ai.google.dev/palm_docs/palm. September 2021 according to https://computercity.com/artificial-intelligence/knowledge-cutoff-dates-llms","""The pre-training corpus is significantly larger than the corpus used to train PaLM"" so greater than 6e+11. According to the leaked documents viewed by CNBC, the corpus was 3.6 trillion tokens or around 2.7*10^12 words.

https://www.cnbc.com/2023/05/16/googles-palm-2-uses-nearly-five-times-more-text-data-than-predecessor.html",,,,,,Google TPU v4,,PaLM 2 was trained on TPU v4 according to the model card (pages 91-92),Industry,Likely,"We introduce PaLM 2, a new state-of-the-art language model that has better multilingual and reasoning capabilities and is more compute-efficient than its predecessor PaLM (Chowdhery et al., 2022). PaLM 2 is a Transformer-based model trained using a mixture of objectives similar to UL2 (Tay et al., 2023). Through extensive evaluations on English and multilingual language, and reasoning tasks, we demonstrate that PaLM 2 has significantly improved quality on downstream tasks across different model sizes, while simultaneously exhibiting faster and more efficient inference compared to PaLM. This improved efficiency enables broader deployment while also allowing the model to respond faster, for a more natural pace of interaction. PaLM 2 demonstrates robust reasoning capabilities exemplified by large improvements over PaLM on BIG-Bench and other reasoning tasks. PaLM 2 exhibits stable performance on a suite of responsible AI evaluations, and enables inference-time control over toxicity without additional overhead or impact on other capabilities. Overall, PaLM 2 achieves state-of-the-art performance across a diverse set of tasks and capabilities.",2025-12-18 05:14:58+00:00,Robi Rahman,,0,United States of America,,,,,,,,,,not listed,Industry,True,,7.34e+24,,Unreleased,,True,5014266.983518871,PaLM 2,,,,340,,,,2023-05-29T21:06:54.000Z,,,True,,1463823131900526600,2021-05-20,1.9712525667351128,,,1,275000000000000,,TF16,275000000000000,Operation counting,,,,Google,,,True,,,,2021-09-01,,,,,,3600000000000
Claude 2,Language,"Language modeling,Chat,Language modeling/generation,Question answering",,Historical significance,,API access,"https://www.anthropic.com/index/claude-2, https://www-files.anthropic.com/production/images/Model-Card-Claude-2.pdf",,,2023-07-11,Anthropic,,,3.866e+24,https://colab.research.google.com/drive/1MdPuhS4Emaf23VXYZ-ooExDW-5GXZkw0#scrollTo=Ds0Q5X8aMnOY,Unspecified unreleased,"From model card: ""Claude models are trained on a proprietary mix of publicly available information from the Internet, datasets
that we license from third party businesses, and data that our users affirmatively share or that crowd workers provide. Some of the human feedback data used to finetune Claude was made public [12] alongside our RLHF [2] and red-teaming [4] research.
Claude 2’s training data cuts off in early 2023, and roughly 10 percent of the data included was non-English.""
This has a knowledge cutoff date of January 2024 - which I assume means January 1, 2024 - https://www.youreverydayai.com/knowledge-cutoff-what-it-is-and-why-it-matters-for-large-language-models/.",,,,,,,,,,,Speculative,,2025-11-28 11:53:44+00:00,Anonymous,,0,United States of America,,,,,,,,,,,Industry,True,,1.22e+26,,Unreleased,,True,4902644.123383027,Claude 2,,,,,,,"$8 / 1M input tokens,$24 / 1M output tokens",2023-07-11T16:26:14.000Z,,,True,,788554074639278700,,NaN,,,0,,,FP32,,"Benchmarks,Hardware",,,,,claude-2.0,True,True,,,,,,,,,,
Falcon-180B,Language,"Language modeling,Language modeling/generation,Question answering","Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Mérouane Debbah, Étienne Goffinet, Daniel Hesslow, Julien Launay, Quentin Malartic, Daniele Mazzotta, Badreddine Noune, Baptiste Pannier, Guilherme Penedo",Training cost,"""It's currently at the top of the Hugging Face Leaderboard for pre-trained Open Large Language Models and is available for both research and commercial use.""

""This model performs exceptionally well in various tasks like reasoning, coding, proficiency, and knowledge tests, even beating competitors like Meta's LLaMA 2.""",Open weights (restricted use),https://falconllm.tii.ae/falcon-180b.html; https://arxiv.org/abs/2311.16867,598.0,The Falcon Series of Open Language Models,2023-09-06,Technology Innovation Institute,180000000000.0,"""Falcon 180B is a super-powerful language model with 180 billion parameters""",3.76e+24,"43,500 petaflop-days per Table 1 of the paper

43500 * 1e15 * 24 * 3600 = 3.76e24


C = 6ND = 6 FLOP/token/parameter * 3.5 trillion tokens * 180 billion parameters = 3.78*10^24 FLOP",RefinedWeb,"""The Falcon series is made of three causal decoder-only models trained on up to 4,096 A100. We assembled a pretraining dataset of 3,500 billion tokens, predominantly sourced from our work on RefinedWeb (Penedo et al., 2023)–a massive filtered and deduplicated web dataset""

Training dataset composition is described in Table 3. Falcon was trained for 1 epoch.",3.5 trillion tokens * (~3 words per 4 tokens) ~= 2.625 trillion words,1.0,360000000000.0,C_inference = 2 FLOP / token / param * N => 360B FLOP per token,4320.0,"Stanford CRFM foundation model ecosystem graph data page https://crfm.stanford.edu/ecosystem-graphs/index.html?asset=Falcon-180B says 9 months, which is the maximum possible amount of time: training began sometime in 2023, and it was released in September. 

However, 6 months is more realistic. That is the length of the gap between Falcon 40B and Falcon 180B. Additionally, the amount of compute is specified in the paper, so there is only one degree of freedom in the uncertain values of training duration and hardware utilization rate. At six months, the utilization is unusually low, so the training was probably not longer than that.",NVIDIA A100 SXM4 40 GB,,"From Hugging Face:
""Falcon-180B was trained on up to 4,096 A100 40GB GPUs, using a 3D parallelism strategy (TP=8, PP=8, DP=64) combined with ZeRO.""
""Falcon-180B was trained on AWS SageMaker, on up to 4,096 A100 40GB GPUs in P4d instances.""
https://huggingface.co/tiiuae/falcon-180B

Utilization must have been at least 12.5%, and they probably did not use the whole 4096 GPU cluster for 9 months, so it was probably higher. Lower bound estimate:
https://www.wolframalpha.com/input?i=%286+FLOP+*+3.5+trillion+*+180+billion%29+%2F+%284096*312+teraFLOPS+*+9+months%29",,Confident,"Falcon 180B is a super-powerful language model with 180 billion parameters, trained on 3.5 trillion tokens. It's currently at the top of the Hugging Face Leaderboard for pre-trained Open Large Language Models and is available for both research and commercial use.

This model performs exceptionally well in various tasks like reasoning, coding, proficiency, and knowledge tests, even beating competitors like Meta's LLaMA 2.

Among closed source models, it ranks just behind OpenAI's GPT 4, and performs on par with Google's PaLM 2 Large, which powers Bard, despite being half the size of the model.",2026-01-01 14:02:53+00:00,Anonymous,,0,United Arab Emirates,,,,4096.0,0.1892,Amazon Web Services,,,4194304.0,"from paper (https://arxiv.org/pdf/2311.16867.pdf):

Batch size 2048 (presumably sequences) per Table 16. Warmed up using smaller batches for first 100B tokens.

""All Falcon models are pretrained with a 2,048 sequence length""

2048*2048 = 4194304",Government,,3.76e+24,3.76e+24,17694720.0,Unreleased,"""Falcon 180b can be commercially used but under very restrictive conditions, excluding any ""hosting use""."" https://huggingface.co/blog/falcon-180b",True,10743500.868805483,Falcon-180B,,"Estimated training compute: 3.76e24
FLOPs at 100% utilization, based on GPU-hours: 4320 * 4096 * 3600 * 3.12e14 = 1.987e25

Therefore MFU is:
3.76e24 / 1.987e25 = 0.1892",,400,,,,2023-09-06T17:06:25.000Z,,BF16,True,3254975.4289709153,349979028802187460,2020-05-14,3.3127994524298425,19490000000000,156000000000000,1,312000000000000,77970000000000,BF16,,"Reported,Operation counting",,,,NVIDIA,falcon-180B,,True,,,,,,,,26751806.453032624,131493433.44544913,3500000000000
Amazon Titan,"Language,Image generation","Semantic search,Image generation,Language modeling/generation,Code generation,Chat,Text-to-image,Translation",,Training cost,,API access,https://aws.amazon.com/bedrock/titan/,,,2023-09-28,Amazon,200000000000.0,"200B dense model
https://importai.substack.com/p/import-ai-365-wmd-benchmark-amazon",4.8e+24,"trained using NVIDIA NeMo: https://blogs.nvidia.com/blog/nemo-amazon-titan/

13,760 NVIDIA A100 chips (using 1,720 P4d nodes). It took 48 days to train.
from https://importai.substack.com/p/import-ai-365-wmd-benchmark-amazon

counting operations: 6*200000000000*4000000000000=4.8e+24

gpu usage: 312000000000000(FLOP/s)*0.3*13760*1152*3600=5.3413281792e+24",,,"4T tokens of data, based on comments from amazon engineer James Hamilton at a 2024 talk: https://perspectives.mvdirona.com/2024/01/cidr-2024/
Also cited here:
https://lifearchitect.ai/titan/",,,,1152.0,,NVIDIA A100,,,Industry,Likely,,2025-11-28 11:53:44+00:00,Anonymous,,0,United States of America,,,,13760.0,0.2696,,,,,,Industry,,,9.9e+24,,Unreleased,,True,7933464.673729055,Amazon Titan,,"6ND gives 4.8e24
Training took 48 days on 13,760 NVIDIA A100 chips –> 3.12e14 * 13760 * 48 * 24 * 3600 = 1.78e25 FLOPs at full utilization
Implies 0.2696 MFU.",,400,,,,2023-12-08T21:26:38.000Z,,,True,10929327.206416877,605032000192143400,2020-03-01,3.575633127994524,19500000000000,156000000000000,1,312000000000000,77970000000000,TF16,312000000000000,"Hardware,Operation counting",,,,NVIDIA,,,True,,,,,,,,23965159.94750839,441735752.9808057,4000000000000
GPT-4 Turbo (Nov 2023),"Multimodal,Vision,Language,Image generation","Chat,Language modeling/generation,Image generation,Speech synthesis,Table tasks,Visual question answering,Image captioning",,SOTA improvement,"""More capable"" than GPT-4 according to OpenAI, with larger context window",API access,https://openai.com/blog/new-models-and-developer-products-announced-at-devday,,New models and developer products announced at DevDay,2023-11-06,OpenAI,,Not known. Maybe smaller/sparser than GPT-4.,,Training compute estimated to be 2.2e25 FLOP using benchmark imputation. https://colab.research.google.com/drive/1r3pUMhB7Kh0Gls9eG-v_XefWrye9fVQR?usp=sharing,Unspecified unreleased,"This model was announced in November 2023, so I assume that this is the preview model, which has a knowledge cutoff date of April 2023 - which I assume means April 1, 2023 - according to https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models?tabs=global-standard%2Cstandard-chat-completions#gpt-35. ",,,,"Possibly ~1/2 GPT-4, since OpenAI charges 1/2 as much for GPT-4 Turbo as for GPT-4

https://openai.com/pricing",,,,,,,Unknown,"Today, we shared dozens of new additions and improvements, and reduced pricing across many parts of our platform. These include:

New GPT-4 Turbo model that is more capable, cheaper and supports a 128K context window",2025-12-04 21:43:30+00:00,Anonymous,,0,United States of America,,,,,,,,,,,Industry,True,,,,Unreleased,,True,,GPT-4 Turbo,,,True,,,,"$10 /1M input tokens,$30 /1M output tokens",2024-01-16T19:38:22.000Z,,,True,,NaN,,NaN,,,0,,,FP32,,Benchmarks,,,,,"gpt-4-turbo,gpt-4-1106-preview,gpt-4-0125-preview",,True,,,,2023-04-01,,,,,,
Inflection-2,Language,"Language modeling,Language modeling/generation,Chat,Question answering",,"Significant use,Training cost","Inflection-2 either already powers Pi or soon will: https://inflection.ai/inflection-2

Inflection has claimed that Pi has >1m users: https://x.com/inflectionAI/status/1699100179390210091?s=20",Hosted access (no API),https://inflection.ai/inflection-2,,Inflection-2: The Next Step Up,2023-11-22,Inflection AI,,,1.001e+25,"""Inflection-2 was trained on 5,000 NVIDIA H100 GPUs in fp8 mixed precision for ~10²⁵ FLOPs""

(the second 1 is there because of airtable being wonky, it's not a real sig fig)",Unspecified unreleased,,,,,,,,NVIDIA H100 SXM5 80GB,,,Industry,Confident,"Today we are proud to announce that we have completed training of Inflection-2, the best model in the world for its compute class and the second most capable LLM in the world today. Our mission at Inflection is to create a personal AI for everyone. Just a few months ago, we announced Inflection-1 — a best-in-class language model that currently powers Pi. Our new model, Inflection-2, is substantially more capable than Inflection-1, demonstrating much improved factual knowledge, better stylistic control, and dramatically improved reasoning.",2025-11-28 11:53:44+00:00,Anonymous,,0,United States of America,,,,5000.0,,,,,,,Industry,True,,,,Unreleased,"via Pi, no API",True,13461144.182562498,Inflection-2,,,True,700,,,,2023-11-30T15:26:00.000Z,,FP8,True,6941464.657305156,743621780157953200,2022-09-20,1.1718001368925393,66910000000000,494700000000000,1,989400000000000,133800000000000,FP8,,"Hardware,Benchmarks",,,,NVIDIA,,,True,,,,,,,,,285986159.97968376,
Gemini 1.0 Ultra,"Multimodal,Language,Vision","Language modeling,Visual question answering,Chat,Translation",Gemini Team,"SOTA improvement,Training cost","""Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined.""

Table 2",API access,https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,633.0,Gemini: A Family of Highly Capable Multimodal Models,2023-12-06,Google DeepMind,,,5.0000000001e+25,"This number is an estimate based on limited evidence. In particular, we combine information about the performance of Gemini Ultra on various benchmarks compared to other models, and guesstimates about the hardware setup used for training to arrive at our estimate. Our reasoning and calculations are detailed in this Colab notebook.
https://colab.research.google.com/drive/1sfG91UfiYpEYnj_xB5YRy07T5dv-9O_c",Unspecified unreleased,"""Gemini models are trained on a dataset that is both multimodal and multilingual. Our pretraining dataset uses data from web documents, books, and code, and includes image, audio, and video data... We find that data quality is critical to a highlyperforming model, and believe that many interesting questions remain around finding the optimal dataset distribution for pretraining.""",,,,,2400.0,"Dylan Patel, author of SemiAnalysis, speculates that the training duration of Gemini may have been 100 days.",Google TPU v4,,,Industry,Speculative,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
",2025-12-16 18:01:50+00:00,Anonymous,,0,"United States of America,United Kingdom of Great Britain and Northern Ireland",,,,57000.0,,,,,,,Industry,,,,132000000.0,Unreleased,API access: https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models,True,30719419.93534952,Gemini 1.0 Ultra,,,True,340,,,,2023-12-12T21:42:02.000Z,,,True,38423900.11232233,1627634900210595600,2021-05-20,2.546201232032854,,,1,275000000000000,,TF16,275000000000000,"Benchmarks,Hardware",,,,Google,,,True,,,,,,,,206082489.8134068,619525432.37549,
GLM-4 (0116),Language,"Language modeling/generation,Question answering,Code generation,Quantitative reasoning,Translation","Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Diego Rojas, Guanyu Feng, Hanlin Zhao, Hanyu Lai, Hao Yu, Hongning Wang, Jiadai Sun, Jiajie Zhang, Jiale Cheng, Jiayi Gui, Jie Tang, Jing Zhang, Juanzi Li, Lei Zhao, Lindong Wu, Lucen Zhong, Mingdao Liu, Minlie Huang, Peng Zhang, Qinkai Zheng, Rui Lu, Shuaiqi Duan, Shudan Zhang, Shulin Cao, Shuxun Yang, Weng Lam Tam, Wenyi Zhao, Xiao Liu, Xiao Xia, Xiaohan Zhang, Xiaotao Gu, Xin Lv, Xinghan Liu, Xinyi Liu, Xinyue Yang, Xixuan Song, Xunkai Zhang, Yifan An, Yifan Xu, Yilin Niu, Yuantao Yang, Yueyan Li, Yushi Bai, Yuxiao Dong, Zehan Qi, Zhaoyu Wang, Zhen Yang, Zhengxiao Du, Zhenyu Hou, Zihan Wang",Training cost,"Trained on 10T tokens with similar architecture to GPT-4, probably >$1M compute cost.",API access,"https://arxiv.org/abs/2406.12793
https://zhipuai.cn/en/devday",,ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools,2024-01-17,Z.ai (Zhipu AI),,"ChatGLM was 130B parameters, and the paper implies GLM-4 was scaled larger than previous models.",,"- 0116 has slightly worse performance than 0520
- “the GLM-4 models are pre-trained on ten trillions of tokens”
- I did not find any information about parameters or compute. 
- “GLM-4 gets close to the state-of-the-art models (GPT-4-Turbo, Gemini 1.5 Pro, and Claude 3 Opus)”  none of these models has parameters disclosed or compute estimation.

6 FLOP / token / parameter * 10000000000000 tokens * 200000000000 parameters = 1.2e+25 FLOP with “Likely” confidence (+/- 1 OOM)",,"""To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage""",,,,,,,,,,,Likely,"We introduce ChatGLM, an evolving family of large language models that we have been developing over time. This report primarily focuses on the GLM-4 language series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. They represent our most capable models that are trained with all the insights and lessons gained from the preceding three generations of ChatGLM. To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage. The high-quality alignment is achieved via a multi-stage post-training process, which involves supervised fine-tuning and learning from human feedback. Evaluations show that GLM-4 1) closely rivals or outperforms GPT-4 in terms of general metrics such as MMLU, GSM8K, MATH, BBH, GPQA, and HumanEval, 2) gets close to GPT-4-Turbo in instruction following as measured by IFEval, 3) matches GPT-4 Turbo (128K) and Claude 3 for long context tasks, and 4) outperforms GPT-4 in Chinese alignments as measured by AlignBench. The GLM-4 All Tools model is further aligned to understand user intent and autonomously decide when and which tool(s) touse -- including web browser, Python interpreter, text-to-image model, and user-defined functions -- to effectively complete complex tasks. In practical applications, it matches and even surpasses GPT-4 All Tools in tasks like accessing online information via web browsing and solving math problems using Python interpreter. Over the course, we have open-sourced a series of models, including ChatGLM-6B (three generations), GLM-4-9B (128K, 1M), GLM-4V-9B, WebGLM, and CodeGeeX, attracting over 10 million downloads on Hugging face in the year 2023 alone. The open models can be accessed through this https URL and this https URL.",2025-12-18 04:30:29+00:00,Natalia Martemianova,,1,China,,,,,,,"The paper does not mention any hardware, GPUs or any information regarding the hardware used.",,,Not listed,Industry,,,,,Unreleased,"GLM-4 (0116) has been made available through the GLM-4 API at
https://bigmodel.cn",True,,GLM-4 (0116),,,True,,,,,2024-05-24T16:15:12.000Z,,,True,,NaN,,NaN,,,0,,,FP32,,Operation counting,,,,,,,True,,,,,,,,,,10000000000000
Gemini 1.5 Pro,"Language,Multimodal","Language modeling,Visual question answering",Gemini Team,Significant use,"Google DeepMind's current best public model, being used for their products.",API access,https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf,,Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context,2024-02-15,Google DeepMind,,MoE architecture,,Training compute imputed to be 1.58e25 FLOP from benchmark scores. https://colab.research.google.com/drive/1r3pUMhB7Kh0Gls9eG-v_XefWrye9fVQR?usp=sharing,Unspecified unreleased,,,,,,,,Google TPU v4,,,,Speculative,,2025-11-28 11:53:44+00:00,Epoch AI,,0,"United States of America,United Kingdom of Great Britain and Northern Ireland",,,,,,,,,,,Industry,,,,,Unreleased,API access: https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models,True,,Gemini 1.5 Pro,,,True,340,,,"$3.5 / 1M input tokens (for <= 128k tokens),$7 / 1M input tokens (for > 128k tokens),$10.5 / 1M output tokens (for <= 128k tokens),$21 / 1M output tokens (for > 128k tokens),$0.875 / 1M cached tokens (for <= 128k tokens),$1.75 / 1M cached tokens (for > 128k tokens),$4.5 / 1M tokens per hour (cached storage),$1.25 / 1M input tokens (for <= 128k tokens),$5 / 1M output tokens (for <= 128k tokens),$0.3125 / 1M cached tokens (for <= 128k tokens),$2.5 / 1M input tokens (for > 128k tokens),$10 / 1M output tokens (for > 128k tokens),$0.625 / 1M cached tokens (for > 128k tokens)",2024-04-03T17:52:18.000Z,,,True,,NaN,2021-05-20,2.7405886379192332,,,1,275000000000000,,TF16,275000000000000,Benchmarks,,,,Google,"gemini-1.5-pro-001,gemini-1.5-pro-002",,True,,,,,,,,,,
Claude 3 Opus,"Multimodal,Language,Vision","Chat,Image captioning,Code generation,Language modeling/generation",,"SOTA improvement,Training cost","Based on leaks, Claude 3 Opus and Sonnet probably cost >$1M to train.

""Claude 3 Opus achieves state-of-the-art results on evaluations like GPQA [1], MMLU [2], MMMU [3] and many more. ""

Table 1, Table 3",API access,https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf,,"The Claude 3 Model Family: Opus, Sonnet, Haiku",2024-03-04,Anthropic,,,,Training compute estimated to be 1.64e25 FLOP from benchmark scores. https://colab.research.google.com/drive/1r3pUMhB7Kh0Gls9eG-v_XefWrye9fVQR?usp=sharing,Unspecified unreleased,"Claude 3 models are trained on a proprietary mix of publicly available information on the Internet as of August 2023, as well as non-public data from third parties, data provided by data labeling services and paid contractors, and data we generate internally. We employ several data cleaning and filtering methods, including deduplication and classification. The Claude 3 suite of models have not been trained on any user prompt or output data submitted to us by users or customers, including free users, Claude Pro users, and API customers.
This has a knowledge cutoff date of August 2023, according to https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf, which I assume means August 1, 2023.",,,,,,"Like its predecessors, Claude 3 models employ various training methods, such as unsupervised learning and Constitutional AI [6]. These models were trained using hardware from Amazon Web Services (AWS) and Google Cloud Platform (GCP)",,,"Per https://time.com/6980000/anthropic/
""Claude 3 cost somewhere between $30 million and $300 million to train""
This would seem to include all three versions.

Ballpark estimate, based on relative API costs:
sqrt($30M * $300M) * (15 / (0.25 + 3 + 15)) = $78.0M
(cost) * (Sonnet share of API cost)

Convert to 2020 dollars: $64.7M",,Speculative,"We introduce Claude 3, a new family of large multimodal models – Claude 3 Opus, our most capable offering, Claude 3 Sonnet, which provides a combination of skills and speed, and Claude 3 Haiku, our fastest and least expensive model. All new models have vision capabilities that enable them to process and analyze image data. The Claude 3 family demonstrates strong performance across benchmark evaluations and sets a new standard on
measures of reasoning, math, and coding. Claude 3 Opus achieves state-of-the-art results on evaluations like GPQA [1], MMLU [2], MMMU [3] and many more. Claude 3 Haiku performs as well or better than Claude 2 [4] on most pure-text tasks, while Sonnet and Opus significantly outperform it. Additionally, these models exhibit improved fluency in non-English languages, making them more versatile for a global audience. In this report, we provide an in-depth analysis of our evaluations, focusing on core capabilities, safety, societal impacts, and the catastrophic risk assessments we committed to in our Responsible Scaling Policy [5].
",2025-11-28 11:53:44+00:00,Robi Rahman,,0,United States of America,,,,,,,,,,,Industry,,,,,Unreleased,,True,,Claude 3 Opus,,,True,,,,"$15 / 1M input tokens,$75 / 1M output tokens",2024-03-06T18:37:28.000Z,,,True,,NaN,,NaN,,,0,,,FP32,,Benchmarks,,,,,claude-3-opus-20240229,,True,,,,2023-08-01,,,,,,
GPT-4 Turbo (Apr 2024),"Multimodal,Vision,Language,Image generation","Chat,Language modeling/generation,Image generation,Speech synthesis,Table tasks,Visual question answering,Image captioning",,SOTA improvement,Improves upon GPT-4 Turbo from Nov 2023,API access,https://x.com/OpenAI/status/1778574613813006610,,"Our new GPT-4 Turbo is now available to paid ChatGPT users. We’ve improved capabilities in writing, math, logical reasoning, and coding",2024-04-09,OpenAI,,Not known. Maybe smaller/sparser than GPT-4.,,Training compute estimated to be 2.2e25 FLOP using benchmark imputation. https://colab.research.google.com/drive/1r3pUMhB7Kh0Gls9eG-v_XefWrye9fVQR?usp=sharing,Unspecified unreleased,"This model was announced in November 2023, so I assume that this is the preview model, which has a knowledge cutoff date of April 2023 - which I assume means April 1, 2023 - according to https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models?tabs=global-standard%2Cstandard-chat-completions#gpt-35. ",,,,"Possibly ~1/2 GPT-4, since OpenAI charges 1/2 as much for GPT-4 Turbo as for GPT-4

https://openai.com/pricing",,,,,,,Unknown,"Today, we shared dozens of new additions and improvements, and reduced pricing across many parts of our platform. These include:

New GPT-4 Turbo model that is more capable, cheaper and supports a 128K context window",2025-12-04 21:45:16+00:00,Luke Frymire,,0,United States of America,GPT-4 Turbo (Nov 2023),,,,,,,,,,Industry,True,,,,Unreleased,,True,,GPT-4 Turbo,,,True,,,0.0,"$10 /1M input tokens,$30 /1M output tokens",2025-12-04T21:23:48.000Z,,,True,,NaN,,NaN,,,0,,,FP32,,Benchmarks,,,,,gpt-4-turbo-2024-04-09,,True,,,,2023-04-01,,,,,,
GPT-4o (May 2024),"Multimodal,Language,Audio,Speech,Vision","Chat,Image generation,Audio generation,Vision-language generation,Table tasks,Language modeling/generation,Question answering,Speech recognition (ASR),Speech-to-text","Aidan Clark, Alex Paino, Jacob Menick, Liam Fedus, Luke Metz, Clemens Winter, Lia Guy, Sam Schoenholz, Daniel Levy, Nitish Keskar, Alex Carney, Alex Paino, Ian Sohl, Qiming Yuan, Reimar Leike, Arka Dhar, Brydon Eastman, Mia Glaese, Ben Sokolowsky, Andrew Kondrich, Felipe Petroski Such, Henrique Ponde de Oliveira Pinto, Jiayi Weng, Randall Lin, Youlong Cheng, Nick Ryder, Lauren Itow, Barret Zoph, John Schulman, Mianna Chen, Adam Lerer, Adam P. Goucher, Adam Perelman, Akila Welihinda, Alec Radford, Alex Borzunov, Alex Carney, Alex Chow, Alex Paino, Alex Renzin, Alex Tachard Passos, Alexi Christakis, Ali Kamali, Allison Moyer, Allison Tam, Amin Tootoonchian, Ananya Kumar, Andrej Karpathy, Andrey Mishchenko, Andrew Cann, Andrew Kondrich, Andrew Tulloch, Angela Jiang, Antoine Pelisse, Anuj Gosalia, Avi Nayak, Avital Oliver, Behrooz Ghorbani, Ben Leimberger, Ben Wang, Blake Samic, Brian Guarraci, Brydon Eastman, Camillo Lugaresi, Chak Li, Charlotte Barette, Chelsea Voss, Chong Zhang, Chris Beaumont, Chris Hallacy, Chris Koch, Christian Gibson, Christopher Hesse, Colin Wei, Daniel Kappler, Daniel Levin, Daniel Levy, David Farhi, David Mely, David Sasaki, Dimitris Tsipras, Doug Li, Duc Phong Nguyen, Duncan Findlay, Edmund Wong, Ehsan Asdar, Elizabeth Proehl, Elizabeth Yang, Eric Peterson, Eric Sigler, Eugene Brevdo, Farzad Khorasani, Francis Zhang, Gene Oden, Geoff Salmon, Hadi Salman, Haiming Bao, Heather Schmidt, Hongyu Ren, Hyung Won Chung, Ian Kivlichan, Ian O'Connell, Ian Osband, Ilya Kostrikov, Ingmar Kanitscheider, Jacob Coxon, James Crooks, James Lennon, Jason Teplitz, Jason Wei, Jason Wolfe, Jay Chen, Jeff Harris, Jiayi Weng, Jie Tang, Joanne Jang, Jonathan Ward, Jonathan McKay, Jong Wook Kim, Josh Gross, Josh Kaplan, Joy Jiao, Joyce Lee, Juntang Zhuang, Kai Fricke, Kavin Karthik, Kenny Hsu, Kiel Howe, Kyle Luther, Larry Kai, Lauren Itow, Leo Chen, Lia Guy, Lien Mamitsuka, Lilian Weng, Long Ouyang, Louis Feuvrier, Lukas Kondraciuk, Lukasz Kaiser, Lyric Doshi, Mada Aflak, Maddie Simens, Madeleine Thompson, Marat Dukhan, Marvin Zhang, Mateusz Litwin, Max Johnson, Mayank Gupta, Mia Glaese, Michael Janner, Michael Petrov, Michael Wu, Michelle Fradin, Michelle Pokrass, Miguel Oom Temudo de Castro, Mikhail Pavlov, Minal Khan, Mo Bavarian, Natalia Gimelshein, Natalie Staudacher, Nick Stathas, Nik Tezak, Nithanth Kudige, Noel Bundick, Ofir Nachum, Oleg Boiko, Oleg Murk, Olivier Godement, Owen Campbell-Moore, Philip Pronin, Philippe Tillet, Rachel Lim, Rajan Troll, Randall Lin, Rapha gontijo lopes, Raul Puri, Reah Miyara, Reimar Leike, Renaud Gaubert, Reza Zamani, Rob Honsby, Rohit Ramchandani, Rory Carmichael, Ruslan Nigmatullin, Ryan Cheu, Scott Gray, Sean Grove, Sean Metzger, Shantanu Jain, Shengjia Zhao, Sherwin Wu, Shuaiqi (Tony) Xia, Sonia Phene, Spencer Papay, Steve Coffey, Steve Lee, Steve Lee, Stewart Hall, Suchir Balaji, Tal Broda, Tal Stramer, Tarun Gogineni, Ted Sanders, Thomas Cunninghman, Thomas Dimson, Thomas Raoux, Tianhao Zheng, Tina Kim, Todd Underwood, Tristan Heywood, Valerie Qi, Vinnie Monaco, Vlad Fomenko, Weiyi Zheng, Wenda Zhou, Wojciech Zaremba, Yash Patil, Yilei, Qian, Yongjik Kim, Youlong Cheng, Yuchen He, Yuchen Zhang, Yujia Jin, Yunxing Dai, Yury Malkov","SOTA improvement,Significant use","Outperforms GPT-4 Turbo and other models on text and especially on multimodal benchmarks, such as MMLU, GPQA, HumanEval, MMMU, etc See Model Evaluations: https://openai.com/index/hello-gpt-4o/ 

GPT-4o is now the default model in ChatGPT, so it's one of the most widely used models.",API access,"https://openai.com/index/hello-gpt-4o/ 
https://openai.com/index/gpt-4o-system-card/",,Hello GPT-4o,2024-05-13,OpenAI,,"Not known.

Inference costs in the API are 2x cheaper than GPT-4 Turbo",,Training compute estimated to be 3.8e25 FLOP from benchmark scores. https://colab.research.google.com/drive/1r3pUMhB7Kh0Gls9eG-v_XefWrye9fVQR?usp=sharing,Unspecified unreleased,"""With GPT-4o, we trained a single new model end-to-end across text, vision, and audio.""
This has a knowledge cutoff date of October 2023 - which I assume means October 1, 2023 - according to https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models?tabs=global-standard%2Cstandard-chat-completions#gpt-4o-and-gpt-4-turbo.",,,,,,,,,,,Speculative,"We’re announcing GPT-4o, our new flagship model that can reason across audio, vision, and text in real time.

GPT-4o (“o” for “omni”) is a step towards much more natural human-computer interaction—it accepts as input any combination of text, audio, image, and video and generates any combination of text, audio, and image outputs. It can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time(opens in a new window) in a conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models.",2025-12-08 15:46:28+00:00,Anonymous,,0,United States of America,,,"Definitely a new model, not a GPT-4 finetune",,,,,,,,Industry,,,,,Unreleased,,True,,GPT-4o,,,True,,,,"$2.50 / 1M input tokens,$10.00 / 1M output tokens,$0.638 / 1k 512^2 px input images,$0.213 / 1k low resolution input images,$5.00 / 1M input tokens ,$15.00 / 1M output tokens,$1.913 / 1k 1024^2 px input images,$3.825 / 1k 1024^2 px input images,$0.425 / 1k low resolution input images,$1.275 / 1k 512^2 px input images,$25 / 1M training tokens",2024-05-29T19:27:03.000Z,,,True,,NaN,,NaN,,,0,,,FP32,,Benchmarks,,,,,gpt-4o-2024-05-13,,True,,,,2023-10-01,,,,,,
Nemotron-4 340B,Language,"Language modeling/generation,Chat,Question answering","Bo Adler, Niket Agarwal, Ashwath Aithal, Dong H. Anh, Pallab Bhattacharya, Annika Brundyn, Jared Casper, Bryan Catanzaro, Sharon Clay, Jonathan Cohen, Sirshak Das, Ayush Dattagupta, Olivier Delalleau, Leon Derczynski, Yi Dong, Daniel Egert, Ellie Evans, Aleksander Ficek, Denys Fridman, Shaona Ghosh, Boris Ginsburg, Igor Gitman, Tomasz Grzegorzek, Robert Hero, Jining Huang, Vibhu Jawa, Joseph Jennings, Aastha Jhunjhunwala, John Kamalu, Sadaf Khan, Oleksii Kuchaiev, Patrick LeGresley, Hui Li, Jiwei Liu, Zihan Liu, Eileen Long, Ameya Sunil Mahabaleshwarkar, Somshubra Majumdar, James Maki, Miguel Martinez, Maer Rodrigues de Melo, Ivan Moshkov, Deepak Narayanan, Sean Narenthiran, Jesus Navarro, Phong Nguyen, Osvald Nitski, Vahid Noroozi, Guruprasad Nutheti, Christopher Parisien, Jupinder Parmar, Mostofa Patwary, Krzysztof Pawelec, Wei Ping, Shrimai Prabhumoye, Rajarshi Roy, Trisha Saar, Vasanth Rao Naik Sabavat, Sanjeev Satheesh, Jane Polak Scowcroft, Jason Sewall, Pavel Shamis, Gerald Shen, Mohammad Shoeybi, Dave Sizer, Misha Smelyanskiy, Felipe Soares, Makesh Narsimhan Sreedhar, Dan Su, Sandeep Subramanian, Shengyang Sun, Shubham Toshniwal, Hao Wang, Zhilin Wang, Jiaxuan You, Jiaqi Zeng, Jimmy Zhang, Jing Zhang, Vivienne Zhang, Yian Zhang, Chen Zhu",Training cost,"~2e25 FLOP, so high training cost, likely >5M",Open weights (unrestricted),https://blogs.nvidia.com/blog/nemotron-4-synthetic-data-generation-llm-training/ ,,NVIDIA Releases Open Synthetic Data Generation Pipeline for Training Large Language Models,2024-06-14,NVIDIA,340000000000.0,340B,1.8e+25,"9 trillion tokens for training
6 * 340B * 9T = 1.8E25

alternatively, can do a hardware estimate with a few extra steps:

According to the technical report, Nemotron-4 340B was trained using up to 6144 H100 GPUs. Helpfully, they also report the model FLOP utilization (MFU), which was 41-42% (Table 2). This is the ratio of the actual output of their GPUs, in FLOP used for training, relative to their theoretical max of 989 teraFLOP/s per GPU. 
Unfortunately, the report omits the last ingredient, which is the duration of the training run. However, in Table 2 they report some relevant data that we can use to infer the training time. 
Nemotron-4 was trained in several stages, but the largest stage used all 6144 GPUs with a batch size of 2304 and an iteration time (time per batch) of 8.0 seconds. This stage involved 7.6T tokens, so it makes up the majority of training. 
A batch size of 2304 means that each batch consists of 2304 sequences, and they report that the sequence length used for training was 4096 tokens. This means that each batch contained 4096 * 2304 = 9,437,184 tokens. 
So, during this stage, it took 8 seconds to train the model on 9.4m tokens. Extrapolating to the entire 9T token dataset, this implies the training run would have taken 7,659,574 seconds, or 89 days. (it actually took longer because they didn't use all their GPUs for the whole run) 
Multiplying 7,659,574 seconds by 41% MFU, 989 peak teraFLOP/s for each H100, and 6144 H100s, we get ~1.9e25 FLOP. This is very close to our first estimate. 
",Unspecified unreleased,"The technical report for the 340B model cites the report for the 15B version (https://arxiv.org/pdf/2402.16819 )

from that paper:

""We train Nemotron-4 15B on a pre-training dataset consisting of 8 trillion tokens. At a high-level,
the data blend is split into three different types of data: English natural language data (70%), multilingual
natural language data (15%), and source-code data (15%).
The English corpus consists of curated documents from a variety of sources and domains including web
documents, news articles, scientific papers, books, etc and the distribution used in our pre-training set is
highlighted in Figure 2. The code and multilingual data consists of a diverse set of natural and programming
languages. We find that appropriately sampling tokens from these languages is key to strong accuracies in
these domains. We share the distributions used for both code and multilingual tokens in our pre-training
dataset in Figure 3 and Figure 4 respectively.
In constructing the pre-training corpus, we remove any possible duplicates via document-level exact and
near-deduplication (Jennings et al., 2023). We additionally applied document-level quality filtering across
our corpus using a language-model based filtering approach similar to (Wenzek et al., 2019) in addition to a
series of heuristic filters as described in (Rae et al., 2022) and (Raffel et al., 2020).""","9T training tokens.

They first train on an 8T token dataset and then an additional 1T tokens, it's slightly unclear if that's more data or a partial second epoch

6.75T words using 1 token = 0.75 words",,,,2200.0,"see training compute notes, this is an inferred estimate",NVIDIA H100 SXM5 80GB,,,,Confident,"We release the Nemotron-4 340B model family, including Nemotron-4-340B-Base, Nemotron-4-
340B-Instruct, and Nemotron-4-340B-Reward. Our models are open access under the NVIDIA Open
Model License Agreement, a permissive model license that allows distribution, modification, and use of
the models and its outputs. These models perform competitively to open access models on a wide range
of evaluation benchmarks, and were sized to fit on a single DGX H100 with 8 GPUs when deployed in
FP8 precision. We believe that the community can benefit from these models in various research studies
and commercial applications, especially for generating synthetic data to train smaller language models.
Notably, over 98% of data used in our model alignment process is synthetically generated, showcasing
the effectiveness of these models in generating synthetic data. To further support open research and
facilitate model development, we are also open-sourcing the synthetic data generation pipeline used in
our model alignment process.

(from technical report: https://d1qx31qr3h6wln.cloudfront.net/publications/Nemotron_4_340B_8T_0.pdf )",2025-12-18 05:28:44+00:00,Anonymous,,0,United States of America,,,,6144.0,0.410675,,,,9437184.0,2304 * 4096,Industry,,,,,Unreleased,Permissive commercial license: https://developer.download.nvidia.com/licenses/nvidia-open-model-license-agreement-june-2024.pdf ,True,21271017.96222655,Nemotron-4 340B,,"Table 2 indicates MFU at different stages of training.

((42.4% * 200B) + (42.3% * 200B) + (41.0% * 7600B)) / (200B + 200B + 7600B) = 0.410675, averaged over training.",True,700,,,,2024-06-17T20:11:41.000Z,,BF16,True,8490820.682633882,846221841943094500,2022-09-20,1.73305954825462,66910000000000,494700000000000,1,989400000000000,133800000000000,BF16,,"Operation counting,Hardware",nvidia,,,NVIDIA,nemotron-4-340b-Instruct,,True,,,,,,,,67898317.22964796,348799800.27266073,9000000000000
Claude 3.5 Sonnet,"Multimodal,Language,Vision","Chat,Image captioning,Code generation,Language modeling/generation",,"Significant use,SOTA improvement","""It also sets new performance standards in evaluations of graduate level science knowledge (GPQA) [1], general reasoning (MMLU) [2], and coding proficiency (HumanEval) [3].""",API access,https://www-cdn.anthropic.com/fed9cc193a14b84131812372d8d5857f8f304c52/Model_Card_Claude_3_Addendum.pdf,,Claude 3.5 Sonnet,2024-06-20,Anthropic,,,2.700000000000001e+25,"Blog post by Dario Amodei includes some info on 3.5 Sonnet compute: https://darioamodei.com/on-deepseek-and-export-controls
""Claude 3.5 Sonnet is a mid-sized model that cost a few $10M's to train (I won't give an exact number). Also, 3.5 Sonnet was not trained in any way that involved a larger or more expensive model (contrary to some rumors).""

Using assumptions about GPU pricing, this lets us estimate compute. https://docs.google.com/spreadsheets/d/1-p-ab6t6dkUM6T7GwnFp85ePTMpZMW7LFY7fW2t8POs/",Unspecified unreleased,Training data cutoff Apr 2024,,,,,,,,,,,Speculative,"This addendum to our Claude 3 Model Card describes Claude 3.5 Sonnet, a new model which outperforms
our previous most capable model, Claude 3 Opus, while operating faster and at a lower cost. Claude 3.5
Sonnet offers improved capabilities, including better coding and visual processing. Since it is an evolution of
the Claude 3 model family, we are providing an addendum rather than a new model card. We provide updated
key evaluations and results from our safety testing",2025-11-28 11:53:44+00:00,James Sanders,,0,United States of America,,,,,,,,,,,Industry,,,,,Unreleased,,True,25870993.12024943,Claude 3.5 Sonnet,,,True,,,,"$3 / 1M input tokens,$15 / 1M output tokens,$3 / 1M input tokens",2024-08-12T20:55:08.000Z,,,True,,1043639873989487200,,NaN,,,0,,,FP32,,Cost,,,,,claude-3-5-sonnet-20240620,True,True,,,,2024-04-01,,,,,,
Llama 3.1-405B,Language,"Language modeling/generation,Question answering,Code generation,Mathematical reasoning","Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,
Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie
Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen
Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux,
Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang
Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle
Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino,
Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip
Radenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, Gregoire
Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov,
Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert,
Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong,
Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe
Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala,
Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer,
Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence
Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat,
Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas,
Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar
Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji,
Olivier Duchenne, Onur Çelebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng,
Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong,
Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta
Raileanu, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor,
Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun
Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan,
Shruti Bhosale, Shun Zhang, Simon Vandenhende, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin
Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas
Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta,
Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vladan Petrovic, Weiwei
Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaoqing Ellen Tan,
Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen
Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, and Zoe
Papakipos.
(core contributors)","SOTA improvement,Training cost","High training compute, exceeds 4o and Claude 3.5 on some benchmarks:

https://ai.meta.com/blog/meta-llama-3-1/ ",Open weights (restricted use),https://ai.meta.com/research/publications/the-llama-3-herd-of-models/,,The Llama 3 Herd of Models,2024-07-23,Meta AI,405000000000.0,405B,3.8e+25,"Stated in paper.

Also, 6 * 405B * 15.6T training tokens = 3.8e25",Llama 3 dataset,"Knowledge cutoff date is December 2023, according to https://huggingface.co/meta-llama/Llama-3.1-405B.",15.6T tokens,1.0,,,2142.0,"Trained on 30.84M GPU hours (https://huggingface.co/blog/llama31) and used ""up to 16K H100 GPU[s]"" so training took at least
30.84M / 16k = 1927.5 hours or ~80 days. 

Section 3.3.4 gives reliability details over a 54 day period during training, for which they had ""higher than 90% effective training time""
1927.5 / 0.9 = 2142 hours

Probably, full training time is somewhat longer, since it sounds like there were periods where not all 16k H100s were running.",NVIDIA H100 SXM5 80GB,,,,Confident,"Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.",2025-11-28 11:53:44+00:00,Anonymous,,0,United States of America,,,,16384.0,0.4042,,,,16000000.0,,Industry,,,,,Open (restricted use),"Llama 3.1 model license:

https://huggingface.co/meta-llama/Meta-Llama-3.1-8B/blob/main/LICENSE 

must seek separate license if over 700m monthly users, acceptable use restrictions

training code here: https://github.com/meta-llama/llama-recipes/blob/main/src/llama_recipes/utils/train_utils.py#L70 
",True,52885433.95402246,Llama 3.1-405B,,MFU ranges between 0.38 and 0.43 depending on the specific parallelism used; I assume the geometric mean: sqrt(0.38 * 0.43) = 0.4042,True,700,,,"$2.7 / 1M input tokens,$2.7 / 1M output tokens,$3 / 1M input tokens,$3 / 1M output tokens,$5 / 1M input tokens,$3 / 1M input tokens,$2.8 / 1M input tokens,$5 / 1M input tokens,$3 / 1M input tokens",2024-07-23T20:25:34.000Z,Open access (restricted use),BF16,True,22622532.159299146,718534332781242600,2022-09-20,1.839835728952772,66910000000000,494700000000000,1,989400000000000,133800000000000,BF16,,"Reported,Operation counting",meta-llama,9.4e+22,"Section 4 gives detail about the post-training process. They do 6 rounds of post-training, using the model from the previous iteration in each successive round. In each round, they fine-tune a copy of the language model into a reward model (RM) using preference data, then use the reward model to do rejection sampling on human annotation prompts. Next they do supervised fine-tuning (SFT) on the rejection sampled data along with some synthetic data (8.5k to 9k steps per round). Next, they do Direct Preference Optimization (DPO) over the most recent generation of preference data. Finally, they do model averaging over experiments using different samples of data or hyperparameters in each of the RM, SFT, and DPO steps. 

We can get a very rough estimate of total post-training compute:
- RM section doesn't give any detail about batch size or training length. In Llama 2, they trained the RM for one epoch over their 1.7B tokens of preference data. We aren't told how many examples they have in Llama 3, though we see that examples are somewhat longer, apparently because they contain more turns on average. Assuming preference data scaled up by a similar amount to pre-training data (2T to 15T tokens from Llama 2 to 3), that suggests around 1.7B * 15T / 2T = 12.75B tokens, which corresponds to 6 * 12.75B * 405B = 3.098e22 FLOPs, if they used the 405b variant for their reward model.
- SFT uses 8.5k-9k steps in each round. Batch size is not given here, but in Llama 2 they used batches of 64 sequences of 4096 = 262,144 tokens. Assuming they scaled up SFT batches by a similar ratio as they did pretraining (4M in Llama 2 -> 16M for most of Llama 3 pretraining), Llama 3's SFT would have used around 1M tokens per batch. That suggests 6 * 9k * 1M * 405B = 2.187e22 FLOP per round of SFT. 
- DPO section gives no detail about quantity of examples or training length. Llama 2 used PPO for 200-400 iterations per model; each iteration was a batch of 512 examples, and took ~330 seconds. Our current estimate is that Llama 2-70B used 1000 A100s, which would suggest 330 * 3.12e14 * 1000 = 4.118e22 FLOP. DPO uses less compute, but presumably they've also scaled up their data here; without further info let's speculate these roughly cancel out.
So total fine-tuning compute is about 3.1e22 + 2.2e22 + 4.1e22 = 9.4E22 FLOPs",NVIDIA,"Llama-3.1-405B-Instruct,llama3.1:405b-instruct-q4_K_M,Llama-3.1-405B",,True,,,,2023-12-01,,,,175966627.18329293,928433368.1802773,15600000000000
GPT-4o (Aug 2024),"Multimodal,Language,Audio,Speech,Vision","Chat,Image generation,Audio generation,Vision-language generation,Table tasks,Language modeling/generation,Question answering,Speech recognition (ASR),Speech-to-text","Aidan Clark, Alex Paino, Jacob Menick, Liam Fedus, Luke Metz, Clemens Winter, Lia Guy, Sam Schoenholz, Daniel Levy, Nitish Keskar, Alex Carney, Alex Paino, Ian Sohl, Qiming Yuan, Reimar Leike, Arka Dhar, Brydon Eastman, Mia Glaese, Ben Sokolowsky, Andrew Kondrich, Felipe Petroski Such, Henrique Ponde de Oliveira Pinto, Jiayi Weng, Randall Lin, Youlong Cheng, Nick Ryder, Lauren Itow, Barret Zoph, John Schulman, Mianna Chen, Adam Lerer, Adam P. Goucher, Adam Perelman, Akila Welihinda, Alec Radford, Alex Borzunov, Alex Carney, Alex Chow, Alex Paino, Alex Renzin, Alex Tachard Passos, Alexi Christakis, Ali Kamali, Allison Moyer, Allison Tam, Amin Tootoonchian, Ananya Kumar, Andrej Karpathy, Andrey Mishchenko, Andrew Cann, Andrew Kondrich, Andrew Tulloch, Angela Jiang, Antoine Pelisse, Anuj Gosalia, Avi Nayak, Avital Oliver, Behrooz Ghorbani, Ben Leimberger, Ben Wang, Blake Samic, Brian Guarraci, Brydon Eastman, Camillo Lugaresi, Chak Li, Charlotte Barette, Chelsea Voss, Chong Zhang, Chris Beaumont, Chris Hallacy, Chris Koch, Christian Gibson, Christopher Hesse, Colin Wei, Daniel Kappler, Daniel Levin, Daniel Levy, David Farhi, David Mely, David Sasaki, Dimitris Tsipras, Doug Li, Duc Phong Nguyen, Duncan Findlay, Edmund Wong, Ehsan Asdar, Elizabeth Proehl, Elizabeth Yang, Eric Peterson, Eric Sigler, Eugene Brevdo, Farzad Khorasani, Francis Zhang, Gene Oden, Geoff Salmon, Hadi Salman, Haiming Bao, Heather Schmidt, Hongyu Ren, Hyung Won Chung, Ian Kivlichan, Ian O'Connell, Ian Osband, Ilya Kostrikov, Ingmar Kanitscheider, Jacob Coxon, James Crooks, James Lennon, Jason Teplitz, Jason Wei, Jason Wolfe, Jay Chen, Jeff Harris, Jiayi Weng, Jie Tang, Joanne Jang, Jonathan Ward, Jonathan McKay, Jong Wook Kim, Josh Gross, Josh Kaplan, Joy Jiao, Joyce Lee, Juntang Zhuang, Kai Fricke, Kavin Karthik, Kenny Hsu, Kiel Howe, Kyle Luther, Larry Kai, Lauren Itow, Leo Chen, Lia Guy, Lien Mamitsuka, Lilian Weng, Long Ouyang, Louis Feuvrier, Lukas Kondraciuk, Lukasz Kaiser, Lyric Doshi, Mada Aflak, Maddie Simens, Madeleine Thompson, Marat Dukhan, Marvin Zhang, Mateusz Litwin, Max Johnson, Mayank Gupta, Mia Glaese, Michael Janner, Michael Petrov, Michael Wu, Michelle Fradin, Michelle Pokrass, Miguel Oom Temudo de Castro, Mikhail Pavlov, Minal Khan, Mo Bavarian, Natalia Gimelshein, Natalie Staudacher, Nick Stathas, Nik Tezak, Nithanth Kudige, Noel Bundick, Ofir Nachum, Oleg Boiko, Oleg Murk, Olivier Godement, Owen Campbell-Moore, Philip Pronin, Philippe Tillet, Rachel Lim, Rajan Troll, Randall Lin, Rapha gontijo lopes, Raul Puri, Reah Miyara, Reimar Leike, Renaud Gaubert, Reza Zamani, Rob Honsby, Rohit Ramchandani, Rory Carmichael, Ruslan Nigmatullin, Ryan Cheu, Scott Gray, Sean Grove, Sean Metzger, Shantanu Jain, Shengjia Zhao, Sherwin Wu, Shuaiqi (Tony) Xia, Sonia Phene, Spencer Papay, Steve Coffey, Steve Lee, Steve Lee, Stewart Hall, Suchir Balaji, Tal Broda, Tal Stramer, Tarun Gogineni, Ted Sanders, Thomas Cunninghman, Thomas Dimson, Thomas Raoux, Tianhao Zheng, Tina Kim, Todd Underwood, Tristan Heywood, Valerie Qi, Vinnie Monaco, Vlad Fomenko, Weiyi Zheng, Wenda Zhou, Wojciech Zaremba, Yash Patil, Yilei, Qian, Yongjik Kim, Youlong Cheng, Yuchen He, Yuchen Zhang, Yujia Jin, Yunxing Dai, Yury Malkov","SOTA improvement,Significant use","Outperforms GPT-4 Turbo and other models on text and especially on multimodal benchmarks, such as MMLU, GPQA, HumanEval, MMMU, etc See Model Evaluations: https://openai.com/index/hello-gpt-4o/ 

GPT-4o is now the default model in ChatGPT, so it's one of the most widely used models.",API access,https://help.openai.com/en/articles/9624314-model-release-notes,,Update to GPT-4o,2024-08-06,OpenAI,,"Not known.

Inference costs in the API are 2x cheaper than GPT-4 Turbo",,Training compute estimated to be 3.8e25 FLOP from benchmark scores. https://colab.research.google.com/drive/1r3pUMhB7Kh0Gls9eG-v_XefWrye9fVQR?usp=sharing,Unspecified unreleased,"""With GPT-4o, we trained a single new model end-to-end across text, vision, and audio.""
This has a knowledge cutoff date of October 2023 - which I assume means October 1, 2023 - according to https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models?tabs=global-standard%2Cstandard-chat-completions#gpt-4o-and-gpt-4-turbo.",,,,,,,,,,,Speculative,"We’re announcing GPT-4o, our new flagship model that can reason across audio, vision, and text in real time.

GPT-4o (“o” for “omni”) is a step towards much more natural human-computer interaction—it accepts as input any combination of text, audio, image, and video and generates any combination of text, audio, and image outputs. It can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time(opens in a new window) in a conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models.",2025-12-08 15:46:29+00:00,Luke Frymire,,0,United States of America,GPT-4o (May 2024),,"Definitely a new model, not a GPT-4 finetune",,,,,,,,Industry,,,,,Unreleased,,True,,GPT-4o,,,True,,,0.0,"$2.50 / 1M input tokens,$10.00 / 1M output tokens,$0.638 / 1k 512^2 px input images,$0.213 / 1k low resolution input images,$5.00 / 1M input tokens ,$15.00 / 1M output tokens,$1.913 / 1k 1024^2 px input images,$3.825 / 1k 1024^2 px input images,$0.425 / 1k low resolution input images,$1.275 / 1k 512^2 px input images,$25 / 1M training tokens",2025-12-04T21:45:55.000Z,,,True,,NaN,,NaN,,,0,,,FP32,,Benchmarks,,,,,gpt-4o-2024-08-06,,True,,,,2023-10-01,,,,,,
Grok-2,"Language,Vision,Multimodal","Chat,Language modeling/generation,Question answering,Code generation,Visual question answering",,Training cost,,API access,https://x.ai/blog/grok-2,,Grok-2 Beta Release,2024-08-13,xAI,,,2.96e+25,Estimate based on xAI statements comparing Grok-2 compute to GPT-4 and Grok-3. Full estimate here: https://docs.google.com/document/d/1C_dABuZrAqYE_ui4_GZ4bRLtq3TBjIGoBSktaPElhEU/edit?usp=sharing,Unspecified unreleased,"Knowledge cutoff date is August 2024, according to https://llm-stats.com/models/grok-2.",,,,,,,NVIDIA H100 SXM5 80GB,,,,Confident,Grok-2 is our frontier language model with state-of-the-art reasoning capabilities. This release includes two members of the Grok family: Grok-2 and Grok-2 mini. Both models are now being released to Grok users on the 𝕏 platform.,2025-11-28 11:53:44+00:00,Natalia Martemianova,,,United States of America,,,,,,,,,,,Industry,,2.1e+25,,,Unreleased,,True,31602310.530835517,Grok-2,,,True,700,,,,2024-08-17T03:34:15.000Z,,,True,,936640375428189300,2022-09-20,1.8973305954825461,66910000000000,494700000000000,1,989400000000000,133800000000000,TF16,989400000000000,"Comparison with other models,Reported",,,,NVIDIA,"grok-2-1212,grok-2-vision-1212,grok-2-0813",True,True,,,,2024-08-01,,,,,,
GLM-4-Plus,Language,Language modeling,Zhipu AI,Training cost,,API access,https://bigmodel.cn/dev/howuse/glm-4,,GLM-4-Plus,2024-08-29,Z.ai (Zhipu AI),,,,Estimated to be 3.6e+25 FLOP using benchmark imputation. https://colab.research.google.com/drive/1r3pUMhB7Kh0Gls9eG-v_XefWrye9fVQR?usp=sharing,,,,,,,,,,,,,Speculative,"At the KDD International Conference on Data Mining and Knowledge Discovery, the Zhipu GLM team unveiled the new generation of base large model—GLM-4-Plus. As the latest version of Zhipu’s fully self-developed GLM large model, GLM-4-Plus signifies Zhipu AI’s continuous dedication in the field of general artificial intelligence, advancing the independent and autonomous innovation of large model technology.",2025-11-28 11:53:44+00:00,Lovis Heindrich,,,China,,,,,,,Check references for hardware details.,,,,Industry,,,,,Unreleased,,True,,GLM-4-Plus,,,True,,,,,2024-11-27T13:30:32.000Z,,,True,,NaN,,NaN,,,0,,,FP32,,Benchmarks,,,,,,True,True,,,,,,,,,,
GPT-4o (Nov 2024),"Multimodal,Language,Audio,Speech,Vision","Chat,Image generation,Audio generation,Vision-language generation,Table tasks,Language modeling/generation,Question answering,Speech recognition (ASR),Speech-to-text","Aidan Clark, Alex Paino, Jacob Menick, Liam Fedus, Luke Metz, Clemens Winter, Lia Guy, Sam Schoenholz, Daniel Levy, Nitish Keskar, Alex Carney, Alex Paino, Ian Sohl, Qiming Yuan, Reimar Leike, Arka Dhar, Brydon Eastman, Mia Glaese, Ben Sokolowsky, Andrew Kondrich, Felipe Petroski Such, Henrique Ponde de Oliveira Pinto, Jiayi Weng, Randall Lin, Youlong Cheng, Nick Ryder, Lauren Itow, Barret Zoph, John Schulman, Mianna Chen, Adam Lerer, Adam P. Goucher, Adam Perelman, Akila Welihinda, Alec Radford, Alex Borzunov, Alex Carney, Alex Chow, Alex Paino, Alex Renzin, Alex Tachard Passos, Alexi Christakis, Ali Kamali, Allison Moyer, Allison Tam, Amin Tootoonchian, Ananya Kumar, Andrej Karpathy, Andrey Mishchenko, Andrew Cann, Andrew Kondrich, Andrew Tulloch, Angela Jiang, Antoine Pelisse, Anuj Gosalia, Avi Nayak, Avital Oliver, Behrooz Ghorbani, Ben Leimberger, Ben Wang, Blake Samic, Brian Guarraci, Brydon Eastman, Camillo Lugaresi, Chak Li, Charlotte Barette, Chelsea Voss, Chong Zhang, Chris Beaumont, Chris Hallacy, Chris Koch, Christian Gibson, Christopher Hesse, Colin Wei, Daniel Kappler, Daniel Levin, Daniel Levy, David Farhi, David Mely, David Sasaki, Dimitris Tsipras, Doug Li, Duc Phong Nguyen, Duncan Findlay, Edmund Wong, Ehsan Asdar, Elizabeth Proehl, Elizabeth Yang, Eric Peterson, Eric Sigler, Eugene Brevdo, Farzad Khorasani, Francis Zhang, Gene Oden, Geoff Salmon, Hadi Salman, Haiming Bao, Heather Schmidt, Hongyu Ren, Hyung Won Chung, Ian Kivlichan, Ian O'Connell, Ian Osband, Ilya Kostrikov, Ingmar Kanitscheider, Jacob Coxon, James Crooks, James Lennon, Jason Teplitz, Jason Wei, Jason Wolfe, Jay Chen, Jeff Harris, Jiayi Weng, Jie Tang, Joanne Jang, Jonathan Ward, Jonathan McKay, Jong Wook Kim, Josh Gross, Josh Kaplan, Joy Jiao, Joyce Lee, Juntang Zhuang, Kai Fricke, Kavin Karthik, Kenny Hsu, Kiel Howe, Kyle Luther, Larry Kai, Lauren Itow, Leo Chen, Lia Guy, Lien Mamitsuka, Lilian Weng, Long Ouyang, Louis Feuvrier, Lukas Kondraciuk, Lukasz Kaiser, Lyric Doshi, Mada Aflak, Maddie Simens, Madeleine Thompson, Marat Dukhan, Marvin Zhang, Mateusz Litwin, Max Johnson, Mayank Gupta, Mia Glaese, Michael Janner, Michael Petrov, Michael Wu, Michelle Fradin, Michelle Pokrass, Miguel Oom Temudo de Castro, Mikhail Pavlov, Minal Khan, Mo Bavarian, Natalia Gimelshein, Natalie Staudacher, Nick Stathas, Nik Tezak, Nithanth Kudige, Noel Bundick, Ofir Nachum, Oleg Boiko, Oleg Murk, Olivier Godement, Owen Campbell-Moore, Philip Pronin, Philippe Tillet, Rachel Lim, Rajan Troll, Randall Lin, Rapha gontijo lopes, Raul Puri, Reah Miyara, Reimar Leike, Renaud Gaubert, Reza Zamani, Rob Honsby, Rohit Ramchandani, Rory Carmichael, Ruslan Nigmatullin, Ryan Cheu, Scott Gray, Sean Grove, Sean Metzger, Shantanu Jain, Shengjia Zhao, Sherwin Wu, Shuaiqi (Tony) Xia, Sonia Phene, Spencer Papay, Steve Coffey, Steve Lee, Steve Lee, Stewart Hall, Suchir Balaji, Tal Broda, Tal Stramer, Tarun Gogineni, Ted Sanders, Thomas Cunninghman, Thomas Dimson, Thomas Raoux, Tianhao Zheng, Tina Kim, Todd Underwood, Tristan Heywood, Valerie Qi, Vinnie Monaco, Vlad Fomenko, Weiyi Zheng, Wenda Zhou, Wojciech Zaremba, Yash Patil, Yilei, Qian, Yongjik Kim, Youlong Cheng, Yuchen He, Yuchen Zhang, Yujia Jin, Yunxing Dai, Yury Malkov","SOTA improvement,Significant use","Outperforms GPT-4 Turbo and other models on text and especially on multimodal benchmarks, such as MMLU, GPQA, HumanEval, MMMU, etc See Model Evaluations: https://openai.com/index/hello-gpt-4o/ 

GPT-4o is now the default model in ChatGPT, so it's one of the most widely used models.",API access,https://help.openai.com/en/articles/9624314-model-release-notes,,"Update to GPT-4o (November 20, 2024)",2024-11-20,OpenAI,,"Not known.

Inference costs in the API are 2x cheaper than GPT-4 Turbo",,Training compute estimated to be 3.8e25 FLOP from benchmark scores. https://colab.research.google.com/drive/1r3pUMhB7Kh0Gls9eG-v_XefWrye9fVQR?usp=sharing,Unspecified unreleased,"""With GPT-4o, we trained a single new model end-to-end across text, vision, and audio.""
This has a knowledge cutoff date of October 2023 - which I assume means October 1, 2023 - according to https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models?tabs=global-standard%2Cstandard-chat-completions#gpt-4o-and-gpt-4-turbo.",,,,,,,,,,,Speculative,"We’re announcing GPT-4o, our new flagship model that can reason across audio, vision, and text in real time.

GPT-4o (“o” for “omni”) is a step towards much more natural human-computer interaction—it accepts as input any combination of text, audio, image, and video and generates any combination of text, audio, and image outputs. It can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time(opens in a new window) in a conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models.",2025-12-08 15:44:29+00:00,Luke Frymire,,0,United States of America,GPT-4o (Aug 2024),,"Definitely a new model, not a GPT-4 finetune",,,,,,,,Industry,,,,,Unreleased,,True,,GPT-4o,,,True,,,0.0,"$2.50 / 1M input tokens,$10.00 / 1M output tokens,$0.638 / 1k 512^2 px input images,$0.213 / 1k low resolution input images,$5.00 / 1M input tokens ,$15.00 / 1M output tokens,$1.913 / 1k 1024^2 px input images,$3.825 / 1k 1024^2 px input images,$0.425 / 1k low resolution input images,$1.275 / 1k 512^2 px input images,$25 / 1M training tokens",2025-12-04T21:46:02.000Z,,,True,,NaN,,NaN,,,0,,,FP32,,Benchmarks,,,,,"gpt-4o-2024-11-20,chatgpt-4o-11-20-2024",,True,,,,2023-10-01,,,,,,
GPT-4o (Jan 2025),"Multimodal,Language,Audio,Speech,Vision","Chat,Image generation,Audio generation,Vision-language generation,Table tasks,Language modeling/generation,Question answering,Speech recognition (ASR),Speech-to-text","Aidan Clark, Alex Paino, Jacob Menick, Liam Fedus, Luke Metz, Clemens Winter, Lia Guy, Sam Schoenholz, Daniel Levy, Nitish Keskar, Alex Carney, Alex Paino, Ian Sohl, Qiming Yuan, Reimar Leike, Arka Dhar, Brydon Eastman, Mia Glaese, Ben Sokolowsky, Andrew Kondrich, Felipe Petroski Such, Henrique Ponde de Oliveira Pinto, Jiayi Weng, Randall Lin, Youlong Cheng, Nick Ryder, Lauren Itow, Barret Zoph, John Schulman, Mianna Chen, Adam Lerer, Adam P. Goucher, Adam Perelman, Akila Welihinda, Alec Radford, Alex Borzunov, Alex Carney, Alex Chow, Alex Paino, Alex Renzin, Alex Tachard Passos, Alexi Christakis, Ali Kamali, Allison Moyer, Allison Tam, Amin Tootoonchian, Ananya Kumar, Andrej Karpathy, Andrey Mishchenko, Andrew Cann, Andrew Kondrich, Andrew Tulloch, Angela Jiang, Antoine Pelisse, Anuj Gosalia, Avi Nayak, Avital Oliver, Behrooz Ghorbani, Ben Leimberger, Ben Wang, Blake Samic, Brian Guarraci, Brydon Eastman, Camillo Lugaresi, Chak Li, Charlotte Barette, Chelsea Voss, Chong Zhang, Chris Beaumont, Chris Hallacy, Chris Koch, Christian Gibson, Christopher Hesse, Colin Wei, Daniel Kappler, Daniel Levin, Daniel Levy, David Farhi, David Mely, David Sasaki, Dimitris Tsipras, Doug Li, Duc Phong Nguyen, Duncan Findlay, Edmund Wong, Ehsan Asdar, Elizabeth Proehl, Elizabeth Yang, Eric Peterson, Eric Sigler, Eugene Brevdo, Farzad Khorasani, Francis Zhang, Gene Oden, Geoff Salmon, Hadi Salman, Haiming Bao, Heather Schmidt, Hongyu Ren, Hyung Won Chung, Ian Kivlichan, Ian O'Connell, Ian Osband, Ilya Kostrikov, Ingmar Kanitscheider, Jacob Coxon, James Crooks, James Lennon, Jason Teplitz, Jason Wei, Jason Wolfe, Jay Chen, Jeff Harris, Jiayi Weng, Jie Tang, Joanne Jang, Jonathan Ward, Jonathan McKay, Jong Wook Kim, Josh Gross, Josh Kaplan, Joy Jiao, Joyce Lee, Juntang Zhuang, Kai Fricke, Kavin Karthik, Kenny Hsu, Kiel Howe, Kyle Luther, Larry Kai, Lauren Itow, Leo Chen, Lia Guy, Lien Mamitsuka, Lilian Weng, Long Ouyang, Louis Feuvrier, Lukas Kondraciuk, Lukasz Kaiser, Lyric Doshi, Mada Aflak, Maddie Simens, Madeleine Thompson, Marat Dukhan, Marvin Zhang, Mateusz Litwin, Max Johnson, Mayank Gupta, Mia Glaese, Michael Janner, Michael Petrov, Michael Wu, Michelle Fradin, Michelle Pokrass, Miguel Oom Temudo de Castro, Mikhail Pavlov, Minal Khan, Mo Bavarian, Natalia Gimelshein, Natalie Staudacher, Nick Stathas, Nik Tezak, Nithanth Kudige, Noel Bundick, Ofir Nachum, Oleg Boiko, Oleg Murk, Olivier Godement, Owen Campbell-Moore, Philip Pronin, Philippe Tillet, Rachel Lim, Rajan Troll, Randall Lin, Rapha gontijo lopes, Raul Puri, Reah Miyara, Reimar Leike, Renaud Gaubert, Reza Zamani, Rob Honsby, Rohit Ramchandani, Rory Carmichael, Ruslan Nigmatullin, Ryan Cheu, Scott Gray, Sean Grove, Sean Metzger, Shantanu Jain, Shengjia Zhao, Sherwin Wu, Shuaiqi (Tony) Xia, Sonia Phene, Spencer Papay, Steve Coffey, Steve Lee, Steve Lee, Stewart Hall, Suchir Balaji, Tal Broda, Tal Stramer, Tarun Gogineni, Ted Sanders, Thomas Cunninghman, Thomas Dimson, Thomas Raoux, Tianhao Zheng, Tina Kim, Todd Underwood, Tristan Heywood, Valerie Qi, Vinnie Monaco, Vlad Fomenko, Weiyi Zheng, Wenda Zhou, Wojciech Zaremba, Yash Patil, Yilei, Qian, Yongjik Kim, Youlong Cheng, Yuchen He, Yuchen Zhang, Yujia Jin, Yunxing Dai, Yury Malkov","SOTA improvement,Significant use","Outperforms GPT-4 Turbo and other models on text and especially on multimodal benchmarks, such as MMLU, GPQA, HumanEval, MMMU, etc See Model Evaluations: https://openai.com/index/hello-gpt-4o/ 

GPT-4o is now the default model in ChatGPT, so it's one of the most widely used models.",API access,https://help.openai.com/en/articles/9624314-model-release-notes,,"Updates to GPT-4o in ChatGPT (January 29, 2025)",2025-01-29,OpenAI,,"Not known.

Inference costs in the API are 2x cheaper than GPT-4 Turbo",,Training compute estimated to be 3.8e25 FLOP from benchmark scores. https://colab.research.google.com/drive/1r3pUMhB7Kh0Gls9eG-v_XefWrye9fVQR?usp=sharing,Unspecified unreleased,"""With GPT-4o, we trained a single new model end-to-end across text, vision, and audio.""
This has a knowledge cutoff date of October 2023 - which I assume means October 1, 2023 - according to https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models?tabs=global-standard%2Cstandard-chat-completions#gpt-4o-and-gpt-4-turbo.",,,,,,,,,,,Speculative,"We’re announcing GPT-4o, our new flagship model that can reason across audio, vision, and text in real time.

GPT-4o (“o” for “omni”) is a step towards much more natural human-computer interaction—it accepts as input any combination of text, audio, image, and video and generates any combination of text, audio, and image outputs. It can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time(opens in a new window) in a conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models.",2025-12-08 15:46:31+00:00,Luke Frymire,,0,United States of America,GPT-4o (Nov 2024),,"Definitely a new model, not a GPT-4 finetune",,,,,,,,Industry,,,,,Unreleased,,True,,GPT-4o,,,True,,,0.0,"$2.50 / 1M input tokens,$10.00 / 1M output tokens,$0.638 / 1k 512^2 px input images,$0.213 / 1k low resolution input images,$5.00 / 1M input tokens ,$15.00 / 1M output tokens,$1.913 / 1k 1024^2 px input images,$3.825 / 1k 1024^2 px input images,$0.425 / 1k low resolution input images,$1.275 / 1k 512^2 px input images,$25 / 1M training tokens",2025-12-08T15:34:42.000Z,,,True,,NaN,,NaN,,,0,,,FP32,,Benchmarks,,,,,chatgpt-4o-01-29-2025,,True,,,,2023-10-01,,,,,,
Grok 3,"Language,Vision,Multimodal","Chat,Language modeling/generation,Question answering,Code generation,Visual question answering",,Training cost,,API access,https://x.ai/blog/grok-3,,Grok 3 Beta — The Age of Reasoning Agents,2025-02-17,xAI,3000000000000.0,,3.5e+26,"Estimate based on a cluster of 80,000 H100s per the xai website and an estimated training time of approximately three months.

Full estimate here: https://docs.google.com/document/d/1MIUFviULJ3YI_XjyzL8cwG0cBRANKNxVEB4DrUcFiNs/edit?usp=sharing",Unspecified unreleased,"Knowledge cutoff date is November 17, 2024, according to https://docs.x.ai/docs/models#models-and-pricing. ",,,,,2160.0,"Estimated to be approximately 3 months. See compute estimate notes for more details.
",NVIDIA H100 SXM5 80GB,,,,Likely,"We are pleased to introduce Grok 3, our most advanced model yet: blending strong reasoning with extensive pretraining knowledge. Trained on our Colossus supercluster with 10x the compute of previous state-of-the-art models, Grok 3 displays significant improvements in reasoning, mathematics, coding, world knowledge, and instruction-following tasks. Grok 3's reasoning capabilities, refined through large scale reinforcement learning, allow it to think for seconds to minutes, correcting errors, exploring alternatives, and delivering accurate answers. Grok 3 has leading performance across both academic benchmarks and real-world user preferences, achieving an Elo score of 1402 in the Chatbot Arena. Alongside it, we’re unveiling Grok 3 mini, which represents a new frontier in cost-efficient reasoning. Both models are still in training and will evolve rapidly with your feedback. We are rolling out Grok 3 to users in the coming days, along with an early preview of its reasoning capabilities.",2025-12-18 02:37:37+00:00,James Sanders,,,United States of America,,,,80000.0,,,xAI Memphis Colossus,,,,Industry,,2.1e+26,1.13e+27,,Unreleased,,True,217835545.5267339,Grok-3,,,True,700,,,,2025-02-19T20:18:12.000Z,,,True,109948656.20196915,1606716659366532000,2022-09-20,2.412046543463381,66910000000000,494700000000000,1,989400000000000,133800000000000,TF16,989400000000000,"Hardware,Comparison with other models",,,,NVIDIA,"grok-3-beta,grok-3",,True,,,,2024-11-17,,,,849737163.9529365,4446013316.520631,
GPT-4.5,"Language,Vision,Multimodal","Language modeling/generation,Question answering,Quantitative reasoning,Translation,Visual question answering,Code generation,Instruction interpretation","Foundational contributors
Alex Paino, Ali Kamali, Amin Tootoonchian, Andrew Tulloch, Ben Sokolowsky, Clemens Winter, Colin Wei, Daniel Kappler, Daniel Levy, Felipe Petroski Such, Geoff Salmon, Ian O’Connell, Jason Teplitz, Kai Chen, Nik Tezak, Prafulla Dhariwal, Rapha Gontijo Lopes, Sam Schoenholz, Youlong Cheng, Yujia Jin, Yunxing Dai

Research
Core contributors

Aiden Low, Alec Radford, Alex Carney, Alex Nichol, Alexis Conneau, Ananya Kumar, Ben Wang, Charlotte Cole , Elizabeth Yang, Gabriel Goh, Hadi Salman, Haitang Hu, Heewoo Jun, Ian Sohl, Ishaan Gulrajani, Jacob Coxon, James Betker, Jamie Kiros, Jessica Landon, Kyle Luther, Lia Guy, Lukas Kondraciuk, Lyric Doshi, Mikhail Pavlov, Qiming Yuan, Reimar Leike, Rowan Zellers, Sean Metzger, Shengjia Zhao, Spencer Papay, Tao Wang

Contributors

Adam Lerer, Aidan McLaughlin, Alexander Prokofiev, Alexandra Barr, Allan Jabri, Ananya Kumar, Andrew Gibiansky, Andrew Schmidt, Casey Chu, Chak Li, Chelsea Voss, Chris Hallacy, Chris Koch, Christine McLeavey, David Mely, Dimitris Tsipras, Eric Sigler, Erin Kavanaugh, Farzad Khorasani, Huiwen Chang, Ilya Kostrikov, Ishaan Singal, Ji Lin, Jiahui Yu, Jing Yu Zhang, John Rizzo, Jong Wook Kim, Joyce Lee, Juntang Zhuang, Leo Liu, Li Jing, Long Ouyang, Louis Feuvrier, Mo Bavarian, Nick Stathas, Nitish Keskar, Oleg Murk, Preston Bowman, Scottie Yan, SQ Mah, Tao Xu, Taylor Gordon, Valerie Qi, Wenda Zhou, Yu Zhang

Scaling
Core contributors

Adam Goucher, Alex Chow, Alex Renzin, Aleksandra Spyra, Avi Nayak, Ben Leimberger, Christopher Hesse, Duc Phong Nguyen, Dinghua Li, Eric Peterson, Francis Zhang, Gene Oden, Kai Fricke, Kai Hayashi, Larry Lv, Leqi Zou, Lin Yang, Madeleine Thompson, Michael Petrov, Miguel Castro, Natalia Gimelshein, Phil Tillet, Reza Zamani, Ryan Cheu Stanley Hsieh, Steve Lee, Stewart Hall, Thomas Raoux, Tianhao Zheng, Vishal Kuo, Yongjik Kim, Yuchen Zhang, Zhuoran Liu

Contributors

Alvin Wan, Andrew Cann, Antoine Pelisse, Anuj Kalia, Aaron Hurst, Avital Oliver, Brad Barnes, Brian Hsu, Chen Ding, Chen Shen, Cheng Chang, Christian Gibson, Duncan Findlay, Fan Wang, Fangyuan Li, Gianluca Borello, Heather Schmidt, Henrique Ponde de Oliveira Pinto, Ikai Lan, Jiayi Weng, James Crooks, Jos Kraaijeveld, Junru Shao, Kenny Hsu, Kenny Nguyen, Kevin King, Leah Burkhardt, Leo Chen, Linden Li, Lu Zhang, Mahmoud Eariby, Marat Dukhan, Mateusz Litwin, Miki Habryn, Natan LaFontaine, Pavel Belov, Peng Su, Prasad Chakka, Rachel Lim, Rajkumar Samuel, Renaud Gaubert, Rory Carmichael, Sarah Dong, Shantanu Jain, Stephen Logsdon, Todd Underwood, Weixing Zhang, Will Sheu, Weiyi Zheng, Yinghai Lu, Yunqiao Zhang

Safety Systems
Andrea Vallone, Andy Applebaum, Cameron Raymond, Chong Zhang, Dan Mossing, Elizabeth Proehl, Eric Wallace, Evan Mays, Grace Zhao, Ian Kivlichan, Irina Kofman, Joel Parish, Kevin Liu, Keren Gu-Lemberg, Kristen Ying, Lama Ahmad, Lilian Weng , Leon Maksin, Leyton Ho, Meghan Shah, Michael Lampe, Michele Wang, Miles Wang, Olivia Watkins, Phillip Guo, Samuel Miserendino, Sam Toizer, Sandhini Agarwal, Tejal Patwardhan, Tom Dupré la Tour, Tong Mu, Tyna Eloundou, Yunyun Wang

Deployment
Adam Brandon, Adam Perelman, Adele Li, Akshay Nathan, Alan Hayes, Alfred Xue, Alison Ben, Alec Gorge, Alex Guziel, Alex Iftimie, Ally Bennett, Andrew Chen, Andy Wang, Andy Wood, Angad Singh, Anoop Kotha, Antonia Woodford, Anuj Saharan, Ashley Tyra, Atty Eleti, Ben Schneider, Bessie Ji, Beth Hoover, Bill Chen, Blake Samic, Britney Smith, Brian Yu, Caleb Wang, Cary Bassin, Cary Hudson, Charlie Jatt, Chengdu Huang, Chris Beaumont, Christina Huang, Cristina Scheau, Dana Palmie, Daniel Levine, Daryl Neubieser, Dave Cummings, David Sasaki, Dibya Bhattacharjee, Dylan Hunn, Edwin Arbus, Elaine Ya Le, Enis Sert, Eric Kramer, Fred von Lohmann, Gaby Janatpour, Garrett McGrath, Garrett Ollinger, Gary Yang, Hao Sheng, Harold Hotelling, Janardhanan Vembunarayanan, Jeff Harris, Jeffrey Sabin Matsumoto, Jennifer Robinson, Jessica Liang, Jessica Shieh, Jiacheng Yang, Joel Morris, Joseph Florencio, Josh Kaplan, Kan Wu, Karan Sharma, Karen Li, Katie Pypes, Kendal Simon, Kendra Rimbach, Kevin Park, Kevin Rao, Laurance Fauconnet, Lauren Workman, Leher Pathak, Liang Wu, Liang Xiong, Lien Mamitsuka, Lindsay McCallum, Lukas Gross, Manoli Liodakis, Matt Nichols, Michelle Fradin, Minal Khan, Mingxuan Wang, Nacho Soto, Natalie Staudacher, Nikunj Handa, Niko Felix, Ning Liu, Olivier Godement, Oona Gleeson, Philip Pronin, Raymond Li, Reah Miyara, Rohan Nuttall, R.J. Marsan, Sara Culver, Scott Ethersmith, Sean Fitzgerald, Shamez Hemani, Sherwin Wu, Shiao Lee, Shuyang Cheng, Siyuan Fu, Spug Golden, Steve Coffey, Steven Heidel, Sundeep Tirumalareddy, Tabarak Khan, Thomas Degry, Thomas Dimson, Tom Stasi, Tomo Hiratsuka, Trevor Creech, Uzair Navid Iftikhar, Victoria Chernova, Victoria Spiegel, Wanning Jiang, Wenlei Xie, Yaming Lin, Yara Khakbaz, Yilei Qian, Yilong Qin, Yo Shavit, Zhi Bie

Executive Leadership
Bob McGrew, Greg Brockman, Hannah Wong, Jakub Pachocki, Johannes Heidecke, Joanne Jang, Kate Rouch, Kevin Weil, Lauren Itow, Liam Fedus, Mark Chen, Mia Glaese, Mira Murati, Nick Ryder, Sam Altman, Srinivas Narayanan, Tal Broda",Training cost,"Described by OpenAI as a ""new order of magnitude of compute""

https://openai.com/index/introducing-gpt-4-5/",API access,https://openai.com/index/introducing-gpt-4-5/,,Introducing GPT-4.5,2025-02-27,OpenAI,,,3.8e+26,"Analysis of GPT-4.5's training cluster yields a median estimate of 187M H100-hours of training. The utilization assumptions we used for the Grok 3 estimate (probably worth revisiting) were 20 to 40% under the H100 FP8 spec of 2000 teraflop/s. This leads to an estimate of 2.7e26 to 5.4e26 FLOP, or a geomean of 3.8e26

Alternatively, using a plausible range of 20 to 50% utilization, given the possibility of FP8 training, yields a median estimate of ~2e25 FLOP. See notebook below for details.

https://colab.research.google.com/drive/1QBmVPm64Ti0xucN0EsZTgSz_I7Mj9hAZ#scrollTo=NYH1ABJuLJlw 

This is consistent with OpenAI's statement that GPT-4.5 was a “new order of magnitude in compute” compared to previous models (e.g. GPT-4, which was ~2e25), suggesting around 2e26 FLOP. But they could have meant this somewhat loosely. 

In the ""Pretraining GPT-4.5"" interview, they state they used multi-cluster training: https://youtu.be/6nJZopACRuQ?si=FFJC-gEmGPZjvoPM&t=617 ",Unspecified unreleased,"""GPT-4.5 was pre-trained and post-trained on diverse datasets, including a mix of publicly available
data, proprietary data from data partnerships, and custom datasets developed in-house, which
collectively contribute to the model’s robust conversational capabilities and world knowledge.""
This model seems to also be known as GPT-4.5 Preview, which has a knowledge cutoff date of October 2023 - which I assume means October 1, 2023 - according to https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models?tabs=global-standard%2Cstandard-chat-completions#computer-use-preview.",,,,,,,,,,,Likely,"We advance AI capabilities by scaling two complementary paradigms: unsupervised learning and reasoning. These represent two axes of intelligence.

Unsupervised learning increases world model accuracy and intuition. Models like GPT‑3.5, GPT‑4, and GPT‑4.5 advance this paradigm.
Scaling reasoning⁠, on the other hand, teaches models to think and produce a chain of thought before they respond, allowing them to tackle complex STEM or logic problems. Models like OpenAI o1 and OpenAI o3‑mini advance this paradigm.
GPT‑4.5 is an example of scaling unsupervised learning by scaling up compute and data, along with architecture and optimization innovations. GPT‑4.5 was trained on Microsoft Azure AI supercomputers. The result is a model that has broader knowledge and a deeper understanding of the world, leading to reduced hallucinations and more reliability across a wide range of topics.",2025-11-28 11:52:13+00:00,Tom Adamczewski,,,United States of America,,,,,,Azure AI,,,,,Industry,,,,,Unreleased,,True,339957479.93550694,GPT-4.5,,,True,,,,,2025-02-27T21:46:15.000Z,,,True,,1117786848143742800,,NaN,,,0,,,FP32,,Benchmarks,,,,,gpt-4.5-preview-2025-02-27,,True,,,,2023-10-01,,,,,,
Llama Nemotron Ultra 253B,Language,"Language modeling/generation,Question answering,Quantitative reasoning,Code generation,Neural Architecture Search - NAS","Akhiad Bercovich, Itay Levy, Izik Golan, Mohammad Dabbah, Ran El-Yaniv, Omri Puny, Ido Galil, Zach Moshe, Tomer Ronen, Najeeb Nabwani, Ido Shahaf, Oren Tropp, Ehud Karpas, Ran Zilberstein, Jiaqi Zeng, Soumye Singhal, Alexander Bukharin, Yian Zhang, Tugrul Konuk, Gerald Shen, Ameya Sunil Mahabaleshwarkar, Bilal Kartal, Yoshi Suhara, Olivier Delalleau, Zijia Chen, Zhilin Wang, David Mosallanezhad, Adi Renduchintala, Haifeng Qian, Dima Rekesh, Fei Jia, Somshubra Majumdar, Vahid Noroozi, Wasi Uddin Ahmad, Sean Narenthiran, Aleksander Ficek, Mehrzad Samadi, Jocelyn Huang, Siddhartha Jain, Igor Gitman, Ivan Moshkov, Wei Du, Shubham Toshniwal, George Armstrong, Branislav Kisacanin, Matvei Novikov, Daria Gitman, Evelina Bakhturina, Jane Polak Scowcroft, John Kamalu, Dan Su, Kezhi Kong, Markus Kliegl, Rabeeh Karimi, Ying Lin, Sanjeev Satheesh, Jupinder Parmar, Pritam Gundecha, Brandon Norick, Joseph Jennings, Shrimai Prabhumoye, Syeda Nahida Akter, Mostofa Patwary, Abhinav Khattar, Deepak Narayanan, Roger Waleffe, Jimmy Zhang, Bor-Yiing Su, Guyue Huang, Terry Kong, Parth Chadha, Sahil Jain, Christine Harvey, Elad Segal, Jining Huang, Sergey Kashirsky, Robert McQueen, Izzy Putterman, George Lam, Arun Venkatesan, Sherry Wu, Vinh Nguyen, Manoj Kilaru, Andrew Wang, Anna Warno, Abhilash Somasamudramath, Sandip Bhaskar, Maka Dong, Nave Assaf, Shahar Mor, Omer Ullman Argov, Scot Junkin, Oleksandr Romanenko, Pedro Larroy, Monika Katariya, Marco Rovinelli, Viji Balas, Nicholas Edelman, Anahita Bhiwandiwalla, Muthu Subramaniam et al. (32 additional authors not shown)",,,Open weights (restricted use),https://arxiv.org/abs/2505.00949,,Ultra is 253B distilled from Llama 3.1 405B for maximum agentic accuracy on multi-GPU data center servers.,2025-03-18,NVIDIA,253000000000.0,"253B
""Dense decoder-only Transformer model Network Architecture: Llama-3.1-405B-Instruct, customized through Neural Architecture Search (NAS)

**This model was developed based on Llama-3.1-405B-Instruct
** This model has 253B model parameters.""",3.911001e+25,"Total training compute: 3.8e+25 FLOP (base model) + 1.11e+24 FLOP (fine-tuning) = 3.9e25 FLOP
See calculation in the finetune compute notes.","Unspecified unreleased,Llama Nemotron Post Training Dataset",,"KD + Continued Training: 
""LN-Ultra is first trained with knowledge distillation for 65B tokens using the same distillation dataset, followed by 88B tokens of continued training on the Nemotron-H phase 4 pretraining dataset (NVIDIA et al., 2025)."" (from the paper)

Reasoning training data (SFT): 
for Super model (from the blog) ""60B tokens of synthetic data (representing 4M of the 30M generated samples)"" -> the entire dataset is ~450B tokens (Ultra model is likely to be trained on the entire dataset (""Likely"" confidence}

65b+88b+450b = 603b tokens

RL for Scientific Reasoning:140k h100 hours (240k samples)

RL for instruction following: 30k prompts

RL for chat: 50k prompts",,,,,,,,,,Likely,"Llama-3.1-Nemotron-Ultra-253B-v1 is a large language model (LLM) which is a derivative of Meta Llama-3.1-405B-Instruct (AKA the reference model). It is a reasoning model that is post trained for reasoning, human chat preferences, and tasks, such as RAG and tool calling. The model supports a context length of 128K tokens. This model fits on a single 8xH100 node for inference.

Llama-3.1-Nemotron-Ultra-253B-v1 is a model which offers a great tradeoff between model accuracy and efficiency. Efficiency (throughput) directly translates to savings. Using a novel Neural Architecture Search (NAS) approach, we greatly reduce the model’s memory footprint, enabling larger workloads, as well as reducing the number of GPUs required to run the model in a data center environment. This NAS approach enables the selection of a desired point in the accuracy-efficiency tradeoff. Furthermore, by using a novel method to vertically compress the model (see details here), it also offers a significant improvement in latency.

The model underwent a multi-phase post-training process to enhance both its reasoning and non-reasoning capabilities. This includes a supervised fine-tuning stage for Math, Code, Reasoning, Chat, and Tool Calling as well as multiple reinforcement learning (RL) stages using Group Relative Policy Optimization (GRPO) algorithms for reasoning, chat, and instruction-following.",2025-11-28 11:52:13+00:00,Natalia Martemianova,,,United States of America,Llama 3.1-405B,1.114817000000001e+24,"Knowledge Distillation + Continued pre-training + SFT: 

6 FLOP / parameter / token * 253000000000 parameters * 6033000000000 tokens [see dataset size notes] = 9.15354e+23 FLOP

RL: ""the whole training takes approximately 140k H100 hours""

989400000000000 FLOP / sec / GPU [bf16] * 140000 GPU-hours * 3600 sec / hour * 0.4 [assumed utilization] = 1.9946304e+23 FLOP

Total: 9.15354e+23 FLOP + 1.9946304e+23 FLOP = 1.114817e+24 FLOP
",,,,,,,,Industry,,,,,Unreleased,"GOVERNING TERMS: Your use of this model is governed by the NVIDIA Open Model License. Additional Information: Llama 3.3 Community License Agreement. Built with Llama.

https://huggingface.co/nvidia/Llama-3_1-Nemotron-Ultra-253B-v1",True,,Llama Nemotron Ultra,,,,,3.8e+25,-4.80699999999935e+21,,2025-04-21T19:30:46.000Z,,,True,,Infinity,,NaN,,,0,,,FP32,,Operation counting,nvidia,,,,,,True,,,,,,,,,,603000000000
GPT-4o (Mar 2025),"Multimodal,Language,Audio,Speech,Vision","Chat,Image generation,Audio generation,Vision-language generation,Table tasks,Language modeling/generation,Question answering,Speech recognition (ASR),Speech-to-text","Aidan Clark, Alex Paino, Jacob Menick, Liam Fedus, Luke Metz, Clemens Winter, Lia Guy, Sam Schoenholz, Daniel Levy, Nitish Keskar, Alex Carney, Alex Paino, Ian Sohl, Qiming Yuan, Reimar Leike, Arka Dhar, Brydon Eastman, Mia Glaese, Ben Sokolowsky, Andrew Kondrich, Felipe Petroski Such, Henrique Ponde de Oliveira Pinto, Jiayi Weng, Randall Lin, Youlong Cheng, Nick Ryder, Lauren Itow, Barret Zoph, John Schulman, Mianna Chen, Adam Lerer, Adam P. Goucher, Adam Perelman, Akila Welihinda, Alec Radford, Alex Borzunov, Alex Carney, Alex Chow, Alex Paino, Alex Renzin, Alex Tachard Passos, Alexi Christakis, Ali Kamali, Allison Moyer, Allison Tam, Amin Tootoonchian, Ananya Kumar, Andrej Karpathy, Andrey Mishchenko, Andrew Cann, Andrew Kondrich, Andrew Tulloch, Angela Jiang, Antoine Pelisse, Anuj Gosalia, Avi Nayak, Avital Oliver, Behrooz Ghorbani, Ben Leimberger, Ben Wang, Blake Samic, Brian Guarraci, Brydon Eastman, Camillo Lugaresi, Chak Li, Charlotte Barette, Chelsea Voss, Chong Zhang, Chris Beaumont, Chris Hallacy, Chris Koch, Christian Gibson, Christopher Hesse, Colin Wei, Daniel Kappler, Daniel Levin, Daniel Levy, David Farhi, David Mely, David Sasaki, Dimitris Tsipras, Doug Li, Duc Phong Nguyen, Duncan Findlay, Edmund Wong, Ehsan Asdar, Elizabeth Proehl, Elizabeth Yang, Eric Peterson, Eric Sigler, Eugene Brevdo, Farzad Khorasani, Francis Zhang, Gene Oden, Geoff Salmon, Hadi Salman, Haiming Bao, Heather Schmidt, Hongyu Ren, Hyung Won Chung, Ian Kivlichan, Ian O'Connell, Ian Osband, Ilya Kostrikov, Ingmar Kanitscheider, Jacob Coxon, James Crooks, James Lennon, Jason Teplitz, Jason Wei, Jason Wolfe, Jay Chen, Jeff Harris, Jiayi Weng, Jie Tang, Joanne Jang, Jonathan Ward, Jonathan McKay, Jong Wook Kim, Josh Gross, Josh Kaplan, Joy Jiao, Joyce Lee, Juntang Zhuang, Kai Fricke, Kavin Karthik, Kenny Hsu, Kiel Howe, Kyle Luther, Larry Kai, Lauren Itow, Leo Chen, Lia Guy, Lien Mamitsuka, Lilian Weng, Long Ouyang, Louis Feuvrier, Lukas Kondraciuk, Lukasz Kaiser, Lyric Doshi, Mada Aflak, Maddie Simens, Madeleine Thompson, Marat Dukhan, Marvin Zhang, Mateusz Litwin, Max Johnson, Mayank Gupta, Mia Glaese, Michael Janner, Michael Petrov, Michael Wu, Michelle Fradin, Michelle Pokrass, Miguel Oom Temudo de Castro, Mikhail Pavlov, Minal Khan, Mo Bavarian, Natalia Gimelshein, Natalie Staudacher, Nick Stathas, Nik Tezak, Nithanth Kudige, Noel Bundick, Ofir Nachum, Oleg Boiko, Oleg Murk, Olivier Godement, Owen Campbell-Moore, Philip Pronin, Philippe Tillet, Rachel Lim, Rajan Troll, Randall Lin, Rapha gontijo lopes, Raul Puri, Reah Miyara, Reimar Leike, Renaud Gaubert, Reza Zamani, Rob Honsby, Rohit Ramchandani, Rory Carmichael, Ruslan Nigmatullin, Ryan Cheu, Scott Gray, Sean Grove, Sean Metzger, Shantanu Jain, Shengjia Zhao, Sherwin Wu, Shuaiqi (Tony) Xia, Sonia Phene, Spencer Papay, Steve Coffey, Steve Lee, Steve Lee, Stewart Hall, Suchir Balaji, Tal Broda, Tal Stramer, Tarun Gogineni, Ted Sanders, Thomas Cunninghman, Thomas Dimson, Thomas Raoux, Tianhao Zheng, Tina Kim, Todd Underwood, Tristan Heywood, Valerie Qi, Vinnie Monaco, Vlad Fomenko, Weiyi Zheng, Wenda Zhou, Wojciech Zaremba, Yash Patil, Yilei, Qian, Yongjik Kim, Youlong Cheng, Yuchen He, Yuchen Zhang, Yujia Jin, Yunxing Dai, Yury Malkov","SOTA improvement,Significant use","Outperforms GPT-4 Turbo and other models on text and especially on multimodal benchmarks, such as MMLU, GPQA, HumanEval, MMMU, etc See Model Evaluations: https://openai.com/index/hello-gpt-4o/ 

GPT-4o is now the default model in ChatGPT, so it's one of the most widely used models.",API access,https://help.openai.com/en/articles/6825453-chatgpt-release-notes,,Improvements to GPT-4o,2025-03-27,OpenAI,,"Not known.

Inference costs in the API are 2x cheaper than GPT-4 Turbo",,Training compute estimated to be 3.8e25 FLOP from benchmark scores. https://colab.research.google.com/drive/1r3pUMhB7Kh0Gls9eG-v_XefWrye9fVQR?usp=sharing,Unspecified unreleased,"""With GPT-4o, we trained a single new model end-to-end across text, vision, and audio.""
This has a knowledge cutoff date of October 2023 - which I assume means October 1, 2023 - according to https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models?tabs=global-standard%2Cstandard-chat-completions#gpt-4o-and-gpt-4-turbo.",,,,,,,,,,,Speculative,"We’re announcing GPT-4o, our new flagship model that can reason across audio, vision, and text in real time.

GPT-4o (“o” for “omni”) is a step towards much more natural human-computer interaction—it accepts as input any combination of text, audio, image, and video and generates any combination of text, audio, and image outputs. It can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time(opens in a new window) in a conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models.",2025-12-08 15:46:30+00:00,Luke Frymire,,0,United States of America,GPT-4o (Jan 2025),,"Definitely a new model, not a GPT-4 finetune",,,,,,,,Industry,,,,,Unreleased,,True,,GPT-4o,,,True,,,0.0,"$2.50 / 1M input tokens,$10.00 / 1M output tokens,$0.638 / 1k 512^2 px input images,$0.213 / 1k low resolution input images,$5.00 / 1M input tokens ,$15.00 / 1M output tokens,$1.913 / 1k 1024^2 px input images,$3.825 / 1k 1024^2 px input images,$0.425 / 1k low resolution input images,$1.275 / 1k 512^2 px input images,$25 / 1M training tokens",2025-12-08T15:34:38.000Z,,,True,,NaN,,NaN,,,0,,,FP32,,Benchmarks,,,,,chatgpt-4o-03-27-2025,,True,,,,2023-10-01,,,,,,
Llama 4 Behemoth (preview),"Multimodal,Language,Vision","Chat,Code generation,Visual question answering,Translation,Language modeling/generation,Quantitative reasoning,Question answering",,Training cost,,Unreleased,https://ai.meta.com/blog/llama-4-multimodal-intelligence/,,The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation,2025-04-05,Meta AI,2000000000000.0,"""Llama 4 Behemoth, a 288 billion active parameter model with 16 experts that is our most powerful yet and among the world’s smartest LLMs.""",5.18400000000001e+25,"Behemoth's training dataset is at least 30T tokens:
https://ai.meta.com/blog/llama-4-multimodal-intelligence/ 

6 FLOP / parameter / token * 288 * 10^9 activated parameters * 30 * 10^12 tokens = 5.184e+25 FLOP",,,"""The overall data mixture for training consisted of more than 30 trillion tokens, which is more than double the Llama 3 pre-training mixture and includes diverse text, image, and video datasets.""",,576000000000.0,2 FLOP/param * 288 billion active params,,"Based on the model cards for Llama 4 Scout and Maverick, they seem to be using H100-80GB GPUs, despite the article saying that 390 TFLOPS/GPU was a high MFU (it is high throughput, but <20% MFU in FP8).",NVIDIA H100 SXM5 80GB,,,,Likely,"We’re sharing the first models in the Llama 4 herd, which will enable people to build more personalized multimodal experiences.
...
Llama 4 Behemoth, a 288 billion active parameter model with 16 experts that is our most powerful yet and among the world’s smartest LLMs. Llama 4 Behemoth outperforms GPT-4.5, Claude Sonnet 3.7, and Gemini 2.0 Pro on several STEM benchmarks. Llama 4 Behemoth is still training, and we’re excited to share more details about it even while it’s still in flight.
Download the Llama 4 Scout and Llama 4 Maverick models today on llama.com and Hugging Face. Try Meta AI built with Llama 4 in WhatsApp, Messenger, Instagram Direct, and on the web.",2025-11-28 11:52:13+00:00,Robi Rahman,,,United States of America,,,,32000.0,,,,,,,Industry,,,,,Unreleased,"""While we’re not yet releasing Llama 4 Behemoth as it is still training""",True,44588963.624007165,Llama 4 Behemoth,,,True,700,,,,2025-04-08T06:25:49.000Z,,,True,43933454.99810563,1162619531531091700,2022-09-20,2.54072553045859,66910000000000,494700000000000,1,989400000000000,133800000000000,TF16,989400000000000,Operation counting,,,,NVIDIA,,True,True,,,,,,,,,1780094856.8197687,30000000000000
Grok 4,"Language,Multimodal,Vision","Language modeling/generation,Question answering,Search,Visual question answering,Character recognition (OCR),Image captioning,Quantitative reasoning",,Training cost,,API access,https://x.ai/news/grok-4,,Grok 4,2025-07-09,xAI,3000000000000.0,Rumored to be 2.4T params (https://x.com/kalomaze/status/1942996555088134592),5.0000000000001e+26,"We think that RL relative to pre-compute is between our estimate for o3 (10% of pre-training) and the 100% implied by this slide in the launch ( https://archive.is/f0vJU ). Assuming the same pre-training as Grok 3 (also implied by that slide, and much more consistent) and that Grok 3 used a tenth as much RL, we get:

2 * (grok3/1.1) in the high case (rl is 10% of grok 3, so grok3/1.1 is grok3 precompute, and in this case twice that is grok 4)
1.1 * (grok3/1.01) in the low case
The geometric mean is (rounded to one sig fig): 5e26
",Unspecified unreleased,,,,,,,,,,"See https://colab.research.google.com/drive/1nAl9CJi6VFLYZszzVEOLIlkxx1NMc_Lv?usp=sharing

and 
https://docs.google.com/document/d/1gF9VLRaQx__TN2pdgs-P4K_UEW5PhMQ2tGwJPwPcV6A/edit?tab=t.0",,Speculative,"Grok 4 is the most intelligent model in the world. It includes native tool use and real-time search integration, and is available now to SuperGrok and Premium+ subscribers, as well as through the xAI API. We are also introducing a new SuperGrok Heavy tier with access to Grok 4 Heavy - the most powerful version of Grok 4.",2025-12-18 02:37:51+00:00,Tom Adamczewski,,,United States of America,,,,200000.0,,,"""we utilized Colossus, our 200,000 GPU cluster, to run reinforcement learning training that refines Grok's reasoning abilities at pretraining scale""",,,,Industry,,,,,Unreleased,,True,387842678.08636147,Grok 4,,,True,,,,,2025-07-08T23:07:05.000Z,,,True,,1289182517166598000,,NaN,,,0,,,FP32,,Comparison with other models,,,,,grok-4-0709,,True,,,,,,,,,10717356897.243427,
